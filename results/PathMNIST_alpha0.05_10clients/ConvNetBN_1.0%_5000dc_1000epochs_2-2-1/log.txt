Data statistics: {0: {2: 41, 4: 5777, 7: 9330}, 1: {0: 9022}, 2: {0: 327, 5: 12176}, 3: {1: 5, 2: 1, 3: 1, 4: 1, 6: 313}, 4: {0: 16, 1: 361, 2: 2, 4: 1048, 6: 7572}, 5: {5: 2, 8: 12151}, 6: {1: 4, 3: 10194}, 7: {1: 9112}, 8: {0: 1, 1: 26, 3: 206, 4: 1179, 5: 4, 7: 71, 8: 734}, 9: {1: 1, 2: 10316, 4: 1, 6: 1}}
client classes: [[4, 7], [0], [0, 5], [6], [1, 4, 6], [8], [3], [1], [3, 4, 8], [2]]
ConvNet(
  (net_act): ReLU(inplace=True)
  (net_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=2048, out_features=9, bias=True)
)
317961
{'seed': 19260817, 'device': 'cuda:4', 'dataset_root': './dataset/torchvision', 'split_file': '/home/ygc/dcfl/cr/dataset/split_file/PathMNIST_client_num=10_alpha=0.05.json', 'dataset': 'PathMNIST', 'client_num': 10, 'partition': 'dirichlet', 'alpha': 0.05, 'num_classes_per_client': 2, 'model': 'ConvNetBN', 'communication_rounds': 10, 'join_ratio': 1.0, 'lr_server': 0.001, 'momentum_server': 0.9, 'weight_decay_server': 1e-06, 'batch_size': 256, 'model_epochs': 1000, 'local_ep': 40, 'ipc': 10, 'compression_ratio': 0.01, 'dc_iterations': 5000, 'dc_batch_size': 256, 'image_lr': 0.2, 'image_momentum': 0.5, 'image_weight_decay': 0, 'init': 'random_noise', 'clip_norm': 10.0, 'weighted_matching': False, 'weighted_sample': False, 'weighted_mmd': True, 'contrastive_way': 'supcon_asym_syn', 'con_beta': 0.05, 'con_temp': 1.0, 'topk': 5, 'lr_head': 0.01, 'momentum_head': 0.9, 'weight_decay_head': 0, 'gamma': 1.0, 'lamda': 0.5, 'b': 0.0, 'kernel': 'linear', 'lr': 0.01, 'momentum': 0.5, 'weight_decay': 0, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'preserve_all': False, 'eval_gap': 1, 'tag': '2-2-1', 'save_root_path': '/home/ygc/dcfl/cr/results/PathMNIST_alpha0.05_10clients/ConvNetBN_1.0%_5000dc_1000epochs_2-2-1', 'dsa_param': <src.utils.ParamDiffAug object at 0x7f1d61f72f70>, 'dsa': True}
 ====== round 0 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 2243.06MB, free 22332.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [   7   14   37   83  338 1211 2699 1219  151   18], bin edged: [0.0001728  0.00017285 0.00017289 0.00017294 0.00017299 0.00017304
 0.00017308 0.00017313 0.00017318 0.00017323 0.00017327]
class 7 have 9330 samples, histogram: [  59  559 1913 2678 2078 1403  518   99   17    6], bin edged: [0.00010711 0.00010713 0.00010714 0.00010716 0.00010718 0.0001072
 0.00010722 0.00010724 0.00010726 0.00010728 0.0001073 ]
client 0, data condensation 0, total loss = 888.395751953125, avg loss = 444.1978759765625
client 0, data condensation 200, total loss = 1978.077392578125, avg loss = 989.0386962890625
client 0, data condensation 400, total loss = 47.70843505859375, avg loss = 23.854217529296875
client 0, data condensation 600, total loss = 25.85833740234375, avg loss = 12.929168701171875
client 0, data condensation 800, total loss = 176.483642578125, avg loss = 88.2418212890625
client 0, data condensation 1000, total loss = 385.2906799316406, avg loss = 192.6453399658203
client 0, data condensation 1200, total loss = 646.8265991210938, avg loss = 323.4132995605469
client 0, data condensation 1400, total loss = 46.657135009765625, avg loss = 23.328567504882812
client 0, data condensation 1600, total loss = 1375.8245849609375, avg loss = 687.9122924804688
client 0, data condensation 1800, total loss = 33.852294921875, avg loss = 16.9261474609375
client 0, data condensation 2000, total loss = 31.6895751953125, avg loss = 15.84478759765625
client 0, data condensation 2200, total loss = 7.40374755859375, avg loss = 3.701873779296875
client 0, data condensation 2400, total loss = 9.546051025390625, avg loss = 4.7730255126953125
client 0, data condensation 2600, total loss = 20.304931640625, avg loss = 10.1524658203125
client 0, data condensation 2800, total loss = 1200.6402587890625, avg loss = 600.3201293945312
client 0, data condensation 3000, total loss = 585.2136840820312, avg loss = 292.6068420410156
client 0, data condensation 3200, total loss = 23.9383544921875, avg loss = 11.96917724609375
client 0, data condensation 3400, total loss = 15.424957275390625, avg loss = 7.7124786376953125
client 0, data condensation 3600, total loss = 20.119842529296875, avg loss = 10.059921264648438
client 0, data condensation 3800, total loss = 27.56036376953125, avg loss = 13.780181884765625
client 0, data condensation 4000, total loss = 502.7486267089844, avg loss = 251.3743133544922
client 0, data condensation 4200, total loss = 24.35125732421875, avg loss = 12.175628662109375
client 0, data condensation 4400, total loss = 19.79962158203125, avg loss = 9.899810791015625
client 0, data condensation 4600, total loss = 7.532440185546875, avg loss = 3.7662200927734375
client 0, data condensation 4800, total loss = 13.009521484375, avg loss = 6.5047607421875
client 0, data condensation 5000, total loss = 13.38861083984375, avg loss = 6.694305419921875
Round 0, client 0 condense time: 459.57729506492615
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3101.06MB, free 21474.94MB
total 24576.0MB, used 3101.06MB, free 21474.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [  31  434 2753 3784 1461  410  123   20    3    3], bin edged: [0.00011074 0.00011077 0.0001108  0.00011083 0.00011086 0.00011089
 0.00011092 0.00011095 0.00011098 0.00011101 0.00011104]
client 1, data condensation 0, total loss = 484.53350830078125, avg loss = 484.53350830078125
client 1, data condensation 200, total loss = 75.180908203125, avg loss = 75.180908203125
client 1, data condensation 400, total loss = 20.71051025390625, avg loss = 20.71051025390625
client 1, data condensation 600, total loss = 46.77294921875, avg loss = 46.77294921875
client 1, data condensation 800, total loss = 12.288116455078125, avg loss = 12.288116455078125
client 1, data condensation 1000, total loss = 5.41424560546875, avg loss = 5.41424560546875
client 1, data condensation 1200, total loss = 20.097900390625, avg loss = 20.097900390625
client 1, data condensation 1400, total loss = 10.62945556640625, avg loss = 10.62945556640625
client 1, data condensation 1600, total loss = 22.89691162109375, avg loss = 22.89691162109375
client 1, data condensation 1800, total loss = 739.7787475585938, avg loss = 739.7787475585938
client 1, data condensation 2000, total loss = 9.83245849609375, avg loss = 9.83245849609375
client 1, data condensation 2200, total loss = 14.4649658203125, avg loss = 14.4649658203125
client 1, data condensation 2400, total loss = 10.384124755859375, avg loss = 10.384124755859375
client 1, data condensation 2600, total loss = 30.04705810546875, avg loss = 30.04705810546875
client 1, data condensation 2800, total loss = 8.018524169921875, avg loss = 8.018524169921875
client 1, data condensation 3000, total loss = 22.74658203125, avg loss = 22.74658203125
client 1, data condensation 3200, total loss = 566.4952392578125, avg loss = 566.4952392578125
client 1, data condensation 3400, total loss = 226.55404663085938, avg loss = 226.55404663085938
client 1, data condensation 3600, total loss = 71.44171142578125, avg loss = 71.44171142578125
client 1, data condensation 3800, total loss = 124.52508544921875, avg loss = 124.52508544921875
client 1, data condensation 4000, total loss = 6.73504638671875, avg loss = 6.73504638671875
client 1, data condensation 4200, total loss = 27.66595458984375, avg loss = 27.66595458984375
client 1, data condensation 4400, total loss = 20.6134033203125, avg loss = 20.6134033203125
client 1, data condensation 4600, total loss = 11.107635498046875, avg loss = 11.107635498046875
client 1, data condensation 4800, total loss = 155.683837890625, avg loss = 155.683837890625
client 1, data condensation 5000, total loss = 13.257110595703125, avg loss = 13.257110595703125
Round 0, client 1 condense time: 250.45225834846497
client 1, class 0 have 9022 samples
total 24576.0MB, used 2701.06MB, free 21874.94MB
total 24576.0MB, used 2701.06MB, free 21874.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [10 26 49 82 56 48 28 14 10  4], bin edged: [0.00305702 0.00305728 0.00305753 0.00305779 0.00305805 0.0030583
 0.00305856 0.00305882 0.00305907 0.00305933 0.00305958]
class 5 have 12176 samples, histogram: [  28   68  147  336  807 2578 4346 3311  529   26], bin edged: [8.19785857e-05 8.20022248e-05 8.20258639e-05 8.20495030e-05
 8.20731421e-05 8.20967812e-05 8.21204203e-05 8.21440594e-05
 8.21676985e-05 8.21913376e-05 8.22149767e-05]
client 2, data condensation 0, total loss = 1039.83544921875, avg loss = 519.917724609375
client 2, data condensation 200, total loss = 282.50030517578125, avg loss = 141.25015258789062
client 2, data condensation 400, total loss = 1760.33544921875, avg loss = 880.167724609375
client 2, data condensation 600, total loss = 1665.0770263671875, avg loss = 832.5385131835938
client 2, data condensation 800, total loss = 39.391265869140625, avg loss = 19.695632934570312
client 2, data condensation 1000, total loss = 34.75238037109375, avg loss = 17.376190185546875
client 2, data condensation 1200, total loss = 26.867462158203125, avg loss = 13.433731079101562
client 2, data condensation 1400, total loss = 288.6703186035156, avg loss = 144.3351593017578
client 2, data condensation 1600, total loss = 14.927093505859375, avg loss = 7.4635467529296875
client 2, data condensation 1800, total loss = 11.89385986328125, avg loss = 5.946929931640625
client 2, data condensation 2000, total loss = 23.242218017578125, avg loss = 11.621109008789062
client 2, data condensation 2200, total loss = 24.2393798828125, avg loss = 12.11968994140625
client 2, data condensation 2400, total loss = 48.86370849609375, avg loss = 24.431854248046875
client 2, data condensation 2600, total loss = 38.4892578125, avg loss = 19.24462890625
client 2, data condensation 2800, total loss = 73.89572143554688, avg loss = 36.94786071777344
client 2, data condensation 3000, total loss = 13.72406005859375, avg loss = 6.862030029296875
client 2, data condensation 3200, total loss = 265.9016418457031, avg loss = 132.95082092285156
client 2, data condensation 3400, total loss = 18.211212158203125, avg loss = 9.105606079101562
client 2, data condensation 3600, total loss = 20.06597900390625, avg loss = 10.032989501953125
client 2, data condensation 3800, total loss = 167.63394165039062, avg loss = 83.81697082519531
client 2, data condensation 4000, total loss = 16.42755126953125, avg loss = 8.213775634765625
client 2, data condensation 4200, total loss = 32.00213623046875, avg loss = 16.001068115234375
client 2, data condensation 4400, total loss = 54.524017333984375, avg loss = 27.262008666992188
client 2, data condensation 4600, total loss = 25.1705322265625, avg loss = 12.58526611328125
client 2, data condensation 4800, total loss = 863.9205322265625, avg loss = 431.96026611328125
client 2, data condensation 5000, total loss = 689.2094116210938, avg loss = 344.6047058105469
Round 0, client 2 condense time: 422.06058382987976
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3087.06MB, free 21488.94MB
total 24576.0MB, used 3087.06MB, free 21488.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [ 8 26 56 74 62 54 26  3  2  2], bin edged: [0.00319056 0.00319165 0.00319274 0.00319383 0.00319492 0.00319601
 0.00319709 0.00319818 0.00319927 0.00320036 0.00320145]
client 3, data condensation 0, total loss = 420.2748107910156, avg loss = 420.2748107910156
client 3, data condensation 200, total loss = 2627.01953125, avg loss = 2627.01953125
client 3, data condensation 400, total loss = 273.21240234375, avg loss = 273.21240234375
client 3, data condensation 600, total loss = 10.601593017578125, avg loss = 10.601593017578125
client 3, data condensation 800, total loss = 27.038818359375, avg loss = 27.038818359375
client 3, data condensation 1000, total loss = 9.24053955078125, avg loss = 9.24053955078125
client 3, data condensation 1200, total loss = 28.589569091796875, avg loss = 28.589569091796875
client 3, data condensation 1400, total loss = 62.799896240234375, avg loss = 62.799896240234375
client 3, data condensation 1600, total loss = 28.39935302734375, avg loss = 28.39935302734375
client 3, data condensation 1800, total loss = 9.891632080078125, avg loss = 9.891632080078125
client 3, data condensation 2000, total loss = 149.97662353515625, avg loss = 149.97662353515625
client 3, data condensation 2200, total loss = 15.4552001953125, avg loss = 15.4552001953125
client 3, data condensation 2400, total loss = 613.345703125, avg loss = 613.345703125
client 3, data condensation 2600, total loss = 56.621337890625, avg loss = 56.621337890625
client 3, data condensation 2800, total loss = 14.969879150390625, avg loss = 14.969879150390625
client 3, data condensation 3000, total loss = 302.7440185546875, avg loss = 302.7440185546875
client 3, data condensation 3200, total loss = 8.378936767578125, avg loss = 8.378936767578125
client 3, data condensation 3400, total loss = 8.127349853515625, avg loss = 8.127349853515625
client 3, data condensation 3600, total loss = 4.936920166015625, avg loss = 4.936920166015625
client 3, data condensation 3800, total loss = 2.1348876953125, avg loss = 2.1348876953125
client 3, data condensation 4000, total loss = 8.41424560546875, avg loss = 8.41424560546875
client 3, data condensation 4200, total loss = 4.36474609375, avg loss = 4.36474609375
client 3, data condensation 4400, total loss = 4.92010498046875, avg loss = 4.92010498046875
client 3, data condensation 4600, total loss = 7.92333984375, avg loss = 7.92333984375
client 3, data condensation 4800, total loss = 8.874969482421875, avg loss = 8.874969482421875
client 3, data condensation 5000, total loss = 8.861541748046875, avg loss = 8.861541748046875
Round 0, client 3 condense time: 171.77852749824524
client 3, class 6 have 313 samples
total 24576.0MB, used 2703.06MB, free 21872.94MB
total 24576.0MB, used 2703.06MB, free 21872.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [ 11  71  45 102 120   9   2   0   0   1], bin edged: [0.0027688  0.00276918 0.00276957 0.00276996 0.00277035 0.00277074
 0.00277113 0.00277151 0.0027719  0.00277229 0.00277268]
class 4 have 1048 samples, histogram: [  2   1  17  64 256 367 223  89  26   3], bin edged: [0.00095293 0.00095316 0.00095339 0.00095362 0.00095384 0.00095407
 0.0009543  0.00095453 0.00095475 0.00095498 0.00095521]
class 6 have 7572 samples, histogram: [  13  354 1207 2125 2096 1259  406   88   17    7], bin edged: [0.0001319  0.00013194 0.00013198 0.00013202 0.00013206 0.0001321
 0.00013214 0.00013218 0.00013222 0.00013226 0.0001323 ]
client 4, data condensation 0, total loss = 1279.024658203125, avg loss = 426.341552734375
client 4, data condensation 200, total loss = 102.694091796875, avg loss = 34.231363932291664
client 4, data condensation 400, total loss = 173.16575622558594, avg loss = 57.72191874186198
client 4, data condensation 600, total loss = 85.52838134765625, avg loss = 28.50946044921875
client 4, data condensation 800, total loss = 20.568695068359375, avg loss = 6.856231689453125
client 4, data condensation 1000, total loss = 45.95458984375, avg loss = 15.318196614583334
client 4, data condensation 1200, total loss = 87.43829345703125, avg loss = 29.146097819010418
client 4, data condensation 1400, total loss = 17.770263671875, avg loss = 5.923421223958333
client 4, data condensation 1600, total loss = 22.739349365234375, avg loss = 7.579783121744792
client 4, data condensation 1800, total loss = 13.183197021484375, avg loss = 4.394399007161458
client 4, data condensation 2000, total loss = 24.828643798828125, avg loss = 8.276214599609375
client 4, data condensation 2200, total loss = 18.48101806640625, avg loss = 6.16033935546875
client 4, data condensation 2400, total loss = 102.0028076171875, avg loss = 34.000935872395836
client 4, data condensation 2600, total loss = 17.411956787109375, avg loss = 5.803985595703125
client 4, data condensation 2800, total loss = 14.937896728515625, avg loss = 4.979298909505208
client 4, data condensation 3000, total loss = 17.46875, avg loss = 5.822916666666667
client 4, data condensation 3200, total loss = 14.16094970703125, avg loss = 4.720316569010417
client 4, data condensation 3400, total loss = 20.63714599609375, avg loss = 6.879048665364583
client 4, data condensation 3600, total loss = 76.5364990234375, avg loss = 25.512166341145832
client 4, data condensation 3800, total loss = 83.749267578125, avg loss = 27.916422526041668
client 4, data condensation 4000, total loss = 124.468017578125, avg loss = 41.489339192708336
client 4, data condensation 4200, total loss = 16.7872314453125, avg loss = 5.595743815104167
client 4, data condensation 4400, total loss = 17.239044189453125, avg loss = 5.746348063151042
client 4, data condensation 4600, total loss = 283.91888427734375, avg loss = 94.63962809244792
client 4, data condensation 4800, total loss = 62.15447998046875, avg loss = 20.718159993489582
client 4, data condensation 5000, total loss = 23.125762939453125, avg loss = 7.708587646484375
Round 0, client 4 condense time: 521.7429029941559
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3351.06MB, free 21224.94MB
total 24576.0MB, used 3351.06MB, free 21224.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [   8   17   55  139  428 1709 4071 4643 1067   14], bin edged: [8.20371662e-05 8.20756045e-05 8.21140429e-05 8.21524812e-05
 8.21909196e-05 8.22293579e-05 8.22677963e-05 8.23062346e-05
 8.23446729e-05 8.23831113e-05 8.24215496e-05]
client 5, data condensation 0, total loss = 351.4508056640625, avg loss = 351.4508056640625
client 5, data condensation 200, total loss = 100.14486694335938, avg loss = 100.14486694335938
client 5, data condensation 400, total loss = 77.03915405273438, avg loss = 77.03915405273438
client 5, data condensation 600, total loss = 19.467376708984375, avg loss = 19.467376708984375
client 5, data condensation 800, total loss = 19.665283203125, avg loss = 19.665283203125
client 5, data condensation 1000, total loss = 30.100830078125, avg loss = 30.100830078125
client 5, data condensation 1200, total loss = 9.35821533203125, avg loss = 9.35821533203125
client 5, data condensation 1400, total loss = 9.908843994140625, avg loss = 9.908843994140625
client 5, data condensation 1600, total loss = 9.04840087890625, avg loss = 9.04840087890625
client 5, data condensation 1800, total loss = 58.181854248046875, avg loss = 58.181854248046875
client 5, data condensation 2000, total loss = 1440.7923583984375, avg loss = 1440.7923583984375
client 5, data condensation 2200, total loss = 13.86236572265625, avg loss = 13.86236572265625
client 5, data condensation 2400, total loss = 23.840911865234375, avg loss = 23.840911865234375
client 5, data condensation 2600, total loss = 18.573089599609375, avg loss = 18.573089599609375
client 5, data condensation 2800, total loss = 11.457366943359375, avg loss = 11.457366943359375
client 5, data condensation 3000, total loss = 322.2552490234375, avg loss = 322.2552490234375
client 5, data condensation 3200, total loss = 18.53271484375, avg loss = 18.53271484375
client 5, data condensation 3400, total loss = 9.870635986328125, avg loss = 9.870635986328125
client 5, data condensation 3600, total loss = 774.8408813476562, avg loss = 774.8408813476562
client 5, data condensation 3800, total loss = 16.194366455078125, avg loss = 16.194366455078125
client 5, data condensation 4000, total loss = 37.19525146484375, avg loss = 37.19525146484375
client 5, data condensation 4200, total loss = 27.98919677734375, avg loss = 27.98919677734375
client 5, data condensation 4400, total loss = 235.96435546875, avg loss = 235.96435546875
client 5, data condensation 4600, total loss = 6.612823486328125, avg loss = 6.612823486328125
client 5, data condensation 4800, total loss = 11.33111572265625, avg loss = 11.33111572265625
client 5, data condensation 5000, total loss = 145.36871337890625, avg loss = 145.36871337890625
Round 0, client 5 condense time: 310.5048735141754
client 5, class 8 have 12151 samples
total 24576.0MB, used 2841.06MB, free 21734.94MB
total 24576.0MB, used 2841.06MB, free 21734.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [  24  161  413 1440 3218 3149 1244  440   93   12], bin edged: [9.79983199e-05 9.80182674e-05 9.80382150e-05 9.80581626e-05
 9.80781101e-05 9.80980577e-05 9.81180053e-05 9.81379528e-05
 9.81579004e-05 9.81778480e-05 9.81977955e-05]
client 6, data condensation 0, total loss = 348.4739074707031, avg loss = 348.4739074707031
client 6, data condensation 200, total loss = 100.62142944335938, avg loss = 100.62142944335938
client 6, data condensation 400, total loss = 13.849822998046875, avg loss = 13.849822998046875
client 6, data condensation 600, total loss = 54.42449951171875, avg loss = 54.42449951171875
client 6, data condensation 800, total loss = 12.91693115234375, avg loss = 12.91693115234375
client 6, data condensation 1000, total loss = 1939.161865234375, avg loss = 1939.161865234375
client 6, data condensation 1200, total loss = 7.32574462890625, avg loss = 7.32574462890625
client 6, data condensation 1400, total loss = 19.802642822265625, avg loss = 19.802642822265625
client 6, data condensation 1600, total loss = 10.51483154296875, avg loss = 10.51483154296875
client 6, data condensation 1800, total loss = 22.207183837890625, avg loss = 22.207183837890625
client 6, data condensation 2000, total loss = 34.354461669921875, avg loss = 34.354461669921875
client 6, data condensation 2200, total loss = 92.5721435546875, avg loss = 92.5721435546875
client 6, data condensation 2400, total loss = 6.310821533203125, avg loss = 6.310821533203125
client 6, data condensation 2600, total loss = 16.808837890625, avg loss = 16.808837890625
client 6, data condensation 2800, total loss = 23.2958984375, avg loss = 23.2958984375
client 6, data condensation 3000, total loss = 66.68988037109375, avg loss = 66.68988037109375
client 6, data condensation 3200, total loss = 119.265380859375, avg loss = 119.265380859375
client 6, data condensation 3400, total loss = 7.558563232421875, avg loss = 7.558563232421875
client 6, data condensation 3600, total loss = 6.547393798828125, avg loss = 6.547393798828125
client 6, data condensation 3800, total loss = 7.46881103515625, avg loss = 7.46881103515625
client 6, data condensation 4000, total loss = 9.290985107421875, avg loss = 9.290985107421875
client 6, data condensation 4200, total loss = 3867.81298828125, avg loss = 3867.81298828125
client 6, data condensation 4400, total loss = 22.422943115234375, avg loss = 22.422943115234375
client 6, data condensation 4600, total loss = 7.020751953125, avg loss = 7.020751953125
client 6, data condensation 4800, total loss = 6.51220703125, avg loss = 6.51220703125
client 6, data condensation 5000, total loss = 11.74444580078125, avg loss = 11.74444580078125
Round 0, client 6 condense time: 243.22997117042542
client 6, class 3 have 10194 samples
total 24576.0MB, used 2721.06MB, free 21854.94MB
total 24576.0MB, used 2721.06MB, free 21854.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [1198  556  452  911 3155 2790   40    6    2    2], bin edged: [0.00010958 0.00010962 0.00010966 0.00010971 0.00010975 0.00010979
 0.00010983 0.00010987 0.00010991 0.00010996 0.00011   ]
client 7, data condensation 0, total loss = 381.1142578125, avg loss = 381.1142578125
client 7, data condensation 200, total loss = 151.01461791992188, avg loss = 151.01461791992188
client 7, data condensation 400, total loss = 103.78253173828125, avg loss = 103.78253173828125
client 7, data condensation 600, total loss = 41.920440673828125, avg loss = 41.920440673828125
client 7, data condensation 800, total loss = 25.2508544921875, avg loss = 25.2508544921875
client 7, data condensation 1000, total loss = 43.353485107421875, avg loss = 43.353485107421875
client 7, data condensation 1200, total loss = 37.770843505859375, avg loss = 37.770843505859375
client 7, data condensation 1400, total loss = 27.307037353515625, avg loss = 27.307037353515625
client 7, data condensation 1600, total loss = 159.29818725585938, avg loss = 159.29818725585938
client 7, data condensation 1800, total loss = 32.629669189453125, avg loss = 32.629669189453125
client 7, data condensation 2000, total loss = 27.686614990234375, avg loss = 27.686614990234375
client 7, data condensation 2200, total loss = 102.29583740234375, avg loss = 102.29583740234375
client 7, data condensation 2400, total loss = 348.97027587890625, avg loss = 348.97027587890625
client 7, data condensation 2600, total loss = 31.339874267578125, avg loss = 31.339874267578125
client 7, data condensation 2800, total loss = 211.70428466796875, avg loss = 211.70428466796875
client 7, data condensation 3000, total loss = 12.794189453125, avg loss = 12.794189453125
client 7, data condensation 3200, total loss = 16.7489013671875, avg loss = 16.7489013671875
client 7, data condensation 3400, total loss = 250.30596923828125, avg loss = 250.30596923828125
client 7, data condensation 3600, total loss = 25.354705810546875, avg loss = 25.354705810546875
client 7, data condensation 3800, total loss = 15.22821044921875, avg loss = 15.22821044921875
client 7, data condensation 4000, total loss = 18.86676025390625, avg loss = 18.86676025390625
client 7, data condensation 4200, total loss = 13.3079833984375, avg loss = 13.3079833984375
client 7, data condensation 4400, total loss = 23.27838134765625, avg loss = 23.27838134765625
client 7, data condensation 4600, total loss = 241.849365234375, avg loss = 241.849365234375
client 7, data condensation 4800, total loss = 9.56610107421875, avg loss = 9.56610107421875
client 7, data condensation 5000, total loss = 8.677215576171875, avg loss = 8.677215576171875
Round 0, client 7 condense time: 243.0790615081787
client 7, class 1 have 9112 samples
total 24576.0MB, used 2725.06MB, free 21850.94MB
total 24576.0MB, used 2725.06MB, free 21850.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [ 5 11 30 50 46 31 20  7  4  2], bin edged: [0.00485188 0.00485246 0.00485304 0.00485362 0.0048542  0.00485478
 0.00485536 0.00485594 0.00485652 0.0048571  0.00485769]
class 4 have 1179 samples, histogram: [  1   1  10  37 179 570 318  52   6   5], bin edged: [0.00084676 0.00084701 0.00084726 0.00084751 0.00084776 0.00084802
 0.00084827 0.00084852 0.00084877 0.00084902 0.00084928]
class 8 have 734 samples, histogram: [  1   3   7  15  31 111 201 263  93   9], bin edged: [0.00136063 0.00136089 0.00136115 0.00136141 0.00136167 0.00136193
 0.00136219 0.00136245 0.00136271 0.00136297 0.00136323]
client 8, data condensation 0, total loss = 1285.44580078125, avg loss = 428.48193359375
client 8, data condensation 200, total loss = 85.11911010742188, avg loss = 28.373036702473957
client 8, data condensation 400, total loss = 17.088531494140625, avg loss = 5.696177164713542
client 8, data condensation 600, total loss = 24.1234130859375, avg loss = 8.0411376953125
client 8, data condensation 800, total loss = 21.032012939453125, avg loss = 7.010670979817708
client 8, data condensation 1000, total loss = 885.0973510742188, avg loss = 295.03245035807294
client 8, data condensation 1200, total loss = 13.540069580078125, avg loss = 4.513356526692708
client 8, data condensation 1400, total loss = 25.253814697265625, avg loss = 8.417938232421875
client 8, data condensation 1600, total loss = 22.591888427734375, avg loss = 7.530629475911458
client 8, data condensation 1800, total loss = 22.807769775390625, avg loss = 7.602589925130208
client 8, data condensation 2000, total loss = 1813.641845703125, avg loss = 604.5472819010416
client 8, data condensation 2200, total loss = 1411.2933349609375, avg loss = 470.4311116536458
client 8, data condensation 2400, total loss = 22.000457763671875, avg loss = 7.333485921223958
client 8, data condensation 2600, total loss = 38.463165283203125, avg loss = 12.821055094401041
client 8, data condensation 2800, total loss = 24.852203369140625, avg loss = 8.284067789713541
client 8, data condensation 3000, total loss = 15.08642578125, avg loss = 5.02880859375
client 8, data condensation 3200, total loss = 18.64544677734375, avg loss = 6.21514892578125
client 8, data condensation 3400, total loss = 1212.5557861328125, avg loss = 404.1852620442708
client 8, data condensation 3600, total loss = 167.98422241210938, avg loss = 55.99474080403646
client 8, data condensation 3800, total loss = 19.496170043945312, avg loss = 6.4987233479817705
client 8, data condensation 4000, total loss = 26.502777099609375, avg loss = 8.834259033203125
client 8, data condensation 4200, total loss = 21.85479736328125, avg loss = 7.284932454427083
client 8, data condensation 4400, total loss = 175.35214233398438, avg loss = 58.450714111328125
client 8, data condensation 4600, total loss = 36.757476806640625, avg loss = 12.252492268880209
client 8, data condensation 4800, total loss = 10.526153564453125, avg loss = 3.5087178548177085
client 8, data condensation 5000, total loss = 1153.7318115234375, avg loss = 384.5772705078125
Round 0, client 8 condense time: 410.410133600235
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3365.06MB, free 21210.94MB
total 24576.0MB, used 3365.06MB, free 21210.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [  10   34   78  243  684 2643 5952  595   69    8], bin edged: [9.68052949e-05 9.68269603e-05 9.68486256e-05 9.68702910e-05
 9.68919564e-05 9.69136218e-05 9.69352871e-05 9.69569525e-05
 9.69786179e-05 9.70002832e-05 9.70219486e-05]
client 9, data condensation 0, total loss = 370.2354736328125, avg loss = 370.2354736328125
client 9, data condensation 200, total loss = 73.64309692382812, avg loss = 73.64309692382812
client 9, data condensation 400, total loss = 118.21182250976562, avg loss = 118.21182250976562
client 9, data condensation 600, total loss = 22.80963134765625, avg loss = 22.80963134765625
client 9, data condensation 800, total loss = 8.77679443359375, avg loss = 8.77679443359375
client 9, data condensation 1000, total loss = 45.215087890625, avg loss = 45.215087890625
client 9, data condensation 1200, total loss = 15.687713623046875, avg loss = 15.687713623046875
client 9, data condensation 1400, total loss = 7.152618408203125, avg loss = 7.152618408203125
client 9, data condensation 1600, total loss = 46.66424560546875, avg loss = 46.66424560546875
client 9, data condensation 1800, total loss = 271.616455078125, avg loss = 271.616455078125
client 9, data condensation 2000, total loss = 15.59063720703125, avg loss = 15.59063720703125
client 9, data condensation 2200, total loss = 13.535614013671875, avg loss = 13.535614013671875
client 9, data condensation 2400, total loss = 31.817352294921875, avg loss = 31.817352294921875
client 9, data condensation 2600, total loss = 319.02593994140625, avg loss = 319.02593994140625
client 9, data condensation 2800, total loss = 15.10662841796875, avg loss = 15.10662841796875
client 9, data condensation 3000, total loss = 11.32098388671875, avg loss = 11.32098388671875
client 9, data condensation 3200, total loss = 1162.2391357421875, avg loss = 1162.2391357421875
client 9, data condensation 3400, total loss = 11.435394287109375, avg loss = 11.435394287109375
client 9, data condensation 3600, total loss = 12.1043701171875, avg loss = 12.1043701171875
client 9, data condensation 3800, total loss = 11.609527587890625, avg loss = 11.609527587890625
client 9, data condensation 4000, total loss = 48.175048828125, avg loss = 48.175048828125
client 9, data condensation 4200, total loss = 1207.154296875, avg loss = 1207.154296875
client 9, data condensation 4400, total loss = 5.0419921875, avg loss = 5.0419921875
client 9, data condensation 4600, total loss = 30.121185302734375, avg loss = 30.121185302734375
client 9, data condensation 4800, total loss = 10.67120361328125, avg loss = 10.67120361328125
client 9, data condensation 5000, total loss = 13.875274658203125, avg loss = 13.875274658203125
Round 0, client 9 condense time: 252.96092557907104
client 9, class 2 have 10316 samples
total 24576.0MB, used 2733.06MB, free 21842.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 3.5617e-02,  1.3310e-04,  3.5467e-02, -3.4232e-02, -3.1212e-02,
         -2.1901e-02,  1.2518e-02, -3.1221e-03, -3.9816e-02],
        [ 1.3229e-02,  2.9007e-02,  9.2064e-03,  9.5809e-03, -1.9711e-02,
          2.3011e-02, -1.6273e-02,  7.2536e-03, -4.1146e-02],
        [ 2.5129e-02,  4.5686e-03, -1.2341e-02,  4.3974e-02,  1.6706e-02,
          4.8930e-03,  1.8995e-02, -3.1291e-02,  6.1034e-04],
        [ 1.2246e-02, -5.3701e-03, -2.5338e-02,  6.5952e-03, -1.3739e-02,
          2.4728e-02,  4.3256e-04,  9.8076e-05, -2.5760e-03],
        [-7.4380e-03, -2.4308e-02,  1.3449e-02, -6.6477e-03,  1.4395e-02,
         -1.3630e-02,  2.2072e-02, -7.9454e-03, -2.1580e-02],
        [-2.4870e-02,  2.9412e-02, -1.4807e-02, -4.4815e-02, -1.6317e-03,
         -3.7413e-03, -1.0072e-02, -2.1480e-02,  1.5633e-02],
        [-3.2541e-02, -5.9958e-03, -4.3306e-03, -1.6716e-02, -1.0206e-02,
          1.4633e-02,  9.7860e-03,  1.2816e-02,  2.8214e-02],
        [-2.5103e-03, -3.4208e-02,  1.9385e-02, -9.5278e-03,  2.1041e-02,
         -2.1006e-02,  1.7808e-02, -1.3314e-02, -3.3954e-02],
        [ 3.9556e-03, -2.2954e-02,  8.8622e-03, -1.4077e-02,  2.3127e-02,
          8.5479e-03,  1.0059e-02,  3.4531e-03,  2.1502e-02]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 2, 6, 1, 7],
        [1, 5, 0, 3, 2],
        [3, 0, 6, 4, 5],
        [5, 0, 3, 6, 7],
        [6, 4, 2, 3, 0],
        [1, 8, 4, 5, 6],
        [8, 5, 7, 6, 2],
        [4, 2, 6, 0, 3],
        [4, 8, 6, 2, 5]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
1
Round 0: # synthetic sample: 912
total 24576.0MB, used 2733.06MB, free 21842.94MB
{0: {0: 1314, 1: 0, 2: 0, 3: 0, 4: 2, 5: 20, 6: 0, 7: 0, 8: 2}, 1: {0: 3, 1: 0, 2: 0, 3: 0, 4: 843, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 168, 1: 0, 2: 0, 3: 0, 4: 2, 5: 169, 6: 0, 7: 0, 8: 0}, 3: {0: 557, 1: 0, 2: 0, 3: 0, 4: 76, 5: 1, 6: 0, 7: 0, 8: 0}, 4: {0: 1027, 1: 0, 2: 0, 3: 0, 4: 3, 5: 4, 6: 0, 7: 0, 8: 1}, 5: {0: 572, 1: 0, 2: 0, 3: 0, 4: 8, 5: 12, 6: 0, 7: 0, 8: 0}, 6: {0: 695, 1: 0, 2: 0, 3: 0, 4: 27, 5: 19, 6: 0, 7: 0, 8: 0}, 7: {0: 375, 1: 0, 2: 0, 3: 0, 4: 27, 5: 19, 6: 0, 7: 0, 8: 0}, 8: {0: 922, 1: 1, 2: 0, 3: 3, 4: 241, 5: 60, 6: 0, 7: 0, 8: 6}}
round 0 evaluation: test acc is 0.1859, test loss = 2.190584
{0: {0: 718, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 620}, 1: {0: 0, 1: 0, 2: 0, 3: 2, 4: 0, 5: 0, 6: 0, 7: 0, 8: 845}, 2: {0: 130, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 209}, 3: {0: 2, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 632}, 4: {0: 15, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 1020}, 5: {0: 2, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 590}, 6: {0: 14, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 727}, 7: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 421}, 8: {0: 25, 1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 1207}}
epoch 0, train loss avg now = 2.166770, train contrast loss now = 0.000000, test acc now = 0.2681, test loss now = 2.177683
{0: {0: 1031, 1: 0, 2: 0, 3: 0, 4: 205, 5: 0, 6: 102, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 31, 1: 140, 2: 76, 3: 0, 4: 0, 5: 78, 6: 0, 7: 5, 8: 9}, 3: {0: 0, 1: 12, 2: 43, 3: 218, 4: 23, 5: 0, 6: 138, 7: 200, 8: 0}, 4: {0: 6, 1: 457, 2: 28, 3: 0, 4: 509, 5: 0, 6: 34, 7: 1, 8: 0}, 5: {0: 0, 1: 13, 2: 327, 3: 0, 4: 0, 5: 246, 6: 2, 7: 1, 8: 3}, 6: {0: 1, 1: 1, 2: 92, 3: 0, 4: 32, 5: 0, 6: 613, 7: 1, 8: 1}, 7: {0: 0, 1: 36, 2: 270, 3: 0, 4: 9, 5: 60, 6: 17, 7: 18, 8: 11}, 8: {0: 19, 1: 30, 2: 561, 3: 0, 4: 10, 5: 2, 6: 535, 7: 3, 8: 73}}
epoch 100, train loss avg now = 0.272250, train contrast loss now = 0.000000, test acc now = 0.5057, test loss now = 3.141007
{0: {0: 329, 1: 107, 2: 0, 3: 0, 4: 369, 5: 0, 6: 532, 7: 1, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 166, 1: 2, 2: 8, 3: 0, 4: 3, 5: 103, 6: 0, 7: 56, 8: 1}, 3: {0: 0, 1: 0, 2: 16, 3: 281, 4: 0, 5: 1, 6: 51, 7: 285, 8: 0}, 4: {0: 0, 1: 364, 2: 11, 3: 3, 4: 569, 5: 0, 6: 69, 7: 19, 8: 0}, 5: {0: 0, 1: 14, 2: 202, 3: 1, 4: 0, 5: 296, 6: 1, 7: 73, 8: 5}, 6: {0: 0, 1: 0, 2: 60, 3: 12, 4: 33, 5: 0, 6: 625, 7: 7, 8: 4}, 7: {0: 0, 1: 35, 2: 90, 3: 2, 4: 6, 5: 89, 6: 24, 7: 174, 8: 1}, 8: {0: 1, 1: 8, 2: 492, 3: 9, 4: 9, 5: 6, 6: 578, 7: 36, 8: 94}}
epoch 200, train loss avg now = 0.056713, train contrast loss now = 0.000000, test acc now = 0.4489, test loss now = 3.582366
{0: {0: 727, 1: 0, 2: 0, 3: 0, 4: 601, 5: 0, 6: 8, 7: 0, 8: 2}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 168, 1: 2, 2: 3, 3: 0, 4: 1, 5: 103, 6: 0, 7: 54, 8: 8}, 3: {0: 1, 1: 1, 2: 4, 3: 325, 4: 8, 5: 1, 6: 80, 7: 214, 8: 0}, 4: {0: 3, 1: 230, 2: 10, 3: 5, 4: 765, 5: 0, 6: 3, 7: 19, 8: 0}, 5: {0: 54, 1: 14, 2: 168, 3: 3, 4: 0, 5: 270, 6: 2, 7: 69, 8: 12}, 6: {0: 3, 1: 0, 2: 37, 3: 29, 4: 110, 5: 0, 6: 529, 7: 8, 8: 25}, 7: {0: 0, 1: 39, 2: 70, 3: 12, 4: 9, 5: 88, 6: 29, 7: 169, 8: 5}, 8: {0: 35, 1: 9, 2: 268, 3: 35, 4: 30, 5: 19, 6: 588, 7: 24, 8: 225}}
epoch 300, train loss avg now = 0.095390, train contrast loss now = 0.000000, test acc now = 0.5376, test loss now = 3.370751
{0: {0: 3, 1: 33, 2: 0, 3: 0, 4: 232, 5: 0, 6: 1061, 7: 9, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 168, 1: 2, 2: 12, 3: 0, 4: 1, 5: 117, 6: 0, 7: 33, 8: 6}, 3: {0: 0, 1: 7, 2: 93, 3: 275, 4: 0, 5: 1, 6: 55, 7: 202, 8: 1}, 4: {0: 0, 1: 451, 2: 16, 3: 69, 4: 307, 5: 2, 6: 118, 7: 72, 8: 0}, 5: {0: 49, 1: 14, 2: 247, 3: 3, 4: 0, 5: 231, 6: 3, 7: 45, 8: 0}, 6: {0: 1, 1: 0, 2: 87, 3: 32, 4: 14, 5: 0, 6: 604, 7: 3, 8: 0}, 7: {0: 0, 1: 39, 2: 111, 3: 29, 4: 0, 5: 92, 6: 24, 7: 125, 8: 1}, 8: {0: 2, 1: 3, 2: 561, 3: 15, 4: 1, 5: 22, 6: 528, 7: 16, 8: 85}}
epoch 400, train loss avg now = 0.026612, train contrast loss now = 0.000000, test acc now = 0.3467, test loss now = 4.880809
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 682, 1: 24, 2: 0, 3: 0, 4: 556, 5: 0, 6: 76, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 167, 1: 1, 2: 97, 3: 0, 4: 2, 5: 56, 6: 0, 7: 14, 8: 2}, 3: {0: 0, 1: 2, 2: 12, 3: 473, 4: 4, 5: 1, 6: 74, 7: 67, 8: 1}, 4: {0: 4, 1: 419, 2: 21, 3: 11, 4: 530, 5: 0, 6: 50, 7: 0, 8: 0}, 5: {0: 52, 1: 14, 2: 362, 3: 1, 4: 0, 5: 154, 6: 1, 7: 0, 8: 8}, 6: {0: 3, 1: 0, 2: 59, 3: 20, 4: 26, 5: 0, 6: 623, 7: 0, 8: 10}, 7: {0: 0, 1: 51, 2: 192, 3: 24, 4: 9, 5: 54, 6: 42, 7: 40, 8: 9}, 8: {0: 19, 1: 10, 2: 388, 3: 20, 4: 9, 5: 8, 6: 487, 7: 1, 8: 291}}
epoch 500, train loss avg now = 0.016667, train contrast loss now = 0.000000, test acc now = 0.5205, test loss now = 3.475624
{0: {0: 989, 1: 16, 2: 0, 3: 0, 4: 219, 5: 0, 6: 114, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 168, 1: 1, 2: 28, 3: 0, 4: 1, 5: 120, 6: 0, 7: 20, 8: 1}, 3: {0: 0, 1: 0, 2: 18, 3: 547, 4: 0, 5: 5, 6: 50, 7: 13, 8: 1}, 4: {0: 5, 1: 503, 2: 28, 3: 28, 4: 406, 5: 0, 6: 62, 7: 2, 8: 1}, 5: {0: 54, 1: 14, 2: 301, 3: 3, 4: 0, 5: 209, 6: 0, 7: 5, 8: 6}, 6: {0: 3, 1: 0, 2: 76, 3: 30, 4: 16, 5: 0, 6: 605, 7: 0, 8: 11}, 7: {0: 0, 1: 45, 2: 169, 3: 50, 4: 2, 5: 80, 6: 33, 7: 39, 8: 3}, 8: {0: 19, 1: 2, 2: 471, 3: 23, 4: 5, 5: 9, 6: 480, 7: 1, 8: 223}}
epoch 600, train loss avg now = 0.009474, train contrast loss now = 0.000000, test acc now = 0.5422, test loss now = 3.509773
{0: {0: 758, 1: 28, 2: 3, 3: 0, 4: 342, 5: 0, 6: 207, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 168, 1: 1, 2: 46, 3: 0, 4: 1, 5: 100, 6: 0, 7: 22, 8: 1}, 3: {0: 0, 1: 0, 2: 21, 3: 543, 4: 0, 5: 3, 6: 50, 7: 16, 8: 1}, 4: {0: 4, 1: 537, 2: 40, 3: 42, 4: 339, 5: 0, 6: 72, 7: 0, 8: 1}, 5: {0: 54, 1: 14, 2: 333, 3: 2, 4: 0, 5: 184, 6: 0, 7: 1, 8: 4}, 6: {0: 2, 1: 0, 2: 96, 3: 31, 4: 16, 5: 0, 6: 586, 7: 0, 8: 10}, 7: {0: 0, 1: 39, 2: 197, 3: 53, 4: 0, 5: 69, 6: 29, 7: 31, 8: 3}, 8: {0: 17, 1: 2, 2: 530, 3: 23, 4: 2, 5: 9, 6: 424, 7: 1, 8: 225}}
epoch 700, train loss avg now = 0.062599, train contrast loss now = 0.000000, test acc now = 0.4957, test loss now = 3.711170
{0: {0: 1032, 1: 9, 2: 0, 3: 0, 4: 152, 5: 0, 6: 145, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 1, 2: 18, 3: 0, 4: 0, 5: 135, 6: 0, 7: 15, 8: 1}, 3: {0: 0, 1: 0, 2: 14, 3: 555, 4: 0, 5: 5, 6: 53, 7: 5, 8: 2}, 4: {0: 7, 1: 542, 2: 30, 3: 48, 4: 321, 5: 0, 6: 85, 7: 0, 8: 2}, 5: {0: 55, 1: 14, 2: 286, 3: 3, 4: 0, 5: 222, 6: 0, 7: 5, 8: 7}, 6: {0: 2, 1: 0, 2: 74, 3: 32, 4: 13, 5: 0, 6: 606, 7: 0, 8: 14}, 7: {0: 1, 1: 39, 2: 152, 3: 71, 4: 0, 5: 101, 6: 33, 7: 21, 8: 3}, 8: {0: 18, 1: 2, 2: 439, 3: 28, 4: 2, 5: 18, 6: 467, 7: 1, 8: 258}}
epoch 800, train loss avg now = 0.009297, train contrast loss now = 0.000000, test acc now = 0.5404, test loss now = 3.526778
{0: {0: 898, 1: 23, 2: 3, 3: 0, 4: 232, 5: 0, 6: 182, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 1, 2: 23, 3: 0, 4: 0, 5: 131, 6: 0, 7: 14, 8: 1}, 3: {0: 0, 1: 0, 2: 17, 3: 553, 4: 0, 5: 3, 6: 51, 7: 8, 8: 2}, 4: {0: 5, 1: 553, 2: 41, 3: 51, 4: 311, 5: 0, 6: 72, 7: 0, 8: 2}, 5: {0: 55, 1: 14, 2: 296, 3: 3, 4: 0, 5: 215, 6: 0, 7: 2, 8: 7}, 6: {0: 2, 1: 0, 2: 85, 3: 31, 4: 14, 5: 0, 6: 596, 7: 0, 8: 13}, 7: {0: 1, 1: 41, 2: 162, 3: 67, 4: 0, 5: 95, 6: 29, 7: 22, 8: 4}, 8: {0: 19, 1: 2, 2: 472, 3: 24, 4: 2, 5: 16, 6: 418, 7: 1, 8: 279}}
epoch 900, train loss avg now = 0.041248, train contrast loss now = 0.000000, test acc now = 0.5214, test loss now = 3.625555
{0: {0: 895, 1: 4, 2: 2, 3: 0, 4: 311, 5: 0, 6: 126, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 1, 2: 31, 3: 0, 4: 0, 5: 125, 6: 0, 7: 12, 8: 1}, 3: {0: 0, 1: 0, 2: 16, 3: 552, 4: 0, 5: 3, 6: 52, 7: 9, 8: 2}, 4: {0: 6, 1: 571, 2: 38, 3: 51, 4: 307, 5: 0, 6: 60, 7: 0, 8: 2}, 5: {0: 55, 1: 14, 2: 307, 3: 2, 4: 0, 5: 207, 6: 0, 7: 1, 8: 6}, 6: {0: 2, 1: 0, 2: 92, 3: 33, 4: 12, 5: 0, 6: 590, 7: 0, 8: 12}, 7: {0: 0, 1: 41, 2: 172, 3: 65, 4: 0, 5: 95, 6: 30, 7: 14, 8: 4}, 8: {0: 19, 1: 2, 2: 476, 3: 24, 4: 3, 5: 17, 6: 414, 7: 1, 8: 277}}
epoch 1000, train loss avg now = 0.034655, train contrast loss now = 0.000000, test acc now = 0.5181, test loss now = 3.560283
epoch avg loss = 3.465471403640613e-05, total time = 3926.6653542518616
total 24576.0MB, used 3609.06MB, free 20966.94MB
Round 0 finish, update the prev_syn_proto
torch.Size([96, 3, 28, 28])
torch.Size([97, 3, 28, 28])
torch.Size([104, 3, 28, 28])
torch.Size([107, 3, 28, 28])
torch.Size([81, 3, 28, 28])
torch.Size([122, 3, 28, 28])
torch.Size([81, 3, 28, 28])
torch.Size([94, 3, 28, 28])
torch.Size([130, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 895, 1: 4, 2: 2, 3: 0, 4: 311, 5: 0, 6: 126, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 1, 2: 31, 3: 0, 4: 0, 5: 125, 6: 0, 7: 12, 8: 1}, 3: {0: 0, 1: 0, 2: 16, 3: 552, 4: 0, 5: 3, 6: 52, 7: 9, 8: 2}, 4: {0: 6, 1: 571, 2: 38, 3: 51, 4: 307, 5: 0, 6: 60, 7: 0, 8: 2}, 5: {0: 55, 1: 14, 2: 307, 3: 2, 4: 0, 5: 207, 6: 0, 7: 1, 8: 6}, 6: {0: 2, 1: 0, 2: 92, 3: 33, 4: 12, 5: 0, 6: 590, 7: 0, 8: 12}, 7: {0: 0, 1: 41, 2: 172, 3: 65, 4: 0, 5: 95, 6: 30, 7: 14, 8: 4}, 8: {0: 19, 1: 2, 2: 476, 3: 24, 4: 3, 5: 17, 6: 414, 7: 1, 8: 277}}
round 0 evaluation: test acc is 0.5181, test loss = 3.560283
 ====== round 1 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3609.06MB, free 20966.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [769 828 604 461 447 406 386 410 525 941], bin edged: [0.00013171 0.0001402  0.0001487  0.0001572  0.0001657  0.0001742
 0.00018269 0.00019119 0.00019969 0.00020819 0.00021669]
class 7 have 9330 samples, histogram: [   7   29   86  155  217  332  531  925 1599 5449], bin edged: [7.41831348e-05 7.80152389e-05 8.18473430e-05 8.56794471e-05
 8.95115513e-05 9.33436554e-05 9.71757595e-05 1.01007864e-04
 1.04839968e-04 1.08672072e-04 1.12504176e-04]
client 0, data condensation 0, total loss = 367.26458740234375, avg loss = 183.63229370117188
client 0, data condensation 200, total loss = 13.86956787109375, avg loss = 6.934783935546875
client 0, data condensation 400, total loss = 17.680877685546875, avg loss = 8.840438842773438
client 0, data condensation 600, total loss = 37.976318359375, avg loss = 18.9881591796875
client 0, data condensation 800, total loss = 376.3751525878906, avg loss = 188.1875762939453
client 0, data condensation 1000, total loss = 42.3173828125, avg loss = 21.15869140625
client 0, data condensation 1200, total loss = 60.436920166015625, avg loss = 30.218460083007812
client 0, data condensation 1400, total loss = 191.99298095703125, avg loss = 95.99649047851562
client 0, data condensation 1600, total loss = 11.320281982421875, avg loss = 5.6601409912109375
client 0, data condensation 1800, total loss = 20.93609619140625, avg loss = 10.468048095703125
client 0, data condensation 2000, total loss = 13.1414794921875, avg loss = 6.57073974609375
client 0, data condensation 2200, total loss = 12.090728759765625, avg loss = 6.0453643798828125
client 0, data condensation 2400, total loss = 8.29669189453125, avg loss = 4.148345947265625
client 0, data condensation 2600, total loss = 17.66851806640625, avg loss = 8.834259033203125
client 0, data condensation 2800, total loss = 20.061920166015625, avg loss = 10.030960083007812
client 0, data condensation 3000, total loss = 9.593780517578125, avg loss = 4.7968902587890625
client 0, data condensation 3200, total loss = 160.19854736328125, avg loss = 80.09927368164062
client 0, data condensation 3400, total loss = 29.357330322265625, avg loss = 14.678665161132812
client 0, data condensation 3600, total loss = 17.108489990234375, avg loss = 8.554244995117188
client 0, data condensation 3800, total loss = 134.2574462890625, avg loss = 67.12872314453125
client 0, data condensation 4000, total loss = 11.361968994140625, avg loss = 5.6809844970703125
client 0, data condensation 4200, total loss = 20.939666748046875, avg loss = 10.469833374023438
client 0, data condensation 4400, total loss = 28.063995361328125, avg loss = 14.031997680664062
client 0, data condensation 4600, total loss = 12.808990478515625, avg loss = 6.4044952392578125
client 0, data condensation 4800, total loss = 16.1240234375, avg loss = 8.06201171875
client 0, data condensation 5000, total loss = 64.27496337890625, avg loss = 32.137481689453125
Round 1, client 0 condense time: 450.869411945343
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3453.06MB, free 21122.94MB
total 24576.0MB, used 3453.06MB, free 21122.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [1148 1693 1289  990  790  721  672  648  633  438], bin edged: [8.81045792e-05 9.37889093e-05 9.94732394e-05 1.05157569e-04
 1.10841900e-04 1.16526230e-04 1.22210560e-04 1.27894890e-04
 1.33579220e-04 1.39263550e-04 1.44947880e-04]
client 1, data condensation 0, total loss = 650.023681640625, avg loss = 650.023681640625
client 1, data condensation 200, total loss = 13.61993408203125, avg loss = 13.61993408203125
client 1, data condensation 400, total loss = 10.533477783203125, avg loss = 10.533477783203125
client 1, data condensation 600, total loss = 10.9556884765625, avg loss = 10.9556884765625
client 1, data condensation 800, total loss = 61.297088623046875, avg loss = 61.297088623046875
client 1, data condensation 1000, total loss = 11.438262939453125, avg loss = 11.438262939453125
client 1, data condensation 1200, total loss = 26.693939208984375, avg loss = 26.693939208984375
client 1, data condensation 1400, total loss = 226.20263671875, avg loss = 226.20263671875
client 1, data condensation 1600, total loss = 14.54327392578125, avg loss = 14.54327392578125
client 1, data condensation 1800, total loss = 22.59515380859375, avg loss = 22.59515380859375
client 1, data condensation 2000, total loss = 15.014068603515625, avg loss = 15.014068603515625
client 1, data condensation 2200, total loss = 166.2525634765625, avg loss = 166.2525634765625
client 1, data condensation 2400, total loss = 21.150726318359375, avg loss = 21.150726318359375
client 1, data condensation 2600, total loss = 8.476654052734375, avg loss = 8.476654052734375
client 1, data condensation 2800, total loss = 6.9456787109375, avg loss = 6.9456787109375
client 1, data condensation 3000, total loss = 26.939910888671875, avg loss = 26.939910888671875
client 1, data condensation 3200, total loss = 11.9130859375, avg loss = 11.9130859375
client 1, data condensation 3400, total loss = 18.993927001953125, avg loss = 18.993927001953125
client 1, data condensation 3600, total loss = 12.565704345703125, avg loss = 12.565704345703125
client 1, data condensation 3800, total loss = 10.641937255859375, avg loss = 10.641937255859375
client 1, data condensation 4000, total loss = 271.1552734375, avg loss = 271.1552734375
client 1, data condensation 4200, total loss = 27.68218994140625, avg loss = 27.68218994140625
client 1, data condensation 4400, total loss = 10.460784912109375, avg loss = 10.460784912109375
client 1, data condensation 4600, total loss = 26.431243896484375, avg loss = 26.431243896484375
client 1, data condensation 4800, total loss = 21.489471435546875, avg loss = 21.489471435546875
client 1, data condensation 5000, total loss = 7.38848876953125, avg loss = 7.38848876953125
Round 1, client 1 condense time: 248.91027903556824
client 1, class 0 have 9022 samples
total 24576.0MB, used 3069.06MB, free 21506.94MB
total 24576.0MB, used 3069.06MB, free 21506.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [38 45 38 41 26 27 27 23 30 32], bin edged: [0.0023743 0.0025226 0.0026709 0.0028192 0.0029675 0.0031158 0.0032641
 0.0034124 0.0035607 0.003709  0.0038573]
class 5 have 12176 samples, histogram: [1234 1478 1151  890  875  912  969 1205 1408 2054], bin edged: [6.15092027e-05 6.53723210e-05 6.92354394e-05 7.30985578e-05
 7.69616762e-05 8.08247945e-05 8.46879129e-05 8.85510313e-05
 9.24141496e-05 9.62772680e-05 1.00140386e-04]
client 2, data condensation 0, total loss = 378.1849060058594, avg loss = 189.0924530029297
client 2, data condensation 200, total loss = 37.334442138671875, avg loss = 18.667221069335938
client 2, data condensation 400, total loss = 61.43695068359375, avg loss = 30.718475341796875
client 2, data condensation 600, total loss = 57.850921630859375, avg loss = 28.925460815429688
client 2, data condensation 800, total loss = 55.269073486328125, avg loss = 27.634536743164062
client 2, data condensation 1000, total loss = 38.547088623046875, avg loss = 19.273544311523438
client 2, data condensation 1200, total loss = 40.494964599609375, avg loss = 20.247482299804688
client 2, data condensation 1400, total loss = 46.327880859375, avg loss = 23.1639404296875
client 2, data condensation 1600, total loss = 94.8060302734375, avg loss = 47.40301513671875
client 2, data condensation 1800, total loss = 22.75152587890625, avg loss = 11.375762939453125
client 2, data condensation 2000, total loss = 209.45819091796875, avg loss = 104.72909545898438
client 2, data condensation 2200, total loss = 193.6849365234375, avg loss = 96.84246826171875
client 2, data condensation 2400, total loss = 329.72381591796875, avg loss = 164.86190795898438
client 2, data condensation 2600, total loss = 26.512237548828125, avg loss = 13.256118774414062
client 2, data condensation 2800, total loss = 62.03076171875, avg loss = 31.015380859375
client 2, data condensation 3000, total loss = 33.669403076171875, avg loss = 16.834701538085938
client 2, data condensation 3200, total loss = 112.68203735351562, avg loss = 56.34101867675781
client 2, data condensation 3400, total loss = 34.520538330078125, avg loss = 17.260269165039062
client 2, data condensation 3600, total loss = 41.410400390625, avg loss = 20.7052001953125
client 2, data condensation 3800, total loss = 355.4370422363281, avg loss = 177.71852111816406
client 2, data condensation 4000, total loss = 23.985809326171875, avg loss = 11.992904663085938
client 2, data condensation 4200, total loss = 27.239288330078125, avg loss = 13.619644165039062
client 2, data condensation 4400, total loss = 36.089141845703125, avg loss = 18.044570922851562
client 2, data condensation 4600, total loss = 50.443603515625, avg loss = 25.2218017578125
client 2, data condensation 4800, total loss = 100.2127685546875, avg loss = 50.10638427734375
client 2, data condensation 5000, total loss = 36.046478271484375, avg loss = 18.023239135742188
Round 1, client 2 condense time: 386.39849495887756
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3453.06MB, free 21122.94MB
total 24576.0MB, used 3453.06MB, free 21122.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [68 42 33 29 22 13 19 27 25 35], bin edged: [0.0025133  0.0026752  0.0028371  0.00299901 0.00316091 0.00332281
 0.00348472 0.00364662 0.00380853 0.00397043 0.00413233]
client 3, data condensation 0, total loss = 364.9188232421875, avg loss = 364.9188232421875
client 3, data condensation 200, total loss = 45.448333740234375, avg loss = 45.448333740234375
client 3, data condensation 400, total loss = 33.829833984375, avg loss = 33.829833984375
client 3, data condensation 600, total loss = 40.86602783203125, avg loss = 40.86602783203125
client 3, data condensation 800, total loss = 38.035430908203125, avg loss = 38.035430908203125
client 3, data condensation 1000, total loss = 91.48208618164062, avg loss = 91.48208618164062
client 3, data condensation 1200, total loss = 44.3419189453125, avg loss = 44.3419189453125
client 3, data condensation 1400, total loss = 22.770782470703125, avg loss = 22.770782470703125
client 3, data condensation 1600, total loss = 117.95147705078125, avg loss = 117.95147705078125
client 3, data condensation 1800, total loss = 24.137664794921875, avg loss = 24.137664794921875
client 3, data condensation 2000, total loss = 17.073974609375, avg loss = 17.073974609375
client 3, data condensation 2200, total loss = 45.868011474609375, avg loss = 45.868011474609375
client 3, data condensation 2400, total loss = 24.518310546875, avg loss = 24.518310546875
client 3, data condensation 2600, total loss = 136.53125, avg loss = 136.53125
client 3, data condensation 2800, total loss = 22.07989501953125, avg loss = 22.07989501953125
client 3, data condensation 3000, total loss = 25.456878662109375, avg loss = 25.456878662109375
client 3, data condensation 3200, total loss = 50.97576904296875, avg loss = 50.97576904296875
client 3, data condensation 3400, total loss = 14.931396484375, avg loss = 14.931396484375
client 3, data condensation 3600, total loss = 114.45846557617188, avg loss = 114.45846557617188
client 3, data condensation 3800, total loss = 25.995208740234375, avg loss = 25.995208740234375
client 3, data condensation 4000, total loss = 10.54400634765625, avg loss = 10.54400634765625
client 3, data condensation 4200, total loss = 25.679351806640625, avg loss = 25.679351806640625
client 3, data condensation 4400, total loss = 52.33099365234375, avg loss = 52.33099365234375
client 3, data condensation 4600, total loss = 25.64324951171875, avg loss = 25.64324951171875
client 3, data condensation 4800, total loss = 17.17901611328125, avg loss = 17.17901611328125
client 3, data condensation 5000, total loss = 21.433685302734375, avg loss = 21.433685302734375
Round 1, client 3 condense time: 143.91173481941223
client 3, class 6 have 313 samples
total 24576.0MB, used 3067.06MB, free 21508.94MB
total 24576.0MB, used 3067.06MB, free 21508.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [215  49  53   7   7   5   5   2   3  15], bin edged: [0.00252679 0.00269005 0.00285331 0.00301656 0.00317982 0.00334308
 0.00350634 0.0036696  0.00383285 0.00399611 0.00415937]
class 4 have 1048 samples, histogram: [144 133 117  86  68  70  81  77  99 173], bin edged: [0.00072946 0.00077509 0.00082073 0.00086636 0.00091199 0.00095763
 0.00100326 0.00104889 0.00109452 0.00114016 0.00118579]
class 6 have 7572 samples, histogram: [1731 1204  792  591  554  529  486  529  528  628], bin edged: [0.00010542 0.00011223 0.00011905 0.00012586 0.00013268 0.00013949
 0.00014631 0.00015312 0.00015994 0.00016675 0.00017356]
client 4, data condensation 0, total loss = 371.2015686035156, avg loss = 123.73385620117188
client 4, data condensation 200, total loss = 53.67425537109375, avg loss = 17.89141845703125
client 4, data condensation 400, total loss = 87.38299560546875, avg loss = 29.127665201822918
client 4, data condensation 600, total loss = 83.43179321289062, avg loss = 27.810597737630207
client 4, data condensation 800, total loss = 32.78265380859375, avg loss = 10.92755126953125
client 4, data condensation 1000, total loss = 107.94012451171875, avg loss = 35.98004150390625
client 4, data condensation 1200, total loss = 65.54339599609375, avg loss = 21.847798665364582
client 4, data condensation 1400, total loss = 80.29342651367188, avg loss = 26.764475504557293
client 4, data condensation 1600, total loss = 38.790374755859375, avg loss = 12.930124918619791
client 4, data condensation 1800, total loss = 238.33395385742188, avg loss = 79.4446512858073
client 4, data condensation 2000, total loss = 71.35012817382812, avg loss = 23.783376057942707
client 4, data condensation 2200, total loss = 29.902435302734375, avg loss = 9.967478434244791
client 4, data condensation 2400, total loss = 128.34182739257812, avg loss = 42.780609130859375
client 4, data condensation 2600, total loss = 33.500823974609375, avg loss = 11.166941324869791
client 4, data condensation 2800, total loss = 41.930450439453125, avg loss = 13.976816813151041
client 4, data condensation 3000, total loss = 189.36471557617188, avg loss = 63.12157185872396
client 4, data condensation 3200, total loss = 54.844635009765625, avg loss = 18.281545003255207
client 4, data condensation 3400, total loss = 26.84417724609375, avg loss = 8.94805908203125
client 4, data condensation 3600, total loss = 74.89462280273438, avg loss = 24.964874267578125
client 4, data condensation 3800, total loss = 84.40219116210938, avg loss = 28.134063720703125
client 4, data condensation 4000, total loss = 43.659942626953125, avg loss = 14.553314208984375
client 4, data condensation 4200, total loss = 44.358673095703125, avg loss = 14.786224365234375
client 4, data condensation 4400, total loss = 32.6488037109375, avg loss = 10.8829345703125
client 4, data condensation 4600, total loss = 61.961669921875, avg loss = 20.653889973958332
client 4, data condensation 4800, total loss = 41.98333740234375, avg loss = 13.99444580078125
client 4, data condensation 5000, total loss = 71.00881958007812, avg loss = 23.669606526692707
Round 1, client 4 condense time: 504.4420897960663
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3711.06MB, free 20864.94MB
total 24576.0MB, used 3711.06MB, free 20864.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [ 570  964 1054 1192 1277 1356 1447 1461 1528 1302], bin edged: [6.07419016e-05 6.46228202e-05 6.85037387e-05 7.23846573e-05
 7.62655758e-05 8.01464944e-05 8.40274129e-05 8.79083315e-05
 9.17892500e-05 9.56701686e-05 9.95510871e-05]
client 5, data condensation 0, total loss = 373.82177734375, avg loss = 373.82177734375
client 5, data condensation 200, total loss = 9.91497802734375, avg loss = 9.91497802734375
client 5, data condensation 400, total loss = 12.430084228515625, avg loss = 12.430084228515625
client 5, data condensation 600, total loss = 7.895111083984375, avg loss = 7.895111083984375
client 5, data condensation 800, total loss = 15.480712890625, avg loss = 15.480712890625
client 5, data condensation 1000, total loss = 5.4576416015625, avg loss = 5.4576416015625
client 5, data condensation 1200, total loss = 4.601715087890625, avg loss = 4.601715087890625
client 5, data condensation 1400, total loss = 7.417816162109375, avg loss = 7.417816162109375
client 5, data condensation 1600, total loss = 9.382293701171875, avg loss = 9.382293701171875
client 5, data condensation 1800, total loss = 15.4083251953125, avg loss = 15.4083251953125
client 5, data condensation 2000, total loss = 93.556396484375, avg loss = 93.556396484375
client 5, data condensation 2200, total loss = 138.54150390625, avg loss = 138.54150390625
client 5, data condensation 2400, total loss = 7.06640625, avg loss = 7.06640625
client 5, data condensation 2600, total loss = 12.838470458984375, avg loss = 12.838470458984375
client 5, data condensation 2800, total loss = 4.281280517578125, avg loss = 4.281280517578125
client 5, data condensation 3000, total loss = 148.4566650390625, avg loss = 148.4566650390625
client 5, data condensation 3200, total loss = 6.32586669921875, avg loss = 6.32586669921875
client 5, data condensation 3400, total loss = 2.981048583984375, avg loss = 2.981048583984375
client 5, data condensation 3600, total loss = 5.77386474609375, avg loss = 5.77386474609375
client 5, data condensation 3800, total loss = 10.08355712890625, avg loss = 10.08355712890625
client 5, data condensation 4000, total loss = 5.5445556640625, avg loss = 5.5445556640625
client 5, data condensation 4200, total loss = 31.509796142578125, avg loss = 31.509796142578125
client 5, data condensation 4400, total loss = 18.0289306640625, avg loss = 18.0289306640625
client 5, data condensation 4600, total loss = 11.15301513671875, avg loss = 11.15301513671875
client 5, data condensation 4800, total loss = 17.705352783203125, avg loss = 17.705352783203125
client 5, data condensation 5000, total loss = 5.148712158203125, avg loss = 5.148712158203125
Round 1, client 5 condense time: 288.31666231155396
client 5, class 8 have 12151 samples
total 24576.0MB, used 3069.06MB, free 21506.94MB
total 24576.0MB, used 3069.06MB, free 21506.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [6686 1366  610  397  271  220  164  160  143  177], bin edged: [8.96519283e-05 9.54545019e-05 1.01257076e-04 1.07059649e-04
 1.12862223e-04 1.18664796e-04 1.24467370e-04 1.30269943e-04
 1.36072517e-04 1.41875091e-04 1.47677664e-04]
client 6, data condensation 0, total loss = 391.5777282714844, avg loss = 391.5777282714844
client 6, data condensation 200, total loss = 11.581451416015625, avg loss = 11.581451416015625
client 6, data condensation 400, total loss = 45.220550537109375, avg loss = 45.220550537109375
client 6, data condensation 600, total loss = 19.3668212890625, avg loss = 19.3668212890625
client 6, data condensation 800, total loss = 31.207366943359375, avg loss = 31.207366943359375
client 6, data condensation 1000, total loss = 22.75250244140625, avg loss = 22.75250244140625
client 6, data condensation 1200, total loss = 47.671234130859375, avg loss = 47.671234130859375
client 6, data condensation 1400, total loss = 158.34552001953125, avg loss = 158.34552001953125
client 6, data condensation 1600, total loss = 564.84765625, avg loss = 564.84765625
client 6, data condensation 1800, total loss = 12.510009765625, avg loss = 12.510009765625
client 6, data condensation 2000, total loss = 12.762603759765625, avg loss = 12.762603759765625
client 6, data condensation 2200, total loss = 13.792694091796875, avg loss = 13.792694091796875
client 6, data condensation 2400, total loss = 8.824737548828125, avg loss = 8.824737548828125
client 6, data condensation 2600, total loss = 20.176025390625, avg loss = 20.176025390625
client 6, data condensation 2800, total loss = 46.07183837890625, avg loss = 46.07183837890625
client 6, data condensation 3000, total loss = 161.52706909179688, avg loss = 161.52706909179688
client 6, data condensation 3200, total loss = 19.843109130859375, avg loss = 19.843109130859375
client 6, data condensation 3400, total loss = 22.26092529296875, avg loss = 22.26092529296875
client 6, data condensation 3600, total loss = 7.94976806640625, avg loss = 7.94976806640625
client 6, data condensation 3800, total loss = 19.370574951171875, avg loss = 19.370574951171875
client 6, data condensation 4000, total loss = 8.144134521484375, avg loss = 8.144134521484375
client 6, data condensation 4200, total loss = 10.07763671875, avg loss = 10.07763671875
client 6, data condensation 4400, total loss = 25.25286865234375, avg loss = 25.25286865234375
client 6, data condensation 4600, total loss = 6.147552490234375, avg loss = 6.147552490234375
client 6, data condensation 4800, total loss = 300.2032470703125, avg loss = 300.2032470703125
client 6, data condensation 5000, total loss = 20.033660888671875, avg loss = 20.033660888671875
Round 1, client 6 condense time: 272.07415652275085
client 6, class 3 have 10194 samples
total 24576.0MB, used 3073.06MB, free 21502.94MB
total 24576.0MB, used 3073.06MB, free 21502.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [5702 1461  577  168  102  125  129  163  155  530], bin edged: [9.93320181e-05 1.05756943e-04 1.12181867e-04 1.18606792e-04
 1.25031717e-04 1.31456641e-04 1.37881566e-04 1.44306490e-04
 1.50731415e-04 1.57156340e-04 1.63581264e-04]
client 7, data condensation 0, total loss = 299.79132080078125, avg loss = 299.79132080078125
client 7, data condensation 200, total loss = 17.043487548828125, avg loss = 17.043487548828125
client 7, data condensation 400, total loss = 9.90045166015625, avg loss = 9.90045166015625
client 7, data condensation 600, total loss = 12.994537353515625, avg loss = 12.994537353515625
client 7, data condensation 800, total loss = 14.469329833984375, avg loss = 14.469329833984375
client 7, data condensation 1000, total loss = 46.785491943359375, avg loss = 46.785491943359375
client 7, data condensation 1200, total loss = 14.517730712890625, avg loss = 14.517730712890625
client 7, data condensation 1400, total loss = 458.04351806640625, avg loss = 458.04351806640625
client 7, data condensation 1600, total loss = 7.6490478515625, avg loss = 7.6490478515625
client 7, data condensation 1800, total loss = 403.994873046875, avg loss = 403.994873046875
client 7, data condensation 2000, total loss = 113.92803955078125, avg loss = 113.92803955078125
client 7, data condensation 2200, total loss = 189.60400390625, avg loss = 189.60400390625
client 7, data condensation 2400, total loss = 31.017974853515625, avg loss = 31.017974853515625
client 7, data condensation 2600, total loss = 38.652374267578125, avg loss = 38.652374267578125
client 7, data condensation 2800, total loss = 15.55572509765625, avg loss = 15.55572509765625
client 7, data condensation 3000, total loss = 4.91064453125, avg loss = 4.91064453125
client 7, data condensation 3200, total loss = 72.78326416015625, avg loss = 72.78326416015625
client 7, data condensation 3400, total loss = 7.725341796875, avg loss = 7.725341796875
client 7, data condensation 3600, total loss = 8.8055419921875, avg loss = 8.8055419921875
client 7, data condensation 3800, total loss = 45.07763671875, avg loss = 45.07763671875
client 7, data condensation 4000, total loss = 18.223785400390625, avg loss = 18.223785400390625
client 7, data condensation 4200, total loss = 6.630340576171875, avg loss = 6.630340576171875
client 7, data condensation 4400, total loss = 7.2039794921875, avg loss = 7.2039794921875
client 7, data condensation 4600, total loss = 16.455352783203125, avg loss = 16.455352783203125
client 7, data condensation 4800, total loss = 5.806427001953125, avg loss = 5.806427001953125
client 7, data condensation 5000, total loss = 10.29052734375, avg loss = 10.29052734375
Round 1, client 7 condense time: 244.7726345062256
client 7, class 1 have 9112 samples
total 24576.0MB, used 3073.06MB, free 21502.94MB
total 24576.0MB, used 3073.06MB, free 21502.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [123  31  13  14   9   4   1   4   2   5], bin edged: [0.00439728 0.00468051 0.00496375 0.00524698 0.00553021 0.00581344
 0.00609667 0.0063799  0.00666313 0.00694637 0.0072296 ]
class 4 have 1179 samples, histogram: [118 153 123  92  71 106 109  85  99 223], bin edged: [0.00063514 0.00067587 0.0007166  0.00075732 0.00079805 0.00083878
 0.0008795  0.00092023 0.00096096 0.00100168 0.00104241]
class 8 have 734 samples, histogram: [ 42  73  70 104  78  78  78  84  69  58], bin edged: [0.00102912 0.00109447 0.00115981 0.00122515 0.00129049 0.00135583
 0.00142117 0.00148651 0.00155185 0.00161719 0.00168254]
client 8, data condensation 0, total loss = 381.02264404296875, avg loss = 127.00754801432292
client 8, data condensation 200, total loss = 333.48297119140625, avg loss = 111.16099039713542
client 8, data condensation 400, total loss = 39.338592529296875, avg loss = 13.112864176432291
client 8, data condensation 600, total loss = 157.18951416015625, avg loss = 52.396504720052086
client 8, data condensation 800, total loss = 46.576812744140625, avg loss = 15.525604248046875
client 8, data condensation 1000, total loss = 54.121337890625, avg loss = 18.040445963541668
client 8, data condensation 1200, total loss = 36.662689208984375, avg loss = 12.220896402994791
client 8, data condensation 1400, total loss = 48.384368896484375, avg loss = 16.128122965494793
client 8, data condensation 1600, total loss = 65.18405151367188, avg loss = 21.728017171223957
client 8, data condensation 1800, total loss = 37.382080078125, avg loss = 12.460693359375
client 8, data condensation 2000, total loss = 273.72857666015625, avg loss = 91.24285888671875
client 8, data condensation 2200, total loss = 70.4449462890625, avg loss = 23.481648763020832
client 8, data condensation 2400, total loss = 37.95172119140625, avg loss = 12.65057373046875
client 8, data condensation 2600, total loss = 76.8575439453125, avg loss = 25.619181315104168
client 8, data condensation 2800, total loss = 35.912078857421875, avg loss = 11.970692952473959
client 8, data condensation 3000, total loss = 50.288238525390625, avg loss = 16.762746175130207
client 8, data condensation 3200, total loss = 56.18450927734375, avg loss = 18.728169759114582
client 8, data condensation 3400, total loss = 69.9420166015625, avg loss = 23.314005533854168
client 8, data condensation 3600, total loss = 30.7891845703125, avg loss = 10.2630615234375
client 8, data condensation 3800, total loss = 97.0904541015625, avg loss = 32.363484700520836
client 8, data condensation 4000, total loss = 277.9732360839844, avg loss = 92.65774536132812
client 8, data condensation 4200, total loss = 108.50506591796875, avg loss = 36.168355305989586
client 8, data condensation 4400, total loss = 49.558685302734375, avg loss = 16.519561767578125
client 8, data condensation 4600, total loss = 67.15167236328125, avg loss = 22.383890787760418
client 8, data condensation 4800, total loss = 175.21737670898438, avg loss = 58.405792236328125
client 8, data condensation 5000, total loss = 36.026947021484375, avg loss = 12.008982340494791
Round 1, client 8 condense time: 464.0460526943207
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3709.06MB, free 20866.94MB
total 24576.0MB, used 3709.06MB, free 20866.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [1059 1476 1283 1098  935  891  847  849  489 1389], bin edged: [7.43184927e-05 7.91276162e-05 8.39367397e-05 8.87458632e-05
 9.35549868e-05 9.83641103e-05 1.03173234e-04 1.07982357e-04
 1.12791481e-04 1.17600604e-04 1.22409728e-04]
client 9, data condensation 0, total loss = 267.3584289550781, avg loss = 267.3584289550781
client 9, data condensation 200, total loss = 7.82763671875, avg loss = 7.82763671875
client 9, data condensation 400, total loss = 5.499481201171875, avg loss = 5.499481201171875
client 9, data condensation 600, total loss = 7.41845703125, avg loss = 7.41845703125
client 9, data condensation 800, total loss = 6.10308837890625, avg loss = 6.10308837890625
client 9, data condensation 1000, total loss = 3.763641357421875, avg loss = 3.763641357421875
client 9, data condensation 1200, total loss = 5.0986328125, avg loss = 5.0986328125
client 9, data condensation 1400, total loss = 4.99420166015625, avg loss = 4.99420166015625
client 9, data condensation 1600, total loss = 3.939849853515625, avg loss = 3.939849853515625
client 9, data condensation 1800, total loss = 3.968780517578125, avg loss = 3.968780517578125
client 9, data condensation 2000, total loss = 4.1480712890625, avg loss = 4.1480712890625
client 9, data condensation 2200, total loss = 1.927825927734375, avg loss = 1.927825927734375
client 9, data condensation 2400, total loss = 7.180816650390625, avg loss = 7.180816650390625
client 9, data condensation 2600, total loss = 7.236541748046875, avg loss = 7.236541748046875
client 9, data condensation 2800, total loss = 4.788818359375, avg loss = 4.788818359375
client 9, data condensation 3000, total loss = 4.787567138671875, avg loss = 4.787567138671875
client 9, data condensation 3200, total loss = 82.76943969726562, avg loss = 82.76943969726562
client 9, data condensation 3400, total loss = 8.90728759765625, avg loss = 8.90728759765625
client 9, data condensation 3600, total loss = 3.200958251953125, avg loss = 3.200958251953125
client 9, data condensation 3800, total loss = 8.2952880859375, avg loss = 8.2952880859375
client 9, data condensation 4000, total loss = 9.888031005859375, avg loss = 9.888031005859375
client 9, data condensation 4200, total loss = 12.805816650390625, avg loss = 12.805816650390625
client 9, data condensation 4400, total loss = 4.89251708984375, avg loss = 4.89251708984375
client 9, data condensation 4600, total loss = 3.53070068359375, avg loss = 3.53070068359375
client 9, data condensation 4800, total loss = 4.14080810546875, avg loss = 4.14080810546875
client 9, data condensation 5000, total loss = 5.36517333984375, avg loss = 5.36517333984375
Round 1, client 9 condense time: 265.2804412841797
client 9, class 2 have 10316 samples
total 24576.0MB, used 3075.06MB, free 21500.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 7.5140,  3.4319, -3.1313, -3.3724,  7.7526, -1.6773, -0.6938, -5.5020,
         -4.0543],
        [ 0.6725,  9.7349, -0.8826, -1.7588,  0.8524,  1.6762, -3.3715, -6.5250,
         -0.5727],
        [-5.4485, -4.5156,  7.6899, -3.8902, -2.0412,  7.3556, -0.6096, -0.1423,
          2.3667],
        [-8.8769, -2.4711,  1.0260,  8.7879, -0.6592,  0.2730,  1.8694,  1.8297,
         -0.5828],
        [-2.4054, -0.1200,  1.0944,  0.5715,  4.2553, -1.4576,  1.6916, -3.1699,
          0.1959],
        [-4.2967, -4.0257,  6.6010, -4.4435, -1.6577,  6.9580, -0.6826, -0.2178,
          2.4941],
        [-4.5301, -3.7125,  2.4907,  1.2132,  1.7393, -1.7706,  5.9779, -3.4994,
          2.9826],
        [-7.3166, -4.4943,  7.5796, -1.9356, -2.1209,  5.2753, -0.4244,  2.1171,
          2.1455],
        [-4.7505, -3.6160,  4.8818, -1.6136, -1.1018,  1.9283,  3.8545, -4.1653,
          5.3808]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[4, 0, 1, 6, 5],
        [1, 5, 4, 0, 8],
        [2, 5, 8, 7, 6],
        [3, 6, 7, 2, 5],
        [4, 6, 2, 3, 8],
        [5, 2, 8, 7, 6],
        [6, 8, 2, 4, 3],
        [2, 5, 8, 7, 6],
        [8, 2, 6, 5, 4]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
2
Round 1: # synthetic sample: 1824
total 24576.0MB, used 3075.06MB, free 21500.94MB
{0: {0: 895, 1: 4, 2: 2, 3: 0, 4: 311, 5: 0, 6: 126, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 1, 2: 31, 3: 0, 4: 0, 5: 125, 6: 0, 7: 12, 8: 1}, 3: {0: 0, 1: 0, 2: 16, 3: 552, 4: 0, 5: 3, 6: 52, 7: 9, 8: 2}, 4: {0: 6, 1: 571, 2: 38, 3: 51, 4: 307, 5: 0, 6: 60, 7: 0, 8: 2}, 5: {0: 55, 1: 14, 2: 307, 3: 2, 4: 0, 5: 207, 6: 0, 7: 1, 8: 6}, 6: {0: 2, 1: 0, 2: 92, 3: 33, 4: 12, 5: 0, 6: 590, 7: 0, 8: 12}, 7: {0: 0, 1: 41, 2: 172, 3: 65, 4: 0, 5: 95, 6: 30, 7: 14, 8: 4}, 8: {0: 19, 1: 2, 2: 476, 3: 24, 4: 3, 5: 17, 6: 414, 7: 1, 8: 277}}
round 1 evaluation: test acc is 0.5181, test loss = 3.560283
{0: {0: 1309, 1: 15, 2: 1, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0, 8: 12}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 169, 1: 0, 2: 147, 3: 0, 4: 0, 5: 7, 6: 0, 7: 0, 8: 16}, 3: {0: 0, 1: 0, 2: 94, 3: 346, 4: 0, 5: 139, 6: 2, 7: 0, 8: 53}, 4: {0: 131, 1: 821, 2: 27, 3: 20, 4: 2, 5: 0, 6: 2, 7: 0, 8: 32}, 5: {0: 52, 1: 14, 2: 464, 3: 1, 4: 0, 5: 51, 6: 0, 7: 0, 8: 10}, 6: {0: 20, 1: 9, 2: 169, 3: 39, 4: 0, 5: 1, 6: 79, 7: 0, 8: 424}, 7: {0: 0, 1: 90, 2: 237, 3: 31, 4: 0, 5: 22, 6: 1, 7: 0, 8: 40}, 8: {0: 36, 1: 6, 2: 648, 3: 17, 4: 0, 5: 2, 6: 18, 7: 0, 8: 506}}
epoch 0, train loss avg now = 1.181456, train contrast loss now = 1.416964, test acc now = 0.4578, test loss now = 3.610963
{0: {0: 388, 1: 90, 2: 1, 3: 0, 4: 227, 5: 612, 6: 19, 7: 1, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 274, 3: 0, 4: 0, 5: 31, 6: 0, 7: 33, 8: 1}, 3: {0: 0, 1: 0, 2: 208, 3: 69, 4: 0, 5: 27, 6: 102, 7: 218, 8: 10}, 4: {0: 0, 1: 200, 2: 41, 3: 0, 4: 275, 5: 32, 6: 53, 7: 432, 8: 2}, 5: {0: 55, 1: 0, 2: 383, 3: 0, 4: 0, 5: 127, 6: 4, 7: 7, 8: 16}, 6: {0: 0, 1: 0, 2: 231, 3: 0, 4: 0, 5: 57, 6: 438, 7: 0, 8: 15}, 7: {0: 0, 1: 2, 2: 198, 3: 6, 4: 0, 5: 86, 6: 4, 7: 107, 8: 18}, 8: {0: 0, 1: 3, 2: 427, 3: 0, 4: 0, 5: 60, 6: 65, 7: 4, 8: 674}}
epoch 100, train loss avg now = 0.185847, train contrast loss now = 1.075278, test acc now = 0.4455, test loss now = 2.110689
{0: {0: 491, 1: 13, 2: 2, 3: 0, 4: 795, 5: 35, 6: 2, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 169, 3: 0, 4: 0, 5: 15, 6: 0, 7: 154, 8: 0}, 3: {0: 0, 1: 0, 2: 25, 3: 41, 4: 1, 5: 0, 6: 67, 7: 500, 8: 0}, 4: {0: 0, 1: 78, 2: 1, 3: 0, 4: 657, 5: 0, 6: 9, 7: 290, 8: 0}, 5: {0: 55, 1: 0, 2: 70, 3: 0, 4: 1, 5: 149, 6: 8, 7: 304, 8: 5}, 6: {0: 0, 1: 0, 2: 148, 3: 0, 4: 52, 5: 31, 6: 424, 7: 83, 8: 3}, 7: {0: 4, 1: 0, 2: 30, 3: 9, 4: 2, 5: 21, 6: 4, 7: 351, 8: 0}, 8: {0: 7, 1: 1, 2: 179, 3: 2, 4: 20, 5: 49, 6: 239, 7: 287, 8: 449}}
epoch 200, train loss avg now = 0.240994, train contrast loss now = 1.066408, test acc now = 0.4983, test loss now = 2.199679
{0: {0: 700, 1: 5, 2: 0, 3: 0, 4: 548, 5: 46, 6: 39, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 170, 3: 0, 4: 0, 5: 91, 6: 0, 7: 77, 8: 0}, 3: {0: 0, 1: 0, 2: 57, 3: 100, 4: 1, 5: 46, 6: 195, 7: 188, 8: 47}, 4: {0: 2, 1: 61, 2: 0, 3: 0, 4: 876, 5: 21, 6: 30, 7: 44, 8: 1}, 5: {0: 55, 1: 0, 2: 26, 3: 1, 4: 0, 5: 235, 6: 16, 7: 226, 8: 33}, 6: {0: 0, 1: 0, 2: 28, 3: 0, 4: 9, 5: 53, 6: 630, 7: 0, 8: 21}, 7: {0: 3, 1: 2, 2: 34, 3: 9, 4: 27, 5: 136, 6: 25, 7: 159, 8: 26}, 8: {0: 6, 1: 2, 2: 59, 3: 0, 4: 6, 5: 128, 6: 131, 7: 59, 8: 842}}
epoch 300, train loss avg now = 0.065790, train contrast loss now = 1.062867, test acc now = 0.6350, test loss now = 1.589391
{0: {0: 914, 1: 5, 2: 0, 3: 0, 4: 57, 5: 348, 6: 14, 7: 0, 8: 0}, 1: {0: 0, 1: 839, 2: 6, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 173, 3: 0, 4: 0, 5: 29, 6: 0, 7: 136, 8: 0}, 3: {0: 0, 1: 0, 2: 11, 3: 196, 4: 0, 5: 79, 6: 248, 7: 95, 8: 5}, 4: {0: 11, 1: 67, 2: 1, 3: 0, 4: 636, 5: 49, 6: 94, 7: 177, 8: 0}, 5: {0: 55, 1: 1, 2: 81, 3: 2, 4: 1, 5: 173, 6: 24, 7: 222, 8: 33}, 6: {0: 0, 1: 0, 2: 21, 3: 1, 4: 1, 5: 49, 6: 655, 7: 0, 8: 14}, 7: {0: 6, 1: 4, 2: 49, 3: 18, 4: 13, 5: 80, 6: 35, 7: 206, 8: 10}, 8: {0: 4, 1: 2, 2: 83, 3: 0, 4: 0, 5: 62, 6: 178, 7: 89, 8: 815}}
epoch 400, train loss avg now = 0.028366, train contrast loss now = 1.054463, test acc now = 0.6416, test loss now = 1.632650
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 757, 1: 42, 2: 0, 3: 0, 4: 178, 5: 349, 6: 12, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 173, 3: 0, 4: 0, 5: 38, 6: 0, 7: 125, 8: 2}, 3: {0: 0, 1: 0, 2: 19, 3: 104, 4: 0, 5: 67, 6: 181, 7: 247, 8: 16}, 4: {0: 8, 1: 144, 2: 0, 3: 0, 4: 488, 5: 49, 6: 69, 7: 277, 8: 0}, 5: {0: 55, 1: 1, 2: 38, 3: 1, 4: 0, 5: 184, 6: 19, 7: 232, 8: 62}, 6: {0: 0, 1: 0, 2: 12, 3: 1, 4: 8, 5: 82, 6: 625, 7: 1, 8: 12}, 7: {0: 6, 1: 9, 2: 36, 3: 10, 4: 2, 5: 93, 6: 29, 7: 207, 8: 29}, 8: {0: 5, 1: 5, 2: 48, 3: 2, 4: 0, 5: 105, 6: 166, 7: 53, 8: 849}}
epoch 500, train loss avg now = 0.047693, train contrast loss now = 1.053760, test acc now = 0.5897, test loss now = 1.821100
{0: {0: 825, 1: 2, 2: 0, 3: 0, 4: 256, 5: 235, 6: 20, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 171, 3: 0, 4: 0, 5: 102, 6: 0, 7: 65, 8: 0}, 3: {0: 0, 1: 0, 2: 29, 3: 119, 4: 0, 5: 103, 6: 183, 7: 189, 8: 11}, 4: {0: 5, 1: 154, 2: 0, 3: 0, 4: 460, 5: 62, 6: 119, 7: 234, 8: 1}, 5: {0: 55, 1: 0, 2: 47, 3: 1, 4: 0, 5: 230, 6: 21, 7: 215, 8: 23}, 6: {0: 0, 1: 0, 2: 30, 3: 0, 4: 5, 5: 85, 6: 606, 7: 0, 8: 15}, 7: {0: 5, 1: 5, 2: 32, 3: 15, 4: 3, 5: 144, 6: 26, 7: 181, 8: 10}, 8: {0: 5, 1: 2, 2: 70, 3: 0, 4: 0, 5: 139, 6: 144, 7: 85, 8: 788}}
epoch 600, train loss avg now = 0.043765, train contrast loss now = 1.052360, test acc now = 0.5887, test loss now = 1.818681
{0: {0: 869, 1: 5, 2: 0, 3: 0, 4: 164, 5: 258, 6: 42, 7: 0, 8: 0}, 1: {0: 0, 1: 839, 2: 7, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 174, 3: 0, 4: 0, 5: 85, 6: 0, 7: 79, 8: 0}, 3: {0: 0, 1: 0, 2: 29, 3: 140, 4: 0, 5: 95, 6: 206, 7: 158, 8: 6}, 4: {0: 6, 1: 123, 2: 0, 3: 0, 4: 468, 5: 49, 6: 140, 7: 249, 8: 0}, 5: {0: 55, 1: 0, 2: 63, 3: 1, 4: 0, 5: 207, 6: 28, 7: 213, 8: 25}, 6: {0: 0, 1: 0, 2: 32, 3: 0, 4: 2, 5: 49, 6: 645, 7: 0, 8: 13}, 7: {0: 6, 1: 3, 2: 50, 3: 16, 4: 2, 5: 117, 6: 34, 7: 181, 8: 12}, 8: {0: 5, 1: 2, 2: 87, 3: 0, 4: 0, 5: 100, 6: 179, 7: 68, 8: 792}}
epoch 700, train loss avg now = 0.032826, train contrast loss now = 1.050380, test acc now = 0.6010, test loss now = 1.844613
{0: {0: 973, 1: 1, 2: 0, 3: 0, 4: 148, 5: 179, 6: 37, 7: 0, 8: 0}, 1: {0: 0, 1: 805, 2: 41, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 170, 3: 0, 4: 0, 5: 107, 6: 0, 7: 61, 8: 0}, 3: {0: 0, 1: 0, 2: 31, 3: 135, 4: 0, 5: 103, 6: 199, 7: 161, 8: 5}, 4: {0: 9, 1: 157, 2: 0, 3: 0, 4: 436, 5: 82, 6: 155, 7: 195, 8: 1}, 5: {0: 55, 1: 0, 2: 34, 3: 1, 4: 0, 5: 229, 6: 25, 7: 217, 8: 31}, 6: {0: 0, 1: 0, 2: 29, 3: 0, 4: 7, 5: 63, 6: 629, 7: 0, 8: 13}, 7: {0: 6, 1: 6, 2: 29, 3: 16, 4: 4, 5: 140, 6: 33, 7: 174, 8: 13}, 8: {0: 6, 1: 1, 2: 74, 3: 2, 4: 0, 5: 118, 6: 167, 7: 78, 8: 787}}
epoch 800, train loss avg now = 0.055397, train contrast loss now = 1.052957, test acc now = 0.6042, test loss now = 1.866140
{0: {0: 790, 1: 0, 2: 0, 3: 0, 4: 219, 5: 289, 6: 40, 7: 0, 8: 0}, 1: {0: 0, 1: 780, 2: 66, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 171, 3: 0, 4: 0, 5: 103, 6: 0, 7: 63, 8: 1}, 3: {0: 0, 1: 0, 2: 24, 3: 97, 4: 0, 5: 126, 6: 203, 7: 172, 8: 12}, 4: {0: 8, 1: 139, 2: 0, 3: 0, 4: 392, 5: 90, 6: 171, 7: 235, 8: 0}, 5: {0: 55, 1: 0, 2: 37, 3: 1, 4: 1, 5: 208, 6: 26, 7: 212, 8: 52}, 6: {0: 0, 1: 0, 2: 28, 3: 0, 4: 5, 5: 66, 6: 629, 7: 0, 8: 13}, 7: {0: 6, 1: 9, 2: 33, 3: 14, 4: 5, 5: 134, 6: 31, 7: 168, 8: 21}, 8: {0: 5, 1: 0, 2: 68, 3: 1, 4: 0, 5: 111, 6: 162, 7: 60, 8: 826}}
epoch 900, train loss avg now = 0.036469, train contrast loss now = 1.055383, test acc now = 0.5656, test loss now = 1.994781
{0: {0: 761, 1: 3, 2: 0, 3: 0, 4: 302, 5: 245, 6: 27, 7: 0, 8: 0}, 1: {0: 0, 1: 757, 2: 89, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 170, 3: 0, 4: 0, 5: 105, 6: 0, 7: 63, 8: 0}, 3: {0: 0, 1: 0, 2: 26, 3: 105, 4: 0, 5: 129, 6: 198, 7: 168, 8: 8}, 4: {0: 4, 1: 91, 2: 0, 3: 0, 4: 389, 5: 102, 6: 95, 7: 353, 8: 1}, 5: {0: 55, 1: 0, 2: 36, 3: 1, 4: 0, 5: 238, 6: 25, 7: 216, 8: 21}, 6: {0: 0, 1: 0, 2: 31, 3: 0, 4: 7, 5: 87, 6: 603, 7: 0, 8: 13}, 7: {0: 6, 1: 5, 2: 34, 3: 15, 4: 2, 5: 142, 6: 31, 7: 175, 8: 11}, 8: {0: 5, 1: 1, 2: 71, 3: 0, 4: 1, 5: 134, 6: 135, 7: 89, 8: 797}}
epoch 1000, train loss avg now = 0.013868, train contrast loss now = 1.049458, test acc now = 0.5564, test loss now = 2.090456
epoch avg loss = 1.3867893239908051e-05, total time = 4471.611050367355
total 24576.0MB, used 3595.06MB, free 20980.94MB
Round 1 finish, update the prev_syn_proto
torch.Size([192, 3, 28, 28])
torch.Size([194, 3, 28, 28])
torch.Size([208, 3, 28, 28])
torch.Size([214, 3, 28, 28])
torch.Size([162, 3, 28, 28])
torch.Size([244, 3, 28, 28])
torch.Size([162, 3, 28, 28])
torch.Size([188, 3, 28, 28])
torch.Size([260, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 761, 1: 3, 2: 0, 3: 0, 4: 302, 5: 245, 6: 27, 7: 0, 8: 0}, 1: {0: 0, 1: 757, 2: 89, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 170, 3: 0, 4: 0, 5: 105, 6: 0, 7: 63, 8: 0}, 3: {0: 0, 1: 0, 2: 26, 3: 105, 4: 0, 5: 129, 6: 198, 7: 168, 8: 8}, 4: {0: 4, 1: 91, 2: 0, 3: 0, 4: 389, 5: 102, 6: 95, 7: 353, 8: 1}, 5: {0: 55, 1: 0, 2: 36, 3: 1, 4: 0, 5: 238, 6: 25, 7: 216, 8: 21}, 6: {0: 0, 1: 0, 2: 31, 3: 0, 4: 7, 5: 87, 6: 603, 7: 0, 8: 13}, 7: {0: 6, 1: 5, 2: 34, 3: 15, 4: 2, 5: 142, 6: 31, 7: 175, 8: 11}, 8: {0: 5, 1: 1, 2: 71, 3: 0, 4: 1, 5: 134, 6: 135, 7: 89, 8: 797}}
round 1 evaluation: test acc is 0.5564, test loss = 2.090456
 ====== round 2 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3595.06MB, free 20980.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [2486  458  276  261  217  212  249  251  298 1069], bin edged: [0.00013958 0.00014863 0.00015769 0.00016674 0.0001758  0.00018485
 0.00019391 0.00020296 0.00021202 0.00022107 0.00023013]
class 7 have 9330 samples, histogram: [3078  915  610  464  406  420  421  508  578 1930], bin edged: [8.39710634e-05 8.94177688e-05 9.48644741e-05 1.00311179e-04
 1.05757885e-04 1.11204590e-04 1.16651295e-04 1.22098001e-04
 1.27544706e-04 1.32991411e-04 1.38438117e-04]
client 0, data condensation 0, total loss = 201.86843872070312, avg loss = 100.93421936035156
client 0, data condensation 200, total loss = 21.858489990234375, avg loss = 10.929244995117188
client 0, data condensation 400, total loss = 18.548736572265625, avg loss = 9.274368286132812
client 0, data condensation 600, total loss = 16.2144775390625, avg loss = 8.10723876953125
client 0, data condensation 800, total loss = 11.277557373046875, avg loss = 5.6387786865234375
client 0, data condensation 1000, total loss = 29.687957763671875, avg loss = 14.843978881835938
client 0, data condensation 1200, total loss = 156.60162353515625, avg loss = 78.30081176757812
client 0, data condensation 1400, total loss = 96.97564697265625, avg loss = 48.487823486328125
client 0, data condensation 1600, total loss = 16.502593994140625, avg loss = 8.251296997070312
client 0, data condensation 1800, total loss = 10.692108154296875, avg loss = 5.3460540771484375
client 0, data condensation 2000, total loss = 20.459197998046875, avg loss = 10.229598999023438
client 0, data condensation 2200, total loss = 34.553070068359375, avg loss = 17.276535034179688
client 0, data condensation 2400, total loss = 41.4591064453125, avg loss = 20.72955322265625
client 0, data condensation 2600, total loss = 69.0965576171875, avg loss = 34.54827880859375
client 0, data condensation 2800, total loss = 15.615814208984375, avg loss = 7.8079071044921875
client 0, data condensation 3000, total loss = 10.247161865234375, avg loss = 5.1235809326171875
client 0, data condensation 3200, total loss = 19.5679931640625, avg loss = 9.78399658203125
client 0, data condensation 3400, total loss = 10.828582763671875, avg loss = 5.4142913818359375
client 0, data condensation 3600, total loss = 16.671600341796875, avg loss = 8.335800170898438
client 0, data condensation 3800, total loss = 71.87716674804688, avg loss = 35.93858337402344
client 0, data condensation 4000, total loss = 14.05743408203125, avg loss = 7.028717041015625
client 0, data condensation 4200, total loss = 498.4874572753906, avg loss = 249.2437286376953
client 0, data condensation 4400, total loss = 16.23822021484375, avg loss = 8.119110107421875
client 0, data condensation 4600, total loss = 10.81353759765625, avg loss = 5.406768798828125
client 0, data condensation 4800, total loss = 10.7833251953125, avg loss = 5.39166259765625
client 0, data condensation 5000, total loss = 228.37539672851562, avg loss = 114.18769836425781
Round 2, client 0 condense time: 453.0173542499542
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3191.06MB, free 21384.94MB
total 24576.0MB, used 3191.06MB, free 21384.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [ 419  433  449  460  464  531  580  694 1007 3985], bin edged: [7.55190124e-05 8.04178346e-05 8.53166568e-05 9.02154790e-05
 9.51143012e-05 1.00013123e-04 1.04911946e-04 1.09810768e-04
 1.14709590e-04 1.19608412e-04 1.24507234e-04]
client 1, data condensation 0, total loss = 346.19610595703125, avg loss = 346.19610595703125
client 1, data condensation 200, total loss = 17.297882080078125, avg loss = 17.297882080078125
client 1, data condensation 400, total loss = 17.345977783203125, avg loss = 17.345977783203125
client 1, data condensation 600, total loss = 12.0499267578125, avg loss = 12.0499267578125
client 1, data condensation 800, total loss = 12.188720703125, avg loss = 12.188720703125
client 1, data condensation 1000, total loss = 249.4349365234375, avg loss = 249.4349365234375
client 1, data condensation 1200, total loss = 23.07098388671875, avg loss = 23.07098388671875
client 1, data condensation 1400, total loss = 148.43359375, avg loss = 148.43359375
client 1, data condensation 1600, total loss = 22.67071533203125, avg loss = 22.67071533203125
client 1, data condensation 1800, total loss = 16.125457763671875, avg loss = 16.125457763671875
client 1, data condensation 2000, total loss = 14.44110107421875, avg loss = 14.44110107421875
client 1, data condensation 2200, total loss = 12.404876708984375, avg loss = 12.404876708984375
client 1, data condensation 2400, total loss = 496.40191650390625, avg loss = 496.40191650390625
client 1, data condensation 2600, total loss = 50.415863037109375, avg loss = 50.415863037109375
client 1, data condensation 2800, total loss = 28.94140625, avg loss = 28.94140625
client 1, data condensation 3000, total loss = 96.80731201171875, avg loss = 96.80731201171875
client 1, data condensation 3200, total loss = 15.78875732421875, avg loss = 15.78875732421875
client 1, data condensation 3400, total loss = 40.315185546875, avg loss = 40.315185546875
client 1, data condensation 3600, total loss = 16.52606201171875, avg loss = 16.52606201171875
client 1, data condensation 3800, total loss = 7.2928466796875, avg loss = 7.2928466796875
client 1, data condensation 4000, total loss = 21.60491943359375, avg loss = 21.60491943359375
client 1, data condensation 4200, total loss = 64.3651123046875, avg loss = 64.3651123046875
client 1, data condensation 4400, total loss = 19.4390869140625, avg loss = 19.4390869140625
client 1, data condensation 4600, total loss = 108.9072265625, avg loss = 108.9072265625
client 1, data condensation 4800, total loss = 36.40386962890625, avg loss = 36.40386962890625
client 1, data condensation 5000, total loss = 7.91326904296875, avg loss = 7.91326904296875
Round 2, client 1 condense time: 248.91714811325073
client 1, class 0 have 9022 samples
total 24576.0MB, used 2807.06MB, free 21768.94MB
total 24576.0MB, used 2807.06MB, free 21768.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [ 27  19  15  12  15  16  26  25  32 140], bin edged: [0.00211215 0.00224833 0.00238451 0.00252069 0.00265687 0.00279305
 0.00292923 0.00306541 0.00320159 0.00333777 0.00347395]
class 5 have 12176 samples, histogram: [6201 1022  670  490  464  417  422  454  624 1412], bin edged: [6.90275376e-05 7.35054316e-05 7.79833256e-05 8.24612196e-05
 8.69391136e-05 9.14170076e-05 9.58949016e-05 1.00372796e-04
 1.04850690e-04 1.09328584e-04 1.13806478e-04]
client 2, data condensation 0, total loss = 486.10540771484375, avg loss = 243.05270385742188
client 2, data condensation 200, total loss = 33.45849609375, avg loss = 16.729248046875
client 2, data condensation 400, total loss = 549.8910522460938, avg loss = 274.9455261230469
client 2, data condensation 600, total loss = 58.990234375, avg loss = 29.4951171875
client 2, data condensation 800, total loss = 48.296112060546875, avg loss = 24.148056030273438
client 2, data condensation 1000, total loss = 44.512939453125, avg loss = 22.2564697265625
client 2, data condensation 1200, total loss = 151.227783203125, avg loss = 75.6138916015625
client 2, data condensation 1400, total loss = 50.337890625, avg loss = 25.1689453125
client 2, data condensation 1600, total loss = 165.60308837890625, avg loss = 82.80154418945312
client 2, data condensation 1800, total loss = 31.22064208984375, avg loss = 15.610321044921875
client 2, data condensation 2000, total loss = 25.153717041015625, avg loss = 12.576858520507812
client 2, data condensation 2200, total loss = 47.2542724609375, avg loss = 23.62713623046875
client 2, data condensation 2400, total loss = 155.89202880859375, avg loss = 77.94601440429688
client 2, data condensation 2600, total loss = 141.270751953125, avg loss = 70.6353759765625
client 2, data condensation 2800, total loss = 99.7010498046875, avg loss = 49.85052490234375
client 2, data condensation 3000, total loss = 30.644134521484375, avg loss = 15.322067260742188
client 2, data condensation 3200, total loss = 25.996673583984375, avg loss = 12.998336791992188
client 2, data condensation 3400, total loss = 51.29400634765625, avg loss = 25.647003173828125
client 2, data condensation 3600, total loss = 34.241485595703125, avg loss = 17.120742797851562
client 2, data condensation 3800, total loss = 37.66400146484375, avg loss = 18.832000732421875
client 2, data condensation 4000, total loss = 54.831695556640625, avg loss = 27.415847778320312
client 2, data condensation 4200, total loss = 301.02154541015625, avg loss = 150.51077270507812
client 2, data condensation 4400, total loss = 30.8358154296875, avg loss = 15.41790771484375
client 2, data condensation 4600, total loss = 101.3310546875, avg loss = 50.66552734375
client 2, data condensation 4800, total loss = 37.881378173828125, avg loss = 18.940689086914062
client 2, data condensation 5000, total loss = 57.13909912109375, avg loss = 28.569549560546875
Round 2, client 2 condense time: 436.8271381855011
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3191.06MB, free 21384.94MB
total 24576.0MB, used 3191.06MB, free 21384.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [110  27  10  14  12  12  15  19  27  67], bin edged: [0.00248184 0.00264278 0.00280372 0.00296466 0.0031256  0.00328654
 0.00344748 0.00360842 0.00376935 0.00393029 0.00409123]
client 3, data condensation 0, total loss = 139.86517333984375, avg loss = 139.86517333984375
client 3, data condensation 200, total loss = 42.54248046875, avg loss = 42.54248046875
client 3, data condensation 400, total loss = 35.226226806640625, avg loss = 35.226226806640625
client 3, data condensation 600, total loss = 30.74127197265625, avg loss = 30.74127197265625
client 3, data condensation 800, total loss = 40.85430908203125, avg loss = 40.85430908203125
client 3, data condensation 1000, total loss = 42.536773681640625, avg loss = 42.536773681640625
client 3, data condensation 1200, total loss = 29.65203857421875, avg loss = 29.65203857421875
client 3, data condensation 1400, total loss = 31.84259033203125, avg loss = 31.84259033203125
client 3, data condensation 1600, total loss = 25.160400390625, avg loss = 25.160400390625
client 3, data condensation 1800, total loss = 74.03213500976562, avg loss = 74.03213500976562
client 3, data condensation 2000, total loss = 39.3065185546875, avg loss = 39.3065185546875
client 3, data condensation 2200, total loss = 49.15484619140625, avg loss = 49.15484619140625
client 3, data condensation 2400, total loss = 68.35223388671875, avg loss = 68.35223388671875
client 3, data condensation 2600, total loss = 40.70440673828125, avg loss = 40.70440673828125
client 3, data condensation 2800, total loss = 22.0723876953125, avg loss = 22.0723876953125
client 3, data condensation 3000, total loss = 25.504638671875, avg loss = 25.504638671875
client 3, data condensation 3200, total loss = 29.8841552734375, avg loss = 29.8841552734375
client 3, data condensation 3400, total loss = 30.661773681640625, avg loss = 30.661773681640625
client 3, data condensation 3600, total loss = 22.929931640625, avg loss = 22.929931640625
client 3, data condensation 3800, total loss = 77.28179931640625, avg loss = 77.28179931640625
client 3, data condensation 4000, total loss = 20.243927001953125, avg loss = 20.243927001953125
client 3, data condensation 4200, total loss = 28.8310546875, avg loss = 28.8310546875
client 3, data condensation 4400, total loss = 89.42263793945312, avg loss = 89.42263793945312
client 3, data condensation 4600, total loss = 45.6849365234375, avg loss = 45.6849365234375
client 3, data condensation 4800, total loss = 49.142608642578125, avg loss = 49.142608642578125
client 3, data condensation 5000, total loss = 51.3114013671875, avg loss = 51.3114013671875
Round 2, client 3 condense time: 173.26703596115112
client 3, class 6 have 313 samples
total 24576.0MB, used 2803.06MB, free 21772.94MB
total 24576.0MB, used 2803.06MB, free 21772.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [272  36  14   4   0   1   3   1   1  29], bin edged: [0.00256191 0.00272811 0.00289431 0.0030605  0.0032267  0.0033929
 0.00355909 0.00372529 0.00389148 0.00405768 0.00422388]
class 4 have 1048 samples, histogram: [315  88  64  40  45  41  33  57  93 272], bin edged: [0.00072794 0.00077516 0.00082238 0.0008696  0.00091682 0.00096405
 0.00101127 0.00105849 0.00110571 0.00115293 0.00120015]
class 6 have 7572 samples, histogram: [3660  675  435  355  314  264  258  312  384  915], bin edged: [0.00011022 0.00011737 0.00012452 0.00013167 0.00013882 0.00014597
 0.00015312 0.00016027 0.00016742 0.00017457 0.00018172]
client 4, data condensation 0, total loss = 278.85638427734375, avg loss = 92.95212809244792
client 4, data condensation 200, total loss = 35.708160400390625, avg loss = 11.902720133463541
client 4, data condensation 400, total loss = 218.5198974609375, avg loss = 72.8399658203125
client 4, data condensation 600, total loss = 192.24172973632812, avg loss = 64.08057657877605
client 4, data condensation 800, total loss = 38.48358154296875, avg loss = 12.827860514322916
client 4, data condensation 1000, total loss = 267.50189208984375, avg loss = 89.16729736328125
client 4, data condensation 1200, total loss = 32.7847900390625, avg loss = 10.928263346354166
client 4, data condensation 1400, total loss = 29.9879150390625, avg loss = 9.9959716796875
client 4, data condensation 1600, total loss = 38.516571044921875, avg loss = 12.838857014973959
client 4, data condensation 1800, total loss = 200.9554443359375, avg loss = 66.98514811197917
client 4, data condensation 2000, total loss = 174.2236328125, avg loss = 58.074544270833336
client 4, data condensation 2200, total loss = 66.9581298828125, avg loss = 22.319376627604168
client 4, data condensation 2400, total loss = 29.075347900390625, avg loss = 9.691782633463541
client 4, data condensation 2600, total loss = 31.115570068359375, avg loss = 10.371856689453125
client 4, data condensation 2800, total loss = 125.07833862304688, avg loss = 41.692779541015625
client 4, data condensation 3000, total loss = 31.536956787109375, avg loss = 10.512318929036459
client 4, data condensation 3200, total loss = 27.8956298828125, avg loss = 9.298543294270834
client 4, data condensation 3400, total loss = 30.427947998046875, avg loss = 10.142649332682291
client 4, data condensation 3600, total loss = 43.1231689453125, avg loss = 14.3743896484375
client 4, data condensation 3800, total loss = 47.93084716796875, avg loss = 15.976949055989584
client 4, data condensation 4000, total loss = 61.15447998046875, avg loss = 20.38482666015625
client 4, data condensation 4200, total loss = 23.25677490234375, avg loss = 7.75225830078125
client 4, data condensation 4400, total loss = 30.33154296875, avg loss = 10.110514322916666
client 4, data condensation 4600, total loss = 68.60726928710938, avg loss = 22.869089762369793
client 4, data condensation 4800, total loss = 43.532562255859375, avg loss = 14.510854085286459
client 4, data condensation 5000, total loss = 24.18121337890625, avg loss = 8.060404459635416
Round 2, client 4 condense time: 485.4699676036835
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3449.06MB, free 21126.94MB
total 24576.0MB, used 3449.06MB, free 21126.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [4402  976  624  547  467  491  478  606  788 2772], bin edged: [6.43870333e-05 6.85639561e-05 7.27408790e-05 7.69178018e-05
 8.10947247e-05 8.52716475e-05 8.94485704e-05 9.36254932e-05
 9.78024161e-05 1.01979339e-04 1.06156262e-04]
client 5, data condensation 0, total loss = 106.753662109375, avg loss = 106.753662109375
client 5, data condensation 200, total loss = 9.13189697265625, avg loss = 9.13189697265625
client 5, data condensation 400, total loss = 15.666412353515625, avg loss = 15.666412353515625
client 5, data condensation 600, total loss = 5.8160400390625, avg loss = 5.8160400390625
client 5, data condensation 800, total loss = 18.50390625, avg loss = 18.50390625
client 5, data condensation 1000, total loss = 28.198883056640625, avg loss = 28.198883056640625
client 5, data condensation 1200, total loss = 36.99560546875, avg loss = 36.99560546875
client 5, data condensation 1400, total loss = 5.11328125, avg loss = 5.11328125
client 5, data condensation 1600, total loss = 9.2120361328125, avg loss = 9.2120361328125
client 5, data condensation 1800, total loss = 4.3372802734375, avg loss = 4.3372802734375
client 5, data condensation 2000, total loss = 11.557647705078125, avg loss = 11.557647705078125
client 5, data condensation 2200, total loss = 7.5889892578125, avg loss = 7.5889892578125
client 5, data condensation 2400, total loss = 5.47119140625, avg loss = 5.47119140625
client 5, data condensation 2600, total loss = 8.183624267578125, avg loss = 8.183624267578125
client 5, data condensation 2800, total loss = 18.555816650390625, avg loss = 18.555816650390625
client 5, data condensation 3000, total loss = 7.345367431640625, avg loss = 7.345367431640625
client 5, data condensation 3200, total loss = 5.420013427734375, avg loss = 5.420013427734375
client 5, data condensation 3400, total loss = 257.21337890625, avg loss = 257.21337890625
client 5, data condensation 3600, total loss = 8.7879638671875, avg loss = 8.7879638671875
client 5, data condensation 3800, total loss = 35.185333251953125, avg loss = 35.185333251953125
client 5, data condensation 4000, total loss = 26.771759033203125, avg loss = 26.771759033203125
client 5, data condensation 4200, total loss = 14.55224609375, avg loss = 14.55224609375
client 5, data condensation 4400, total loss = 7.828460693359375, avg loss = 7.828460693359375
client 5, data condensation 4600, total loss = 5.64239501953125, avg loss = 5.64239501953125
client 5, data condensation 4800, total loss = 6.887054443359375, avg loss = 6.887054443359375
client 5, data condensation 5000, total loss = 6.14410400390625, avg loss = 6.14410400390625
Round 2, client 5 condense time: 242.8221538066864
client 5, class 8 have 12151 samples
total 24576.0MB, used 2809.06MB, free 21766.94MB
total 24576.0MB, used 2809.06MB, free 21766.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [1439  876  701  643  602  608  648  752  996 2929], bin edged: [7.13718820e-05 7.60016399e-05 8.06313978e-05 8.52611557e-05
 8.98909135e-05 9.45206714e-05 9.91504293e-05 1.03780187e-04
 1.08409945e-04 1.13039703e-04 1.17669461e-04]
client 6, data condensation 0, total loss = 293.9768981933594, avg loss = 293.9768981933594
client 6, data condensation 200, total loss = 15.32666015625, avg loss = 15.32666015625
client 6, data condensation 400, total loss = 480.85736083984375, avg loss = 480.85736083984375
client 6, data condensation 600, total loss = 14.342803955078125, avg loss = 14.342803955078125
client 6, data condensation 800, total loss = 15.169097900390625, avg loss = 15.169097900390625
client 6, data condensation 1000, total loss = 352.95770263671875, avg loss = 352.95770263671875
client 6, data condensation 1200, total loss = 36.69659423828125, avg loss = 36.69659423828125
client 6, data condensation 1400, total loss = 606.211669921875, avg loss = 606.211669921875
client 6, data condensation 1600, total loss = 295.5761413574219, avg loss = 295.5761413574219
client 6, data condensation 1800, total loss = 15.907623291015625, avg loss = 15.907623291015625
client 6, data condensation 2000, total loss = 9.759979248046875, avg loss = 9.759979248046875
client 6, data condensation 2200, total loss = 13.6116943359375, avg loss = 13.6116943359375
client 6, data condensation 2400, total loss = 9.35791015625, avg loss = 9.35791015625
client 6, data condensation 2600, total loss = 213.7005615234375, avg loss = 213.7005615234375
client 6, data condensation 2800, total loss = 13.237518310546875, avg loss = 13.237518310546875
client 6, data condensation 3000, total loss = 20.631072998046875, avg loss = 20.631072998046875
client 6, data condensation 3200, total loss = 20.955352783203125, avg loss = 20.955352783203125
client 6, data condensation 3400, total loss = 19.612396240234375, avg loss = 19.612396240234375
client 6, data condensation 3600, total loss = 17.932769775390625, avg loss = 17.932769775390625
client 6, data condensation 3800, total loss = 24.2845458984375, avg loss = 24.2845458984375
client 6, data condensation 4000, total loss = 36.04486083984375, avg loss = 36.04486083984375
client 6, data condensation 4200, total loss = 19.955322265625, avg loss = 19.955322265625
client 6, data condensation 4400, total loss = 7.333404541015625, avg loss = 7.333404541015625
client 6, data condensation 4600, total loss = 18.490997314453125, avg loss = 18.490997314453125
client 6, data condensation 4800, total loss = 32.445343017578125, avg loss = 32.445343017578125
client 6, data condensation 5000, total loss = 9.083343505859375, avg loss = 9.083343505859375
Round 2, client 6 condense time: 241.61977529525757
client 6, class 3 have 10194 samples
total 24576.0MB, used 2811.06MB, free 21764.94MB
total 24576.0MB, used 2811.06MB, free 21764.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [5736  884  358  659   99   60   47   54   73 1142], bin edged: [9.69477074e-05 1.03236164e-04 1.09524621e-04 1.15813077e-04
 1.22101534e-04 1.28389991e-04 1.34678447e-04 1.40966904e-04
 1.47255361e-04 1.53543817e-04 1.59832274e-04]
client 7, data condensation 0, total loss = 217.37872314453125, avg loss = 217.37872314453125
client 7, data condensation 200, total loss = 7.636444091796875, avg loss = 7.636444091796875
client 7, data condensation 400, total loss = 17.613525390625, avg loss = 17.613525390625
client 7, data condensation 600, total loss = 3.068634033203125, avg loss = 3.068634033203125
client 7, data condensation 800, total loss = 12.886871337890625, avg loss = 12.886871337890625
client 7, data condensation 1000, total loss = 8.836578369140625, avg loss = 8.836578369140625
client 7, data condensation 1200, total loss = 19.214996337890625, avg loss = 19.214996337890625
client 7, data condensation 1400, total loss = 120.43246459960938, avg loss = 120.43246459960938
client 7, data condensation 1600, total loss = 16.72918701171875, avg loss = 16.72918701171875
client 7, data condensation 1800, total loss = 5.81231689453125, avg loss = 5.81231689453125
client 7, data condensation 2000, total loss = 115.39187622070312, avg loss = 115.39187622070312
client 7, data condensation 2200, total loss = 8.102996826171875, avg loss = 8.102996826171875
client 7, data condensation 2400, total loss = 4.63055419921875, avg loss = 4.63055419921875
client 7, data condensation 2600, total loss = 25.92474365234375, avg loss = 25.92474365234375
client 7, data condensation 2800, total loss = 6.26629638671875, avg loss = 6.26629638671875
client 7, data condensation 3000, total loss = 21.848907470703125, avg loss = 21.848907470703125
client 7, data condensation 3200, total loss = 8.419342041015625, avg loss = 8.419342041015625
client 7, data condensation 3400, total loss = 8.915557861328125, avg loss = 8.915557861328125
client 7, data condensation 3600, total loss = 11.113616943359375, avg loss = 11.113616943359375
client 7, data condensation 3800, total loss = 9.302734375, avg loss = 9.302734375
client 7, data condensation 4000, total loss = 92.34963989257812, avg loss = 92.34963989257812
client 7, data condensation 4200, total loss = 7.1558837890625, avg loss = 7.1558837890625
client 7, data condensation 4400, total loss = 15.66314697265625, avg loss = 15.66314697265625
client 7, data condensation 4600, total loss = 6.395050048828125, avg loss = 6.395050048828125
client 7, data condensation 4800, total loss = 5.85357666015625, avg loss = 5.85357666015625
client 7, data condensation 5000, total loss = 14.2181396484375, avg loss = 14.2181396484375
Round 2, client 7 condense time: 259.904657125473
client 7, class 1 have 9112 samples
total 24576.0MB, used 2811.06MB, free 21764.94MB
total 24576.0MB, used 2811.06MB, free 21764.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [69 26 18 17 15 12  8  7  8 26], bin edged: [0.00395516 0.00421119 0.00446723 0.00472326 0.0049793  0.00523534
 0.00549137 0.00574741 0.00600344 0.00625948 0.00651551]
class 4 have 1179 samples, histogram: [575  82  55  50  52  36  41  50  51 187], bin edged: [0.00069913 0.00074449 0.00078984 0.00083519 0.00088054 0.00092589
 0.00097125 0.0010166  0.00106195 0.0011073  0.00115265]
class 8 have 734 samples, histogram: [316  73  53  34  33  33  26  30  35 101], bin edged: [0.00112154 0.0011943  0.00126705 0.00133981 0.00141257 0.00148532
 0.00155808 0.00163084 0.00170359 0.00177635 0.00184911]
client 8, data condensation 0, total loss = 309.66094970703125, avg loss = 103.22031656901042
client 8, data condensation 200, total loss = 157.3729248046875, avg loss = 52.4576416015625
client 8, data condensation 400, total loss = 65.60330200195312, avg loss = 21.867767333984375
client 8, data condensation 600, total loss = 42.023406982421875, avg loss = 14.007802327473959
client 8, data condensation 800, total loss = 49.380279541015625, avg loss = 16.460093180338543
client 8, data condensation 1000, total loss = 52.951995849609375, avg loss = 17.650665283203125
client 8, data condensation 1200, total loss = 59.660888671875, avg loss = 19.886962890625
client 8, data condensation 1400, total loss = 159.77438354492188, avg loss = 53.25812784830729
client 8, data condensation 1600, total loss = 81.69363403320312, avg loss = 27.231211344401043
client 8, data condensation 1800, total loss = 56.0767822265625, avg loss = 18.6922607421875
client 8, data condensation 2000, total loss = 170.11236572265625, avg loss = 56.704121907552086
client 8, data condensation 2200, total loss = 471.7555847167969, avg loss = 157.25186157226562
client 8, data condensation 2400, total loss = 81.57662963867188, avg loss = 27.192209879557293
client 8, data condensation 2600, total loss = 209.7637939453125, avg loss = 69.9212646484375
client 8, data condensation 2800, total loss = 60.3189697265625, avg loss = 20.1063232421875
client 8, data condensation 3000, total loss = 64.921630859375, avg loss = 21.640543619791668
client 8, data condensation 3200, total loss = 57.906158447265625, avg loss = 19.302052815755207
client 8, data condensation 3400, total loss = 103.8720703125, avg loss = 34.6240234375
client 8, data condensation 3600, total loss = 52.16607666015625, avg loss = 17.388692220052082
client 8, data condensation 3800, total loss = 60.534759521484375, avg loss = 20.178253173828125
client 8, data condensation 4000, total loss = 68.40023803710938, avg loss = 22.800079345703125
client 8, data condensation 4200, total loss = 131.57244873046875, avg loss = 43.85748291015625
client 8, data condensation 4400, total loss = 243.74810791015625, avg loss = 81.24936930338542
client 8, data condensation 4600, total loss = 59.7841796875, avg loss = 19.928059895833332
client 8, data condensation 4800, total loss = 36.010223388671875, avg loss = 12.003407796223959
client 8, data condensation 5000, total loss = 146.35491943359375, avg loss = 48.78497314453125
Round 2, client 8 condense time: 507.5889587402344
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3447.06MB, free 21128.94MB
total 24576.0MB, used 3447.06MB, free 21128.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [ 277   56   57   90   99  151  197  383  778 8228], bin edged: [6.09970303e-05 6.49540373e-05 6.89110444e-05 7.28680514e-05
 7.68250585e-05 8.07820655e-05 8.47390726e-05 8.86960796e-05
 9.26530866e-05 9.66100937e-05 1.00567101e-04]
client 9, data condensation 0, total loss = 139.36834716796875, avg loss = 139.36834716796875
client 9, data condensation 200, total loss = 7.509765625, avg loss = 7.509765625
client 9, data condensation 400, total loss = 10.172576904296875, avg loss = 10.172576904296875
client 9, data condensation 600, total loss = 6.984527587890625, avg loss = 6.984527587890625
client 9, data condensation 800, total loss = 100.9193115234375, avg loss = 100.9193115234375
client 9, data condensation 1000, total loss = 10.390045166015625, avg loss = 10.390045166015625
client 9, data condensation 1200, total loss = 23.204681396484375, avg loss = 23.204681396484375
client 9, data condensation 1400, total loss = 15.2718505859375, avg loss = 15.2718505859375
client 9, data condensation 1600, total loss = 10.640533447265625, avg loss = 10.640533447265625
client 9, data condensation 1800, total loss = 5.913543701171875, avg loss = 5.913543701171875
client 9, data condensation 2000, total loss = 34.07879638671875, avg loss = 34.07879638671875
client 9, data condensation 2200, total loss = 7.39129638671875, avg loss = 7.39129638671875
client 9, data condensation 2400, total loss = 92.83969116210938, avg loss = 92.83969116210938
client 9, data condensation 2600, total loss = 4.751922607421875, avg loss = 4.751922607421875
client 9, data condensation 2800, total loss = 7.84600830078125, avg loss = 7.84600830078125
client 9, data condensation 3000, total loss = 216.32452392578125, avg loss = 216.32452392578125
client 9, data condensation 3200, total loss = 21.6617431640625, avg loss = 21.6617431640625
client 9, data condensation 3400, total loss = 6.877197265625, avg loss = 6.877197265625
client 9, data condensation 3600, total loss = 141.29476928710938, avg loss = 141.29476928710938
client 9, data condensation 3800, total loss = 9.6318359375, avg loss = 9.6318359375
client 9, data condensation 4000, total loss = 6.261199951171875, avg loss = 6.261199951171875
client 9, data condensation 4200, total loss = 9.33477783203125, avg loss = 9.33477783203125
client 9, data condensation 4400, total loss = 4.035797119140625, avg loss = 4.035797119140625
client 9, data condensation 4600, total loss = 5.756988525390625, avg loss = 5.756988525390625
client 9, data condensation 4800, total loss = 20.83709716796875, avg loss = 20.83709716796875
client 9, data condensation 5000, total loss = 7.388916015625, avg loss = 7.388916015625
Round 2, client 9 condense time: 267.763592004776
client 9, class 2 have 10316 samples
total 24576.0MB, used 2813.06MB, free 21762.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[  7.1885,   0.7658,  -3.9674,  -7.8018,   6.7382,   3.5250,   0.9898,
          -0.6565,  -6.7528],
        [  1.1426,  11.1233,  -3.9072,  -4.8661,  -0.0960,   3.1493,  -2.1723,
          -0.7192,  -4.0694],
        [ -9.9524,  -5.9469,   4.5934,  -5.8037,   0.0791,   9.4471,   2.3874,
           6.0166,  -0.0389],
        [-11.6110,  -6.1011,   0.7732,   5.3034,  -3.7887,   5.5111,   5.3292,
           5.2768,   1.0504],
        [ -5.3997,  -4.5761,  -2.1353,  -5.3121,   5.7308,   5.2142,   6.1433,
           3.1785,  -2.3818],
        [ -8.3215,  -4.9238,   2.6517,  -7.1243,   0.2163,  11.6194,   2.1492,
           4.5687,  -0.2997],
        [ -7.8494,  -6.5867,   1.1740,  -3.3207,   0.3989,   5.0709,   9.3134,
           0.3287,   2.4008],
        [-10.8061,  -6.2825,   0.5588,  -3.8353,   1.3692,   8.1734,   2.6946,
           8.9189,  -0.1010],
        [ -9.5235,  -5.6023,   1.9640,  -4.6529,  -1.2371,   5.8302,   5.7045,
          -0.4100,   8.6443]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 6, 1],
        [1, 5, 0, 4, 7],
        [5, 7, 2, 6, 4],
        [5, 6, 3, 7, 8],
        [6, 4, 5, 7, 2],
        [5, 7, 2, 6, 4],
        [6, 5, 8, 2, 4],
        [7, 5, 6, 4, 2],
        [8, 5, 6, 2, 7]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
3
Round 2: # synthetic sample: 2736
total 24576.0MB, used 2813.06MB, free 21762.94MB
{0: {0: 761, 1: 3, 2: 0, 3: 0, 4: 302, 5: 245, 6: 27, 7: 0, 8: 0}, 1: {0: 0, 1: 757, 2: 89, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 170, 3: 0, 4: 0, 5: 105, 6: 0, 7: 63, 8: 0}, 3: {0: 0, 1: 0, 2: 26, 3: 105, 4: 0, 5: 129, 6: 198, 7: 168, 8: 8}, 4: {0: 4, 1: 91, 2: 0, 3: 0, 4: 389, 5: 102, 6: 95, 7: 353, 8: 1}, 5: {0: 55, 1: 0, 2: 36, 3: 1, 4: 0, 5: 238, 6: 25, 7: 216, 8: 21}, 6: {0: 0, 1: 0, 2: 31, 3: 0, 4: 7, 5: 87, 6: 603, 7: 0, 8: 13}, 7: {0: 6, 1: 5, 2: 34, 3: 15, 4: 2, 5: 142, 6: 31, 7: 175, 8: 11}, 8: {0: 5, 1: 1, 2: 71, 3: 0, 4: 1, 5: 134, 6: 135, 7: 89, 8: 797}}
round 2 evaluation: test acc is 0.5564, test loss = 2.090456
{0: {0: 1276, 1: 0, 2: 0, 3: 0, 4: 29, 5: 31, 6: 0, 7: 0, 8: 2}, 1: {0: 0, 1: 844, 2: 3, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 175, 3: 2, 4: 0, 5: 37, 6: 0, 7: 112, 8: 12}, 3: {0: 0, 1: 0, 2: 0, 3: 595, 4: 1, 5: 2, 6: 6, 7: 0, 8: 30}, 4: {0: 61, 1: 47, 2: 0, 3: 0, 4: 894, 5: 6, 6: 7, 7: 16, 8: 4}, 5: {0: 55, 1: 1, 2: 36, 3: 71, 4: 1, 5: 150, 6: 1, 7: 118, 8: 159}, 6: {0: 18, 1: 0, 2: 3, 3: 93, 4: 72, 5: 6, 6: 258, 7: 0, 8: 291}, 7: {0: 2, 1: 16, 2: 25, 3: 70, 4: 22, 5: 59, 6: 2, 7: 137, 8: 88}, 8: {0: 9, 1: 1, 2: 4, 3: 57, 4: 3, 5: 16, 6: 2, 7: 5, 8: 1136}}
epoch 0, train loss avg now = 0.970525, train contrast loss now = 1.362654, test acc now = 0.7611, test loss now = 1.395624
{0: {0: 1267, 1: 0, 2: 0, 3: 0, 4: 14, 5: 50, 6: 7, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 148, 3: 6, 4: 0, 5: 96, 6: 0, 7: 80, 8: 8}, 3: {0: 0, 1: 0, 2: 0, 3: 562, 4: 1, 5: 5, 6: 64, 7: 0, 8: 2}, 4: {0: 31, 1: 74, 2: 0, 3: 3, 4: 808, 5: 11, 6: 99, 7: 9, 8: 0}, 5: {0: 55, 1: 0, 2: 49, 3: 75, 4: 0, 5: 195, 6: 13, 7: 141, 8: 64}, 6: {0: 19, 1: 0, 2: 0, 3: 2, 4: 18, 5: 54, 6: 643, 7: 0, 8: 5}, 7: {0: 2, 1: 6, 2: 7, 3: 53, 4: 26, 5: 88, 6: 35, 7: 168, 8: 36}, 8: {0: 11, 1: 1, 2: 5, 3: 21, 4: 3, 5: 168, 6: 189, 7: 40, 8: 795}}
epoch 100, train loss avg now = 0.136448, train contrast loss now = 1.112151, test acc now = 0.7567, test loss now = 1.077294
{0: {0: 1314, 1: 11, 2: 0, 3: 0, 4: 9, 5: 1, 6: 3, 7: 0, 8: 0}, 1: {0: 0, 1: 846, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 217, 3: 6, 4: 0, 5: 16, 6: 0, 7: 92, 8: 7}, 3: {0: 1, 1: 0, 2: 0, 3: 536, 4: 1, 5: 0, 6: 95, 7: 1, 8: 0}, 4: {0: 38, 1: 157, 2: 0, 3: 0, 4: 743, 5: 12, 6: 76, 7: 9, 8: 0}, 5: {0: 57, 1: 0, 2: 80, 3: 92, 4: 1, 5: 162, 6: 20, 7: 134, 8: 46}, 6: {0: 36, 1: 0, 2: 0, 3: 1, 4: 32, 5: 8, 6: 661, 7: 0, 8: 3}, 7: {0: 4, 1: 13, 2: 21, 3: 40, 4: 66, 5: 49, 6: 45, 7: 154, 8: 29}, 8: {0: 33, 1: 2, 2: 28, 3: 37, 4: 8, 5: 99, 6: 375, 7: 28, 8: 623}}
epoch 200, train loss avg now = 0.049919, train contrast loss now = 1.108566, test acc now = 0.7320, test loss now = 1.457969
{0: {0: 1308, 1: 3, 2: 0, 3: 0, 4: 25, 5: 1, 6: 1, 7: 0, 8: 0}, 1: {0: 0, 1: 840, 2: 7, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 268, 3: 5, 4: 0, 5: 12, 6: 0, 7: 49, 8: 4}, 3: {0: 2, 1: 0, 2: 1, 3: 536, 4: 1, 5: 1, 6: 91, 7: 1, 8: 1}, 4: {0: 30, 1: 136, 2: 0, 3: 0, 4: 831, 5: 18, 6: 20, 7: 0, 8: 0}, 5: {0: 57, 1: 0, 2: 228, 3: 50, 4: 3, 5: 158, 6: 16, 7: 31, 8: 49}, 6: {0: 27, 1: 0, 2: 3, 3: 1, 4: 55, 5: 7, 6: 640, 7: 0, 8: 8}, 7: {0: 6, 1: 22, 2: 80, 3: 27, 4: 115, 5: 39, 6: 43, 7: 45, 8: 44}, 8: {0: 25, 1: 3, 2: 65, 3: 20, 4: 31, 5: 68, 6: 218, 7: 8, 8: 795}}
epoch 300, train loss avg now = 0.039314, train contrast loss now = 1.103793, test acc now = 0.7550, test loss now = 1.490131
{0: {0: 1292, 1: 10, 2: 0, 3: 0, 4: 32, 5: 2, 6: 2, 7: 0, 8: 0}, 1: {0: 0, 1: 844, 2: 3, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 291, 3: 13, 4: 0, 5: 11, 6: 0, 7: 24, 8: 0}, 3: {0: 0, 1: 0, 2: 11, 3: 592, 4: 3, 5: 0, 6: 28, 7: 0, 8: 0}, 4: {0: 31, 1: 126, 2: 0, 3: 4, 4: 823, 5: 11, 6: 23, 7: 17, 8: 0}, 5: {0: 55, 1: 12, 2: 366, 3: 39, 4: 2, 5: 65, 6: 3, 7: 46, 8: 4}, 6: {0: 23, 1: 0, 2: 75, 3: 16, 4: 52, 5: 3, 6: 569, 7: 1, 8: 2}, 7: {0: 1, 1: 27, 2: 158, 3: 56, 4: 32, 5: 10, 6: 13, 7: 116, 8: 8}, 8: {0: 22, 1: 6, 2: 388, 3: 47, 4: 38, 5: 46, 6: 91, 7: 15, 8: 580}}
epoch 400, train loss avg now = 0.078929, train contrast loss now = 1.104958, test acc now = 0.7203, test loss now = 1.732618
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1300, 1: 6, 2: 0, 3: 0, 4: 19, 5: 7, 6: 6, 7: 0, 8: 0}, 1: {0: 6, 1: 814, 2: 27, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 244, 3: 10, 4: 0, 5: 54, 6: 0, 7: 27, 8: 3}, 3: {0: 0, 1: 0, 2: 0, 3: 563, 4: 1, 5: 1, 6: 67, 7: 0, 8: 2}, 4: {0: 36, 1: 148, 2: 0, 3: 1, 4: 761, 5: 29, 6: 59, 7: 1, 8: 0}, 5: {0: 55, 1: 0, 2: 151, 3: 70, 4: 0, 5: 176, 6: 19, 7: 73, 8: 48}, 6: {0: 19, 1: 0, 2: 0, 3: 1, 4: 24, 5: 10, 6: 683, 7: 0, 8: 4}, 7: {0: 4, 1: 16, 2: 55, 3: 44, 4: 44, 5: 52, 6: 49, 7: 114, 8: 43}, 8: {0: 17, 1: 3, 2: 50, 3: 27, 4: 6, 5: 96, 6: 285, 7: 7, 8: 742}}
epoch 500, train loss avg now = 0.081047, train contrast loss now = 1.104289, test acc now = 0.7517, test loss now = 1.297420
{0: {0: 1318, 1: 5, 2: 0, 3: 0, 4: 5, 5: 6, 6: 4, 7: 0, 8: 0}, 1: {0: 2, 1: 811, 2: 33, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 174, 3: 10, 4: 0, 5: 110, 6: 1, 7: 42, 8: 2}, 3: {0: 1, 1: 0, 2: 0, 3: 542, 4: 1, 5: 1, 6: 88, 7: 0, 8: 1}, 4: {0: 47, 1: 209, 2: 0, 3: 1, 4: 672, 5: 31, 6: 74, 7: 1, 8: 0}, 5: {0: 55, 1: 0, 2: 167, 3: 73, 4: 0, 5: 167, 6: 25, 7: 74, 8: 31}, 6: {0: 20, 1: 0, 2: 1, 3: 0, 4: 26, 5: 13, 6: 679, 7: 0, 8: 2}, 7: {0: 1, 1: 11, 2: 66, 3: 46, 4: 48, 5: 44, 6: 60, 7: 120, 8: 25}, 8: {0: 19, 1: 0, 2: 59, 3: 31, 4: 9, 5: 109, 6: 354, 7: 12, 8: 640}}
epoch 600, train loss avg now = 0.025007, train contrast loss now = 1.100512, test acc now = 0.7135, test loss now = 1.588104
{0: {0: 1320, 1: 5, 2: 0, 3: 0, 4: 2, 5: 7, 6: 4, 7: 0, 8: 0}, 1: {0: 2, 1: 804, 2: 41, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 225, 3: 9, 4: 0, 5: 55, 6: 2, 7: 45, 8: 2}, 3: {0: 0, 1: 0, 2: 0, 3: 538, 4: 1, 5: 2, 6: 91, 7: 0, 8: 2}, 4: {0: 54, 1: 219, 2: 0, 3: 1, 4: 641, 5: 27, 6: 92, 7: 1, 8: 0}, 5: {0: 55, 1: 5, 2: 164, 3: 68, 4: 0, 5: 161, 6: 28, 7: 76, 8: 35}, 6: {0: 19, 1: 0, 2: 0, 3: 0, 4: 25, 5: 12, 6: 682, 7: 0, 8: 3}, 7: {0: 2, 1: 20, 2: 69, 3: 44, 4: 52, 5: 33, 6: 63, 7: 114, 8: 24}, 8: {0: 20, 1: 0, 2: 69, 3: 31, 4: 4, 5: 119, 6: 350, 7: 11, 8: 629}}
epoch 700, train loss avg now = 0.031258, train contrast loss now = 1.101478, test acc now = 0.7123, test loss now = 1.619577
{0: {0: 1318, 1: 3, 2: 0, 3: 0, 4: 6, 5: 6, 6: 5, 7: 0, 8: 0}, 1: {0: 1, 1: 723, 2: 123, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 264, 3: 10, 4: 0, 5: 19, 6: 7, 7: 35, 8: 3}, 3: {0: 0, 1: 0, 2: 0, 3: 555, 4: 1, 5: 1, 6: 75, 7: 0, 8: 2}, 4: {0: 51, 1: 208, 2: 0, 3: 1, 4: 643, 5: 38, 6: 94, 7: 0, 8: 0}, 5: {0: 55, 1: 6, 2: 120, 3: 79, 4: 0, 5: 161, 6: 31, 7: 87, 8: 53}, 6: {0: 18, 1: 0, 2: 0, 3: 0, 4: 27, 5: 10, 6: 683, 7: 0, 8: 3}, 7: {0: 2, 1: 20, 2: 58, 3: 49, 4: 59, 5: 30, 6: 73, 7: 101, 8: 29}, 8: {0: 18, 1: 0, 2: 52, 3: 36, 4: 7, 5: 91, 6: 366, 7: 10, 8: 653}}
epoch 800, train loss avg now = 0.030587, train contrast loss now = 1.102027, test acc now = 0.7104, test loss now = 1.625745
{0: {0: 1319, 1: 5, 2: 0, 3: 0, 4: 4, 5: 4, 6: 6, 7: 0, 8: 0}, 1: {0: 3, 1: 610, 2: 234, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 270, 3: 10, 4: 0, 5: 15, 6: 7, 7: 35, 8: 1}, 3: {0: 0, 1: 0, 2: 1, 3: 552, 4: 1, 5: 1, 6: 79, 7: 0, 8: 0}, 4: {0: 52, 1: 193, 2: 0, 3: 1, 4: 659, 5: 41, 6: 89, 7: 0, 8: 0}, 5: {0: 55, 1: 6, 2: 160, 3: 78, 4: 0, 5: 162, 6: 32, 7: 75, 8: 24}, 6: {0: 19, 1: 0, 2: 1, 3: 0, 4: 26, 5: 9, 6: 684, 7: 0, 8: 2}, 7: {0: 6, 1: 18, 2: 67, 3: 50, 4: 67, 5: 30, 6: 77, 7: 86, 8: 20}, 8: {0: 20, 1: 0, 2: 81, 3: 41, 4: 6, 5: 97, 6: 408, 7: 9, 8: 571}}
epoch 900, train loss avg now = 0.027957, train contrast loss now = 1.101689, test acc now = 0.6843, test loss now = 1.748957
{0: {0: 1319, 1: 5, 2: 0, 3: 0, 4: 5, 5: 7, 6: 2, 7: 0, 8: 0}, 1: {0: 1, 1: 591, 2: 255, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 261, 3: 5, 4: 0, 5: 15, 6: 4, 7: 49, 8: 4}, 3: {0: 2, 1: 0, 2: 0, 3: 522, 4: 1, 5: 2, 6: 105, 7: 0, 8: 2}, 4: {0: 48, 1: 165, 2: 0, 3: 0, 4: 705, 5: 41, 6: 75, 7: 1, 8: 0}, 5: {0: 55, 1: 5, 2: 147, 3: 61, 4: 0, 5: 164, 6: 30, 7: 93, 8: 37}, 6: {0: 21, 1: 0, 2: 0, 3: 0, 4: 28, 5: 15, 6: 673, 7: 0, 8: 4}, 7: {0: 5, 1: 18, 2: 63, 3: 37, 4: 65, 5: 33, 6: 64, 7: 111, 8: 25}, 8: {0: 20, 1: 1, 2: 65, 3: 27, 4: 9, 5: 118, 6: 308, 7: 15, 8: 670}}
epoch 1000, train loss avg now = 0.012614, train contrast loss now = 1.102048, test acc now = 0.6986, test loss now = 1.666406
epoch avg loss = 1.2613541811530352e-05, total time = 5092.299854755402
total 24576.0MB, used 3537.06MB, free 21038.94MB
Round 2 finish, update the prev_syn_proto
torch.Size([288, 3, 28, 28])
torch.Size([291, 3, 28, 28])
torch.Size([312, 3, 28, 28])
torch.Size([321, 3, 28, 28])
torch.Size([243, 3, 28, 28])
torch.Size([366, 3, 28, 28])
torch.Size([243, 3, 28, 28])
torch.Size([282, 3, 28, 28])
torch.Size([390, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1319, 1: 5, 2: 0, 3: 0, 4: 5, 5: 7, 6: 2, 7: 0, 8: 0}, 1: {0: 1, 1: 591, 2: 255, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 261, 3: 5, 4: 0, 5: 15, 6: 4, 7: 49, 8: 4}, 3: {0: 2, 1: 0, 2: 0, 3: 522, 4: 1, 5: 2, 6: 105, 7: 0, 8: 2}, 4: {0: 48, 1: 165, 2: 0, 3: 0, 4: 705, 5: 41, 6: 75, 7: 1, 8: 0}, 5: {0: 55, 1: 5, 2: 147, 3: 61, 4: 0, 5: 164, 6: 30, 7: 93, 8: 37}, 6: {0: 21, 1: 0, 2: 0, 3: 0, 4: 28, 5: 15, 6: 673, 7: 0, 8: 4}, 7: {0: 5, 1: 18, 2: 63, 3: 37, 4: 65, 5: 33, 6: 64, 7: 111, 8: 25}, 8: {0: 20, 1: 1, 2: 65, 3: 27, 4: 9, 5: 118, 6: 308, 7: 15, 8: 670}}
round 2 evaluation: test acc is 0.6986, test loss = 1.666406
 ====== round 3 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3537.06MB, free 21038.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [2567  486  300  259  203  209  214  248  318  973], bin edged: [0.00014082 0.00014996 0.00015909 0.00016823 0.00017736 0.0001865
 0.00019563 0.00020477 0.0002139  0.00022304 0.00023217]
class 7 have 9330 samples, histogram: [2150  772  561  391  411  374  416  483  683 3089], bin edged: [7.91570290e-05 8.42906965e-05 8.94243640e-05 9.45580315e-05
 9.96916991e-05 1.04825367e-04 1.09959034e-04 1.15092702e-04
 1.20226369e-04 1.25360037e-04 1.30493704e-04]
client 0, data condensation 0, total loss = 104.44485473632812, avg loss = 52.22242736816406
client 0, data condensation 200, total loss = 40.766082763671875, avg loss = 20.383041381835938
client 0, data condensation 400, total loss = 15.134124755859375, avg loss = 7.5670623779296875
client 0, data condensation 600, total loss = 80.43136596679688, avg loss = 40.21568298339844
client 0, data condensation 800, total loss = 16.945404052734375, avg loss = 8.472702026367188
client 0, data condensation 1000, total loss = 33.413330078125, avg loss = 16.7066650390625
client 0, data condensation 1200, total loss = 430.55682373046875, avg loss = 215.27841186523438
client 0, data condensation 1400, total loss = 16.056243896484375, avg loss = 8.028121948242188
client 0, data condensation 1600, total loss = 37.556671142578125, avg loss = 18.778335571289062
client 0, data condensation 1800, total loss = 12.333740234375, avg loss = 6.1668701171875
client 0, data condensation 2000, total loss = 13.1910400390625, avg loss = 6.59552001953125
client 0, data condensation 2200, total loss = 10.4884033203125, avg loss = 5.24420166015625
client 0, data condensation 2400, total loss = 15.461334228515625, avg loss = 7.7306671142578125
client 0, data condensation 2600, total loss = 20.458953857421875, avg loss = 10.229476928710938
client 0, data condensation 2800, total loss = 24.648223876953125, avg loss = 12.324111938476562
client 0, data condensation 3000, total loss = 5.510101318359375, avg loss = 2.7550506591796875
client 0, data condensation 3200, total loss = 41.703399658203125, avg loss = 20.851699829101562
client 0, data condensation 3400, total loss = 19.195404052734375, avg loss = 9.597702026367188
client 0, data condensation 3600, total loss = 22.197998046875, avg loss = 11.0989990234375
client 0, data condensation 3800, total loss = 20.995086669921875, avg loss = 10.497543334960938
client 0, data condensation 4000, total loss = 23.803985595703125, avg loss = 11.901992797851562
client 0, data condensation 4200, total loss = 23.753692626953125, avg loss = 11.876846313476562
client 0, data condensation 4400, total loss = 25.76251220703125, avg loss = 12.881256103515625
client 0, data condensation 4600, total loss = 21.174530029296875, avg loss = 10.587265014648438
client 0, data condensation 4800, total loss = 13.77911376953125, avg loss = 6.889556884765625
client 0, data condensation 5000, total loss = 23.931610107421875, avg loss = 11.965805053710938
Round 3, client 0 condense time: 431.5361661911011
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3217.06MB, free 21358.94MB
total 24576.0MB, used 3217.06MB, free 21358.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [4576  864  507  332  273  266  270  279  383 1272], bin edged: [9.28477165e-05 9.88709434e-05 1.04894170e-04 1.10917397e-04
 1.16940624e-04 1.22963851e-04 1.28987078e-04 1.35010305e-04
 1.41033532e-04 1.47056759e-04 1.53079986e-04]
client 1, data condensation 0, total loss = 347.90283203125, avg loss = 347.90283203125
client 1, data condensation 200, total loss = 13.6650390625, avg loss = 13.6650390625
client 1, data condensation 400, total loss = 9.6163330078125, avg loss = 9.6163330078125
client 1, data condensation 600, total loss = 28.063720703125, avg loss = 28.063720703125
client 1, data condensation 800, total loss = 32.7449951171875, avg loss = 32.7449951171875
client 1, data condensation 1000, total loss = 24.0184326171875, avg loss = 24.0184326171875
client 1, data condensation 1200, total loss = 12.47393798828125, avg loss = 12.47393798828125
client 1, data condensation 1400, total loss = 4.462890625, avg loss = 4.462890625
client 1, data condensation 1600, total loss = 57.9464111328125, avg loss = 57.9464111328125
client 1, data condensation 1800, total loss = 7.598388671875, avg loss = 7.598388671875
client 1, data condensation 2000, total loss = 55.99090576171875, avg loss = 55.99090576171875
client 1, data condensation 2200, total loss = 16.85394287109375, avg loss = 16.85394287109375
client 1, data condensation 2400, total loss = 35.8138427734375, avg loss = 35.8138427734375
client 1, data condensation 2600, total loss = 276.65716552734375, avg loss = 276.65716552734375
client 1, data condensation 2800, total loss = 11.3583984375, avg loss = 11.3583984375
client 1, data condensation 3000, total loss = 37.55908203125, avg loss = 37.55908203125
client 1, data condensation 3200, total loss = 18.9027099609375, avg loss = 18.9027099609375
client 1, data condensation 3400, total loss = 126.69390869140625, avg loss = 126.69390869140625
client 1, data condensation 3600, total loss = 8.434967041015625, avg loss = 8.434967041015625
client 1, data condensation 3800, total loss = 151.66079711914062, avg loss = 151.66079711914062
client 1, data condensation 4000, total loss = 11.29010009765625, avg loss = 11.29010009765625
client 1, data condensation 4200, total loss = 6.90948486328125, avg loss = 6.90948486328125
client 1, data condensation 4400, total loss = 42.45855712890625, avg loss = 42.45855712890625
client 1, data condensation 4600, total loss = 15.6328125, avg loss = 15.6328125
client 1, data condensation 4800, total loss = 16.74322509765625, avg loss = 16.74322509765625
client 1, data condensation 5000, total loss = 59.585601806640625, avg loss = 59.585601806640625
Round 3, client 1 condense time: 247.51294541358948
client 1, class 0 have 9022 samples
total 24576.0MB, used 2835.06MB, free 21740.94MB
total 24576.0MB, used 2835.06MB, free 21740.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [232  22  12  13   4   2   4   6   8  24], bin edged: [0.00276428 0.0029436  0.00312291 0.00330223 0.00348154 0.00366086
 0.00384017 0.00401949 0.0041988  0.00437812 0.00455743]
class 5 have 12176 samples, histogram: [4025  595  390  320  317  301  349  435  684 4760], bin edged: [6.10192101e-05 6.49776543e-05 6.89360984e-05 7.28945426e-05
 7.68529867e-05 8.08114309e-05 8.47698750e-05 8.87283192e-05
 9.26867633e-05 9.66452075e-05 1.00603652e-04]
client 2, data condensation 0, total loss = 307.4515075683594, avg loss = 153.7257537841797
client 2, data condensation 200, total loss = 107.7061767578125, avg loss = 53.85308837890625
client 2, data condensation 400, total loss = 48.17279052734375, avg loss = 24.086395263671875
client 2, data condensation 600, total loss = 61.627899169921875, avg loss = 30.813949584960938
client 2, data condensation 800, total loss = 83.0123291015625, avg loss = 41.50616455078125
client 2, data condensation 1000, total loss = 57.830810546875, avg loss = 28.9154052734375
client 2, data condensation 1200, total loss = 43.365478515625, avg loss = 21.6827392578125
client 2, data condensation 1400, total loss = 49.6180419921875, avg loss = 24.80902099609375
client 2, data condensation 1600, total loss = 85.9476318359375, avg loss = 42.97381591796875
client 2, data condensation 1800, total loss = 42.05377197265625, avg loss = 21.026885986328125
client 2, data condensation 2000, total loss = 60.1285400390625, avg loss = 30.06427001953125
client 2, data condensation 2200, total loss = 40.17462158203125, avg loss = 20.087310791015625
client 2, data condensation 2400, total loss = 57.6944580078125, avg loss = 28.84722900390625
client 2, data condensation 2600, total loss = 34.9444580078125, avg loss = 17.47222900390625
client 2, data condensation 2800, total loss = 41.6090087890625, avg loss = 20.80450439453125
client 2, data condensation 3000, total loss = 285.3026123046875, avg loss = 142.65130615234375
client 2, data condensation 3200, total loss = 33.8297119140625, avg loss = 16.91485595703125
client 2, data condensation 3400, total loss = 198.56143188476562, avg loss = 99.28071594238281
client 2, data condensation 3600, total loss = 35.947998046875, avg loss = 17.9739990234375
client 2, data condensation 3800, total loss = 85.4837646484375, avg loss = 42.74188232421875
client 2, data condensation 4000, total loss = 73.19122314453125, avg loss = 36.595611572265625
client 2, data condensation 4200, total loss = 53.73797607421875, avg loss = 26.868988037109375
client 2, data condensation 4400, total loss = 59.11834716796875, avg loss = 29.559173583984375
client 2, data condensation 4600, total loss = 80.83489990234375, avg loss = 40.417449951171875
client 2, data condensation 4800, total loss = 206.5177001953125, avg loss = 103.25885009765625
client 2, data condensation 5000, total loss = 153.40960693359375, avg loss = 76.70480346679688
Round 3, client 2 condense time: 447.72424578666687
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3219.06MB, free 21356.94MB
total 24576.0MB, used 3219.06MB, free 21356.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [177  17  11   9  11  14  12  12  11  39], bin edged: [0.00270318 0.00287845 0.00305373 0.003229   0.00340428 0.00357955
 0.00375483 0.0039301  0.00410538 0.00428065 0.00445593]
client 3, data condensation 0, total loss = 87.410888671875, avg loss = 87.410888671875
client 3, data condensation 200, total loss = 43.045867919921875, avg loss = 43.045867919921875
client 3, data condensation 400, total loss = 24.1611328125, avg loss = 24.1611328125
client 3, data condensation 600, total loss = 62.938446044921875, avg loss = 62.938446044921875
client 3, data condensation 800, total loss = 29.849578857421875, avg loss = 29.849578857421875
client 3, data condensation 1000, total loss = 25.83612060546875, avg loss = 25.83612060546875
client 3, data condensation 1200, total loss = 27.45965576171875, avg loss = 27.45965576171875
client 3, data condensation 1400, total loss = 38.14556884765625, avg loss = 38.14556884765625
client 3, data condensation 1600, total loss = 24.851959228515625, avg loss = 24.851959228515625
client 3, data condensation 1800, total loss = 20.28387451171875, avg loss = 20.28387451171875
client 3, data condensation 2000, total loss = 99.25970458984375, avg loss = 99.25970458984375
client 3, data condensation 2200, total loss = 30.1416015625, avg loss = 30.1416015625
client 3, data condensation 2400, total loss = 61.080596923828125, avg loss = 61.080596923828125
client 3, data condensation 2600, total loss = 51.92633056640625, avg loss = 51.92633056640625
client 3, data condensation 2800, total loss = 132.81423950195312, avg loss = 132.81423950195312
client 3, data condensation 3000, total loss = 33.360321044921875, avg loss = 33.360321044921875
client 3, data condensation 3200, total loss = 44.9749755859375, avg loss = 44.9749755859375
client 3, data condensation 3400, total loss = 23.42437744140625, avg loss = 23.42437744140625
client 3, data condensation 3600, total loss = 28.958160400390625, avg loss = 28.958160400390625
client 3, data condensation 3800, total loss = 33.645355224609375, avg loss = 33.645355224609375
client 3, data condensation 4000, total loss = 22.77911376953125, avg loss = 22.77911376953125
client 3, data condensation 4200, total loss = 26.007781982421875, avg loss = 26.007781982421875
client 3, data condensation 4400, total loss = 42.59149169921875, avg loss = 42.59149169921875
client 3, data condensation 4600, total loss = 24.701934814453125, avg loss = 24.701934814453125
client 3, data condensation 4800, total loss = 60.296539306640625, avg loss = 60.296539306640625
client 3, data condensation 5000, total loss = 39.733123779296875, avg loss = 39.733123779296875
Round 3, client 3 condense time: 169.32321572303772
client 3, class 6 have 313 samples
total 24576.0MB, used 2831.06MB, free 21744.94MB
total 24576.0MB, used 2831.06MB, free 21744.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [315   4   3   2   0   1   1   1   4  30], bin edged: [0.0025869  0.00275472 0.00292253 0.00309035 0.00325817 0.00342599
 0.0035938  0.00376162 0.00392944 0.00409725 0.00426507]
class 4 have 1048 samples, histogram: [452 111  70  41  43  37  43  43  45 163], bin edged: [0.00078224 0.00083299 0.00088373 0.00093448 0.00098522 0.00103597
 0.00108671 0.00113746 0.00118821 0.00123895 0.0012897 ]
class 6 have 7572 samples, histogram: [4996  517  347  252  211  193  149  178  230  499], bin edged: [0.00011757 0.0001252  0.00013283 0.00014045 0.00014808 0.00015571
 0.00016333 0.00017096 0.00017859 0.00018621 0.00019384]
client 4, data condensation 0, total loss = 361.5146484375, avg loss = 120.5048828125
client 4, data condensation 200, total loss = 45.672576904296875, avg loss = 15.224192301432291
client 4, data condensation 400, total loss = 50.857391357421875, avg loss = 16.952463785807293
client 4, data condensation 600, total loss = 49.672637939453125, avg loss = 16.557545979817707
client 4, data condensation 800, total loss = 104.26223754882812, avg loss = 34.75407918294271
client 4, data condensation 1000, total loss = 38.026153564453125, avg loss = 12.675384521484375
client 4, data condensation 1200, total loss = 170.0472412109375, avg loss = 56.682413736979164
client 4, data condensation 1400, total loss = 226.15191650390625, avg loss = 75.38397216796875
client 4, data condensation 1600, total loss = 246.67587280273438, avg loss = 82.2252909342448
client 4, data condensation 1800, total loss = 45.824462890625, avg loss = 15.274820963541666
client 4, data condensation 2000, total loss = 203.61581420898438, avg loss = 67.87193806966145
client 4, data condensation 2200, total loss = 216.57711791992188, avg loss = 72.19237263997395
client 4, data condensation 2400, total loss = 43.861785888671875, avg loss = 14.620595296223959
client 4, data condensation 2600, total loss = 71.23123168945312, avg loss = 23.743743896484375
client 4, data condensation 2800, total loss = 45.83306884765625, avg loss = 15.277689615885416
client 4, data condensation 3000, total loss = 67.0052490234375, avg loss = 22.3350830078125
client 4, data condensation 3200, total loss = 36.796722412109375, avg loss = 12.265574137369791
client 4, data condensation 3400, total loss = 53.088836669921875, avg loss = 17.696278889973957
client 4, data condensation 3600, total loss = 48.743743896484375, avg loss = 16.247914632161457
client 4, data condensation 3800, total loss = 59.188018798828125, avg loss = 19.729339599609375
client 4, data condensation 4000, total loss = 31.61138916015625, avg loss = 10.537129720052084
client 4, data condensation 4200, total loss = 40.1248779296875, avg loss = 13.374959309895834
client 4, data condensation 4400, total loss = 39.760772705078125, avg loss = 13.253590901692709
client 4, data condensation 4600, total loss = 53.048309326171875, avg loss = 17.682769775390625
client 4, data condensation 4800, total loss = 68.13442993164062, avg loss = 22.711476643880207
client 4, data condensation 5000, total loss = 80.9366455078125, avg loss = 26.9788818359375
Round 3, client 4 condense time: 514.1254813671112
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3475.06MB, free 21100.94MB
total 24576.0MB, used 3475.06MB, free 21100.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [4042  771  495  372  358  343  391  501  677 4201], bin edged: [6.20116265e-05 6.60344526e-05 7.00572787e-05 7.40801049e-05
 7.81029310e-05 8.21257571e-05 8.61485832e-05 9.01714093e-05
 9.41942354e-05 9.82170616e-05 1.02239888e-04]
client 5, data condensation 0, total loss = 164.42425537109375, avg loss = 164.42425537109375
client 5, data condensation 200, total loss = 42.074737548828125, avg loss = 42.074737548828125
client 5, data condensation 400, total loss = 6.93109130859375, avg loss = 6.93109130859375
client 5, data condensation 600, total loss = 18.81329345703125, avg loss = 18.81329345703125
client 5, data condensation 800, total loss = 27.665924072265625, avg loss = 27.665924072265625
client 5, data condensation 1000, total loss = 31.10284423828125, avg loss = 31.10284423828125
client 5, data condensation 1200, total loss = 12.78460693359375, avg loss = 12.78460693359375
client 5, data condensation 1400, total loss = 5.7572021484375, avg loss = 5.7572021484375
client 5, data condensation 1600, total loss = 9.618682861328125, avg loss = 9.618682861328125
client 5, data condensation 1800, total loss = 8.728668212890625, avg loss = 8.728668212890625
client 5, data condensation 2000, total loss = 6.982330322265625, avg loss = 6.982330322265625
client 5, data condensation 2200, total loss = 5.842681884765625, avg loss = 5.842681884765625
client 5, data condensation 2400, total loss = 26.556304931640625, avg loss = 26.556304931640625
client 5, data condensation 2600, total loss = 9.35870361328125, avg loss = 9.35870361328125
client 5, data condensation 2800, total loss = 7.209747314453125, avg loss = 7.209747314453125
client 5, data condensation 3000, total loss = 17.3292236328125, avg loss = 17.3292236328125
client 5, data condensation 3200, total loss = 23.05743408203125, avg loss = 23.05743408203125
client 5, data condensation 3400, total loss = 9.611236572265625, avg loss = 9.611236572265625
client 5, data condensation 3600, total loss = 17.138885498046875, avg loss = 17.138885498046875
client 5, data condensation 3800, total loss = 24.169158935546875, avg loss = 24.169158935546875
client 5, data condensation 4000, total loss = 14.444854736328125, avg loss = 14.444854736328125
client 5, data condensation 4200, total loss = 60.69097900390625, avg loss = 60.69097900390625
client 5, data condensation 4400, total loss = 6.00543212890625, avg loss = 6.00543212890625
client 5, data condensation 4600, total loss = 6.144775390625, avg loss = 6.144775390625
client 5, data condensation 4800, total loss = 5.128387451171875, avg loss = 5.128387451171875
client 5, data condensation 5000, total loss = 6.99676513671875, avg loss = 6.99676513671875
Round 3, client 5 condense time: 262.3251326084137
client 5, class 8 have 12151 samples
total 24576.0MB, used 2837.06MB, free 21738.94MB
total 24576.0MB, used 2837.06MB, free 21738.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [5442  819  462  392  313  261  278  312  392 1523], bin edged: [8.23523757e-05 8.76947381e-05 9.30371006e-05 9.83794630e-05
 1.03721825e-04 1.09064188e-04 1.14406550e-04 1.19748913e-04
 1.25091275e-04 1.30433638e-04 1.35776000e-04]
client 6, data condensation 0, total loss = 205.39529418945312, avg loss = 205.39529418945312
client 6, data condensation 200, total loss = 8.444091796875, avg loss = 8.444091796875
client 6, data condensation 400, total loss = 20.427947998046875, avg loss = 20.427947998046875
client 6, data condensation 600, total loss = 16.57855224609375, avg loss = 16.57855224609375
client 6, data condensation 800, total loss = 15.56768798828125, avg loss = 15.56768798828125
client 6, data condensation 1000, total loss = 11.573944091796875, avg loss = 11.573944091796875
client 6, data condensation 1200, total loss = 9.882720947265625, avg loss = 9.882720947265625
client 6, data condensation 1400, total loss = 8.88818359375, avg loss = 8.88818359375
client 6, data condensation 1600, total loss = 21.714324951171875, avg loss = 21.714324951171875
client 6, data condensation 1800, total loss = 26.340118408203125, avg loss = 26.340118408203125
client 6, data condensation 2000, total loss = 10.894775390625, avg loss = 10.894775390625
client 6, data condensation 2200, total loss = 511.05474853515625, avg loss = 511.05474853515625
client 6, data condensation 2400, total loss = 17.442291259765625, avg loss = 17.442291259765625
client 6, data condensation 2600, total loss = 14.2047119140625, avg loss = 14.2047119140625
client 6, data condensation 2800, total loss = 31.721527099609375, avg loss = 31.721527099609375
client 6, data condensation 3000, total loss = 6.88287353515625, avg loss = 6.88287353515625
client 6, data condensation 3200, total loss = 12.1817626953125, avg loss = 12.1817626953125
client 6, data condensation 3400, total loss = 10.328887939453125, avg loss = 10.328887939453125
client 6, data condensation 3600, total loss = 8.340545654296875, avg loss = 8.340545654296875
client 6, data condensation 3800, total loss = 14.3604736328125, avg loss = 14.3604736328125
client 6, data condensation 4000, total loss = 15.185211181640625, avg loss = 15.185211181640625
client 6, data condensation 4200, total loss = 14.484222412109375, avg loss = 14.484222412109375
client 6, data condensation 4400, total loss = 7.371337890625, avg loss = 7.371337890625
client 6, data condensation 4600, total loss = 161.36639404296875, avg loss = 161.36639404296875
client 6, data condensation 4800, total loss = 16.258514404296875, avg loss = 16.258514404296875
client 6, data condensation 5000, total loss = 7.4794921875, avg loss = 7.4794921875
Round 3, client 6 condense time: 235.8002471923828
client 6, class 3 have 10194 samples
total 24576.0MB, used 2839.06MB, free 21736.94MB
total 24576.0MB, used 2839.06MB, free 21736.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [6916  343  132  100  104   80   98   88  109 1142], bin edged: [9.78637545e-05 1.04212079e-04 1.10560404e-04 1.16908728e-04
 1.23257053e-04 1.29605377e-04 1.35953702e-04 1.42302027e-04
 1.48650351e-04 1.54998676e-04 1.61347000e-04]
client 7, data condensation 0, total loss = 192.8385009765625, avg loss = 192.8385009765625
client 7, data condensation 200, total loss = 6.103515625, avg loss = 6.103515625
client 7, data condensation 400, total loss = 7.49615478515625, avg loss = 7.49615478515625
client 7, data condensation 600, total loss = 365.7013854980469, avg loss = 365.7013854980469
client 7, data condensation 800, total loss = 4.279815673828125, avg loss = 4.279815673828125
client 7, data condensation 1000, total loss = 200.1539306640625, avg loss = 200.1539306640625
client 7, data condensation 1200, total loss = 176.95977783203125, avg loss = 176.95977783203125
client 7, data condensation 1400, total loss = 9.959625244140625, avg loss = 9.959625244140625
client 7, data condensation 1600, total loss = 6.855010986328125, avg loss = 6.855010986328125
client 7, data condensation 1800, total loss = 13.29339599609375, avg loss = 13.29339599609375
client 7, data condensation 2000, total loss = 2.390106201171875, avg loss = 2.390106201171875
client 7, data condensation 2200, total loss = 6.97137451171875, avg loss = 6.97137451171875
client 7, data condensation 2400, total loss = 8.702972412109375, avg loss = 8.702972412109375
client 7, data condensation 2600, total loss = 19.122772216796875, avg loss = 19.122772216796875
client 7, data condensation 2800, total loss = 6.258453369140625, avg loss = 6.258453369140625
client 7, data condensation 3000, total loss = 6.06170654296875, avg loss = 6.06170654296875
client 7, data condensation 3200, total loss = 8.461395263671875, avg loss = 8.461395263671875
client 7, data condensation 3400, total loss = 31.16375732421875, avg loss = 31.16375732421875
client 7, data condensation 3600, total loss = 2.661712646484375, avg loss = 2.661712646484375
client 7, data condensation 3800, total loss = 4.86083984375, avg loss = 4.86083984375
client 7, data condensation 4000, total loss = 10.5008544921875, avg loss = 10.5008544921875
client 7, data condensation 4200, total loss = 14.522613525390625, avg loss = 14.522613525390625
client 7, data condensation 4400, total loss = 19.2725830078125, avg loss = 19.2725830078125
client 7, data condensation 4600, total loss = 19.246673583984375, avg loss = 19.246673583984375
client 7, data condensation 4800, total loss = 5.520263671875, avg loss = 5.520263671875
client 7, data condensation 5000, total loss = 6.673553466796875, avg loss = 6.673553466796875
Round 3, client 7 condense time: 245.56897282600403
client 7, class 1 have 9112 samples
total 24576.0MB, used 2837.06MB, free 21738.94MB
total 24576.0MB, used 2837.06MB, free 21738.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [170  12   3   4   0   1   5   1   3   7], bin edged: [0.00457844 0.00487545 0.00517246 0.00546947 0.00576648 0.00606349
 0.0063605  0.00665752 0.00695453 0.00725154 0.00754855]
class 4 have 1179 samples, histogram: [654  85  55  36  36  38  35  50  44 146], bin edged: [0.00071858 0.00076519 0.00081181 0.00085842 0.00090504 0.00095166
 0.00099827 0.00104489 0.0010915  0.00113812 0.00118473]
class 8 have 734 samples, histogram: [418  44  32  21  25  14  21  28  23 108], bin edged: [0.00115144 0.00122613 0.00130083 0.00137552 0.00145022 0.00152492
 0.00159961 0.00167431 0.001749   0.0018237  0.0018984 ]
client 8, data condensation 0, total loss = 316.15972900390625, avg loss = 105.38657633463542
client 8, data condensation 200, total loss = 85.03427124023438, avg loss = 28.344757080078125
client 8, data condensation 400, total loss = 38.23046875, avg loss = 12.743489583333334
client 8, data condensation 600, total loss = 168.55551147460938, avg loss = 56.18517049153646
client 8, data condensation 800, total loss = 157.4046630859375, avg loss = 52.468221028645836
client 8, data condensation 1000, total loss = 39.726318359375, avg loss = 13.242106119791666
client 8, data condensation 1200, total loss = 50.330810546875, avg loss = 16.776936848958332
client 8, data condensation 1400, total loss = 57.41357421875, avg loss = 19.137858072916668
client 8, data condensation 1600, total loss = 36.510162353515625, avg loss = 12.170054117838541
client 8, data condensation 1800, total loss = 141.2708740234375, avg loss = 47.090291341145836
client 8, data condensation 2000, total loss = 113.18984985351562, avg loss = 37.729949951171875
client 8, data condensation 2200, total loss = 47.992950439453125, avg loss = 15.997650146484375
client 8, data condensation 2400, total loss = 73.95736694335938, avg loss = 24.652455647786457
client 8, data condensation 2600, total loss = 60.360076904296875, avg loss = 20.120025634765625
client 8, data condensation 2800, total loss = 70.052734375, avg loss = 23.350911458333332
client 8, data condensation 3000, total loss = 136.09375, avg loss = 45.364583333333336
client 8, data condensation 3200, total loss = 38.6253662109375, avg loss = 12.8751220703125
client 8, data condensation 3400, total loss = 53.435791015625, avg loss = 17.811930338541668
client 8, data condensation 3600, total loss = 145.31240844726562, avg loss = 48.437469482421875
client 8, data condensation 3800, total loss = 36.852142333984375, avg loss = 12.284047444661459
client 8, data condensation 4000, total loss = 95.06451416015625, avg loss = 31.68817138671875
client 8, data condensation 4200, total loss = 29.473846435546875, avg loss = 9.824615478515625
client 8, data condensation 4400, total loss = 40.413604736328125, avg loss = 13.471201578776041
client 8, data condensation 4600, total loss = 43.937744140625, avg loss = 14.645914713541666
client 8, data condensation 4800, total loss = 82.6697998046875, avg loss = 27.556599934895832
client 8, data condensation 5000, total loss = 53.65350341796875, avg loss = 17.884501139322918
Round 3, client 8 condense time: 376.97965264320374
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3477.06MB, free 21098.94MB
total 24576.0MB, used 3477.06MB, free 21098.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [ 540  337  322  390  447  476  649  790 1150 5215], bin edged: [6.48985464e-05 6.91086531e-05 7.33187598e-05 7.75288666e-05
 8.17389733e-05 8.59490801e-05 9.01591868e-05 9.43692936e-05
 9.85794003e-05 1.02789507e-04 1.06999614e-04]
client 9, data condensation 0, total loss = 262.58624267578125, avg loss = 262.58624267578125
client 9, data condensation 200, total loss = 9.891815185546875, avg loss = 9.891815185546875
client 9, data condensation 400, total loss = 16.68603515625, avg loss = 16.68603515625
client 9, data condensation 600, total loss = 8.885772705078125, avg loss = 8.885772705078125
client 9, data condensation 800, total loss = 4.458892822265625, avg loss = 4.458892822265625
client 9, data condensation 1000, total loss = 18.126312255859375, avg loss = 18.126312255859375
client 9, data condensation 1200, total loss = 11.2064208984375, avg loss = 11.2064208984375
client 9, data condensation 1400, total loss = 341.4378662109375, avg loss = 341.4378662109375
client 9, data condensation 1600, total loss = 13.556793212890625, avg loss = 13.556793212890625
client 9, data condensation 1800, total loss = 3.98150634765625, avg loss = 3.98150634765625
client 9, data condensation 2000, total loss = 11.9970703125, avg loss = 11.9970703125
client 9, data condensation 2200, total loss = 3.439666748046875, avg loss = 3.439666748046875
client 9, data condensation 2400, total loss = 156.490234375, avg loss = 156.490234375
client 9, data condensation 2600, total loss = 6.056060791015625, avg loss = 6.056060791015625
client 9, data condensation 2800, total loss = 167.96426391601562, avg loss = 167.96426391601562
client 9, data condensation 3000, total loss = 9.568328857421875, avg loss = 9.568328857421875
client 9, data condensation 3200, total loss = 33.60107421875, avg loss = 33.60107421875
client 9, data condensation 3400, total loss = 3.36199951171875, avg loss = 3.36199951171875
client 9, data condensation 3600, total loss = 11.35235595703125, avg loss = 11.35235595703125
client 9, data condensation 3800, total loss = 12.191192626953125, avg loss = 12.191192626953125
client 9, data condensation 4000, total loss = 21.539337158203125, avg loss = 21.539337158203125
client 9, data condensation 4200, total loss = 12.27947998046875, avg loss = 12.27947998046875
client 9, data condensation 4400, total loss = 27.056610107421875, avg loss = 27.056610107421875
client 9, data condensation 4600, total loss = 5.5577392578125, avg loss = 5.5577392578125
client 9, data condensation 4800, total loss = 84.69158935546875, avg loss = 84.69158935546875
client 9, data condensation 5000, total loss = 8.0460205078125, avg loss = 8.0460205078125
Round 3, client 9 condense time: 238.1573610305786
client 9, class 2 have 10316 samples
total 24576.0MB, used 2841.06MB, free 21734.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 1.6853e+01,  2.6613e+00, -6.3775e+00, -1.2992e+01,  8.8291e+00,
          4.3707e+00,  2.5844e+00, -5.2737e+00, -1.0782e+01],
        [ 2.1425e+00,  1.2783e+01, -3.2972e+00, -5.3520e+00, -2.6480e-01,
          2.8975e+00, -2.0588e+00, -1.4235e+00, -5.1082e+00],
        [-8.2133e+00, -6.3714e+00,  7.5434e+00, -3.2589e+00,  1.3369e+00,
          4.3350e+00,  1.7527e+00,  2.6623e+00,  8.8418e-01],
        [-1.0426e+01, -7.4035e+00,  6.0757e-01,  1.3681e+01,  1.7231e-02,
         -2.8562e+00,  8.4921e+00,  2.1926e+00, -3.4644e+00],
        [ 2.8971e-03, -3.8221e+00, -4.5542e+00, -5.8485e+00,  1.0501e+01,
          1.8382e+00,  8.2528e+00, -3.3656e-01, -6.1058e+00],
        [-5.1236e+00, -4.6397e+00,  3.1129e+00, -7.7379e+00,  1.9781e+00,
          9.1972e+00,  1.7491e+00,  2.3888e+00, -5.2300e-01],
        [-4.6964e+00, -6.7894e+00, -8.8167e-01, -1.7873e+00,  3.7632e+00,
          9.3116e-01,  1.2189e+01, -2.0614e+00, -3.6039e-01],
        [-8.4867e+00, -6.9043e+00,  1.3204e+00, -8.7372e-01,  3.9909e+00,
          2.3571e+00,  2.7079e+00,  6.3266e+00,  4.7393e-02],
        [-8.7260e+00, -6.2647e+00,  1.5744e+00, -3.0013e+00,  1.7330e+00,
          1.6412e+00,  6.5905e+00, -1.3197e+00,  7.8672e+00]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 0, 4, 7],
        [2, 5, 7, 6, 4],
        [3, 6, 7, 2, 4],
        [4, 6, 5, 0, 7],
        [5, 2, 7, 4, 6],
        [6, 4, 5, 8, 2],
        [7, 4, 6, 5, 2],
        [8, 6, 4, 5, 2]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
4
Round 3: # synthetic sample: 3648
total 24576.0MB, used 2841.06MB, free 21734.94MB
{0: {0: 1319, 1: 5, 2: 0, 3: 0, 4: 5, 5: 7, 6: 2, 7: 0, 8: 0}, 1: {0: 1, 1: 591, 2: 255, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 261, 3: 5, 4: 0, 5: 15, 6: 4, 7: 49, 8: 4}, 3: {0: 2, 1: 0, 2: 0, 3: 522, 4: 1, 5: 2, 6: 105, 7: 0, 8: 2}, 4: {0: 48, 1: 165, 2: 0, 3: 0, 4: 705, 5: 41, 6: 75, 7: 1, 8: 0}, 5: {0: 55, 1: 5, 2: 147, 3: 61, 4: 0, 5: 164, 6: 30, 7: 93, 8: 37}, 6: {0: 21, 1: 0, 2: 0, 3: 0, 4: 28, 5: 15, 6: 673, 7: 0, 8: 4}, 7: {0: 5, 1: 18, 2: 63, 3: 37, 4: 65, 5: 33, 6: 64, 7: 111, 8: 25}, 8: {0: 20, 1: 1, 2: 65, 3: 27, 4: 9, 5: 118, 6: 308, 7: 15, 8: 670}}
round 3 evaluation: test acc is 0.6986, test loss = 1.666406
{0: {0: 1153, 1: 5, 2: 0, 3: 0, 4: 12, 5: 94, 6: 74, 7: 0, 8: 0}, 1: {0: 0, 1: 847, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 219, 3: 5, 4: 0, 5: 89, 6: 1, 7: 24, 8: 1}, 3: {0: 0, 1: 0, 2: 3, 3: 577, 4: 0, 5: 0, 6: 53, 7: 1, 8: 0}, 4: {0: 17, 1: 186, 2: 0, 3: 27, 4: 435, 5: 85, 6: 228, 7: 57, 8: 0}, 5: {0: 55, 1: 9, 2: 84, 3: 50, 4: 0, 5: 272, 6: 32, 7: 63, 8: 27}, 6: {0: 2, 1: 0, 2: 12, 3: 4, 4: 7, 5: 6, 6: 710, 7: 0, 8: 0}, 7: {0: 0, 1: 22, 2: 39, 3: 41, 4: 0, 5: 96, 6: 67, 7: 129, 8: 27}, 8: {0: 5, 1: 4, 2: 100, 3: 38, 4: 0, 5: 94, 6: 357, 7: 6, 8: 629}}
epoch 0, train loss avg now = 0.515844, train contrast loss now = 1.350028, test acc now = 0.6923, test loss now = 1.541166
{0: {0: 1313, 1: 5, 2: 0, 3: 0, 4: 5, 5: 2, 6: 13, 7: 0, 8: 0}, 1: {0: 11, 1: 820, 2: 14, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 159, 3: 12, 4: 0, 5: 45, 6: 1, 7: 121, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 577, 4: 0, 5: 0, 6: 57, 7: 0, 8: 0}, 4: {0: 58, 1: 226, 2: 0, 3: 2, 4: 566, 5: 26, 6: 151, 7: 6, 8: 0}, 5: {0: 0, 1: 48, 2: 24, 3: 71, 4: 0, 5: 142, 6: 68, 7: 237, 8: 2}, 6: {0: 13, 1: 0, 2: 0, 3: 4, 4: 5, 5: 6, 6: 713, 7: 0, 8: 0}, 7: {0: 0, 1: 12, 2: 7, 3: 66, 4: 19, 5: 45, 6: 93, 7: 173, 8: 6}, 8: {0: 15, 1: 7, 2: 5, 3: 39, 4: 3, 5: 99, 6: 758, 7: 104, 8: 203}}
epoch 100, train loss avg now = 0.079553, train contrast loss now = 1.152516, test acc now = 0.6499, test loss now = 1.881519
{0: {0: 1279, 1: 5, 2: 0, 3: 0, 4: 19, 5: 23, 6: 12, 7: 0, 8: 0}, 1: {0: 26, 1: 778, 2: 43, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 256, 3: 3, 4: 0, 5: 14, 6: 0, 7: 66, 8: 0}, 3: {0: 0, 1: 0, 2: 7, 3: 593, 4: 1, 5: 1, 6: 26, 7: 5, 8: 1}, 4: {0: 31, 1: 68, 2: 0, 3: 5, 4: 799, 5: 22, 6: 85, 7: 25, 8: 0}, 5: {0: 44, 1: 7, 2: 151, 3: 38, 4: 1, 5: 124, 6: 6, 7: 216, 8: 5}, 6: {0: 7, 1: 0, 2: 42, 3: 13, 4: 34, 5: 42, 6: 580, 7: 22, 8: 1}, 7: {0: 3, 1: 6, 2: 60, 3: 50, 4: 5, 5: 35, 6: 21, 7: 235, 8: 6}, 8: {0: 9, 1: 0, 2: 233, 3: 43, 4: 8, 5: 162, 6: 144, 7: 157, 8: 477}}
epoch 200, train loss avg now = 0.120085, train contrast loss now = 1.149337, test acc now = 0.7132, test loss now = 1.368494
{0: {0: 1304, 1: 2, 2: 0, 3: 0, 4: 12, 5: 6, 6: 14, 7: 0, 8: 0}, 1: {0: 35, 1: 578, 2: 231, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 253, 3: 24, 4: 0, 5: 25, 6: 0, 7: 36, 8: 0}, 3: {0: 0, 1: 0, 2: 1, 3: 591, 4: 1, 5: 2, 6: 39, 7: 0, 8: 0}, 4: {0: 46, 1: 250, 2: 0, 3: 12, 4: 613, 5: 12, 6: 102, 7: 0, 8: 0}, 5: {0: 51, 1: 1, 2: 133, 3: 82, 4: 1, 5: 162, 6: 32, 7: 129, 8: 1}, 6: {0: 11, 1: 0, 2: 9, 3: 9, 4: 27, 5: 10, 6: 675, 7: 0, 8: 0}, 7: {0: 4, 1: 23, 2: 63, 3: 133, 4: 12, 5: 42, 6: 69, 7: 70, 8: 5}, 8: {0: 14, 1: 2, 2: 121, 3: 91, 4: 13, 5: 176, 6: 536, 7: 46, 8: 234}}
epoch 300, train loss avg now = 0.061879, train contrast loss now = 1.146140, test acc now = 0.6240, test loss now = 1.929858
{0: {0: 1315, 1: 7, 2: 0, 3: 0, 4: 8, 5: 3, 6: 5, 7: 0, 8: 0}, 1: {0: 10, 1: 835, 2: 1, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 190, 3: 6, 4: 0, 5: 79, 6: 0, 7: 60, 8: 4}, 3: {0: 0, 1: 0, 2: 0, 3: 598, 4: 0, 5: 1, 6: 30, 7: 0, 8: 5}, 4: {0: 40, 1: 160, 2: 0, 3: 11, 4: 744, 5: 18, 6: 40, 7: 22, 8: 0}, 5: {0: 19, 1: 35, 2: 115, 3: 46, 4: 1, 5: 143, 6: 10, 7: 198, 8: 25}, 6: {0: 16, 1: 0, 2: 18, 3: 15, 4: 20, 5: 14, 6: 646, 7: 3, 8: 9}, 7: {0: 0, 1: 20, 2: 32, 3: 89, 4: 6, 5: 40, 6: 29, 7: 166, 8: 39}, 8: {0: 17, 1: 4, 2: 30, 3: 37, 4: 5, 5: 91, 6: 151, 7: 69, 8: 829}}
epoch 400, train loss avg now = 0.055273, train contrast loss now = 1.145015, test acc now = 0.7613, test loss now = 1.129582
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1275, 1: 5, 2: 0, 3: 0, 4: 50, 5: 6, 6: 2, 7: 0, 8: 0}, 1: {0: 107, 1: 738, 2: 0, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 245, 3: 6, 4: 0, 5: 11, 6: 1, 7: 75, 8: 0}, 3: {0: 0, 1: 0, 2: 1, 3: 569, 4: 4, 5: 0, 6: 57, 7: 1, 8: 2}, 4: {0: 26, 1: 121, 2: 0, 3: 1, 4: 837, 5: 4, 6: 44, 7: 2, 8: 0}, 5: {0: 36, 1: 17, 2: 105, 3: 49, 4: 2, 5: 152, 6: 14, 7: 209, 8: 8}, 6: {0: 9, 1: 0, 2: 18, 3: 5, 4: 72, 5: 22, 6: 610, 7: 1, 8: 4}, 7: {0: 7, 1: 25, 2: 30, 3: 56, 4: 22, 5: 30, 6: 53, 7: 176, 8: 22}, 8: {0: 15, 1: 2, 2: 88, 3: 36, 4: 33, 5: 127, 6: 238, 7: 47, 8: 647}}
epoch 500, train loss avg now = 0.067867, train contrast loss now = 1.144505, test acc now = 0.7311, test loss now = 1.379795
{0: {0: 1299, 1: 5, 2: 0, 3: 0, 4: 25, 5: 8, 6: 1, 7: 0, 8: 0}, 1: {0: 47, 1: 789, 2: 8, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 206, 3: 2, 4: 0, 5: 32, 6: 0, 7: 95, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 574, 4: 1, 5: 2, 6: 55, 7: 0, 8: 2}, 4: {0: 34, 1: 191, 2: 0, 3: 1, 4: 747, 5: 22, 6: 38, 7: 2, 8: 0}, 5: {0: 0, 1: 55, 2: 79, 3: 23, 4: 2, 5: 159, 6: 13, 7: 256, 8: 5}, 6: {0: 10, 1: 0, 2: 5, 3: 5, 4: 41, 5: 19, 6: 658, 7: 1, 8: 2}, 7: {0: 0, 1: 26, 2: 28, 3: 35, 4: 42, 5: 42, 6: 51, 7: 170, 8: 27}, 8: {0: 15, 1: 2, 2: 43, 3: 33, 4: 17, 5: 140, 6: 262, 7: 64, 8: 657}}
epoch 600, train loss avg now = 0.029689, train contrast loss now = 1.143573, test acc now = 0.7325, test loss now = 1.374511
{0: {0: 1314, 1: 5, 2: 0, 3: 0, 4: 12, 5: 6, 6: 1, 7: 0, 8: 0}, 1: {0: 26, 1: 801, 2: 19, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 5, 2: 236, 3: 4, 4: 0, 5: 13, 6: 0, 7: 79, 8: 2}, 3: {0: 1, 1: 0, 2: 0, 3: 566, 4: 1, 5: 2, 6: 61, 7: 1, 8: 2}, 4: {0: 42, 1: 229, 2: 0, 3: 1, 4: 718, 5: 14, 6: 29, 7: 2, 8: 0}, 5: {0: 0, 1: 63, 2: 91, 3: 33, 4: 2, 5: 151, 6: 15, 7: 230, 8: 7}, 6: {0: 14, 1: 0, 2: 5, 3: 5, 4: 39, 5: 20, 6: 657, 7: 0, 8: 1}, 7: {0: 1, 1: 37, 2: 29, 3: 35, 4: 42, 5: 28, 6: 49, 7: 171, 8: 29}, 8: {0: 20, 1: 2, 2: 38, 3: 34, 4: 17, 5: 151, 6: 297, 7: 59, 8: 615}}
epoch 700, train loss avg now = 0.026950, train contrast loss now = 1.145519, test acc now = 0.7283, test loss now = 1.491671
{0: {0: 1314, 1: 5, 2: 0, 3: 0, 4: 11, 5: 6, 6: 2, 7: 0, 8: 0}, 1: {0: 32, 1: 807, 2: 8, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 6, 2: 214, 3: 7, 4: 0, 5: 26, 6: 2, 7: 80, 8: 4}, 3: {0: 0, 1: 0, 2: 0, 3: 573, 4: 1, 5: 2, 6: 56, 7: 0, 8: 2}, 4: {0: 47, 1: 181, 2: 0, 3: 2, 4: 744, 5: 15, 6: 44, 7: 2, 8: 0}, 5: {0: 0, 1: 63, 2: 75, 3: 38, 4: 2, 5: 156, 6: 20, 7: 227, 8: 11}, 6: {0: 15, 1: 0, 2: 4, 3: 5, 4: 34, 5: 18, 6: 663, 7: 0, 8: 2}, 7: {0: 0, 1: 33, 2: 26, 3: 53, 4: 37, 5: 29, 6: 53, 7: 156, 8: 34}, 8: {0: 18, 1: 1, 2: 28, 3: 33, 4: 13, 5: 139, 6: 292, 7: 56, 8: 653}}
epoch 800, train loss avg now = 0.035499, train contrast loss now = 1.142374, test acc now = 0.7354, test loss now = 1.416803
{0: {0: 1295, 1: 5, 2: 0, 3: 0, 4: 23, 5: 5, 6: 10, 7: 0, 8: 0}, 1: {0: 18, 1: 558, 2: 270, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 6, 2: 219, 3: 4, 4: 0, 5: 7, 6: 0, 7: 102, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 576, 4: 1, 5: 2, 6: 53, 7: 0, 8: 2}, 4: {0: 31, 1: 210, 2: 0, 3: 2, 4: 732, 5: 10, 6: 46, 7: 4, 8: 0}, 5: {0: 0, 1: 61, 2: 63, 3: 33, 4: 2, 5: 144, 6: 17, 7: 264, 8: 8}, 6: {0: 5, 1: 0, 2: 4, 3: 5, 4: 37, 5: 12, 6: 675, 7: 1, 8: 2}, 7: {0: 0, 1: 31, 2: 18, 3: 49, 4: 32, 5: 29, 6: 52, 7: 185, 8: 25}, 8: {0: 15, 1: 1, 2: 27, 3: 39, 4: 16, 5: 130, 6: 322, 7: 84, 8: 599}}
epoch 900, train loss avg now = 0.020922, train contrast loss now = 1.146922, test acc now = 0.6940, test loss now = 1.516687
{0: {0: 1312, 1: 5, 2: 0, 3: 0, 4: 12, 5: 6, 6: 3, 7: 0, 8: 0}, 1: {0: 20, 1: 600, 2: 224, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 5, 2: 219, 3: 7, 4: 0, 5: 11, 6: 0, 7: 96, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 580, 4: 1, 5: 2, 6: 49, 7: 0, 8: 2}, 4: {0: 38, 1: 174, 2: 0, 3: 2, 4: 750, 5: 23, 6: 44, 7: 4, 8: 0}, 5: {0: 0, 1: 62, 2: 72, 3: 49, 4: 2, 5: 131, 6: 18, 7: 250, 8: 8}, 6: {0: 10, 1: 0, 2: 5, 3: 7, 4: 34, 5: 14, 6: 669, 7: 0, 8: 2}, 7: {0: 0, 1: 29, 2: 28, 3: 61, 4: 30, 5: 33, 6: 52, 7: 166, 8: 22}, 8: {0: 17, 1: 2, 2: 35, 3: 46, 4: 12, 5: 132, 6: 297, 7: 69, 8: 623}}
epoch 1000, train loss avg now = 0.017770, train contrast loss now = 1.143703, test acc now = 0.7033, test loss now = 1.496328
epoch avg loss = 1.7769871900479e-05, total time = 5282.509140729904
total 24576.0MB, used 3553.06MB, free 21022.94MB
Round 3 finish, update the prev_syn_proto
torch.Size([384, 3, 28, 28])
torch.Size([388, 3, 28, 28])
torch.Size([416, 3, 28, 28])
torch.Size([428, 3, 28, 28])
torch.Size([324, 3, 28, 28])
torch.Size([488, 3, 28, 28])
torch.Size([324, 3, 28, 28])
torch.Size([376, 3, 28, 28])
torch.Size([520, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1312, 1: 5, 2: 0, 3: 0, 4: 12, 5: 6, 6: 3, 7: 0, 8: 0}, 1: {0: 20, 1: 600, 2: 224, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 5, 2: 219, 3: 7, 4: 0, 5: 11, 6: 0, 7: 96, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 580, 4: 1, 5: 2, 6: 49, 7: 0, 8: 2}, 4: {0: 38, 1: 174, 2: 0, 3: 2, 4: 750, 5: 23, 6: 44, 7: 4, 8: 0}, 5: {0: 0, 1: 62, 2: 72, 3: 49, 4: 2, 5: 131, 6: 18, 7: 250, 8: 8}, 6: {0: 10, 1: 0, 2: 5, 3: 7, 4: 34, 5: 14, 6: 669, 7: 0, 8: 2}, 7: {0: 0, 1: 29, 2: 28, 3: 61, 4: 30, 5: 33, 6: 52, 7: 166, 8: 22}, 8: {0: 17, 1: 2, 2: 35, 3: 46, 4: 12, 5: 132, 6: 297, 7: 69, 8: 623}}
round 3 evaluation: test acc is 0.7033, test loss = 1.496328
 ====== round 4 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3553.06MB, free 21022.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [2671  442  267  242  169  163  190  234  288 1111], bin edged: [0.00014051 0.00014962 0.00015874 0.00016785 0.00017697 0.00018608
 0.0001952  0.00020431 0.00021343 0.00022254 0.00023166]
class 7 have 9330 samples, histogram: [3590  663  451  356  300  309  310  379  528 2444], bin edged: [8.36098101e-05 8.90336960e-05 9.44575818e-05 9.98814677e-05
 1.05305354e-04 1.10729239e-04 1.16153125e-04 1.21577011e-04
 1.27000897e-04 1.32424783e-04 1.37848669e-04]
client 0, data condensation 0, total loss = 137.90338134765625, avg loss = 68.95169067382812
client 0, data condensation 200, total loss = 12.912384033203125, avg loss = 6.4561920166015625
client 0, data condensation 400, total loss = 12.49017333984375, avg loss = 6.245086669921875
client 0, data condensation 600, total loss = 16.34326171875, avg loss = 8.171630859375
client 0, data condensation 800, total loss = 29.895599365234375, avg loss = 14.947799682617188
client 0, data condensation 1000, total loss = 111.90777587890625, avg loss = 55.953887939453125
client 0, data condensation 1200, total loss = 12.665435791015625, avg loss = 6.3327178955078125
client 0, data condensation 1400, total loss = 7.831695556640625, avg loss = 3.9158477783203125
client 0, data condensation 1600, total loss = 63.606689453125, avg loss = 31.8033447265625
client 0, data condensation 1800, total loss = 23.0765380859375, avg loss = 11.53826904296875
client 0, data condensation 2000, total loss = 12.548858642578125, avg loss = 6.2744293212890625
client 0, data condensation 2200, total loss = 11.82330322265625, avg loss = 5.911651611328125
client 0, data condensation 2400, total loss = 8.210784912109375, avg loss = 4.1053924560546875
client 0, data condensation 2600, total loss = 25.87017822265625, avg loss = 12.935089111328125
client 0, data condensation 2800, total loss = 175.0020751953125, avg loss = 87.50103759765625
client 0, data condensation 3000, total loss = 70.55291748046875, avg loss = 35.276458740234375
client 0, data condensation 3200, total loss = 157.2154541015625, avg loss = 78.60772705078125
client 0, data condensation 3400, total loss = 23.117889404296875, avg loss = 11.558944702148438
client 0, data condensation 3600, total loss = 20.825042724609375, avg loss = 10.412521362304688
client 0, data condensation 3800, total loss = 16.30029296875, avg loss = 8.150146484375
client 0, data condensation 4000, total loss = 23.391387939453125, avg loss = 11.695693969726562
client 0, data condensation 4200, total loss = 43.300750732421875, avg loss = 21.650375366210938
client 0, data condensation 4400, total loss = 18.7919921875, avg loss = 9.39599609375
client 0, data condensation 4600, total loss = 16.680389404296875, avg loss = 8.340194702148438
client 0, data condensation 4800, total loss = 32.055328369140625, avg loss = 16.027664184570312
client 0, data condensation 5000, total loss = 19.350494384765625, avg loss = 9.675247192382812
Round 4, client 0 condense time: 432.0505793094635
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3195.06MB, free 21380.94MB
total 24576.0MB, used 3195.06MB, free 21380.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [6044  525  267  202  191  190  208  198  235  962], bin edged: [9.74593243e-05 1.03781718e-04 1.10104112e-04 1.16426505e-04
 1.22748899e-04 1.29071292e-04 1.35393686e-04 1.41716080e-04
 1.48038473e-04 1.54360867e-04 1.60683261e-04]
client 1, data condensation 0, total loss = 313.1352844238281, avg loss = 313.1352844238281
client 1, data condensation 200, total loss = 8.39166259765625, avg loss = 8.39166259765625
client 1, data condensation 400, total loss = 13.38153076171875, avg loss = 13.38153076171875
client 1, data condensation 600, total loss = 12.65106201171875, avg loss = 12.65106201171875
client 1, data condensation 800, total loss = 9.8890380859375, avg loss = 9.8890380859375
client 1, data condensation 1000, total loss = 9.52783203125, avg loss = 9.52783203125
client 1, data condensation 1200, total loss = 46.59417724609375, avg loss = 46.59417724609375
client 1, data condensation 1400, total loss = 8.7926025390625, avg loss = 8.7926025390625
client 1, data condensation 1600, total loss = 3.769775390625, avg loss = 3.769775390625
client 1, data condensation 1800, total loss = 7.45355224609375, avg loss = 7.45355224609375
client 1, data condensation 2000, total loss = 54.752288818359375, avg loss = 54.752288818359375
client 1, data condensation 2200, total loss = 19.5615234375, avg loss = 19.5615234375
client 1, data condensation 2400, total loss = 13.3782958984375, avg loss = 13.3782958984375
client 1, data condensation 2600, total loss = 12.02972412109375, avg loss = 12.02972412109375
client 1, data condensation 2800, total loss = 20.57159423828125, avg loss = 20.57159423828125
client 1, data condensation 3000, total loss = 20.4345703125, avg loss = 20.4345703125
client 1, data condensation 3200, total loss = 110.6136474609375, avg loss = 110.6136474609375
client 1, data condensation 3400, total loss = 25.47711181640625, avg loss = 25.47711181640625
client 1, data condensation 3600, total loss = 7.7572021484375, avg loss = 7.7572021484375
client 1, data condensation 3800, total loss = 20.7159423828125, avg loss = 20.7159423828125
client 1, data condensation 4000, total loss = 42.01251220703125, avg loss = 42.01251220703125
client 1, data condensation 4200, total loss = 8.576263427734375, avg loss = 8.576263427734375
client 1, data condensation 4400, total loss = 13.2503662109375, avg loss = 13.2503662109375
client 1, data condensation 4600, total loss = 15.968505859375, avg loss = 15.968505859375
client 1, data condensation 4800, total loss = 408.7547607421875, avg loss = 408.7547607421875
client 1, data condensation 5000, total loss = 21.33013916015625, avg loss = 21.33013916015625
Round 4, client 1 condense time: 249.72288966178894
client 1, class 0 have 9022 samples
total 24576.0MB, used 2813.06MB, free 21762.94MB
total 24576.0MB, used 2813.06MB, free 21762.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [247  10  12   5   6   3   4   3  11  26], bin edged: [0.00277268 0.00295255 0.00313241 0.00331228 0.00349215 0.00367202
 0.00385188 0.00403175 0.00421162 0.00439149 0.00457136]
class 5 have 12176 samples, histogram: [3378  729  520  380  376  359  365  462  580 5027], bin edged: [6.01708502e-05 6.40742613e-05 6.79776723e-05 7.18810834e-05
 7.57844944e-05 7.96879055e-05 8.35913165e-05 8.74947275e-05
 9.13981386e-05 9.53015496e-05 9.92049607e-05]
client 2, data condensation 0, total loss = 253.44976806640625, avg loss = 126.72488403320312
client 2, data condensation 200, total loss = 233.6597900390625, avg loss = 116.82989501953125
client 2, data condensation 400, total loss = 128.00848388671875, avg loss = 64.00424194335938
client 2, data condensation 600, total loss = 43.091796875, avg loss = 21.5458984375
client 2, data condensation 800, total loss = 38.32379150390625, avg loss = 19.161895751953125
client 2, data condensation 1000, total loss = 531.5547485351562, avg loss = 265.7773742675781
client 2, data condensation 1200, total loss = 76.19317626953125, avg loss = 38.096588134765625
client 2, data condensation 1400, total loss = 47.215576171875, avg loss = 23.6077880859375
client 2, data condensation 1600, total loss = 29.0745849609375, avg loss = 14.53729248046875
client 2, data condensation 1800, total loss = 26.075927734375, avg loss = 13.0379638671875
client 2, data condensation 2000, total loss = 40.2174072265625, avg loss = 20.10870361328125
client 2, data condensation 2200, total loss = 33.1551513671875, avg loss = 16.57757568359375
client 2, data condensation 2400, total loss = 42.13287353515625, avg loss = 21.066436767578125
client 2, data condensation 2600, total loss = 62.617919921875, avg loss = 31.3089599609375
client 2, data condensation 2800, total loss = 53.57611083984375, avg loss = 26.788055419921875
client 2, data condensation 3000, total loss = 48.4493408203125, avg loss = 24.22467041015625
client 2, data condensation 3200, total loss = 31.93182373046875, avg loss = 15.965911865234375
client 2, data condensation 3400, total loss = 71.26068115234375, avg loss = 35.630340576171875
client 2, data condensation 3600, total loss = 87.50576782226562, avg loss = 43.75288391113281
client 2, data condensation 3800, total loss = 47.615814208984375, avg loss = 23.807907104492188
client 2, data condensation 4000, total loss = 30.4039306640625, avg loss = 15.20196533203125
client 2, data condensation 4200, total loss = 32.5032958984375, avg loss = 16.25164794921875
client 2, data condensation 4400, total loss = 36.4461669921875, avg loss = 18.22308349609375
client 2, data condensation 4600, total loss = 36.55364990234375, avg loss = 18.276824951171875
client 2, data condensation 4800, total loss = 72.70263671875, avg loss = 36.351318359375
client 2, data condensation 5000, total loss = 41.635467529296875, avg loss = 20.817733764648438
Round 4, client 2 condense time: 392.0831093788147
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3195.06MB, free 21380.94MB
total 24576.0MB, used 3195.06MB, free 21380.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [156  21  14  12   9   9  11  13  20  48], bin edged: [0.00263223 0.00280292 0.00297361 0.00314431 0.003315   0.00348569
 0.00365638 0.00382708 0.00399777 0.00416846 0.00433916]
client 3, data condensation 0, total loss = 93.6737060546875, avg loss = 93.6737060546875
client 3, data condensation 200, total loss = 19.91937255859375, avg loss = 19.91937255859375
client 3, data condensation 400, total loss = 60.488983154296875, avg loss = 60.488983154296875
client 3, data condensation 600, total loss = 29.566314697265625, avg loss = 29.566314697265625
client 3, data condensation 800, total loss = 43.2454833984375, avg loss = 43.2454833984375
client 3, data condensation 1000, total loss = 93.92898559570312, avg loss = 93.92898559570312
client 3, data condensation 1200, total loss = 39.901092529296875, avg loss = 39.901092529296875
client 3, data condensation 1400, total loss = 34.077056884765625, avg loss = 34.077056884765625
client 3, data condensation 1600, total loss = 34.401947021484375, avg loss = 34.401947021484375
client 3, data condensation 1800, total loss = 65.213134765625, avg loss = 65.213134765625
client 3, data condensation 2000, total loss = 30.5311279296875, avg loss = 30.5311279296875
client 3, data condensation 2200, total loss = 16.328643798828125, avg loss = 16.328643798828125
client 3, data condensation 2400, total loss = 33.80560302734375, avg loss = 33.80560302734375
client 3, data condensation 2600, total loss = 20.311767578125, avg loss = 20.311767578125
client 3, data condensation 2800, total loss = 21.89178466796875, avg loss = 21.89178466796875
client 3, data condensation 3000, total loss = 24.3099365234375, avg loss = 24.3099365234375
client 3, data condensation 3200, total loss = 31.29840087890625, avg loss = 31.29840087890625
client 3, data condensation 3400, total loss = 32.4583740234375, avg loss = 32.4583740234375
client 3, data condensation 3600, total loss = 127.1126708984375, avg loss = 127.1126708984375
client 3, data condensation 3800, total loss = 59.80975341796875, avg loss = 59.80975341796875
client 3, data condensation 4000, total loss = 22.653961181640625, avg loss = 22.653961181640625
client 3, data condensation 4200, total loss = 32.89923095703125, avg loss = 32.89923095703125
client 3, data condensation 4400, total loss = 48.5472412109375, avg loss = 48.5472412109375
client 3, data condensation 4600, total loss = 32.409393310546875, avg loss = 32.409393310546875
client 3, data condensation 4800, total loss = 41.403076171875, avg loss = 41.403076171875
client 3, data condensation 5000, total loss = 248.9879150390625, avg loss = 248.9879150390625
Round 4, client 3 condense time: 129.44648361206055
client 3, class 6 have 313 samples
total 24576.0MB, used 2937.06MB, free 21638.94MB
total 24576.0MB, used 2937.06MB, free 21638.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [312   5   2   1   1   0   3   2   2  33], bin edged: [0.00257797 0.00274521 0.00291244 0.00307968 0.00324692 0.00341416
 0.0035814  0.00374864 0.00391587 0.00408311 0.00425035]
class 4 have 1048 samples, histogram: [490  73  53  36  39  37  44  45  48 183], bin edged: [0.00077784 0.0008283  0.00087876 0.00092922 0.00097968 0.00103014
 0.0010806  0.00113106 0.00118152 0.00123198 0.00128244]
class 6 have 7572 samples, histogram: [5120  470  278  197  201  157  155  181  220  593], bin edged: [0.00011743 0.00012505 0.00013267 0.00014029 0.00014791 0.00015553
 0.00016314 0.00017076 0.00017838 0.000186   0.00019362]
client 4, data condensation 0, total loss = 297.8399658203125, avg loss = 99.27998860677083
client 4, data condensation 200, total loss = 51.309417724609375, avg loss = 17.103139241536457
client 4, data condensation 400, total loss = 546.10205078125, avg loss = 182.03401692708334
client 4, data condensation 600, total loss = 40.17242431640625, avg loss = 13.39080810546875
client 4, data condensation 800, total loss = 44.381591796875, avg loss = 14.793863932291666
client 4, data condensation 1000, total loss = 58.342742919921875, avg loss = 19.447580973307293
client 4, data condensation 1200, total loss = 93.409423828125, avg loss = 31.136474609375
client 4, data condensation 1400, total loss = 236.58920288085938, avg loss = 78.86306762695312
client 4, data condensation 1600, total loss = 65.66473388671875, avg loss = 21.88824462890625
client 4, data condensation 1800, total loss = 123.55905151367188, avg loss = 41.18635050455729
client 4, data condensation 2000, total loss = 39.20623779296875, avg loss = 13.068745930989584
client 4, data condensation 2200, total loss = 175.26858520507812, avg loss = 58.42286173502604
client 4, data condensation 2400, total loss = 83.916748046875, avg loss = 27.972249348958332
client 4, data condensation 2600, total loss = 43.9503173828125, avg loss = 14.650105794270834
client 4, data condensation 2800, total loss = 131.396728515625, avg loss = 43.798909505208336
client 4, data condensation 3000, total loss = 47.945220947265625, avg loss = 15.981740315755209
client 4, data condensation 3200, total loss = 278.7464904785156, avg loss = 92.91549682617188
client 4, data condensation 3400, total loss = 111.5347900390625, avg loss = 37.178263346354164
client 4, data condensation 3600, total loss = 71.91403198242188, avg loss = 23.971343994140625
client 4, data condensation 3800, total loss = 115.529541015625, avg loss = 38.509847005208336
client 4, data condensation 4000, total loss = 182.83612060546875, avg loss = 60.94537353515625
client 4, data condensation 4200, total loss = 45.770233154296875, avg loss = 15.256744384765625
client 4, data condensation 4400, total loss = 36.293701171875, avg loss = 12.097900390625
client 4, data condensation 4600, total loss = 51.813812255859375, avg loss = 17.271270751953125
client 4, data condensation 4800, total loss = 55.060943603515625, avg loss = 18.353647867838543
client 4, data condensation 5000, total loss = 48.809051513671875, avg loss = 16.269683837890625
Round 4, client 4 condense time: 530.622801065445
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3453.06MB, free 21122.94MB
total 24576.0MB, used 3453.06MB, free 21122.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [3480  577  357  311  316  297  324  418  617 5454], bin edged: [5.97109275e-05 6.35845018e-05 6.74580760e-05 7.13316503e-05
 7.52052246e-05 7.90787989e-05 8.29523732e-05 8.68259475e-05
 9.06995218e-05 9.45730961e-05 9.84466704e-05]
client 5, data condensation 0, total loss = 121.36105346679688, avg loss = 121.36105346679688
client 5, data condensation 200, total loss = 14.26092529296875, avg loss = 14.26092529296875
client 5, data condensation 400, total loss = 10.728118896484375, avg loss = 10.728118896484375
client 5, data condensation 600, total loss = 14.63470458984375, avg loss = 14.63470458984375
client 5, data condensation 800, total loss = 150.9794921875, avg loss = 150.9794921875
client 5, data condensation 1000, total loss = 12.118408203125, avg loss = 12.118408203125
client 5, data condensation 1200, total loss = 8.028289794921875, avg loss = 8.028289794921875
client 5, data condensation 1400, total loss = 157.28604125976562, avg loss = 157.28604125976562
client 5, data condensation 1600, total loss = 116.45150756835938, avg loss = 116.45150756835938
client 5, data condensation 1800, total loss = 9.392120361328125, avg loss = 9.392120361328125
client 5, data condensation 2000, total loss = 8.303802490234375, avg loss = 8.303802490234375
client 5, data condensation 2200, total loss = 3.657501220703125, avg loss = 3.657501220703125
client 5, data condensation 2400, total loss = 9.1951904296875, avg loss = 9.1951904296875
client 5, data condensation 2600, total loss = 103.769775390625, avg loss = 103.769775390625
client 5, data condensation 2800, total loss = 5.771453857421875, avg loss = 5.771453857421875
client 5, data condensation 3000, total loss = 92.51187133789062, avg loss = 92.51187133789062
client 5, data condensation 3200, total loss = 5.5406494140625, avg loss = 5.5406494140625
client 5, data condensation 3400, total loss = 6.839447021484375, avg loss = 6.839447021484375
client 5, data condensation 3600, total loss = 5.08197021484375, avg loss = 5.08197021484375
client 5, data condensation 3800, total loss = 6.0465087890625, avg loss = 6.0465087890625
client 5, data condensation 4000, total loss = 135.36056518554688, avg loss = 135.36056518554688
client 5, data condensation 4200, total loss = 5.80548095703125, avg loss = 5.80548095703125
client 5, data condensation 4400, total loss = 23.911224365234375, avg loss = 23.911224365234375
client 5, data condensation 4600, total loss = 50.337158203125, avg loss = 50.337158203125
client 5, data condensation 4800, total loss = 5.283538818359375, avg loss = 5.283538818359375
client 5, data condensation 5000, total loss = 11.160400390625, avg loss = 11.160400390625
Round 4, client 5 condense time: 264.71165204048157
client 5, class 8 have 12151 samples
total 24576.0MB, used 2941.06MB, free 21634.94MB
total 24576.0MB, used 2941.06MB, free 21634.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [7357  458  265  168  171  156  155  165  224 1075], bin edged: [8.75266211e-05 9.32046592e-05 9.88826973e-05 1.04560735e-04
 1.10238773e-04 1.15916812e-04 1.21594850e-04 1.27272888e-04
 1.32950926e-04 1.38628964e-04 1.44307002e-04]
client 6, data condensation 0, total loss = 162.099365234375, avg loss = 162.099365234375
client 6, data condensation 200, total loss = 8.223175048828125, avg loss = 8.223175048828125
client 6, data condensation 400, total loss = 6.98565673828125, avg loss = 6.98565673828125
client 6, data condensation 600, total loss = 21.46221923828125, avg loss = 21.46221923828125
client 6, data condensation 800, total loss = 14.232208251953125, avg loss = 14.232208251953125
client 6, data condensation 1000, total loss = 6.775665283203125, avg loss = 6.775665283203125
client 6, data condensation 1200, total loss = 23.77264404296875, avg loss = 23.77264404296875
client 6, data condensation 1400, total loss = 27.58660888671875, avg loss = 27.58660888671875
client 6, data condensation 1600, total loss = 19.2652587890625, avg loss = 19.2652587890625
client 6, data condensation 1800, total loss = 4.615753173828125, avg loss = 4.615753173828125
client 6, data condensation 2000, total loss = 11.00958251953125, avg loss = 11.00958251953125
client 6, data condensation 2200, total loss = 17.64013671875, avg loss = 17.64013671875
client 6, data condensation 2400, total loss = 4.568359375, avg loss = 4.568359375
client 6, data condensation 2600, total loss = 26.95263671875, avg loss = 26.95263671875
client 6, data condensation 2800, total loss = 103.98513793945312, avg loss = 103.98513793945312
client 6, data condensation 3000, total loss = 11.103851318359375, avg loss = 11.103851318359375
client 6, data condensation 3200, total loss = 465.90887451171875, avg loss = 465.90887451171875
client 6, data condensation 3400, total loss = 23.058441162109375, avg loss = 23.058441162109375
client 6, data condensation 3600, total loss = 17.320159912109375, avg loss = 17.320159912109375
client 6, data condensation 3800, total loss = 55.609649658203125, avg loss = 55.609649658203125
client 6, data condensation 4000, total loss = 16.7757568359375, avg loss = 16.7757568359375
client 6, data condensation 4200, total loss = 20.500213623046875, avg loss = 20.500213623046875
client 6, data condensation 4400, total loss = 11.10693359375, avg loss = 11.10693359375
client 6, data condensation 4600, total loss = 16.47430419921875, avg loss = 16.47430419921875
client 6, data condensation 4800, total loss = 17.217254638671875, avg loss = 17.217254638671875
client 6, data condensation 5000, total loss = 13.516021728515625, avg loss = 13.516021728515625
Round 4, client 6 condense time: 264.48115038871765
client 6, class 3 have 10194 samples
total 24576.0MB, used 2945.06MB, free 21630.94MB
total 24576.0MB, used 2945.06MB, free 21630.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [5180  349  209  208  225  294  484  369  353 1441], bin edged: [9.13557012e-05 9.72816888e-05 1.03207676e-04 1.09133664e-04
 1.15059651e-04 1.20985639e-04 1.26911627e-04 1.32837614e-04
 1.38763602e-04 1.44689589e-04 1.50615577e-04]
client 7, data condensation 0, total loss = 154.59588623046875, avg loss = 154.59588623046875
client 7, data condensation 200, total loss = 8.460906982421875, avg loss = 8.460906982421875
client 7, data condensation 400, total loss = 19.63848876953125, avg loss = 19.63848876953125
client 7, data condensation 600, total loss = 10.567657470703125, avg loss = 10.567657470703125
client 7, data condensation 800, total loss = 11.6705322265625, avg loss = 11.6705322265625
client 7, data condensation 1000, total loss = 12.906280517578125, avg loss = 12.906280517578125
client 7, data condensation 1200, total loss = 10.0174560546875, avg loss = 10.0174560546875
client 7, data condensation 1400, total loss = 12.98388671875, avg loss = 12.98388671875
client 7, data condensation 1600, total loss = 247.17144775390625, avg loss = 247.17144775390625
client 7, data condensation 1800, total loss = 21.4456787109375, avg loss = 21.4456787109375
client 7, data condensation 2000, total loss = 12.77349853515625, avg loss = 12.77349853515625
client 7, data condensation 2200, total loss = 31.8115234375, avg loss = 31.8115234375
client 7, data condensation 2400, total loss = 9.949432373046875, avg loss = 9.949432373046875
client 7, data condensation 2600, total loss = 12.540374755859375, avg loss = 12.540374755859375
client 7, data condensation 2800, total loss = 4.79205322265625, avg loss = 4.79205322265625
client 7, data condensation 3000, total loss = 46.110626220703125, avg loss = 46.110626220703125
client 7, data condensation 3200, total loss = 7.023681640625, avg loss = 7.023681640625
client 7, data condensation 3400, total loss = 23.40838623046875, avg loss = 23.40838623046875
client 7, data condensation 3600, total loss = 232.66888427734375, avg loss = 232.66888427734375
client 7, data condensation 3800, total loss = 20.491668701171875, avg loss = 20.491668701171875
client 7, data condensation 4000, total loss = 39.426849365234375, avg loss = 39.426849365234375
client 7, data condensation 4200, total loss = 5.553131103515625, avg loss = 5.553131103515625
client 7, data condensation 4400, total loss = 12.870880126953125, avg loss = 12.870880126953125
client 7, data condensation 4600, total loss = 5.521087646484375, avg loss = 5.521087646484375
client 7, data condensation 4800, total loss = 16.227294921875, avg loss = 16.227294921875
client 7, data condensation 5000, total loss = 8.2359619140625, avg loss = 8.2359619140625
Round 4, client 7 condense time: 253.49627804756165
client 7, class 1 have 9112 samples
total 24576.0MB, used 2943.06MB, free 21632.94MB
total 24576.0MB, used 2943.06MB, free 21632.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [188   3   4   2   1   2   0   0   0   6], bin edged: [0.00470256 0.00500763 0.00531269 0.00561776 0.00592282 0.00622789
 0.00653295 0.00683802 0.00714308 0.00744815 0.00775321]
class 4 have 1179 samples, histogram: [633  99  58  38  30  33  33  34  54 167], bin edged: [0.0007142  0.00076053 0.00080686 0.0008532  0.00089953 0.00094586
 0.00099219 0.00103852 0.00108485 0.00113119 0.00117752]
class 8 have 734 samples, histogram: [392  31  21  27  13  14  20  26  33 157], bin edged: [0.00111365 0.00118589 0.00125814 0.00133038 0.00140263 0.00147487
 0.00154711 0.00161936 0.0016916  0.00176385 0.00183609]
client 8, data condensation 0, total loss = 223.88198852539062, avg loss = 74.62732950846355
client 8, data condensation 200, total loss = 111.42489624023438, avg loss = 37.141632080078125
client 8, data condensation 400, total loss = 321.4329528808594, avg loss = 107.14431762695312
client 8, data condensation 600, total loss = 71.65756225585938, avg loss = 23.885854085286457
client 8, data condensation 800, total loss = 83.87359619140625, avg loss = 27.957865397135418
client 8, data condensation 1000, total loss = 144.19839477539062, avg loss = 48.066131591796875
client 8, data condensation 1200, total loss = 47.9842529296875, avg loss = 15.9947509765625
client 8, data condensation 1400, total loss = 142.12286376953125, avg loss = 47.374287923177086
client 8, data condensation 1600, total loss = 113.74105834960938, avg loss = 37.91368611653646
client 8, data condensation 1800, total loss = 62.35693359375, avg loss = 20.78564453125
client 8, data condensation 2000, total loss = 36.410003662109375, avg loss = 12.136667887369791
client 8, data condensation 2200, total loss = 104.96353149414062, avg loss = 34.98784383138021
client 8, data condensation 2400, total loss = 39.5635986328125, avg loss = 13.1878662109375
client 8, data condensation 2600, total loss = 376.15081787109375, avg loss = 125.38360595703125
client 8, data condensation 2800, total loss = 52.11309814453125, avg loss = 17.37103271484375
client 8, data condensation 3000, total loss = 38.5777587890625, avg loss = 12.8592529296875
client 8, data condensation 3200, total loss = 115.54428100585938, avg loss = 38.51476033528646
client 8, data condensation 3400, total loss = 34.37493896484375, avg loss = 11.45831298828125
client 8, data condensation 3600, total loss = 62.573699951171875, avg loss = 20.857899983723957
client 8, data condensation 3800, total loss = 306.9860534667969, avg loss = 102.3286844889323
client 8, data condensation 4000, total loss = 153.38088989257812, avg loss = 51.12696329752604
client 8, data condensation 4200, total loss = 672.7437133789062, avg loss = 224.2479044596354
client 8, data condensation 4400, total loss = 85.1339111328125, avg loss = 28.377970377604168
client 8, data condensation 4600, total loss = 81.88409423828125, avg loss = 27.294698079427082
client 8, data condensation 4800, total loss = 47.81488037109375, avg loss = 15.93829345703125
client 8, data condensation 5000, total loss = 72.96505737304688, avg loss = 24.321685791015625
Round 4, client 8 condense time: 428.5710413455963
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3453.06MB, free 21122.94MB
total 24576.0MB, used 3453.06MB, free 21122.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2546  445  290  263  231  240  292  366  543 5100], bin edged: [6.88737401e-05 7.33417261e-05 7.78097121e-05 8.22776981e-05
 8.67456841e-05 9.12136702e-05 9.56816562e-05 1.00149642e-04
 1.04617628e-04 1.09085614e-04 1.13553600e-04]
client 9, data condensation 0, total loss = 132.3348388671875, avg loss = 132.3348388671875
client 9, data condensation 200, total loss = 9.025146484375, avg loss = 9.025146484375
client 9, data condensation 400, total loss = 116.759033203125, avg loss = 116.759033203125
client 9, data condensation 600, total loss = 19.142242431640625, avg loss = 19.142242431640625
client 9, data condensation 800, total loss = 6.81280517578125, avg loss = 6.81280517578125
client 9, data condensation 1000, total loss = 17.625213623046875, avg loss = 17.625213623046875
client 9, data condensation 1200, total loss = 11.907806396484375, avg loss = 11.907806396484375
client 9, data condensation 1400, total loss = 16.232025146484375, avg loss = 16.232025146484375
client 9, data condensation 1600, total loss = 6.169891357421875, avg loss = 6.169891357421875
client 9, data condensation 1800, total loss = 3.042236328125, avg loss = 3.042236328125
client 9, data condensation 2000, total loss = 5.255767822265625, avg loss = 5.255767822265625
client 9, data condensation 2200, total loss = 3.48004150390625, avg loss = 3.48004150390625
client 9, data condensation 2400, total loss = 24.767974853515625, avg loss = 24.767974853515625
client 9, data condensation 2600, total loss = 14.907257080078125, avg loss = 14.907257080078125
client 9, data condensation 2800, total loss = 115.40850830078125, avg loss = 115.40850830078125
client 9, data condensation 3000, total loss = 8.615020751953125, avg loss = 8.615020751953125
client 9, data condensation 3200, total loss = 2.595550537109375, avg loss = 2.595550537109375
client 9, data condensation 3400, total loss = 10.160675048828125, avg loss = 10.160675048828125
client 9, data condensation 3600, total loss = 9.304931640625, avg loss = 9.304931640625
client 9, data condensation 3800, total loss = 13.504669189453125, avg loss = 13.504669189453125
client 9, data condensation 4000, total loss = 5.807159423828125, avg loss = 5.807159423828125
client 9, data condensation 4200, total loss = 250.26144409179688, avg loss = 250.26144409179688
client 9, data condensation 4400, total loss = 4.401031494140625, avg loss = 4.401031494140625
client 9, data condensation 4600, total loss = 8.388031005859375, avg loss = 8.388031005859375
client 9, data condensation 4800, total loss = 5.44366455078125, avg loss = 5.44366455078125
client 9, data condensation 5000, total loss = 3.6142578125, avg loss = 3.6142578125
Round 4, client 9 condense time: 246.399480342865
client 9, class 2 have 10316 samples
total 24576.0MB, used 2945.06MB, free 21630.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 19.0981,   3.1631,  -6.4634, -15.3124,   8.8986,   5.0905,   1.5148,
          -6.3323,  -9.6509],
        [  1.8238,  12.5591,  -1.3144,  -5.6572,  -0.8913,   2.9518,  -3.2707,
          -2.2814,  -3.2778],
        [ -9.6600,  -7.6138,   7.9012,  -2.6953,   0.0302,   5.0890,   1.8541,
           5.9638,   0.0410],
        [-12.4284,  -6.8832,  -0.6668,  17.5280,  -0.5311,  -3.9175,   6.8070,
           4.9072,  -3.8166],
        [ -1.4853,  -3.8782,  -4.3445,  -4.7234,   9.5076,   0.6722,   7.2075,
           0.3711,  -3.0873],
        [ -6.7634,  -5.1990,   4.1203,  -7.2085,   0.8151,   9.8622,   0.8847,
           4.3517,  -0.0302],
        [ -6.8918,  -7.8527,  -0.5196,   0.0278,   3.5920,  -0.2338,  11.9066,
           0.9137,  -0.5888],
        [-10.4329,  -8.7069,   2.1625,   0.5657,   2.0154,   2.7160,   3.2867,
           9.1105,   0.2025],
        [-10.2607,  -7.2080,   1.0533,  -2.1588,   0.8943,   1.4360,   7.0190,
           1.4505,   8.2460]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 0, 4, 2],
        [2, 7, 5, 6, 8],
        [3, 6, 7, 4, 2],
        [4, 6, 5, 7, 0],
        [5, 7, 2, 6, 4],
        [6, 4, 7, 3, 5],
        [7, 6, 5, 2, 4],
        [8, 6, 7, 5, 2]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
5
Round 4: # synthetic sample: 4560
total 24576.0MB, used 2945.06MB, free 21630.94MB
{0: {0: 1312, 1: 5, 2: 0, 3: 0, 4: 12, 5: 6, 6: 3, 7: 0, 8: 0}, 1: {0: 20, 1: 600, 2: 224, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 5, 2: 219, 3: 7, 4: 0, 5: 11, 6: 0, 7: 96, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 580, 4: 1, 5: 2, 6: 49, 7: 0, 8: 2}, 4: {0: 38, 1: 174, 2: 0, 3: 2, 4: 750, 5: 23, 6: 44, 7: 4, 8: 0}, 5: {0: 0, 1: 62, 2: 72, 3: 49, 4: 2, 5: 131, 6: 18, 7: 250, 8: 8}, 6: {0: 10, 1: 0, 2: 5, 3: 7, 4: 34, 5: 14, 6: 669, 7: 0, 8: 2}, 7: {0: 0, 1: 29, 2: 28, 3: 61, 4: 30, 5: 33, 6: 52, 7: 166, 8: 22}, 8: {0: 17, 1: 2, 2: 35, 3: 46, 4: 12, 5: 132, 6: 297, 7: 69, 8: 623}}
round 4 evaluation: test acc is 0.7033, test loss = 1.496328
{0: {0: 1318, 1: 5, 2: 0, 3: 0, 4: 1, 5: 12, 6: 2, 7: 0, 8: 0}, 1: {0: 1, 1: 846, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 6, 2: 259, 3: 44, 4: 0, 5: 6, 6: 0, 7: 4, 8: 20}, 3: {0: 2, 1: 0, 2: 0, 3: 605, 4: 0, 5: 1, 6: 10, 7: 0, 8: 16}, 4: {0: 96, 1: 410, 2: 2, 3: 30, 4: 349, 5: 109, 6: 33, 7: 0, 8: 6}, 5: {0: 0, 1: 66, 2: 68, 3: 61, 4: 0, 5: 167, 6: 1, 7: 104, 8: 125}, 6: {0: 20, 1: 1, 2: 4, 3: 50, 4: 4, 5: 42, 6: 518, 7: 0, 8: 102}, 7: {0: 0, 1: 25, 2: 23, 3: 138, 4: 4, 5: 37, 6: 11, 7: 31, 8: 152}, 8: {0: 9, 1: 3, 2: 2, 3: 39, 4: 1, 5: 106, 6: 26, 7: 2, 8: 1045}}
epoch 0, train loss avg now = 0.407398, train contrast loss now = 1.429726, test acc now = 0.7156, test loss now = 1.524726
{0: {0: 1319, 1: 9, 2: 0, 3: 0, 4: 3, 5: 3, 6: 4, 7: 0, 8: 0}, 1: {0: 5, 1: 840, 2: 2, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 245, 3: 13, 4: 0, 5: 27, 6: 1, 7: 53, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 42, 7: 0, 8: 4}, 4: {0: 52, 1: 106, 2: 0, 3: 7, 4: 804, 5: 5, 6: 57, 7: 1, 8: 3}, 5: {0: 0, 1: 13, 2: 98, 3: 66, 4: 3, 5: 201, 6: 13, 7: 187, 8: 11}, 6: {0: 21, 1: 0, 2: 0, 3: 18, 4: 20, 5: 16, 6: 659, 7: 0, 8: 7}, 7: {0: 1, 1: 18, 2: 34, 3: 112, 4: 17, 5: 54, 6: 50, 7: 91, 8: 44}, 8: {0: 22, 1: 6, 2: 20, 3: 36, 4: 8, 5: 59, 6: 237, 7: 12, 8: 833}}
epoch 100, train loss avg now = 0.065871, train contrast loss now = 1.194896, test acc now = 0.7770, test loss now = 1.045214
{0: {0: 1311, 1: 4, 2: 0, 3: 0, 4: 16, 5: 5, 6: 2, 7: 0, 8: 0}, 1: {0: 12, 1: 835, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 28, 2: 200, 3: 55, 4: 0, 5: 52, 6: 0, 7: 4, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 613, 4: 1, 5: 1, 6: 14, 7: 0, 8: 5}, 4: {0: 63, 1: 193, 2: 0, 3: 12, 4: 739, 5: 1, 6: 26, 7: 0, 8: 1}, 5: {0: 0, 1: 12, 2: 151, 3: 88, 4: 3, 5: 170, 6: 4, 7: 151, 8: 13}, 6: {0: 6, 1: 0, 2: 4, 3: 47, 4: 48, 5: 15, 6: 609, 7: 0, 8: 12}, 7: {0: 1, 1: 49, 2: 36, 3: 164, 4: 22, 5: 14, 6: 38, 7: 54, 8: 43}, 8: {0: 10, 1: 9, 2: 13, 3: 87, 4: 12, 5: 52, 6: 162, 7: 9, 8: 879}}
epoch 200, train loss avg now = 0.073240, train contrast loss now = 1.194676, test acc now = 0.7535, test loss now = 1.429264
{0: {0: 1316, 1: 5, 2: 0, 3: 0, 4: 1, 5: 14, 6: 2, 7: 0, 8: 0}, 1: {0: 6, 1: 841, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 214, 3: 7, 4: 0, 5: 18, 6: 1, 7: 98, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 569, 4: 0, 5: 1, 6: 61, 7: 0, 8: 3}, 4: {0: 107, 1: 54, 2: 0, 3: 2, 4: 813, 5: 12, 6: 42, 7: 5, 8: 0}, 5: {0: 0, 1: 0, 2: 69, 3: 22, 4: 1, 5: 136, 6: 9, 7: 351, 8: 4}, 6: {0: 12, 1: 0, 2: 3, 3: 8, 4: 15, 5: 27, 6: 662, 7: 10, 8: 4}, 7: {0: 1, 1: 12, 2: 12, 3: 29, 4: 10, 5: 52, 6: 38, 7: 238, 8: 29}, 8: {0: 12, 1: 6, 2: 6, 3: 23, 4: 5, 5: 110, 6: 271, 7: 90, 8: 710}}
epoch 300, train loss avg now = 0.053358, train contrast loss now = 1.193558, test acc now = 0.7659, test loss now = 1.250614
{0: {0: 1318, 1: 4, 2: 0, 3: 0, 4: 5, 5: 3, 6: 8, 7: 0, 8: 0}, 1: {0: 6, 1: 789, 2: 50, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 209, 3: 27, 4: 0, 5: 13, 6: 3, 7: 85, 8: 2}, 3: {0: 0, 1: 0, 2: 0, 3: 571, 4: 0, 5: 1, 6: 60, 7: 0, 8: 2}, 4: {0: 48, 1: 31, 2: 0, 3: 3, 4: 825, 5: 9, 6: 116, 7: 3, 8: 0}, 5: {0: 52, 1: 1, 2: 42, 3: 66, 4: 2, 5: 93, 6: 23, 7: 307, 8: 6}, 6: {0: 16, 1: 0, 2: 0, 3: 11, 4: 14, 5: 5, 6: 693, 7: 0, 8: 2}, 7: {0: 2, 1: 0, 2: 13, 3: 93, 4: 13, 5: 67, 6: 58, 7: 137, 8: 38}, 8: {0: 15, 1: 14, 2: 0, 3: 40, 4: 4, 5: 71, 6: 420, 7: 26, 8: 643}}
epoch 400, train loss avg now = 0.042289, train contrast loss now = 1.193371, test acc now = 0.7351, test loss now = 1.512899
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1307, 1: 5, 2: 0, 3: 0, 4: 11, 5: 10, 6: 5, 7: 0, 8: 0}, 1: {0: 1, 1: 832, 2: 14, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 222, 3: 11, 4: 0, 5: 13, 6: 4, 7: 89, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 564, 4: 1, 5: 1, 6: 65, 7: 0, 8: 3}, 4: {0: 52, 1: 96, 2: 0, 3: 3, 4: 820, 5: 18, 6: 38, 7: 5, 8: 3}, 5: {0: 6, 1: 3, 2: 57, 3: 40, 4: 3, 5: 147, 6: 17, 7: 310, 8: 9}, 6: {0: 11, 1: 0, 2: 2, 3: 7, 4: 22, 5: 9, 6: 689, 7: 0, 8: 1}, 7: {0: 1, 1: 12, 2: 9, 3: 80, 4: 18, 5: 58, 6: 56, 7: 148, 8: 39}, 8: {0: 13, 1: 6, 2: 6, 3: 31, 4: 9, 5: 75, 6: 342, 7: 47, 8: 704}}
epoch 500, train loss avg now = 0.022352, train contrast loss now = 1.192867, test acc now = 0.7567, test loss now = 1.297548
{0: {0: 1317, 1: 5, 2: 0, 3: 0, 4: 9, 5: 4, 6: 3, 7: 0, 8: 0}, 1: {0: 9, 1: 793, 2: 45, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 257, 3: 29, 4: 0, 5: 11, 6: 3, 7: 39, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 588, 4: 1, 5: 1, 6: 41, 7: 0, 8: 3}, 4: {0: 47, 1: 62, 2: 0, 3: 4, 4: 863, 5: 7, 6: 48, 7: 3, 8: 1}, 5: {0: 0, 1: 3, 2: 98, 3: 54, 4: 3, 5: 151, 6: 11, 7: 265, 8: 7}, 6: {0: 12, 1: 0, 2: 4, 3: 15, 4: 26, 5: 10, 6: 670, 7: 0, 8: 4}, 7: {0: 2, 1: 14, 2: 21, 3: 108, 4: 17, 5: 52, 6: 58, 7: 120, 8: 29}, 8: {0: 16, 1: 5, 2: 11, 3: 46, 4: 8, 5: 71, 6: 342, 7: 31, 8: 703}}
epoch 600, train loss avg now = 0.021188, train contrast loss now = 1.194595, test acc now = 0.7607, test loss now = 1.307430
{0: {0: 1312, 1: 5, 2: 0, 3: 0, 4: 9, 5: 6, 6: 6, 7: 0, 8: 0}, 1: {0: 8, 1: 660, 2: 179, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 247, 3: 20, 4: 0, 5: 12, 6: 7, 7: 53, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 571, 4: 1, 5: 1, 6: 58, 7: 0, 8: 3}, 4: {0: 48, 1: 58, 2: 0, 3: 4, 4: 858, 5: 4, 6: 59, 7: 3, 8: 1}, 5: {0: 1, 1: 3, 2: 84, 3: 46, 4: 3, 5: 159, 6: 20, 7: 269, 8: 7}, 6: {0: 12, 1: 0, 2: 1, 3: 8, 4: 21, 5: 12, 6: 686, 7: 0, 8: 1}, 7: {0: 2, 1: 14, 2: 19, 3: 94, 4: 25, 5: 53, 6: 77, 7: 106, 8: 31}, 8: {0: 14, 1: 7, 2: 8, 3: 38, 4: 8, 5: 76, 6: 386, 7: 25, 8: 671}}
epoch 700, train loss avg now = 0.019946, train contrast loss now = 1.191637, test acc now = 0.7340, test loss now = 1.369277
{0: {0: 1313, 1: 5, 2: 0, 3: 0, 4: 9, 5: 5, 6: 6, 7: 0, 8: 0}, 1: {0: 8, 1: 770, 2: 67, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 246, 3: 16, 4: 0, 5: 12, 6: 4, 7: 61, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 570, 4: 1, 5: 1, 6: 59, 7: 0, 8: 3}, 4: {0: 48, 1: 54, 2: 0, 3: 2, 4: 865, 5: 4, 6: 58, 7: 3, 8: 1}, 5: {0: 2, 1: 4, 2: 79, 3: 42, 4: 3, 5: 153, 6: 16, 7: 286, 8: 7}, 6: {0: 12, 1: 0, 2: 1, 3: 8, 4: 22, 5: 10, 6: 686, 7: 0, 8: 2}, 7: {0: 1, 1: 14, 2: 16, 3: 81, 4: 24, 5: 56, 6: 64, 7: 133, 8: 32}, 8: {0: 13, 1: 7, 2: 5, 3: 29, 4: 8, 5: 81, 6: 362, 7: 35, 8: 693}}
epoch 800, train loss avg now = 0.019307, train contrast loss now = 1.190882, test acc now = 0.7561, test loss now = 1.329654
{0: {0: 1311, 1: 5, 2: 0, 3: 0, 4: 10, 5: 9, 6: 3, 7: 0, 8: 0}, 1: {0: 4, 1: 783, 2: 60, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 246, 3: 16, 4: 0, 5: 12, 6: 4, 7: 61, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 566, 4: 1, 5: 1, 6: 63, 7: 0, 8: 3}, 4: {0: 40, 1: 42, 2: 0, 3: 2, 4: 898, 5: 3, 6: 47, 7: 2, 8: 1}, 5: {0: 1, 1: 1, 2: 85, 3: 39, 4: 3, 5: 149, 6: 16, 7: 290, 8: 8}, 6: {0: 12, 1: 0, 2: 2, 3: 8, 4: 23, 5: 14, 6: 678, 7: 0, 8: 4}, 7: {0: 1, 1: 9, 2: 18, 3: 83, 4: 26, 5: 56, 6: 67, 7: 129, 8: 32}, 8: {0: 14, 1: 7, 2: 6, 3: 32, 4: 8, 5: 73, 6: 363, 7: 34, 8: 696}}
epoch 900, train loss avg now = 0.018257, train contrast loss now = 1.193460, test acc now = 0.7599, test loss now = 1.311951
{0: {0: 1315, 1: 5, 2: 0, 3: 0, 4: 7, 5: 7, 6: 4, 7: 0, 8: 0}, 1: {0: 8, 1: 678, 2: 160, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 236, 3: 16, 4: 0, 5: 13, 6: 5, 7: 68, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 573, 4: 1, 5: 1, 6: 56, 7: 0, 8: 3}, 4: {0: 50, 1: 49, 2: 0, 3: 3, 4: 864, 5: 6, 6: 58, 7: 4, 8: 1}, 5: {0: 0, 1: 4, 2: 68, 3: 43, 4: 3, 5: 164, 6: 18, 7: 283, 8: 9}, 6: {0: 11, 1: 0, 2: 1, 3: 10, 4: 24, 5: 13, 6: 677, 7: 0, 8: 5}, 7: {0: 2, 1: 13, 2: 16, 3: 90, 4: 22, 5: 58, 6: 58, 7: 136, 8: 26}, 8: {0: 13, 1: 7, 2: 4, 3: 34, 4: 9, 5: 77, 6: 366, 7: 37, 8: 686}}
epoch 1000, train loss avg now = 0.015060, train contrast loss now = 1.189470, test acc now = 0.7422, test loss now = 1.358720
epoch avg loss = 1.5059807880275082e-05, total time = 5845.84858083725
total 24576.0MB, used 3581.06MB, free 20994.94MB
Round 4 finish, update the prev_syn_proto
torch.Size([480, 3, 28, 28])
torch.Size([485, 3, 28, 28])
torch.Size([520, 3, 28, 28])
torch.Size([535, 3, 28, 28])
torch.Size([405, 3, 28, 28])
torch.Size([610, 3, 28, 28])
torch.Size([405, 3, 28, 28])
torch.Size([470, 3, 28, 28])
torch.Size([650, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1315, 1: 5, 2: 0, 3: 0, 4: 7, 5: 7, 6: 4, 7: 0, 8: 0}, 1: {0: 8, 1: 678, 2: 160, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 236, 3: 16, 4: 0, 5: 13, 6: 5, 7: 68, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 573, 4: 1, 5: 1, 6: 56, 7: 0, 8: 3}, 4: {0: 50, 1: 49, 2: 0, 3: 3, 4: 864, 5: 6, 6: 58, 7: 4, 8: 1}, 5: {0: 0, 1: 4, 2: 68, 3: 43, 4: 3, 5: 164, 6: 18, 7: 283, 8: 9}, 6: {0: 11, 1: 0, 2: 1, 3: 10, 4: 24, 5: 13, 6: 677, 7: 0, 8: 5}, 7: {0: 2, 1: 13, 2: 16, 3: 90, 4: 22, 5: 58, 6: 58, 7: 136, 8: 26}, 8: {0: 13, 1: 7, 2: 4, 3: 34, 4: 9, 5: 77, 6: 366, 7: 37, 8: 686}}
round 4 evaluation: test acc is 0.7422, test loss = 1.358720
 ====== round 5 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3581.06MB, free 20994.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [3040  423  276  194  171  152  167  203  201  950], bin edged: [0.00014429 0.00015365 0.00016301 0.00017237 0.00018173 0.00019109
 0.00020045 0.00020982 0.00021918 0.00022854 0.0002379 ]
class 7 have 9330 samples, histogram: [4043  635  438  339  287  275  281  328  428 2276], bin edged: [8.52053051e-05 9.07327306e-05 9.62601561e-05 1.01787582e-04
 1.07315007e-04 1.12842433e-04 1.18369858e-04 1.23897284e-04
 1.29424709e-04 1.34952135e-04 1.40479560e-04]
client 0, data condensation 0, total loss = 102.20706176757812, avg loss = 51.10353088378906
client 0, data condensation 200, total loss = 22.879852294921875, avg loss = 11.439926147460938
client 0, data condensation 400, total loss = 260.172607421875, avg loss = 130.0863037109375
client 0, data condensation 600, total loss = 14.479949951171875, avg loss = 7.2399749755859375
client 0, data condensation 800, total loss = 174.1265869140625, avg loss = 87.06329345703125
client 0, data condensation 1000, total loss = 15.30145263671875, avg loss = 7.650726318359375
client 0, data condensation 1200, total loss = 14.518829345703125, avg loss = 7.2594146728515625
client 0, data condensation 1400, total loss = 172.66485595703125, avg loss = 86.33242797851562
client 0, data condensation 1600, total loss = 13.082763671875, avg loss = 6.5413818359375
client 0, data condensation 1800, total loss = 23.016510009765625, avg loss = 11.508255004882812
client 0, data condensation 2000, total loss = 14.546142578125, avg loss = 7.2730712890625
client 0, data condensation 2200, total loss = 25.108489990234375, avg loss = 12.554244995117188
client 0, data condensation 2400, total loss = 25.91448974609375, avg loss = 12.957244873046875
client 0, data condensation 2600, total loss = 95.35906982421875, avg loss = 47.679534912109375
client 0, data condensation 2800, total loss = 11.97393798828125, avg loss = 5.986968994140625
client 0, data condensation 3000, total loss = 182.84439086914062, avg loss = 91.42219543457031
client 0, data condensation 3200, total loss = 25.897369384765625, avg loss = 12.948684692382812
client 0, data condensation 3400, total loss = 19.8677978515625, avg loss = 9.93389892578125
client 0, data condensation 3600, total loss = 10.619720458984375, avg loss = 5.3098602294921875
client 0, data condensation 3800, total loss = 13.5703125, avg loss = 6.78515625
client 0, data condensation 4000, total loss = 8.578765869140625, avg loss = 4.2893829345703125
client 0, data condensation 4200, total loss = 24.86285400390625, avg loss = 12.431427001953125
client 0, data condensation 4400, total loss = 17.169952392578125, avg loss = 8.584976196289062
client 0, data condensation 4600, total loss = 11.64794921875, avg loss = 5.823974609375
client 0, data condensation 4800, total loss = 18.989105224609375, avg loss = 9.494552612304688
client 0, data condensation 5000, total loss = 17.095062255859375, avg loss = 8.547531127929688
Round 5, client 0 condense time: 373.2463483810425
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3475.06MB, free 21100.94MB
total 24576.0MB, used 3475.06MB, free 21100.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [5847  490  272  228  198  181  186  192  272 1156], bin edged: [9.62290205e-05 1.02471602e-04 1.08714183e-04 1.14956764e-04
 1.21199345e-04 1.27441927e-04 1.33684508e-04 1.39927089e-04
 1.46169670e-04 1.52412252e-04 1.58654833e-04]
client 1, data condensation 0, total loss = 310.13336181640625, avg loss = 310.13336181640625
client 1, data condensation 200, total loss = 39.83465576171875, avg loss = 39.83465576171875
client 1, data condensation 400, total loss = 9.37982177734375, avg loss = 9.37982177734375
client 1, data condensation 600, total loss = 15.4498291015625, avg loss = 15.4498291015625
client 1, data condensation 800, total loss = 14.251220703125, avg loss = 14.251220703125
client 1, data condensation 1000, total loss = 23.64056396484375, avg loss = 23.64056396484375
client 1, data condensation 1200, total loss = 27.73876953125, avg loss = 27.73876953125
client 1, data condensation 1400, total loss = 16.500732421875, avg loss = 16.500732421875
client 1, data condensation 1600, total loss = 369.7469482421875, avg loss = 369.7469482421875
client 1, data condensation 1800, total loss = 12.390625, avg loss = 12.390625
client 1, data condensation 2000, total loss = 47.82073974609375, avg loss = 47.82073974609375
client 1, data condensation 2200, total loss = 224.22906494140625, avg loss = 224.22906494140625
client 1, data condensation 2400, total loss = 3.18994140625, avg loss = 3.18994140625
client 1, data condensation 2600, total loss = 34.62603759765625, avg loss = 34.62603759765625
client 1, data condensation 2800, total loss = 12.8973388671875, avg loss = 12.8973388671875
client 1, data condensation 3000, total loss = 53.7841796875, avg loss = 53.7841796875
client 1, data condensation 3200, total loss = 18.48248291015625, avg loss = 18.48248291015625
client 1, data condensation 3400, total loss = 233.7811279296875, avg loss = 233.7811279296875
client 1, data condensation 3600, total loss = 17.439453125, avg loss = 17.439453125
client 1, data condensation 3800, total loss = 18.1846923828125, avg loss = 18.1846923828125
client 1, data condensation 4000, total loss = 5.43145751953125, avg loss = 5.43145751953125
client 1, data condensation 4200, total loss = 20.4793701171875, avg loss = 20.4793701171875
client 1, data condensation 4400, total loss = 10.2685546875, avg loss = 10.2685546875
client 1, data condensation 4600, total loss = 31.62420654296875, avg loss = 31.62420654296875
client 1, data condensation 4800, total loss = 2.29937744140625, avg loss = 2.29937744140625
client 1, data condensation 5000, total loss = 9.45294189453125, avg loss = 9.45294189453125
Round 5, client 1 condense time: 206.35250616073608
client 1, class 0 have 9022 samples
total 24576.0MB, used 2835.06MB, free 21740.94MB
total 24576.0MB, used 2835.06MB, free 21740.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [242  13  11   5   5   1   5   9   4  32], bin edged: [0.00275828 0.00293721 0.00311615 0.00329508 0.00347401 0.00365295
 0.00383188 0.00401082 0.00418975 0.00436869 0.00454762]
class 5 have 12176 samples, histogram: [3425  671  475  395  365  345  412  407  640 5041], bin edged: [6.01326903e-05 6.40336258e-05 6.79345614e-05 7.18354969e-05
 7.57364324e-05 7.96373680e-05 8.35383035e-05 8.74392390e-05
 9.13401745e-05 9.52411101e-05 9.91420456e-05]
client 2, data condensation 0, total loss = 228.620849609375, avg loss = 114.3104248046875
client 2, data condensation 200, total loss = 51.94659423828125, avg loss = 25.973297119140625
client 2, data condensation 400, total loss = 70.3603515625, avg loss = 35.18017578125
client 2, data condensation 600, total loss = 55.8580322265625, avg loss = 27.92901611328125
client 2, data condensation 800, total loss = 49.43994140625, avg loss = 24.719970703125
client 2, data condensation 1000, total loss = 66.0267333984375, avg loss = 33.01336669921875
client 2, data condensation 1200, total loss = 73.95574951171875, avg loss = 36.977874755859375
client 2, data condensation 1400, total loss = 33.42913818359375, avg loss = 16.714569091796875
client 2, data condensation 1600, total loss = 54.753021240234375, avg loss = 27.376510620117188
client 2, data condensation 1800, total loss = 65.60311889648438, avg loss = 32.80155944824219
client 2, data condensation 2000, total loss = 48.88702392578125, avg loss = 24.443511962890625
client 2, data condensation 2200, total loss = 152.9722900390625, avg loss = 76.48614501953125
client 2, data condensation 2400, total loss = 73.7425537109375, avg loss = 36.87127685546875
client 2, data condensation 2600, total loss = 93.52142333984375, avg loss = 46.760711669921875
client 2, data condensation 2800, total loss = 25.6876220703125, avg loss = 12.84381103515625
client 2, data condensation 3000, total loss = 57.4371337890625, avg loss = 28.71856689453125
client 2, data condensation 3200, total loss = 53.93011474609375, avg loss = 26.965057373046875
client 2, data condensation 3400, total loss = 41.6605224609375, avg loss = 20.83026123046875
client 2, data condensation 3600, total loss = 36.43768310546875, avg loss = 18.218841552734375
client 2, data condensation 3800, total loss = 56.8870849609375, avg loss = 28.44354248046875
client 2, data condensation 4000, total loss = 42.5721435546875, avg loss = 21.28607177734375
client 2, data condensation 4200, total loss = 65.8775634765625, avg loss = 32.93878173828125
client 2, data condensation 4400, total loss = 40.0074462890625, avg loss = 20.00372314453125
client 2, data condensation 4600, total loss = 49.24700927734375, avg loss = 24.623504638671875
client 2, data condensation 4800, total loss = 49.45611572265625, avg loss = 24.728057861328125
client 2, data condensation 5000, total loss = 60.1058349609375, avg loss = 30.05291748046875
Round 5, client 2 condense time: 363.25076818466187
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3217.06MB, free 21358.94MB
total 24576.0MB, used 3217.06MB, free 21358.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [168  22  13  11   6   7   5  15  14  52], bin edged: [0.00265967 0.0028322  0.00300474 0.00317727 0.00334981 0.00352234
 0.00369488 0.00386741 0.00403995 0.00421248 0.00438502]
client 3, data condensation 0, total loss = 91.47940063476562, avg loss = 91.47940063476562
client 3, data condensation 200, total loss = 57.1343994140625, avg loss = 57.1343994140625
client 3, data condensation 400, total loss = 29.760772705078125, avg loss = 29.760772705078125
client 3, data condensation 600, total loss = 71.62942504882812, avg loss = 71.62942504882812
client 3, data condensation 800, total loss = 39.337432861328125, avg loss = 39.337432861328125
client 3, data condensation 1000, total loss = 35.087738037109375, avg loss = 35.087738037109375
client 3, data condensation 1200, total loss = 67.81939697265625, avg loss = 67.81939697265625
client 3, data condensation 1400, total loss = 153.786865234375, avg loss = 153.786865234375
client 3, data condensation 1600, total loss = 40.362213134765625, avg loss = 40.362213134765625
client 3, data condensation 1800, total loss = 272.0751037597656, avg loss = 272.0751037597656
client 3, data condensation 2000, total loss = 44.456298828125, avg loss = 44.456298828125
client 3, data condensation 2200, total loss = 45.318939208984375, avg loss = 45.318939208984375
client 3, data condensation 2400, total loss = 31.102447509765625, avg loss = 31.102447509765625
client 3, data condensation 2600, total loss = 68.05963134765625, avg loss = 68.05963134765625
client 3, data condensation 2800, total loss = 53.6175537109375, avg loss = 53.6175537109375
client 3, data condensation 3000, total loss = 23.255889892578125, avg loss = 23.255889892578125
client 3, data condensation 3200, total loss = 59.82745361328125, avg loss = 59.82745361328125
client 3, data condensation 3400, total loss = 31.068023681640625, avg loss = 31.068023681640625
client 3, data condensation 3600, total loss = 38.13775634765625, avg loss = 38.13775634765625
client 3, data condensation 3800, total loss = 18.4793701171875, avg loss = 18.4793701171875
client 3, data condensation 4000, total loss = 47.508056640625, avg loss = 47.508056640625
client 3, data condensation 4200, total loss = 207.14312744140625, avg loss = 207.14312744140625
client 3, data condensation 4400, total loss = 45.128753662109375, avg loss = 45.128753662109375
client 3, data condensation 4600, total loss = 28.852081298828125, avg loss = 28.852081298828125
client 3, data condensation 4800, total loss = 51.7852783203125, avg loss = 51.7852783203125
client 3, data condensation 5000, total loss = 29.705718994140625, avg loss = 29.705718994140625
Round 5, client 3 condense time: 122.44378137588501
client 3, class 6 have 313 samples
total 24576.0MB, used 2831.06MB, free 21744.94MB
total 24576.0MB, used 2831.06MB, free 21744.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [309   8   3   0   3   1   2   1   0  34], bin edged: [0.00258079 0.00274821 0.00291563 0.00308305 0.00325047 0.00341789
 0.00358531 0.00375274 0.00392016 0.00408758 0.004255  ]
class 4 have 1048 samples, histogram: [548  67  44  40  28  26  31  38  43 183], bin edged: [0.00079016 0.00084142 0.00089267 0.00094393 0.00099519 0.00104645
 0.00109771 0.00114897 0.00120023 0.00125149 0.00130275]
class 6 have 7572 samples, histogram: [5217  398  261  211  164  145  153  166  191  666], bin edged: [0.00011747 0.0001251  0.00013272 0.00014034 0.00014796 0.00015558
 0.0001632  0.00017082 0.00017844 0.00018606 0.00019368]
client 4, data condensation 0, total loss = 177.8740234375, avg loss = 59.291341145833336
client 4, data condensation 200, total loss = 50.21539306640625, avg loss = 16.73846435546875
client 4, data condensation 400, total loss = 62.24420166015625, avg loss = 20.748067220052082
client 4, data condensation 600, total loss = 80.79244995117188, avg loss = 26.930816650390625
client 4, data condensation 800, total loss = 46.478057861328125, avg loss = 15.492685953776041
client 4, data condensation 1000, total loss = 39.6549072265625, avg loss = 13.218302408854166
client 4, data condensation 1200, total loss = 41.92236328125, avg loss = 13.97412109375
client 4, data condensation 1400, total loss = 51.16131591796875, avg loss = 17.05377197265625
client 4, data condensation 1600, total loss = 53.266876220703125, avg loss = 17.755625406901043
client 4, data condensation 1800, total loss = 226.18194580078125, avg loss = 75.39398193359375
client 4, data condensation 2000, total loss = 47.727264404296875, avg loss = 15.909088134765625
client 4, data condensation 2200, total loss = 209.65554809570312, avg loss = 69.8851826985677
client 4, data condensation 2400, total loss = 44.735931396484375, avg loss = 14.911977132161459
client 4, data condensation 2600, total loss = 48.125457763671875, avg loss = 16.041819254557293
client 4, data condensation 2800, total loss = 38.37811279296875, avg loss = 12.792704264322916
client 4, data condensation 3000, total loss = 42.794525146484375, avg loss = 14.264841715494791
client 4, data condensation 3200, total loss = 27.942047119140625, avg loss = 9.314015706380209
client 4, data condensation 3400, total loss = 73.2386474609375, avg loss = 24.412882486979168
client 4, data condensation 3600, total loss = 37.367889404296875, avg loss = 12.455963134765625
client 4, data condensation 3800, total loss = 61.72222900390625, avg loss = 20.574076334635418
client 4, data condensation 4000, total loss = 43.3306884765625, avg loss = 14.443562825520834
client 4, data condensation 4200, total loss = 402.46759033203125, avg loss = 134.1558634440104
client 4, data condensation 4400, total loss = 98.69134521484375, avg loss = 32.897115071614586
client 4, data condensation 4600, total loss = 119.6171875, avg loss = 39.872395833333336
client 4, data condensation 4800, total loss = 49.042449951171875, avg loss = 16.347483317057293
client 4, data condensation 5000, total loss = 96.9052734375, avg loss = 32.3017578125
Round 5, client 4 condense time: 458.76523184776306
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3475.06MB, free 21100.94MB
total 24576.0MB, used 3475.06MB, free 21100.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [3641  576  356  337  303  323  346  460  653 5156], bin edged: [6.01902303e-05 6.40948986e-05 6.79995668e-05 7.19042351e-05
 7.58089034e-05 7.97135716e-05 8.36182399e-05 8.75229082e-05
 9.14275764e-05 9.53322447e-05 9.92369130e-05]
client 5, data condensation 0, total loss = 131.90814208984375, avg loss = 131.90814208984375
client 5, data condensation 200, total loss = 7.344970703125, avg loss = 7.344970703125
client 5, data condensation 400, total loss = 8.8590087890625, avg loss = 8.8590087890625
client 5, data condensation 600, total loss = 4.553131103515625, avg loss = 4.553131103515625
client 5, data condensation 800, total loss = 79.99932861328125, avg loss = 79.99932861328125
client 5, data condensation 1000, total loss = 44.4119873046875, avg loss = 44.4119873046875
client 5, data condensation 1200, total loss = 12.289764404296875, avg loss = 12.289764404296875
client 5, data condensation 1400, total loss = 10.471954345703125, avg loss = 10.471954345703125
client 5, data condensation 1600, total loss = 7.1590576171875, avg loss = 7.1590576171875
client 5, data condensation 1800, total loss = 6.68865966796875, avg loss = 6.68865966796875
client 5, data condensation 2000, total loss = 9.577362060546875, avg loss = 9.577362060546875
client 5, data condensation 2200, total loss = 10.733154296875, avg loss = 10.733154296875
client 5, data condensation 2400, total loss = 9.99139404296875, avg loss = 9.99139404296875
client 5, data condensation 2600, total loss = 6.425872802734375, avg loss = 6.425872802734375
client 5, data condensation 2800, total loss = 22.77593994140625, avg loss = 22.77593994140625
client 5, data condensation 3000, total loss = 14.039306640625, avg loss = 14.039306640625
client 5, data condensation 3200, total loss = 547.0963134765625, avg loss = 547.0963134765625
client 5, data condensation 3400, total loss = 3.6719970703125, avg loss = 3.6719970703125
client 5, data condensation 3600, total loss = 8.623138427734375, avg loss = 8.623138427734375
client 5, data condensation 3800, total loss = 14.8924560546875, avg loss = 14.8924560546875
client 5, data condensation 4000, total loss = 7.848907470703125, avg loss = 7.848907470703125
client 5, data condensation 4200, total loss = 9.229705810546875, avg loss = 9.229705810546875
client 5, data condensation 4400, total loss = 18.631256103515625, avg loss = 18.631256103515625
client 5, data condensation 4600, total loss = 6.7523193359375, avg loss = 6.7523193359375
client 5, data condensation 4800, total loss = 14.974395751953125, avg loss = 14.974395751953125
client 5, data condensation 5000, total loss = 19.845458984375, avg loss = 19.845458984375
Round 5, client 5 condense time: 240.21902012825012
client 5, class 8 have 12151 samples
total 24576.0MB, used 2963.06MB, free 21612.94MB
total 24576.0MB, used 2963.06MB, free 21612.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [7223  420  236  194  162  162  184  150  233 1230], bin edged: [8.67263513e-05 9.23524742e-05 9.79785971e-05 1.03604720e-04
 1.09230843e-04 1.14856966e-04 1.20483089e-04 1.26109212e-04
 1.31735334e-04 1.37361457e-04 1.42987580e-04]
client 6, data condensation 0, total loss = 171.46499633789062, avg loss = 171.46499633789062
client 6, data condensation 200, total loss = 11.879241943359375, avg loss = 11.879241943359375
client 6, data condensation 400, total loss = 222.97021484375, avg loss = 222.97021484375
client 6, data condensation 600, total loss = 16.04296875, avg loss = 16.04296875
client 6, data condensation 800, total loss = 25.35345458984375, avg loss = 25.35345458984375
client 6, data condensation 1000, total loss = 13.14447021484375, avg loss = 13.14447021484375
client 6, data condensation 1200, total loss = 35.29937744140625, avg loss = 35.29937744140625
client 6, data condensation 1400, total loss = 19.22015380859375, avg loss = 19.22015380859375
client 6, data condensation 1600, total loss = 7.92254638671875, avg loss = 7.92254638671875
client 6, data condensation 1800, total loss = 11.497039794921875, avg loss = 11.497039794921875
client 6, data condensation 2000, total loss = 10.539581298828125, avg loss = 10.539581298828125
client 6, data condensation 2200, total loss = 13.28680419921875, avg loss = 13.28680419921875
client 6, data condensation 2400, total loss = 11.142852783203125, avg loss = 11.142852783203125
client 6, data condensation 2600, total loss = 10.994598388671875, avg loss = 10.994598388671875
client 6, data condensation 2800, total loss = 11.526275634765625, avg loss = 11.526275634765625
client 6, data condensation 3000, total loss = 4.9718017578125, avg loss = 4.9718017578125
client 6, data condensation 3200, total loss = 18.77886962890625, avg loss = 18.77886962890625
client 6, data condensation 3400, total loss = 8.204620361328125, avg loss = 8.204620361328125
client 6, data condensation 3600, total loss = 21.629669189453125, avg loss = 21.629669189453125
client 6, data condensation 3800, total loss = 9.06109619140625, avg loss = 9.06109619140625
client 6, data condensation 4000, total loss = 18.77227783203125, avg loss = 18.77227783203125
client 6, data condensation 4200, total loss = 21.353271484375, avg loss = 21.353271484375
client 6, data condensation 4400, total loss = 38.6739501953125, avg loss = 38.6739501953125
client 6, data condensation 4600, total loss = 20.57501220703125, avg loss = 20.57501220703125
client 6, data condensation 4800, total loss = 26.31195068359375, avg loss = 26.31195068359375
client 6, data condensation 5000, total loss = 16.4473876953125, avg loss = 16.4473876953125
Round 5, client 6 condense time: 218.73836040496826
client 6, class 3 have 10194 samples
total 24576.0MB, used 2967.06MB, free 21608.94MB
total 24576.0MB, used 2967.06MB, free 21608.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [5425  606  579  247  217  172  139  177  173 1377], bin edged: [9.43231648e-05 1.00442101e-04 1.06561038e-04 1.12679974e-04
 1.18798911e-04 1.24917847e-04 1.31036783e-04 1.37155720e-04
 1.43274656e-04 1.49393593e-04 1.55512529e-04]
client 7, data condensation 0, total loss = 196.827880859375, avg loss = 196.827880859375
client 7, data condensation 200, total loss = 13.892242431640625, avg loss = 13.892242431640625
client 7, data condensation 400, total loss = 29.819610595703125, avg loss = 29.819610595703125
client 7, data condensation 600, total loss = 9.6654052734375, avg loss = 9.6654052734375
client 7, data condensation 800, total loss = 87.89389038085938, avg loss = 87.89389038085938
client 7, data condensation 1000, total loss = 7.7760009765625, avg loss = 7.7760009765625
client 7, data condensation 1200, total loss = 9.64141845703125, avg loss = 9.64141845703125
client 7, data condensation 1400, total loss = 4.84381103515625, avg loss = 4.84381103515625
client 7, data condensation 1600, total loss = 13.499755859375, avg loss = 13.499755859375
client 7, data condensation 1800, total loss = 41.2666015625, avg loss = 41.2666015625
client 7, data condensation 2000, total loss = 23.33453369140625, avg loss = 23.33453369140625
client 7, data condensation 2200, total loss = 182.192626953125, avg loss = 182.192626953125
client 7, data condensation 2400, total loss = 16.683319091796875, avg loss = 16.683319091796875
client 7, data condensation 2600, total loss = 12.8271484375, avg loss = 12.8271484375
client 7, data condensation 2800, total loss = 7.43939208984375, avg loss = 7.43939208984375
client 7, data condensation 3000, total loss = 47.61151123046875, avg loss = 47.61151123046875
client 7, data condensation 3200, total loss = 21.821868896484375, avg loss = 21.821868896484375
client 7, data condensation 3400, total loss = 16.115509033203125, avg loss = 16.115509033203125
client 7, data condensation 3600, total loss = 35.136871337890625, avg loss = 35.136871337890625
client 7, data condensation 3800, total loss = 21.714599609375, avg loss = 21.714599609375
client 7, data condensation 4000, total loss = 11.630340576171875, avg loss = 11.630340576171875
client 7, data condensation 4200, total loss = 36.228057861328125, avg loss = 36.228057861328125
client 7, data condensation 4400, total loss = 8.88214111328125, avg loss = 8.88214111328125
client 7, data condensation 4600, total loss = 7.913330078125, avg loss = 7.913330078125
client 7, data condensation 4800, total loss = 11.92633056640625, avg loss = 11.92633056640625
client 7, data condensation 5000, total loss = 9.728271484375, avg loss = 9.728271484375
Round 5, client 7 condense time: 212.66840243339539
client 7, class 1 have 9112 samples
total 24576.0MB, used 2965.06MB, free 21610.94MB
total 24576.0MB, used 2965.06MB, free 21610.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [191   4   0   5   1   0   0   0   1   4], bin edged: [0.00473392 0.00504102 0.00534812 0.00565522 0.00596232 0.00626942
 0.00657652 0.00688362 0.00719072 0.00749782 0.00780492]
class 4 have 1179 samples, histogram: [675  98  42  37  30  24  31  33  42 167], bin edged: [0.00072127 0.00076806 0.00081485 0.00086164 0.00090843 0.00095522
 0.00100201 0.0010488  0.00109559 0.00114238 0.00118917]
class 8 have 734 samples, histogram: [378  35  32  11  11  18  27  24  35 163], bin edged: [0.00110724 0.00117907 0.0012509  0.00132273 0.00139455 0.00146638
 0.00153821 0.00161004 0.00168187 0.0017537  0.00182553]
client 8, data condensation 0, total loss = 199.58123779296875, avg loss = 66.52707926432292
client 8, data condensation 200, total loss = 64.40341186523438, avg loss = 21.467803955078125
client 8, data condensation 400, total loss = 66.9622802734375, avg loss = 22.320760091145832
client 8, data condensation 600, total loss = 47.648162841796875, avg loss = 15.882720947265625
client 8, data condensation 800, total loss = 62.572296142578125, avg loss = 20.857432047526043
client 8, data condensation 1000, total loss = 40.62677001953125, avg loss = 13.542256673177084
client 8, data condensation 1200, total loss = 67.4154052734375, avg loss = 22.4718017578125
client 8, data condensation 1400, total loss = 47.0645751953125, avg loss = 15.688191731770834
client 8, data condensation 1600, total loss = 88.07833862304688, avg loss = 29.359446207682293
client 8, data condensation 1800, total loss = 38.73388671875, avg loss = 12.911295572916666
client 8, data condensation 2000, total loss = 52.6658935546875, avg loss = 17.5552978515625
client 8, data condensation 2200, total loss = 284.65380859375, avg loss = 94.88460286458333
client 8, data condensation 2400, total loss = 71.40365600585938, avg loss = 23.801218668619793
client 8, data condensation 2600, total loss = 66.02532958984375, avg loss = 22.008443196614582
client 8, data condensation 2800, total loss = 55.347198486328125, avg loss = 18.449066162109375
client 8, data condensation 3000, total loss = 205.32821655273438, avg loss = 68.44273885091145
client 8, data condensation 3200, total loss = 288.74725341796875, avg loss = 96.24908447265625
client 8, data condensation 3400, total loss = 54.7032470703125, avg loss = 18.234415690104168
client 8, data condensation 3600, total loss = 57.6749267578125, avg loss = 19.2249755859375
client 8, data condensation 3800, total loss = 57.264495849609375, avg loss = 19.088165283203125
client 8, data condensation 4000, total loss = 32.331756591796875, avg loss = 10.777252197265625
client 8, data condensation 4200, total loss = 58.35565185546875, avg loss = 19.451883951822918
client 8, data condensation 4400, total loss = 57.047515869140625, avg loss = 19.015838623046875
client 8, data condensation 4600, total loss = 203.4930419921875, avg loss = 67.83101399739583
client 8, data condensation 4800, total loss = 42.3807373046875, avg loss = 14.126912434895834
client 8, data condensation 5000, total loss = 57.57415771484375, avg loss = 19.191385904947918
Round 5, client 8 condense time: 452.94129371643066
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3473.06MB, free 21102.94MB
total 24576.0MB, used 3473.06MB, free 21102.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2213  373  255  234  220  227  285  316  495 5698], bin edged: [6.74837770e-05 7.18615932e-05 7.62394093e-05 8.06172255e-05
 8.49950416e-05 8.93728578e-05 9.37506739e-05 9.81284901e-05
 1.02506306e-04 1.06884122e-04 1.11261939e-04]
client 9, data condensation 0, total loss = 90.16940307617188, avg loss = 90.16940307617188
client 9, data condensation 200, total loss = 7.568389892578125, avg loss = 7.568389892578125
client 9, data condensation 400, total loss = 5.24713134765625, avg loss = 5.24713134765625
client 9, data condensation 600, total loss = 6.011260986328125, avg loss = 6.011260986328125
client 9, data condensation 800, total loss = 13.138763427734375, avg loss = 13.138763427734375
client 9, data condensation 1000, total loss = 11.99224853515625, avg loss = 11.99224853515625
client 9, data condensation 1200, total loss = 7.603118896484375, avg loss = 7.603118896484375
client 9, data condensation 1400, total loss = 22.85662841796875, avg loss = 22.85662841796875
client 9, data condensation 1600, total loss = 266.72113037109375, avg loss = 266.72113037109375
client 9, data condensation 1800, total loss = 16.855255126953125, avg loss = 16.855255126953125
client 9, data condensation 2000, total loss = 28.984527587890625, avg loss = 28.984527587890625
client 9, data condensation 2200, total loss = 2.86956787109375, avg loss = 2.86956787109375
client 9, data condensation 2400, total loss = 8.8050537109375, avg loss = 8.8050537109375
client 9, data condensation 2600, total loss = 8.20062255859375, avg loss = 8.20062255859375
client 9, data condensation 2800, total loss = 55.501983642578125, avg loss = 55.501983642578125
client 9, data condensation 3000, total loss = 351.2462158203125, avg loss = 351.2462158203125
client 9, data condensation 3200, total loss = 2.769805908203125, avg loss = 2.769805908203125
client 9, data condensation 3400, total loss = 22.2154541015625, avg loss = 22.2154541015625
client 9, data condensation 3600, total loss = 14.489959716796875, avg loss = 14.489959716796875
client 9, data condensation 3800, total loss = 7.99700927734375, avg loss = 7.99700927734375
client 9, data condensation 4000, total loss = 16.066986083984375, avg loss = 16.066986083984375
client 9, data condensation 4200, total loss = 9.87762451171875, avg loss = 9.87762451171875
client 9, data condensation 4400, total loss = 3.86444091796875, avg loss = 3.86444091796875
client 9, data condensation 4600, total loss = 4.326751708984375, avg loss = 4.326751708984375
client 9, data condensation 4800, total loss = 17.15338134765625, avg loss = 17.15338134765625
client 9, data condensation 5000, total loss = 8.954254150390625, avg loss = 8.954254150390625
Round 5, client 9 condense time: 252.14090967178345
client 9, class 2 have 10316 samples
total 24576.0MB, used 2969.06MB, free 21606.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 20.1764,   3.1478,  -5.4530, -15.3273,   9.0498,   4.8031,   1.7987,
          -7.5353, -10.6446],
        [  1.8583,  14.0506,  -0.0365,  -6.7974,  -0.6528,   2.8987,  -4.7891,
          -2.4186,  -3.1266],
        [ -9.9510,  -8.0498,   8.4921,  -1.8293,  -0.2083,   5.1935,   1.8133,
           5.1659,   0.3386],
        [-11.8944,  -7.7860,  -1.4261,  17.4350,   0.3355,  -1.8305,   6.5243,
           2.3138,  -2.8377],
        [ -1.8127,  -5.0764,  -4.0594,  -3.7764,  10.2956,   0.1593,   7.7405,
          -0.3601,  -2.9207],
        [ -7.3747,  -5.9846,   3.3287,  -6.1764,   1.4353,  10.0551,   0.6512,
           4.4579,   0.4395],
        [ -7.3144,  -8.5351,  -0.9141,   1.4432,   3.2404,  -0.4412,  12.6015,
          -0.2917,   0.5079],
        [-11.3418, -10.2236,   2.2151,   1.6364,   1.9156,   2.6189,   3.2553,
           9.8265,   1.0556],
        [-10.5457,  -7.5918,   0.3974,  -1.4830,   0.6680,   1.1685,   8.4663,
           0.2569,   9.3334]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 0, 2, 4],
        [2, 5, 7, 6, 8],
        [3, 6, 7, 4, 2],
        [4, 6, 5, 7, 0],
        [5, 7, 2, 4, 6],
        [6, 4, 3, 8, 7],
        [7, 6, 5, 2, 4],
        [8, 6, 5, 4, 2]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
6
Round 5: # synthetic sample: 5472
total 24576.0MB, used 2969.06MB, free 21606.94MB
{0: {0: 1315, 1: 5, 2: 0, 3: 0, 4: 7, 5: 7, 6: 4, 7: 0, 8: 0}, 1: {0: 8, 1: 678, 2: 160, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 236, 3: 16, 4: 0, 5: 13, 6: 5, 7: 68, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 573, 4: 1, 5: 1, 6: 56, 7: 0, 8: 3}, 4: {0: 50, 1: 49, 2: 0, 3: 3, 4: 864, 5: 6, 6: 58, 7: 4, 8: 1}, 5: {0: 0, 1: 4, 2: 68, 3: 43, 4: 3, 5: 164, 6: 18, 7: 283, 8: 9}, 6: {0: 11, 1: 0, 2: 1, 3: 10, 4: 24, 5: 13, 6: 677, 7: 0, 8: 5}, 7: {0: 2, 1: 13, 2: 16, 3: 90, 4: 22, 5: 58, 6: 58, 7: 136, 8: 26}, 8: {0: 13, 1: 7, 2: 4, 3: 34, 4: 9, 5: 77, 6: 366, 7: 37, 8: 686}}
round 5 evaluation: test acc is 0.7422, test loss = 1.358720
{0: {0: 1283, 1: 6, 2: 0, 3: 0, 4: 32, 5: 14, 6: 2, 7: 0, 8: 1}, 1: {0: 4, 1: 637, 2: 206, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 213, 3: 11, 4: 0, 5: 107, 6: 0, 7: 5, 8: 3}, 3: {0: 1, 1: 0, 2: 1, 3: 609, 4: 5, 5: 0, 6: 7, 7: 0, 8: 11}, 4: {0: 22, 1: 27, 2: 3, 3: 6, 4: 851, 5: 94, 6: 24, 7: 3, 8: 5}, 5: {0: 20, 1: 1, 2: 221, 3: 12, 4: 2, 5: 68, 6: 1, 7: 231, 8: 36}, 6: {0: 10, 1: 4, 2: 70, 3: 46, 4: 73, 5: 4, 6: 346, 7: 1, 8: 187}, 7: {0: 2, 1: 3, 2: 75, 3: 75, 4: 17, 5: 56, 6: 6, 7: 99, 8: 88}, 8: {0: 11, 1: 22, 2: 35, 3: 33, 4: 8, 5: 13, 6: 27, 7: 19, 8: 1065}}
epoch 0, train loss avg now = 0.186497, train contrast loss now = 1.506041, test acc now = 0.7202, test loss now = 1.559828
{0: {0: 1317, 1: 10, 2: 0, 3: 0, 4: 5, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 6, 1: 822, 2: 19, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 274, 3: 8, 4: 0, 5: 20, 6: 0, 7: 36, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 598, 4: 1, 5: 0, 6: 29, 7: 1, 8: 5}, 4: {0: 59, 1: 111, 2: 0, 3: 64, 4: 740, 5: 4, 6: 49, 7: 4, 8: 4}, 5: {0: 0, 1: 0, 2: 98, 3: 24, 4: 1, 5: 249, 6: 3, 7: 189, 8: 28}, 6: {0: 12, 1: 2, 2: 5, 3: 26, 4: 25, 5: 16, 6: 632, 7: 1, 8: 22}, 7: {0: 2, 1: 7, 2: 22, 3: 93, 4: 3, 5: 80, 6: 37, 7: 137, 8: 40}, 8: {0: 15, 1: 7, 2: 4, 3: 58, 4: 3, 5: 39, 6: 177, 7: 34, 8: 896}}
epoch 100, train loss avg now = 0.045506, train contrast loss now = 1.217111, test acc now = 0.7890, test loss now = 0.996676
{0: {0: 1331, 1: 2, 2: 0, 3: 0, 4: 2, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 11, 1: 638, 2: 36, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 162}, 2: {0: 0, 1: 14, 2: 294, 3: 9, 4: 0, 5: 14, 6: 1, 7: 7, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 601, 4: 3, 5: 1, 6: 24, 7: 0, 8: 4}, 4: {0: 82, 1: 56, 2: 0, 3: 7, 4: 846, 5: 13, 6: 29, 7: 1, 8: 1}, 5: {0: 0, 1: 39, 2: 262, 3: 22, 4: 3, 5: 149, 6: 4, 7: 106, 8: 7}, 6: {0: 17, 1: 2, 2: 7, 3: 23, 4: 23, 5: 4, 6: 654, 7: 0, 8: 11}, 7: {0: 2, 1: 2, 2: 77, 3: 105, 4: 24, 5: 73, 6: 59, 7: 57, 8: 22}, 8: {0: 19, 1: 3, 2: 23, 3: 58, 4: 9, 5: 30, 6: 307, 7: 14, 8: 770}}
epoch 200, train loss avg now = 0.059952, train contrast loss now = 1.213625, test acc now = 0.7437, test loss now = 1.203904
{0: {0: 1328, 1: 1, 2: 0, 3: 0, 4: 4, 5: 4, 6: 1, 7: 0, 8: 0}, 1: {0: 7, 1: 832, 2: 7, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 49, 2: 250, 3: 4, 4: 0, 5: 14, 6: 3, 7: 19, 8: 0}, 3: {0: 2, 1: 0, 2: 1, 3: 525, 4: 0, 5: 3, 6: 102, 7: 0, 8: 1}, 4: {0: 49, 1: 48, 2: 0, 3: 2, 4: 901, 5: 8, 6: 25, 7: 2, 8: 0}, 5: {0: 0, 1: 11, 2: 125, 3: 4, 4: 2, 5: 228, 6: 12, 7: 208, 8: 2}, 6: {0: 25, 1: 1, 2: 3, 3: 3, 4: 23, 5: 11, 6: 675, 7: 0, 8: 0}, 7: {0: 2, 1: 2, 2: 44, 3: 54, 4: 20, 5: 88, 6: 70, 7: 134, 8: 7}, 8: {0: 20, 1: 1, 2: 18, 3: 39, 4: 11, 5: 92, 6: 456, 7: 49, 8: 547}}
epoch 300, train loss avg now = 0.044553, train contrast loss now = 1.213203, test acc now = 0.7549, test loss now = 1.300419
{0: {0: 1319, 1: 6, 2: 0, 3: 0, 4: 7, 5: 4, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 840, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 21, 2: 278, 3: 14, 4: 0, 5: 17, 6: 1, 7: 8, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 577, 4: 0, 5: 2, 6: 51, 7: 0, 8: 4}, 4: {0: 81, 1: 65, 2: 0, 3: 1, 4: 843, 5: 11, 6: 30, 7: 2, 8: 2}, 5: {0: 0, 1: 1, 2: 112, 3: 39, 4: 2, 5: 286, 6: 8, 7: 133, 8: 11}, 6: {0: 16, 1: 1, 2: 0, 3: 14, 4: 24, 5: 15, 6: 663, 7: 0, 8: 8}, 7: {0: 2, 1: 18, 2: 38, 3: 82, 4: 17, 5: 76, 6: 65, 7: 93, 8: 30}, 8: {0: 11, 1: 3, 2: 4, 3: 45, 4: 11, 5: 59, 6: 290, 7: 13, 8: 797}}
epoch 400, train loss avg now = 0.027081, train contrast loss now = 1.213537, test acc now = 0.7933, test loss now = 1.080690
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1318, 1: 5, 2: 0, 3: 0, 4: 9, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 25, 1: 820, 2: 0, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 6, 1: 0, 2: 289, 3: 5, 4: 0, 5: 13, 6: 4, 7: 22, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 565, 4: 0, 5: 1, 6: 65, 7: 0, 8: 3}, 4: {0: 39, 1: 58, 2: 0, 3: 9, 4: 858, 5: 7, 6: 60, 7: 3, 8: 1}, 5: {0: 0, 1: 35, 2: 126, 3: 15, 4: 3, 5: 161, 6: 9, 7: 231, 8: 12}, 6: {0: 12, 1: 1, 2: 3, 3: 11, 4: 29, 5: 7, 6: 676, 7: 0, 8: 2}, 7: {0: 2, 1: 3, 2: 31, 3: 74, 4: 10, 5: 67, 6: 67, 7: 140, 8: 27}, 8: {0: 11, 1: 4, 2: 5, 3: 38, 4: 8, 5: 53, 6: 393, 7: 24, 8: 697}}
epoch 500, train loss avg now = 0.031948, train contrast loss now = 1.215641, test acc now = 0.7694, test loss now = 1.263374
{0: {0: 1321, 1: 4, 2: 0, 3: 0, 4: 6, 5: 2, 6: 5, 7: 0, 8: 0}, 1: {0: 15, 1: 829, 2: 3, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 267, 3: 14, 4: 0, 5: 14, 6: 8, 7: 32, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 573, 4: 0, 5: 0, 6: 57, 7: 0, 8: 4}, 4: {0: 36, 1: 47, 2: 0, 3: 8, 4: 893, 5: 4, 6: 43, 7: 2, 8: 2}, 5: {0: 0, 1: 19, 2: 59, 3: 21, 4: 3, 5: 188, 6: 17, 7: 264, 8: 21}, 6: {0: 8, 1: 1, 2: 1, 3: 13, 4: 26, 5: 7, 6: 678, 7: 0, 8: 7}, 7: {0: 2, 1: 11, 2: 12, 3: 99, 4: 16, 5: 53, 6: 63, 7: 135, 8: 30}, 8: {0: 13, 1: 1, 2: 0, 3: 46, 4: 7, 5: 25, 6: 342, 7: 32, 8: 767}}
epoch 600, train loss avg now = 0.023823, train contrast loss now = 1.214831, test acc now = 0.7870, test loss now = 1.146563
{0: {0: 1322, 1: 5, 2: 0, 3: 0, 4: 5, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 6, 1: 703, 2: 15, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 123}, 2: {0: 0, 1: 1, 2: 272, 3: 12, 4: 0, 5: 13, 6: 5, 7: 36, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 561, 4: 0, 5: 1, 6: 69, 7: 0, 8: 3}, 4: {0: 37, 1: 57, 2: 0, 3: 6, 4: 881, 5: 5, 6: 46, 7: 2, 8: 1}, 5: {0: 0, 1: 2, 2: 82, 3: 14, 4: 3, 5: 209, 6: 9, 7: 264, 8: 9}, 6: {0: 12, 1: 1, 2: 2, 3: 10, 4: 29, 5: 7, 6: 674, 7: 0, 8: 6}, 7: {0: 2, 1: 14, 2: 23, 3: 73, 4: 12, 5: 59, 6: 61, 7: 154, 8: 23}, 8: {0: 12, 1: 3, 2: 2, 3: 43, 4: 7, 5: 41, 6: 378, 7: 30, 8: 717}}
epoch 700, train loss avg now = 0.013540, train contrast loss now = 1.212061, test acc now = 0.7650, test loss now = 1.258844
{0: {0: 1322, 1: 5, 2: 0, 3: 0, 4: 5, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 6, 1: 754, 2: 19, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 68}, 2: {0: 0, 1: 0, 2: 272, 3: 13, 4: 0, 5: 13, 6: 7, 7: 33, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 566, 4: 1, 5: 1, 6: 62, 7: 0, 8: 4}, 4: {0: 43, 1: 48, 2: 0, 3: 6, 4: 887, 5: 5, 6: 42, 7: 2, 8: 2}, 5: {0: 0, 1: 4, 2: 83, 3: 17, 4: 3, 5: 214, 6: 10, 7: 250, 8: 11}, 6: {0: 18, 1: 1, 2: 2, 3: 11, 4: 27, 5: 7, 6: 669, 7: 0, 8: 6}, 7: {0: 2, 1: 13, 2: 16, 3: 76, 4: 13, 5: 64, 6: 69, 7: 144, 8: 24}, 8: {0: 16, 1: 3, 2: 4, 3: 43, 4: 7, 5: 33, 6: 371, 7: 25, 8: 731}}
epoch 800, train loss avg now = 0.007181, train contrast loss now = 1.215909, test acc now = 0.7742, test loss now = 1.233717
{0: {0: 1322, 1: 5, 2: 0, 3: 0, 4: 5, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 6, 1: 661, 2: 22, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 158}, 2: {0: 0, 1: 0, 2: 265, 3: 13, 4: 0, 5: 13, 6: 10, 7: 37, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 566, 4: 1, 5: 1, 6: 63, 7: 0, 8: 3}, 4: {0: 45, 1: 49, 2: 0, 3: 3, 4: 885, 5: 6, 6: 43, 7: 2, 8: 2}, 5: {0: 0, 1: 7, 2: 76, 3: 16, 4: 3, 5: 194, 6: 10, 7: 275, 8: 11}, 6: {0: 13, 1: 1, 2: 3, 3: 11, 4: 29, 5: 6, 6: 674, 7: 0, 8: 4}, 7: {0: 2, 1: 15, 2: 15, 3: 77, 4: 14, 5: 57, 6: 72, 7: 148, 8: 21}, 8: {0: 15, 1: 2, 2: 2, 3: 44, 4: 9, 5: 32, 6: 404, 7: 34, 8: 691}}
epoch 900, train loss avg now = 0.011969, train contrast loss now = 1.212994, test acc now = 0.7529, test loss now = 1.325969
{0: {0: 1314, 1: 5, 2: 0, 3: 0, 4: 11, 5: 2, 6: 6, 7: 0, 8: 0}, 1: {0: 15, 1: 774, 2: 18, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 40}, 2: {0: 0, 1: 0, 2: 265, 3: 9, 4: 0, 5: 15, 6: 6, 7: 44, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 561, 4: 0, 5: 0, 6: 69, 7: 0, 8: 4}, 4: {0: 29, 1: 49, 2: 0, 3: 1, 4: 912, 5: 4, 6: 37, 7: 2, 8: 1}, 5: {0: 0, 1: 4, 2: 54, 3: 10, 4: 3, 5: 194, 6: 15, 7: 300, 8: 12}, 6: {0: 6, 1: 1, 2: 1, 3: 10, 4: 27, 5: 7, 6: 681, 7: 0, 8: 8}, 7: {0: 2, 1: 14, 2: 12, 3: 74, 4: 18, 5: 57, 6: 53, 7: 167, 8: 24}, 8: {0: 11, 1: 2, 2: 0, 3: 37, 4: 12, 5: 30, 6: 360, 7: 50, 8: 731}}
epoch 1000, train loss avg now = 0.014735, train contrast loss now = 1.214007, test acc now = 0.7798, test loss now = 1.194000
epoch avg loss = 1.473494901251026e-05, total time = 5981.532301187515
total 24576.0MB, used 3609.06MB, free 20966.94MB
Round 5 finish, update the prev_syn_proto
torch.Size([576, 3, 28, 28])
torch.Size([582, 3, 28, 28])
torch.Size([624, 3, 28, 28])
torch.Size([642, 3, 28, 28])
torch.Size([486, 3, 28, 28])
torch.Size([732, 3, 28, 28])
torch.Size([486, 3, 28, 28])
torch.Size([564, 3, 28, 28])
torch.Size([780, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1314, 1: 5, 2: 0, 3: 0, 4: 11, 5: 2, 6: 6, 7: 0, 8: 0}, 1: {0: 15, 1: 774, 2: 18, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 40}, 2: {0: 0, 1: 0, 2: 265, 3: 9, 4: 0, 5: 15, 6: 6, 7: 44, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 561, 4: 0, 5: 0, 6: 69, 7: 0, 8: 4}, 4: {0: 29, 1: 49, 2: 0, 3: 1, 4: 912, 5: 4, 6: 37, 7: 2, 8: 1}, 5: {0: 0, 1: 4, 2: 54, 3: 10, 4: 3, 5: 194, 6: 15, 7: 300, 8: 12}, 6: {0: 6, 1: 1, 2: 1, 3: 10, 4: 27, 5: 7, 6: 681, 7: 0, 8: 8}, 7: {0: 2, 1: 14, 2: 12, 3: 74, 4: 18, 5: 57, 6: 53, 7: 167, 8: 24}, 8: {0: 11, 1: 2, 2: 0, 3: 37, 4: 12, 5: 30, 6: 360, 7: 50, 8: 731}}
round 5 evaluation: test acc is 0.7798, test loss = 1.194000
 ====== round 6 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3609.06MB, free 20966.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [3321  402  243  150  141  158  128  166  195  873], bin edged: [0.00014675 0.00015627 0.00016579 0.00017531 0.00018483 0.00019435
 0.00020387 0.00021339 0.00022291 0.00023243 0.00024195]
class 7 have 9330 samples, histogram: [4315  552  371  290  258  250  234  287  402 2371], bin edged: [8.55457966e-05 9.10953310e-05 9.66448654e-05 1.02194400e-04
 1.07743934e-04 1.13293469e-04 1.18843003e-04 1.24392538e-04
 1.29942072e-04 1.35491606e-04 1.41041141e-04]
client 0, data condensation 0, total loss = 107.70849609375, avg loss = 53.854248046875
client 0, data condensation 200, total loss = 13.347747802734375, avg loss = 6.6738739013671875
client 0, data condensation 400, total loss = 21.621002197265625, avg loss = 10.810501098632812
client 0, data condensation 600, total loss = 24.427886962890625, avg loss = 12.213943481445312
client 0, data condensation 800, total loss = 16.591766357421875, avg loss = 8.295883178710938
client 0, data condensation 1000, total loss = 265.544921875, avg loss = 132.7724609375
client 0, data condensation 1200, total loss = 22.15350341796875, avg loss = 11.076751708984375
client 0, data condensation 1400, total loss = 31.84283447265625, avg loss = 15.921417236328125
client 0, data condensation 1600, total loss = 12.9361572265625, avg loss = 6.46807861328125
client 0, data condensation 1800, total loss = 40.409576416015625, avg loss = 20.204788208007812
client 0, data condensation 2000, total loss = 15.106170654296875, avg loss = 7.5530853271484375
client 0, data condensation 2200, total loss = 14.6444091796875, avg loss = 7.32220458984375
client 0, data condensation 2400, total loss = 11.343292236328125, avg loss = 5.6716461181640625
client 0, data condensation 2600, total loss = 78.123779296875, avg loss = 39.0618896484375
client 0, data condensation 2800, total loss = 101.97134399414062, avg loss = 50.98567199707031
client 0, data condensation 3000, total loss = 26.8348388671875, avg loss = 13.41741943359375
client 0, data condensation 3200, total loss = 48.63067626953125, avg loss = 24.315338134765625
client 0, data condensation 3400, total loss = 15.62579345703125, avg loss = 7.812896728515625
client 0, data condensation 3600, total loss = 22.7974853515625, avg loss = 11.39874267578125
client 0, data condensation 3800, total loss = 16.546875, avg loss = 8.2734375
client 0, data condensation 4000, total loss = 14.798736572265625, avg loss = 7.3993682861328125
client 0, data condensation 4200, total loss = 57.645172119140625, avg loss = 28.822586059570312
client 0, data condensation 4400, total loss = 247.414794921875, avg loss = 123.7073974609375
client 0, data condensation 4600, total loss = 103.283447265625, avg loss = 51.6417236328125
client 0, data condensation 4800, total loss = 9.310089111328125, avg loss = 4.6550445556640625
client 0, data condensation 5000, total loss = 44.0966796875, avg loss = 22.04833984375
Round 6, client 0 condense time: 418.2719056606293
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3229.06MB, free 21346.94MB
total 24576.0MB, used 3229.06MB, free 21346.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [5729  505  266  211  205  160  167  194  262 1323], bin edged: [9.54540545e-05 1.01646362e-04 1.07838670e-04 1.14030977e-04
 1.20223285e-04 1.26415592e-04 1.32607900e-04 1.38800207e-04
 1.44992515e-04 1.51184823e-04 1.57377130e-04]
client 1, data condensation 0, total loss = 355.34588623046875, avg loss = 355.34588623046875
client 1, data condensation 200, total loss = 21.99603271484375, avg loss = 21.99603271484375
client 1, data condensation 400, total loss = 174.42681884765625, avg loss = 174.42681884765625
client 1, data condensation 600, total loss = 11.499755859375, avg loss = 11.499755859375
client 1, data condensation 800, total loss = 23.19525146484375, avg loss = 23.19525146484375
client 1, data condensation 1000, total loss = 155.94342041015625, avg loss = 155.94342041015625
client 1, data condensation 1200, total loss = 12.44818115234375, avg loss = 12.44818115234375
client 1, data condensation 1400, total loss = 22.06805419921875, avg loss = 22.06805419921875
client 1, data condensation 1600, total loss = 29.40521240234375, avg loss = 29.40521240234375
client 1, data condensation 1800, total loss = 16.56298828125, avg loss = 16.56298828125
client 1, data condensation 2000, total loss = 13.12823486328125, avg loss = 13.12823486328125
client 1, data condensation 2200, total loss = 32.43804931640625, avg loss = 32.43804931640625
client 1, data condensation 2400, total loss = 21.70599365234375, avg loss = 21.70599365234375
client 1, data condensation 2600, total loss = 15.5997314453125, avg loss = 15.5997314453125
client 1, data condensation 2800, total loss = 7.633056640625, avg loss = 7.633056640625
client 1, data condensation 3000, total loss = 19.3802490234375, avg loss = 19.3802490234375
client 1, data condensation 3200, total loss = 40.42047119140625, avg loss = 40.42047119140625
client 1, data condensation 3400, total loss = 22.14129638671875, avg loss = 22.14129638671875
client 1, data condensation 3600, total loss = 12.55694580078125, avg loss = 12.55694580078125
client 1, data condensation 3800, total loss = 18.59588623046875, avg loss = 18.59588623046875
client 1, data condensation 4000, total loss = 15.7860107421875, avg loss = 15.7860107421875
client 1, data condensation 4200, total loss = 18.50250244140625, avg loss = 18.50250244140625
client 1, data condensation 4400, total loss = 15.07293701171875, avg loss = 15.07293701171875
client 1, data condensation 4600, total loss = 18.90106201171875, avg loss = 18.90106201171875
client 1, data condensation 4800, total loss = 22.2313232421875, avg loss = 22.2313232421875
client 1, data condensation 5000, total loss = 16.7039794921875, avg loss = 16.7039794921875
Round 6, client 1 condense time: 246.25497341156006
client 1, class 0 have 9022 samples
total 24576.0MB, used 2845.06MB, free 21730.94MB
total 24576.0MB, used 2845.06MB, free 21730.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [241   9   8   6   6   4   5   2  10  36], bin edged: [0.00273155 0.00290875 0.00308595 0.00326315 0.00344035 0.00361755
 0.00379475 0.00397195 0.00414916 0.00432636 0.00450356]
class 5 have 12176 samples, histogram: [4321  694  435  346  292  317  298  385  566 4522], bin edged: [6.18871319e-05 6.59018818e-05 6.99166317e-05 7.39313815e-05
 7.79461314e-05 8.19608813e-05 8.59756312e-05 8.99903811e-05
 9.40051310e-05 9.80198808e-05 1.02034631e-04]
client 2, data condensation 0, total loss = 235.52774047851562, avg loss = 117.76387023925781
client 2, data condensation 200, total loss = 32.81689453125, avg loss = 16.408447265625
client 2, data condensation 400, total loss = 67.51641845703125, avg loss = 33.758209228515625
client 2, data condensation 600, total loss = 78.5047607421875, avg loss = 39.25238037109375
client 2, data condensation 800, total loss = 30.407470703125, avg loss = 15.2037353515625
client 2, data condensation 1000, total loss = 65.59039306640625, avg loss = 32.795196533203125
client 2, data condensation 1200, total loss = 327.3309020996094, avg loss = 163.6654510498047
client 2, data condensation 1400, total loss = 70.57574462890625, avg loss = 35.287872314453125
client 2, data condensation 1600, total loss = 49.9739990234375, avg loss = 24.98699951171875
client 2, data condensation 1800, total loss = 39.16986083984375, avg loss = 19.584930419921875
client 2, data condensation 2000, total loss = 290.86602783203125, avg loss = 145.43301391601562
client 2, data condensation 2200, total loss = 90.3665771484375, avg loss = 45.18328857421875
client 2, data condensation 2400, total loss = 58.11248779296875, avg loss = 29.056243896484375
client 2, data condensation 2600, total loss = 35.61077880859375, avg loss = 17.805389404296875
client 2, data condensation 2800, total loss = 44.01617431640625, avg loss = 22.008087158203125
client 2, data condensation 3000, total loss = 47.14227294921875, avg loss = 23.571136474609375
client 2, data condensation 3200, total loss = 61.54510498046875, avg loss = 30.772552490234375
client 2, data condensation 3400, total loss = 40.55047607421875, avg loss = 20.275238037109375
client 2, data condensation 3600, total loss = 63.06103515625, avg loss = 31.530517578125
client 2, data condensation 3800, total loss = 43.89996337890625, avg loss = 21.949981689453125
client 2, data condensation 4000, total loss = 65.85467529296875, avg loss = 32.927337646484375
client 2, data condensation 4200, total loss = 50.8682861328125, avg loss = 25.43414306640625
client 2, data condensation 4400, total loss = 45.43310546875, avg loss = 22.716552734375
client 2, data condensation 4600, total loss = 201.44403076171875, avg loss = 100.72201538085938
client 2, data condensation 4800, total loss = 117.40167236328125, avg loss = 58.700836181640625
client 2, data condensation 5000, total loss = 39.4619140625, avg loss = 19.73095703125
Round 6, client 2 condense time: 415.6329894065857
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3229.06MB, free 21346.94MB
total 24576.0MB, used 3229.06MB, free 21346.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [153  17  17   9   5  14   3  15  15  65], bin edged: [0.00258813 0.00275603 0.00292392 0.00309182 0.00325972 0.00342761
 0.00359551 0.00376341 0.0039313  0.0040992  0.00426709]
client 3, data condensation 0, total loss = 85.8564453125, avg loss = 85.8564453125
client 3, data condensation 200, total loss = 35.2913818359375, avg loss = 35.2913818359375
client 3, data condensation 400, total loss = 39.41107177734375, avg loss = 39.41107177734375
client 3, data condensation 600, total loss = 128.33377075195312, avg loss = 128.33377075195312
client 3, data condensation 800, total loss = 48.767913818359375, avg loss = 48.767913818359375
client 3, data condensation 1000, total loss = 62.5799560546875, avg loss = 62.5799560546875
client 3, data condensation 1200, total loss = 39.4288330078125, avg loss = 39.4288330078125
client 3, data condensation 1400, total loss = 279.86688232421875, avg loss = 279.86688232421875
client 3, data condensation 1600, total loss = 50.27093505859375, avg loss = 50.27093505859375
client 3, data condensation 1800, total loss = 49.732513427734375, avg loss = 49.732513427734375
client 3, data condensation 2000, total loss = 45.7677001953125, avg loss = 45.7677001953125
client 3, data condensation 2200, total loss = 38.487213134765625, avg loss = 38.487213134765625
client 3, data condensation 2400, total loss = 55.309814453125, avg loss = 55.309814453125
client 3, data condensation 2600, total loss = 28.290985107421875, avg loss = 28.290985107421875
client 3, data condensation 2800, total loss = 222.7718505859375, avg loss = 222.7718505859375
client 3, data condensation 3000, total loss = 32.538818359375, avg loss = 32.538818359375
client 3, data condensation 3200, total loss = 32.73028564453125, avg loss = 32.73028564453125
client 3, data condensation 3400, total loss = 47.201019287109375, avg loss = 47.201019287109375
client 3, data condensation 3600, total loss = 268.09100341796875, avg loss = 268.09100341796875
client 3, data condensation 3800, total loss = 113.45956420898438, avg loss = 113.45956420898438
client 3, data condensation 4000, total loss = 92.48724365234375, avg loss = 92.48724365234375
client 3, data condensation 4200, total loss = 90.00271606445312, avg loss = 90.00271606445312
client 3, data condensation 4400, total loss = 34.2901611328125, avg loss = 34.2901611328125
client 3, data condensation 4600, total loss = 29.20562744140625, avg loss = 29.20562744140625
client 3, data condensation 4800, total loss = 52.58123779296875, avg loss = 52.58123779296875
client 3, data condensation 5000, total loss = 117.09982299804688, avg loss = 117.09982299804688
Round 6, client 3 condense time: 164.39194083213806
client 3, class 6 have 313 samples
total 24576.0MB, used 2841.06MB, free 21734.94MB
total 24576.0MB, used 2841.06MB, free 21734.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [312   6   1   1   1   0   4   0   3  33], bin edged: [0.00258138 0.00274883 0.00291629 0.00308375 0.00325121 0.00341867
 0.00358613 0.00375359 0.00392105 0.00408851 0.00425597]
class 4 have 1048 samples, histogram: [587  72  49  39  23  23  21  27  34 173], bin edged: [0.00080452 0.00085671 0.0009089  0.00096109 0.00101328 0.00106547
 0.00111766 0.00116985 0.00122204 0.00127423 0.00132642]
class 6 have 7572 samples, histogram: [5195  429  251  177  145  146  147  165  199  718], bin edged: [0.00011719 0.00012479 0.00013239 0.00013999 0.0001476  0.0001552
 0.0001628  0.0001704  0.00017801 0.00018561 0.00019321]
client 4, data condensation 0, total loss = 150.01412963867188, avg loss = 50.00470987955729
client 4, data condensation 200, total loss = 45.51837158203125, avg loss = 15.17279052734375
client 4, data condensation 400, total loss = 154.49237060546875, avg loss = 51.497456868489586
client 4, data condensation 600, total loss = 74.9561767578125, avg loss = 24.985392252604168
client 4, data condensation 800, total loss = 52.583892822265625, avg loss = 17.527964274088543
client 4, data condensation 1000, total loss = 57.115234375, avg loss = 19.038411458333332
client 4, data condensation 1200, total loss = 60.9281005859375, avg loss = 20.309366861979168
client 4, data condensation 1400, total loss = 88.002685546875, avg loss = 29.334228515625
client 4, data condensation 1600, total loss = 60.07550048828125, avg loss = 20.025166829427082
client 4, data condensation 1800, total loss = 47.772705078125, avg loss = 15.924235026041666
client 4, data condensation 2000, total loss = 44.79168701171875, avg loss = 14.930562337239584
client 4, data condensation 2200, total loss = 43.02886962890625, avg loss = 14.34295654296875
client 4, data condensation 2400, total loss = 38.41400146484375, avg loss = 12.804667154947916
client 4, data condensation 2600, total loss = 47.09320068359375, avg loss = 15.697733561197916
client 4, data condensation 2800, total loss = 37.3214111328125, avg loss = 12.440470377604166
client 4, data condensation 3000, total loss = 35.72064208984375, avg loss = 11.906880696614584
client 4, data condensation 3200, total loss = 252.5318603515625, avg loss = 84.17728678385417
client 4, data condensation 3400, total loss = 46.281524658203125, avg loss = 15.427174886067709
client 4, data condensation 3600, total loss = 61.529571533203125, avg loss = 20.509857177734375
client 4, data condensation 3800, total loss = 103.2786865234375, avg loss = 34.426228841145836
client 4, data condensation 4000, total loss = 116.09619140625, avg loss = 38.69873046875
client 4, data condensation 4200, total loss = 52.6763916015625, avg loss = 17.558797200520832
client 4, data condensation 4400, total loss = 44.06103515625, avg loss = 14.68701171875
client 4, data condensation 4600, total loss = 118.51406860351562, avg loss = 39.50468953450521
client 4, data condensation 4800, total loss = 53.698974609375, avg loss = 17.899658203125
client 4, data condensation 5000, total loss = 217.77774047851562, avg loss = 72.5925801595052
Round 6, client 4 condense time: 522.3207921981812
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3485.06MB, free 21090.94MB
total 24576.0MB, used 3485.06MB, free 21090.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [3938  608  378  294  277  289  335  423  623 4986], bin edged: [6.08408293e-05 6.47877034e-05 6.87345774e-05 7.26814514e-05
 7.66283254e-05 8.05751994e-05 8.45220734e-05 8.84689474e-05
 9.24158214e-05 9.63626955e-05 1.00309569e-04]
client 5, data condensation 0, total loss = 90.320068359375, avg loss = 90.320068359375
client 5, data condensation 200, total loss = 19.718414306640625, avg loss = 19.718414306640625
client 5, data condensation 400, total loss = 5.582183837890625, avg loss = 5.582183837890625
client 5, data condensation 600, total loss = 7.109527587890625, avg loss = 7.109527587890625
client 5, data condensation 800, total loss = 4.535064697265625, avg loss = 4.535064697265625
client 5, data condensation 1000, total loss = 6.600341796875, avg loss = 6.600341796875
client 5, data condensation 1200, total loss = 169.6536865234375, avg loss = 169.6536865234375
client 5, data condensation 1400, total loss = 8.2076416015625, avg loss = 8.2076416015625
client 5, data condensation 1600, total loss = 10.115234375, avg loss = 10.115234375
client 5, data condensation 1800, total loss = 6.19793701171875, avg loss = 6.19793701171875
client 5, data condensation 2000, total loss = 5.59600830078125, avg loss = 5.59600830078125
client 5, data condensation 2200, total loss = 8.602081298828125, avg loss = 8.602081298828125
client 5, data condensation 2400, total loss = 15.19415283203125, avg loss = 15.19415283203125
client 5, data condensation 2600, total loss = 13.310546875, avg loss = 13.310546875
client 5, data condensation 2800, total loss = 129.45892333984375, avg loss = 129.45892333984375
client 5, data condensation 3000, total loss = 6.407196044921875, avg loss = 6.407196044921875
client 5, data condensation 3200, total loss = 13.3331298828125, avg loss = 13.3331298828125
client 5, data condensation 3400, total loss = 168.59762573242188, avg loss = 168.59762573242188
client 5, data condensation 3600, total loss = 6.462310791015625, avg loss = 6.462310791015625
client 5, data condensation 3800, total loss = 7.000091552734375, avg loss = 7.000091552734375
client 5, data condensation 4000, total loss = 17.66949462890625, avg loss = 17.66949462890625
client 5, data condensation 4200, total loss = 14.422576904296875, avg loss = 14.422576904296875
client 5, data condensation 4400, total loss = 9.05511474609375, avg loss = 9.05511474609375
client 5, data condensation 4600, total loss = 8.1693115234375, avg loss = 8.1693115234375
client 5, data condensation 4800, total loss = 18.3720703125, avg loss = 18.3720703125
client 5, data condensation 5000, total loss = 8.33831787109375, avg loss = 8.33831787109375
Round 6, client 5 condense time: 297.22597670555115
client 5, class 8 have 12151 samples
total 24576.0MB, used 2973.06MB, free 21602.94MB
total 24576.0MB, used 2973.06MB, free 21602.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [6556  487  273  223  171  192  177  204  274 1637], bin edged: [8.42457161e-05 8.97109149e-05 9.51761137e-05 1.00641312e-04
 1.06106511e-04 1.11571710e-04 1.17036909e-04 1.22502108e-04
 1.27967306e-04 1.33432505e-04 1.38897704e-04]
client 6, data condensation 0, total loss = 131.0146484375, avg loss = 131.0146484375
client 6, data condensation 200, total loss = 24.625030517578125, avg loss = 24.625030517578125
client 6, data condensation 400, total loss = 11.560699462890625, avg loss = 11.560699462890625
client 6, data condensation 600, total loss = 44.862579345703125, avg loss = 44.862579345703125
client 6, data condensation 800, total loss = 19.3839111328125, avg loss = 19.3839111328125
client 6, data condensation 1000, total loss = 16.73388671875, avg loss = 16.73388671875
client 6, data condensation 1200, total loss = 16.80499267578125, avg loss = 16.80499267578125
client 6, data condensation 1400, total loss = 13.900360107421875, avg loss = 13.900360107421875
client 6, data condensation 1600, total loss = 12.21649169921875, avg loss = 12.21649169921875
client 6, data condensation 1800, total loss = 16.0499267578125, avg loss = 16.0499267578125
client 6, data condensation 2000, total loss = 15.90728759765625, avg loss = 15.90728759765625
client 6, data condensation 2200, total loss = 16.16259765625, avg loss = 16.16259765625
client 6, data condensation 2400, total loss = 20.586090087890625, avg loss = 20.586090087890625
client 6, data condensation 2600, total loss = 32.3399658203125, avg loss = 32.3399658203125
client 6, data condensation 2800, total loss = 3.74298095703125, avg loss = 3.74298095703125
client 6, data condensation 3000, total loss = 18.06524658203125, avg loss = 18.06524658203125
client 6, data condensation 3200, total loss = 7.63909912109375, avg loss = 7.63909912109375
client 6, data condensation 3400, total loss = 11.178558349609375, avg loss = 11.178558349609375
client 6, data condensation 3600, total loss = 13.82489013671875, avg loss = 13.82489013671875
client 6, data condensation 3800, total loss = 5.53076171875, avg loss = 5.53076171875
client 6, data condensation 4000, total loss = 15.70703125, avg loss = 15.70703125
client 6, data condensation 4200, total loss = 28.355987548828125, avg loss = 28.355987548828125
client 6, data condensation 4400, total loss = 30.48748779296875, avg loss = 30.48748779296875
client 6, data condensation 4600, total loss = 10.6248779296875, avg loss = 10.6248779296875
client 6, data condensation 4800, total loss = 15.68634033203125, avg loss = 15.68634033203125
client 6, data condensation 5000, total loss = 12.54119873046875, avg loss = 12.54119873046875
Round 6, client 6 condense time: 276.66538524627686
client 6, class 3 have 10194 samples
total 24576.0MB, used 2977.06MB, free 21598.94MB
total 24576.0MB, used 2977.06MB, free 21598.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [6259  375  179  132   97   92   99   99  135 1645], bin edged: [9.47981064e-05 1.00947847e-04 1.07097587e-04 1.13247328e-04
 1.19397068e-04 1.25546808e-04 1.31696549e-04 1.37846289e-04
 1.43996029e-04 1.50145770e-04 1.56295510e-04]
client 7, data condensation 0, total loss = 174.67913818359375, avg loss = 174.67913818359375
client 7, data condensation 200, total loss = 11.86968994140625, avg loss = 11.86968994140625
client 7, data condensation 400, total loss = 15.4755859375, avg loss = 15.4755859375
client 7, data condensation 600, total loss = 19.68634033203125, avg loss = 19.68634033203125
client 7, data condensation 800, total loss = 15.1175537109375, avg loss = 15.1175537109375
client 7, data condensation 1000, total loss = 4.526458740234375, avg loss = 4.526458740234375
client 7, data condensation 1200, total loss = 12.6956787109375, avg loss = 12.6956787109375
client 7, data condensation 1400, total loss = 8.005615234375, avg loss = 8.005615234375
client 7, data condensation 1600, total loss = 20.53887939453125, avg loss = 20.53887939453125
client 7, data condensation 1800, total loss = 18.108551025390625, avg loss = 18.108551025390625
client 7, data condensation 2000, total loss = 20.808929443359375, avg loss = 20.808929443359375
client 7, data condensation 2200, total loss = 5.669525146484375, avg loss = 5.669525146484375
client 7, data condensation 2400, total loss = 8.680419921875, avg loss = 8.680419921875
client 7, data condensation 2600, total loss = 14.18548583984375, avg loss = 14.18548583984375
client 7, data condensation 2800, total loss = 19.353240966796875, avg loss = 19.353240966796875
client 7, data condensation 3000, total loss = 27.940521240234375, avg loss = 27.940521240234375
client 7, data condensation 3200, total loss = 6.8094482421875, avg loss = 6.8094482421875
client 7, data condensation 3400, total loss = 8.473480224609375, avg loss = 8.473480224609375
client 7, data condensation 3600, total loss = 3.785400390625, avg loss = 3.785400390625
client 7, data condensation 3800, total loss = 26.80426025390625, avg loss = 26.80426025390625
client 7, data condensation 4000, total loss = 27.1400146484375, avg loss = 27.1400146484375
client 7, data condensation 4200, total loss = 8.81427001953125, avg loss = 8.81427001953125
client 7, data condensation 4400, total loss = 18.72833251953125, avg loss = 18.72833251953125
client 7, data condensation 4600, total loss = 15.539642333984375, avg loss = 15.539642333984375
client 7, data condensation 4800, total loss = 13.37591552734375, avg loss = 13.37591552734375
client 7, data condensation 5000, total loss = 3.952545166015625, avg loss = 3.952545166015625
Round 6, client 7 condense time: 253.8129243850708
client 7, class 1 have 9112 samples
total 24576.0MB, used 2977.06MB, free 21598.94MB
total 24576.0MB, used 2977.06MB, free 21598.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [186   5   0   1   2   1   0   3   3   5], bin edged: [0.00466403 0.0049666  0.00526917 0.00557173 0.0058743  0.00617686
 0.00647943 0.006782   0.00708456 0.00738713 0.00768969]
class 4 have 1179 samples, histogram: [743  67  39  33  24  25  23  26  36 163], bin edged: [0.0007313  0.00077874 0.00082618 0.00087363 0.00092107 0.00096851
 0.00101595 0.00106339 0.00111083 0.00115827 0.00120571]
class 8 have 734 samples, histogram: [386  35  19  17  14  16  20  25  29 173], bin edged: [0.00110583 0.00117757 0.00124931 0.00132105 0.00139279 0.00146452
 0.00153626 0.001608   0.00167974 0.00175147 0.00182321]
client 8, data condensation 0, total loss = 227.03106689453125, avg loss = 75.67702229817708
client 8, data condensation 200, total loss = 163.82791137695312, avg loss = 54.60930379231771
client 8, data condensation 400, total loss = 235.308837890625, avg loss = 78.436279296875
client 8, data condensation 600, total loss = 56.564697265625, avg loss = 18.854899088541668
client 8, data condensation 800, total loss = 60.13104248046875, avg loss = 20.043680826822918
client 8, data condensation 1000, total loss = 89.45379638671875, avg loss = 29.81793212890625
client 8, data condensation 1200, total loss = 148.637451171875, avg loss = 49.545817057291664
client 8, data condensation 1400, total loss = 153.646728515625, avg loss = 51.215576171875
client 8, data condensation 1600, total loss = 58.21917724609375, avg loss = 19.406392415364582
client 8, data condensation 1800, total loss = 86.974365234375, avg loss = 28.991455078125
client 8, data condensation 2000, total loss = 135.82498168945312, avg loss = 45.274993896484375
client 8, data condensation 2200, total loss = 48.410797119140625, avg loss = 16.136932373046875
client 8, data condensation 2400, total loss = 131.07595825195312, avg loss = 43.691986083984375
client 8, data condensation 2600, total loss = 41.364105224609375, avg loss = 13.788035074869791
client 8, data condensation 2800, total loss = 68.32443237304688, avg loss = 22.774810791015625
client 8, data condensation 3000, total loss = 71.28802490234375, avg loss = 23.762674967447918
client 8, data condensation 3200, total loss = 297.13262939453125, avg loss = 99.04420979817708
client 8, data condensation 3400, total loss = 116.00274658203125, avg loss = 38.667582194010414
client 8, data condensation 3600, total loss = 59.197235107421875, avg loss = 19.732411702473957
client 8, data condensation 3800, total loss = 56.50787353515625, avg loss = 18.835957845052082
client 8, data condensation 4000, total loss = 55.1453857421875, avg loss = 18.381795247395832
client 8, data condensation 4200, total loss = 55.69293212890625, avg loss = 18.564310709635418
client 8, data condensation 4400, total loss = 79.931884765625, avg loss = 26.643961588541668
client 8, data condensation 4600, total loss = 46.8314208984375, avg loss = 15.6104736328125
client 8, data condensation 4800, total loss = 74.92416381835938, avg loss = 24.974721272786457
client 8, data condensation 5000, total loss = 65.43975830078125, avg loss = 21.813252766927082
Round 6, client 8 condense time: 521.2089042663574
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3487.06MB, free 21088.94MB
total 24576.0MB, used 3487.06MB, free 21088.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2612  358  249  187  203  180  232  296  446 5553], bin edged: [6.83055194e-05 7.27366437e-05 7.71677681e-05 8.15988924e-05
 8.60300167e-05 9.04611411e-05 9.48922654e-05 9.93233897e-05
 1.03754514e-04 1.08185638e-04 1.12616763e-04]
client 9, data condensation 0, total loss = 105.17984008789062, avg loss = 105.17984008789062
client 9, data condensation 200, total loss = 7.20831298828125, avg loss = 7.20831298828125
client 9, data condensation 400, total loss = 9.20257568359375, avg loss = 9.20257568359375
client 9, data condensation 600, total loss = 4.477203369140625, avg loss = 4.477203369140625
client 9, data condensation 800, total loss = 30.474884033203125, avg loss = 30.474884033203125
client 9, data condensation 1000, total loss = 111.92218017578125, avg loss = 111.92218017578125
client 9, data condensation 1200, total loss = 9.07940673828125, avg loss = 9.07940673828125
client 9, data condensation 1400, total loss = 6.234619140625, avg loss = 6.234619140625
client 9, data condensation 1600, total loss = 4.27203369140625, avg loss = 4.27203369140625
client 9, data condensation 1800, total loss = 10.466033935546875, avg loss = 10.466033935546875
client 9, data condensation 2000, total loss = 5.5052490234375, avg loss = 5.5052490234375
client 9, data condensation 2200, total loss = 5.4619140625, avg loss = 5.4619140625
client 9, data condensation 2400, total loss = 10.222900390625, avg loss = 10.222900390625
client 9, data condensation 2600, total loss = 4.048004150390625, avg loss = 4.048004150390625
client 9, data condensation 2800, total loss = 64.955322265625, avg loss = 64.955322265625
client 9, data condensation 3000, total loss = 13.249359130859375, avg loss = 13.249359130859375
client 9, data condensation 3200, total loss = 114.613525390625, avg loss = 114.613525390625
client 9, data condensation 3400, total loss = 7.957061767578125, avg loss = 7.957061767578125
client 9, data condensation 3600, total loss = 4.19439697265625, avg loss = 4.19439697265625
client 9, data condensation 3800, total loss = 199.0611572265625, avg loss = 199.0611572265625
client 9, data condensation 4000, total loss = 6.251617431640625, avg loss = 6.251617431640625
client 9, data condensation 4200, total loss = 17.224853515625, avg loss = 17.224853515625
client 9, data condensation 4400, total loss = 6.499237060546875, avg loss = 6.499237060546875
client 9, data condensation 4600, total loss = 20.94500732421875, avg loss = 20.94500732421875
client 9, data condensation 4800, total loss = 139.80960083007812, avg loss = 139.80960083007812
client 9, data condensation 5000, total loss = 15.333831787109375, avg loss = 15.333831787109375
Round 6, client 9 condense time: 246.03031635284424
client 9, class 2 have 10316 samples
total 24576.0MB, used 2979.06MB, free 21596.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 20.6042,   3.4101,  -6.5359, -13.8360,   9.2361,   4.3313,   1.3403,
          -7.4192, -11.0060],
        [  1.9943,  14.2003,  -0.0349,  -4.8934,  -0.4363,   3.4150,  -6.8042,
          -2.4440,  -3.9259],
        [-10.5609,  -8.8298,   9.5377,  -3.3356,   0.6280,   5.6073,   1.5454,
           5.7131,   0.5045],
        [-12.1776,  -8.2722,  -2.1948,  17.1446,   0.5438,  -1.8560,   7.3089,
           3.1292,  -3.0437],
        [ -2.3409,  -5.4324,  -3.7382,  -2.8765,  11.5512,  -1.0901,   8.0254,
          -0.2839,  -3.4812],
        [ -8.1185,  -6.6771,   4.8056,  -6.9068,   1.6765,  10.4630,   0.3215,
           4.5715,   0.7110],
        [ -8.1663,  -9.4729,  -0.7419,   1.4572,   4.0954,  -1.5166,  13.3071,
           0.3542,   0.8321],
        [-11.7459, -10.4721,   3.1350,   0.4672,   2.5893,   3.2452,   2.7170,
           9.9157,   1.0557],
        [-11.4155,  -8.0588,   0.1669,  -1.7975,   1.1099,   1.5959,   9.2925,
          -0.0314,   9.6239]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 0, 2, 4],
        [2, 7, 5, 6, 4],
        [3, 6, 7, 4, 5],
        [4, 6, 7, 5, 0],
        [5, 2, 7, 4, 8],
        [6, 4, 3, 8, 7],
        [7, 5, 2, 6, 4],
        [8, 6, 5, 4, 2]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
7
Round 6: # synthetic sample: 6384
total 24576.0MB, used 2979.06MB, free 21596.94MB
{0: {0: 1314, 1: 5, 2: 0, 3: 0, 4: 11, 5: 2, 6: 6, 7: 0, 8: 0}, 1: {0: 15, 1: 774, 2: 18, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 40}, 2: {0: 0, 1: 0, 2: 265, 3: 9, 4: 0, 5: 15, 6: 6, 7: 44, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 561, 4: 0, 5: 0, 6: 69, 7: 0, 8: 4}, 4: {0: 29, 1: 49, 2: 0, 3: 1, 4: 912, 5: 4, 6: 37, 7: 2, 8: 1}, 5: {0: 0, 1: 4, 2: 54, 3: 10, 4: 3, 5: 194, 6: 15, 7: 300, 8: 12}, 6: {0: 6, 1: 1, 2: 1, 3: 10, 4: 27, 5: 7, 6: 681, 7: 0, 8: 8}, 7: {0: 2, 1: 14, 2: 12, 3: 74, 4: 18, 5: 57, 6: 53, 7: 167, 8: 24}, 8: {0: 11, 1: 2, 2: 0, 3: 37, 4: 12, 5: 30, 6: 360, 7: 50, 8: 731}}
round 6 evaluation: test acc is 0.7798, test loss = 1.194000
{0: {0: 1313, 1: 2, 2: 0, 3: 0, 4: 17, 5: 4, 6: 2, 7: 0, 8: 0}, 1: {0: 1, 1: 846, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 4, 2: 174, 3: 1, 4: 0, 5: 73, 6: 2, 7: 85, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 562, 4: 1, 5: 2, 6: 61, 7: 5, 8: 3}, 4: {0: 24, 1: 47, 2: 0, 3: 3, 4: 917, 5: 5, 6: 24, 7: 11, 8: 4}, 5: {0: 0, 1: 9, 2: 20, 3: 6, 4: 2, 5: 265, 6: 7, 7: 280, 8: 3}, 6: {0: 16, 1: 1, 2: 0, 3: 10, 4: 32, 5: 12, 6: 661, 7: 5, 8: 4}, 7: {0: 1, 1: 11, 2: 10, 3: 38, 4: 8, 5: 65, 6: 30, 7: 255, 8: 3}, 8: {0: 18, 1: 2, 2: 0, 3: 45, 4: 8, 5: 62, 6: 335, 7: 145, 8: 618}}
epoch 0, train loss avg now = 0.124690, train contrast loss now = 1.546279, test acc now = 0.7815, test loss now = 1.264982
{0: {0: 1331, 1: 4, 2: 0, 3: 0, 4: 0, 5: 2, 6: 1, 7: 0, 8: 0}, 1: {0: 6, 1: 840, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 279, 3: 1, 4: 0, 5: 14, 6: 1, 7: 42, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 564, 4: 0, 5: 2, 6: 64, 7: 1, 8: 3}, 4: {0: 85, 1: 77, 2: 0, 3: 0, 4: 825, 5: 14, 6: 30, 7: 2, 8: 2}, 5: {0: 0, 1: 35, 2: 114, 3: 4, 4: 0, 5: 155, 6: 3, 7: 267, 8: 14}, 6: {0: 28, 1: 1, 2: 4, 3: 9, 4: 21, 5: 18, 6: 646, 7: 4, 8: 10}, 7: {0: 2, 1: 10, 2: 22, 3: 33, 4: 16, 5: 57, 6: 33, 7: 233, 8: 15}, 8: {0: 18, 1: 2, 2: 6, 3: 37, 4: 9, 5: 76, 6: 279, 7: 78, 8: 728}}
epoch 100, train loss avg now = 0.036194, train contrast loss now = 1.239705, test acc now = 0.7801, test loss now = 1.175225
{0: {0: 1326, 1: 5, 2: 0, 3: 0, 4: 4, 5: 0, 6: 3, 7: 0, 8: 0}, 1: {0: 8, 1: 839, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 189, 3: 5, 4: 0, 5: 66, 6: 23, 7: 56, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 559, 4: 3, 5: 0, 6: 68, 7: 0, 8: 4}, 4: {0: 44, 1: 53, 2: 0, 3: 1, 4: 901, 5: 4, 6: 32, 7: 0, 8: 0}, 5: {0: 0, 1: 18, 2: 47, 3: 28, 4: 3, 5: 164, 6: 33, 7: 265, 8: 34}, 6: {0: 13, 1: 1, 2: 0, 3: 7, 4: 41, 5: 3, 6: 674, 7: 0, 8: 2}, 7: {0: 2, 1: 11, 2: 6, 3: 69, 4: 31, 5: 74, 6: 98, 7: 121, 8: 9}, 8: {0: 23, 1: 6, 2: 0, 3: 49, 4: 12, 5: 12, 6: 532, 7: 35, 8: 564}}
epoch 200, train loss avg now = 0.028327, train contrast loss now = 1.238335, test acc now = 0.7433, test loss now = 1.513669
{0: {0: 1329, 1: 4, 2: 0, 3: 0, 4: 3, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 3, 1: 844, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 49, 2: 191, 3: 21, 4: 0, 5: 24, 6: 5, 7: 45, 8: 3}, 3: {0: 2, 1: 0, 2: 0, 3: 597, 4: 4, 5: 1, 6: 26, 7: 0, 8: 4}, 4: {0: 47, 1: 77, 2: 0, 3: 1, 4: 897, 5: 2, 6: 9, 7: 1, 8: 1}, 5: {0: 23, 1: 39, 2: 58, 3: 68, 4: 3, 5: 123, 6: 11, 7: 251, 8: 16}, 6: {0: 20, 1: 2, 2: 0, 3: 24, 4: 81, 5: 3, 6: 603, 7: 0, 8: 8}, 7: {0: 2, 1: 24, 2: 7, 3: 113, 4: 42, 5: 47, 6: 46, 7: 117, 8: 23}, 8: {0: 23, 1: 5, 2: 0, 3: 115, 4: 25, 5: 24, 6: 281, 7: 25, 8: 735}}
epoch 300, train loss avg now = 0.026025, train contrast loss now = 1.238622, test acc now = 0.7571, test loss now = 1.491434
{0: {0: 1321, 1: 5, 2: 0, 3: 0, 4: 7, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 6, 1: 841, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 4, 1: 18, 2: 209, 3: 17, 4: 0, 5: 33, 6: 6, 7: 49, 8: 3}, 3: {0: 0, 1: 0, 2: 0, 3: 587, 4: 3, 5: 0, 6: 39, 7: 0, 8: 5}, 4: {0: 23, 1: 67, 2: 0, 3: 9, 4: 893, 5: 4, 6: 32, 7: 4, 8: 3}, 5: {0: 1, 1: 55, 2: 64, 3: 52, 4: 2, 5: 182, 6: 10, 7: 217, 8: 9}, 6: {0: 12, 1: 3, 2: 1, 3: 16, 4: 34, 5: 4, 6: 662, 7: 0, 8: 9}, 7: {0: 2, 1: 16, 2: 9, 3: 115, 4: 14, 5: 65, 6: 55, 7: 127, 8: 18}, 8: {0: 11, 1: 4, 2: 0, 3: 54, 4: 9, 5: 23, 6: 345, 7: 23, 8: 764}}
epoch 400, train loss avg now = 0.014857, train contrast loss now = 1.238839, test acc now = 0.7780, test loss now = 1.415508
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1327, 1: 4, 2: 0, 3: 0, 4: 2, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 3, 1: 842, 2: 0, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 109, 3: 9, 4: 0, 5: 172, 6: 8, 7: 39, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 562, 4: 1, 5: 1, 6: 69, 7: 0, 8: 1}, 4: {0: 43, 1: 59, 2: 0, 3: 3, 4: 882, 5: 8, 6: 38, 7: 2, 8: 0}, 5: {0: 0, 1: 55, 2: 26, 3: 37, 4: 2, 5: 247, 6: 18, 7: 197, 8: 10}, 6: {0: 13, 1: 1, 2: 0, 3: 7, 4: 21, 5: 7, 6: 688, 7: 1, 8: 3}, 7: {0: 2, 1: 13, 2: 8, 3: 86, 4: 16, 5: 92, 6: 57, 7: 137, 8: 10}, 8: {0: 11, 1: 2, 2: 0, 3: 45, 4: 8, 5: 75, 6: 425, 7: 42, 8: 625}}
epoch 500, train loss avg now = 0.021299, train contrast loss now = 1.238033, test acc now = 0.7547, test loss now = 1.419430
{0: {0: 1327, 1: 5, 2: 0, 3: 0, 4: 1, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 4, 1: 843, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 203, 3: 9, 4: 0, 5: 64, 6: 10, 7: 49, 8: 2}, 3: {0: 0, 1: 0, 2: 0, 3: 561, 4: 1, 5: 0, 6: 67, 7: 0, 8: 5}, 4: {0: 44, 1: 57, 2: 0, 3: 1, 4: 889, 5: 5, 6: 36, 7: 2, 8: 1}, 5: {0: 12, 1: 45, 2: 53, 3: 41, 4: 3, 5: 175, 6: 13, 7: 239, 8: 11}, 6: {0: 21, 1: 2, 2: 0, 3: 9, 4: 23, 5: 6, 6: 674, 7: 1, 8: 5}, 7: {0: 2, 1: 11, 2: 9, 3: 84, 4: 22, 5: 67, 6: 62, 7: 142, 8: 22}, 8: {0: 18, 1: 1, 2: 0, 3: 50, 4: 9, 5: 37, 6: 377, 7: 33, 8: 708}}
epoch 600, train loss avg now = 0.009449, train contrast loss now = 1.237461, test acc now = 0.7691, test loss now = 1.371524
{0: {0: 1331, 1: 5, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 5, 1: 834, 2: 8, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 222, 3: 19, 4: 0, 5: 47, 6: 5, 7: 43, 8: 3}, 3: {0: 1, 1: 0, 2: 0, 3: 583, 4: 2, 5: 0, 6: 43, 7: 0, 8: 5}, 4: {0: 54, 1: 54, 2: 0, 3: 3, 4: 883, 5: 6, 6: 31, 7: 2, 8: 2}, 5: {0: 3, 1: 51, 2: 60, 3: 51, 4: 1, 5: 183, 6: 9, 7: 221, 8: 13}, 6: {0: 29, 1: 2, 2: 1, 3: 17, 4: 27, 5: 7, 6: 647, 7: 1, 8: 10}, 7: {0: 2, 1: 10, 2: 8, 3: 109, 4: 15, 5: 71, 6: 54, 7: 133, 8: 19}, 8: {0: 21, 1: 1, 2: 0, 3: 75, 4: 5, 5: 32, 6: 323, 7: 28, 8: 748}}
epoch 700, train loss avg now = 0.011643, train contrast loss now = 1.236374, test acc now = 0.7749, test loss now = 1.345766
{0: {0: 1330, 1: 2, 2: 0, 3: 0, 4: 1, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 6, 1: 841, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 10, 2: 173, 3: 6, 4: 0, 5: 101, 6: 16, 7: 33, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 559, 4: 1, 5: 0, 6: 69, 7: 0, 8: 5}, 4: {0: 42, 1: 57, 2: 0, 3: 1, 4: 889, 5: 7, 6: 35, 7: 2, 8: 2}, 5: {0: 4, 1: 37, 2: 23, 3: 33, 4: 0, 5: 253, 6: 23, 7: 203, 8: 16}, 6: {0: 17, 1: 1, 2: 0, 3: 9, 4: 23, 5: 6, 6: 677, 7: 1, 8: 7}, 7: {0: 2, 1: 10, 2: 8, 3: 99, 4: 24, 5: 83, 6: 64, 7: 113, 8: 18}, 8: {0: 12, 1: 1, 2: 0, 3: 52, 4: 8, 5: 53, 6: 365, 7: 32, 8: 710}}
epoch 800, train loss avg now = 0.024445, train contrast loss now = 1.238240, test acc now = 0.7723, test loss now = 1.263379
{0: {0: 1327, 1: 4, 2: 0, 3: 0, 4: 2, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 7, 1: 827, 2: 12, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 200, 3: 8, 4: 0, 5: 75, 6: 8, 7: 45, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 574, 4: 2, 5: 0, 6: 53, 7: 0, 8: 5}, 4: {0: 31, 1: 51, 2: 0, 3: 1, 4: 912, 5: 6, 6: 31, 7: 2, 8: 1}, 5: {0: 2, 1: 47, 2: 44, 3: 40, 4: 1, 5: 208, 6: 10, 7: 225, 8: 15}, 6: {0: 16, 1: 1, 2: 0, 3: 15, 4: 28, 5: 6, 6: 665, 7: 1, 8: 9}, 7: {0: 2, 1: 10, 2: 9, 3: 99, 4: 20, 5: 77, 6: 51, 7: 131, 8: 22}, 8: {0: 13, 1: 1, 2: 0, 3: 55, 4: 10, 5: 43, 6: 342, 7: 35, 8: 734}}
epoch 900, train loss avg now = 0.009824, train contrast loss now = 1.237428, test acc now = 0.7769, test loss now = 1.281121
{0: {0: 1330, 1: 4, 2: 0, 3: 0, 4: 1, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 828, 2: 11, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 208, 3: 10, 4: 0, 5: 60, 6: 13, 7: 44, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 571, 4: 1, 5: 1, 6: 57, 7: 0, 8: 4}, 4: {0: 53, 1: 51, 2: 0, 3: 1, 4: 889, 5: 6, 6: 32, 7: 2, 8: 1}, 5: {0: 2, 1: 44, 2: 45, 3: 44, 4: 1, 5: 208, 6: 12, 7: 225, 8: 11}, 6: {0: 20, 1: 2, 2: 0, 3: 14, 4: 25, 5: 6, 6: 669, 7: 0, 8: 5}, 7: {0: 2, 1: 10, 2: 8, 3: 95, 4: 18, 5: 74, 6: 61, 7: 133, 8: 20}, 8: {0: 16, 1: 1, 2: 0, 3: 57, 4: 9, 5: 43, 6: 381, 7: 32, 8: 694}}
epoch 1000, train loss avg now = 0.006669, train contrast loss now = 1.236212, test acc now = 0.7702, test loss now = 1.372532
epoch avg loss = 6.66925152775395e-06, total time = 6575.037678003311
total 24576.0MB, used 3627.06MB, free 20948.94MB
Round 6 finish, update the prev_syn_proto
torch.Size([672, 3, 28, 28])
torch.Size([679, 3, 28, 28])
torch.Size([728, 3, 28, 28])
torch.Size([749, 3, 28, 28])
torch.Size([567, 3, 28, 28])
torch.Size([854, 3, 28, 28])
torch.Size([567, 3, 28, 28])
torch.Size([658, 3, 28, 28])
torch.Size([910, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1330, 1: 4, 2: 0, 3: 0, 4: 1, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 828, 2: 11, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 208, 3: 10, 4: 0, 5: 60, 6: 13, 7: 44, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 571, 4: 1, 5: 1, 6: 57, 7: 0, 8: 4}, 4: {0: 53, 1: 51, 2: 0, 3: 1, 4: 889, 5: 6, 6: 32, 7: 2, 8: 1}, 5: {0: 2, 1: 44, 2: 45, 3: 44, 4: 1, 5: 208, 6: 12, 7: 225, 8: 11}, 6: {0: 20, 1: 2, 2: 0, 3: 14, 4: 25, 5: 6, 6: 669, 7: 0, 8: 5}, 7: {0: 2, 1: 10, 2: 8, 3: 95, 4: 18, 5: 74, 6: 61, 7: 133, 8: 20}, 8: {0: 16, 1: 1, 2: 0, 3: 57, 4: 9, 5: 43, 6: 381, 7: 32, 8: 694}}
round 6 evaluation: test acc is 0.7702, test loss = 1.372532
 ====== round 7 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3627.06MB, free 20948.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [3630  352  215  157  136  106  129  122  136  794], bin edged: [0.00014966 0.00015936 0.00016907 0.00017878 0.00018849 0.0001982
 0.00020791 0.00021761 0.00022732 0.00023703 0.00024674]
class 7 have 9330 samples, histogram: [4479  596  347  289  256  256  248  289  373 2197], bin edged: [8.63889474e-05 9.19931758e-05 9.75974042e-05 1.03201633e-04
 1.08805861e-04 1.14410089e-04 1.20014318e-04 1.25618546e-04
 1.31222775e-04 1.36827003e-04 1.42431231e-04]
client 0, data condensation 0, total loss = 74.83203125, avg loss = 37.416015625
client 0, data condensation 200, total loss = 169.69903564453125, avg loss = 84.84951782226562
client 0, data condensation 400, total loss = 27.771392822265625, avg loss = 13.885696411132812
client 0, data condensation 600, total loss = 44.107421875, avg loss = 22.0537109375
client 0, data condensation 800, total loss = 25.861328125, avg loss = 12.9306640625
client 0, data condensation 1000, total loss = 16.169189453125, avg loss = 8.0845947265625
client 0, data condensation 1200, total loss = 18.050872802734375, avg loss = 9.025436401367188
client 0, data condensation 1400, total loss = 25.180755615234375, avg loss = 12.590377807617188
client 0, data condensation 1600, total loss = 23.638153076171875, avg loss = 11.819076538085938
client 0, data condensation 1800, total loss = 17.677703857421875, avg loss = 8.838851928710938
client 0, data condensation 2000, total loss = 59.572998046875, avg loss = 29.7864990234375
client 0, data condensation 2200, total loss = 15.70587158203125, avg loss = 7.852935791015625
client 0, data condensation 2400, total loss = 17.96514892578125, avg loss = 8.982574462890625
client 0, data condensation 2600, total loss = 10.5396728515625, avg loss = 5.26983642578125
client 0, data condensation 2800, total loss = 24.361328125, avg loss = 12.1806640625
client 0, data condensation 3000, total loss = 12.10064697265625, avg loss = 6.050323486328125
client 0, data condensation 3200, total loss = 19.22528076171875, avg loss = 9.612640380859375
client 0, data condensation 3400, total loss = 22.7373046875, avg loss = 11.36865234375
client 0, data condensation 3600, total loss = 17.75640869140625, avg loss = 8.878204345703125
client 0, data condensation 3800, total loss = 21.22784423828125, avg loss = 10.613922119140625
client 0, data condensation 4000, total loss = 18.261016845703125, avg loss = 9.130508422851562
client 0, data condensation 4200, total loss = 89.7408447265625, avg loss = 44.87042236328125
client 0, data condensation 4400, total loss = 42.956329345703125, avg loss = 21.478164672851562
client 0, data condensation 4600, total loss = 14.595428466796875, avg loss = 7.2977142333984375
client 0, data condensation 4800, total loss = 64.9635009765625, avg loss = 32.48175048828125
client 0, data condensation 5000, total loss = 19.000457763671875, avg loss = 9.500228881835938
Round 7, client 0 condense time: 436.77625012397766
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3445.06MB, free 21130.94MB
total 24576.0MB, used 3445.06MB, free 21130.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [6490  356  224  152  119  128  131  176  192 1054], bin edged: [9.84131719e-05 1.04797444e-04 1.11181715e-04 1.17565987e-04
 1.23950259e-04 1.30334531e-04 1.36718803e-04 1.43103074e-04
 1.49487346e-04 1.55871618e-04 1.62255890e-04]
client 1, data condensation 0, total loss = 305.24127197265625, avg loss = 305.24127197265625
client 1, data condensation 200, total loss = 13.71038818359375, avg loss = 13.71038818359375
client 1, data condensation 400, total loss = 46.0911865234375, avg loss = 46.0911865234375
client 1, data condensation 600, total loss = 7.43988037109375, avg loss = 7.43988037109375
client 1, data condensation 800, total loss = 12.66278076171875, avg loss = 12.66278076171875
client 1, data condensation 1000, total loss = 21.0865478515625, avg loss = 21.0865478515625
client 1, data condensation 1200, total loss = 24.21942138671875, avg loss = 24.21942138671875
client 1, data condensation 1400, total loss = 16.39154052734375, avg loss = 16.39154052734375
client 1, data condensation 1600, total loss = 24.62371826171875, avg loss = 24.62371826171875
client 1, data condensation 1800, total loss = 26.10955810546875, avg loss = 26.10955810546875
client 1, data condensation 2000, total loss = 26.2646484375, avg loss = 26.2646484375
client 1, data condensation 2200, total loss = 45.2100830078125, avg loss = 45.2100830078125
client 1, data condensation 2400, total loss = 10.57940673828125, avg loss = 10.57940673828125
client 1, data condensation 2600, total loss = 30.80181884765625, avg loss = 30.80181884765625
client 1, data condensation 2800, total loss = 17.79998779296875, avg loss = 17.79998779296875
client 1, data condensation 3000, total loss = 53.60711669921875, avg loss = 53.60711669921875
client 1, data condensation 3200, total loss = 113.4918212890625, avg loss = 113.4918212890625
client 1, data condensation 3400, total loss = 26.599853515625, avg loss = 26.599853515625
client 1, data condensation 3600, total loss = 107.81597900390625, avg loss = 107.81597900390625
client 1, data condensation 3800, total loss = 20.8841552734375, avg loss = 20.8841552734375
client 1, data condensation 4000, total loss = 14.49432373046875, avg loss = 14.49432373046875
client 1, data condensation 4200, total loss = 25.36102294921875, avg loss = 25.36102294921875
client 1, data condensation 4400, total loss = 9.73333740234375, avg loss = 9.73333740234375
client 1, data condensation 4600, total loss = 261.95587158203125, avg loss = 261.95587158203125
client 1, data condensation 4800, total loss = 17.630615234375, avg loss = 17.630615234375
client 1, data condensation 5000, total loss = 7.26214599609375, avg loss = 7.26214599609375
Round 7, client 1 condense time: 244.71835017204285
client 1, class 0 have 9022 samples
total 24576.0MB, used 3061.06MB, free 21514.94MB
total 24576.0MB, used 3061.06MB, free 21514.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [266   2   8   6   3   3   4   3   8  24], bin edged: [0.00281193 0.00299434 0.00317676 0.00335917 0.00354159 0.003724
 0.00390642 0.00408884 0.00427125 0.00445367 0.00463608]
class 5 have 12176 samples, histogram: [5557  621  375  317  272  270  282  319  506 3657], bin edged: [6.45821202e-05 6.87716997e-05 7.29612792e-05 7.71508587e-05
 8.13404382e-05 8.55300177e-05 8.97195972e-05 9.39091767e-05
 9.80987562e-05 1.02288336e-04 1.06477915e-04]
client 2, data condensation 0, total loss = 140.555419921875, avg loss = 70.2777099609375
client 2, data condensation 200, total loss = 44.4083251953125, avg loss = 22.20416259765625
client 2, data condensation 400, total loss = 131.2821044921875, avg loss = 65.64105224609375
client 2, data condensation 600, total loss = 83.50970458984375, avg loss = 41.754852294921875
client 2, data condensation 800, total loss = 111.15264892578125, avg loss = 55.576324462890625
client 2, data condensation 1000, total loss = 52.0721435546875, avg loss = 26.03607177734375
client 2, data condensation 1200, total loss = 67.25152587890625, avg loss = 33.625762939453125
client 2, data condensation 1400, total loss = 42.7451171875, avg loss = 21.37255859375
client 2, data condensation 1600, total loss = 44.19793701171875, avg loss = 22.098968505859375
client 2, data condensation 1800, total loss = 44.976318359375, avg loss = 22.4881591796875
client 2, data condensation 2000, total loss = 77.2742919921875, avg loss = 38.63714599609375
client 2, data condensation 2200, total loss = 124.42852783203125, avg loss = 62.214263916015625
client 2, data condensation 2400, total loss = 128.66717529296875, avg loss = 64.33358764648438
client 2, data condensation 2600, total loss = 63.88714599609375, avg loss = 31.943572998046875
client 2, data condensation 2800, total loss = 83.178955078125, avg loss = 41.5894775390625
client 2, data condensation 3000, total loss = 149.61184692382812, avg loss = 74.80592346191406
client 2, data condensation 3200, total loss = 67.84246826171875, avg loss = 33.921234130859375
client 2, data condensation 3400, total loss = 145.5977783203125, avg loss = 72.79888916015625
client 2, data condensation 3600, total loss = 75.150146484375, avg loss = 37.5750732421875
client 2, data condensation 3800, total loss = 66.06817626953125, avg loss = 33.034088134765625
client 2, data condensation 4000, total loss = 49.4263916015625, avg loss = 24.71319580078125
client 2, data condensation 4200, total loss = 167.76373291015625, avg loss = 83.88186645507812
client 2, data condensation 4400, total loss = 91.12158203125, avg loss = 45.560791015625
client 2, data condensation 4600, total loss = 85.44744873046875, avg loss = 42.723724365234375
client 2, data condensation 4800, total loss = 102.4373779296875, avg loss = 51.21868896484375
client 2, data condensation 5000, total loss = 97.51885986328125, avg loss = 48.759429931640625
Round 7, client 2 condense time: 424.7202322483063
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3445.06MB, free 21130.94MB
total 24576.0MB, used 3445.06MB, free 21130.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [177  16   9   9  12   9   4   7  19  51], bin edged: [0.00267101 0.00284428 0.00301755 0.00319083 0.0033641  0.00353737
 0.00371065 0.00388392 0.00405719 0.00423047 0.00440374]
client 3, data condensation 0, total loss = 142.74591064453125, avg loss = 142.74591064453125
client 3, data condensation 200, total loss = 18.4918212890625, avg loss = 18.4918212890625
client 3, data condensation 400, total loss = 91.69903564453125, avg loss = 91.69903564453125
client 3, data condensation 600, total loss = 89.2808837890625, avg loss = 89.2808837890625
client 3, data condensation 800, total loss = 56.3814697265625, avg loss = 56.3814697265625
client 3, data condensation 1000, total loss = 63.07568359375, avg loss = 63.07568359375
client 3, data condensation 1200, total loss = 64.73681640625, avg loss = 64.73681640625
client 3, data condensation 1400, total loss = 136.68896484375, avg loss = 136.68896484375
client 3, data condensation 1600, total loss = 54.91217041015625, avg loss = 54.91217041015625
client 3, data condensation 1800, total loss = 86.74444580078125, avg loss = 86.74444580078125
client 3, data condensation 2000, total loss = 57.04425048828125, avg loss = 57.04425048828125
client 3, data condensation 2200, total loss = 41.5482177734375, avg loss = 41.5482177734375
client 3, data condensation 2400, total loss = 44.88214111328125, avg loss = 44.88214111328125
client 3, data condensation 2600, total loss = 147.14093017578125, avg loss = 147.14093017578125
client 3, data condensation 2800, total loss = 48.2498779296875, avg loss = 48.2498779296875
client 3, data condensation 3000, total loss = 37.5335693359375, avg loss = 37.5335693359375
client 3, data condensation 3200, total loss = 32.1978759765625, avg loss = 32.1978759765625
client 3, data condensation 3400, total loss = 71.991943359375, avg loss = 71.991943359375
client 3, data condensation 3600, total loss = 107.57122802734375, avg loss = 107.57122802734375
client 3, data condensation 3800, total loss = 31.097259521484375, avg loss = 31.097259521484375
client 3, data condensation 4000, total loss = 47.19580078125, avg loss = 47.19580078125
client 3, data condensation 4200, total loss = 53.27984619140625, avg loss = 53.27984619140625
client 3, data condensation 4400, total loss = 39.4886474609375, avg loss = 39.4886474609375
client 3, data condensation 4600, total loss = 50.73443603515625, avg loss = 50.73443603515625
client 3, data condensation 4800, total loss = 32.8013916015625, avg loss = 32.8013916015625
client 3, data condensation 5000, total loss = 34.7742919921875, avg loss = 34.7742919921875
Round 7, client 3 condense time: 114.14434099197388
client 3, class 6 have 313 samples
total 24576.0MB, used 3059.06MB, free 21516.94MB
total 24576.0MB, used 3059.06MB, free 21516.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [316   2   1   2   3   0   3   0   2  32], bin edged: [0.00258877 0.00275671 0.00292465 0.00309259 0.00326053 0.00342847
 0.00359641 0.00376435 0.00393229 0.00410023 0.00426817]
class 4 have 1048 samples, histogram: [614  73  32  35  28  20  26  26  26 168], bin edged: [0.00081012 0.00086267 0.00091523 0.00096778 0.00102033 0.00107289
 0.00112544 0.001178   0.00123055 0.00128311 0.00133566]
class 6 have 7572 samples, histogram: [5478  362  205  137  168  124  129  129  187  653], bin edged: [0.0001186  0.00012629 0.00013398 0.00014168 0.00014937 0.00015706
 0.00016476 0.00017245 0.00018014 0.00018784 0.00019553]
client 4, data condensation 0, total loss = 246.7381591796875, avg loss = 82.24605305989583
client 4, data condensation 200, total loss = 301.9468994140625, avg loss = 100.64896647135417
client 4, data condensation 400, total loss = 79.8463134765625, avg loss = 26.615437825520832
client 4, data condensation 600, total loss = 74.26593017578125, avg loss = 24.75531005859375
client 4, data condensation 800, total loss = 93.3992919921875, avg loss = 31.133097330729168
client 4, data condensation 1000, total loss = 138.78656005859375, avg loss = 46.262186686197914
client 4, data condensation 1200, total loss = 119.0550537109375, avg loss = 39.685017903645836
client 4, data condensation 1400, total loss = 49.13763427734375, avg loss = 16.37921142578125
client 4, data condensation 1600, total loss = 55.123077392578125, avg loss = 18.374359130859375
client 4, data condensation 1800, total loss = 148.7296142578125, avg loss = 49.5765380859375
client 4, data condensation 2000, total loss = 37.8033447265625, avg loss = 12.601114908854166
client 4, data condensation 2200, total loss = 57.28204345703125, avg loss = 19.094014485677082
client 4, data condensation 2400, total loss = 87.83251953125, avg loss = 29.277506510416668
client 4, data condensation 2600, total loss = 46.9808349609375, avg loss = 15.6602783203125
client 4, data condensation 2800, total loss = 69.42233276367188, avg loss = 23.140777587890625
client 4, data condensation 3000, total loss = 77.08871459960938, avg loss = 25.696238199869793
client 4, data condensation 3200, total loss = 52.278564453125, avg loss = 17.426188151041668
client 4, data condensation 3400, total loss = 172.7474365234375, avg loss = 57.582478841145836
client 4, data condensation 3600, total loss = 38.10211181640625, avg loss = 12.700703938802084
client 4, data condensation 3800, total loss = 42.77557373046875, avg loss = 14.258524576822916
client 4, data condensation 4000, total loss = 69.38421630859375, avg loss = 23.128072102864582
client 4, data condensation 4200, total loss = 53.6964111328125, avg loss = 17.8988037109375
client 4, data condensation 4400, total loss = 80.81488037109375, avg loss = 26.93829345703125
client 4, data condensation 4600, total loss = 94.15057373046875, avg loss = 31.383524576822918
client 4, data condensation 4800, total loss = 43.7176513671875, avg loss = 14.572550455729166
client 4, data condensation 5000, total loss = 50.01123046875, avg loss = 16.67041015625
Round 7, client 4 condense time: 419.16730189323425
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3701.06MB, free 20874.94MB
total 24576.0MB, used 3701.06MB, free 20874.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [3192  506  359  325  292  271  331  417  567 5891], bin edged: [5.89070786e-05 6.27285060e-05 6.65499335e-05 7.03713610e-05
 7.41927885e-05 7.80142160e-05 8.18356435e-05 8.56570710e-05
 8.94784984e-05 9.32999259e-05 9.71213534e-05]
client 5, data condensation 0, total loss = 127.10498046875, avg loss = 127.10498046875
client 5, data condensation 200, total loss = 9.671966552734375, avg loss = 9.671966552734375
client 5, data condensation 400, total loss = 21.358123779296875, avg loss = 21.358123779296875
client 5, data condensation 600, total loss = 7.2149658203125, avg loss = 7.2149658203125
client 5, data condensation 800, total loss = 12.49383544921875, avg loss = 12.49383544921875
client 5, data condensation 1000, total loss = 9.1268310546875, avg loss = 9.1268310546875
client 5, data condensation 1200, total loss = 8.04132080078125, avg loss = 8.04132080078125
client 5, data condensation 1400, total loss = 18.532440185546875, avg loss = 18.532440185546875
client 5, data condensation 1600, total loss = 290.46173095703125, avg loss = 290.46173095703125
client 5, data condensation 1800, total loss = 17.347320556640625, avg loss = 17.347320556640625
client 5, data condensation 2000, total loss = 45.37872314453125, avg loss = 45.37872314453125
client 5, data condensation 2200, total loss = 7.416748046875, avg loss = 7.416748046875
client 5, data condensation 2400, total loss = 18.33355712890625, avg loss = 18.33355712890625
client 5, data condensation 2600, total loss = 10.916351318359375, avg loss = 10.916351318359375
client 5, data condensation 2800, total loss = 8.7772216796875, avg loss = 8.7772216796875
client 5, data condensation 3000, total loss = 19.475860595703125, avg loss = 19.475860595703125
client 5, data condensation 3200, total loss = 5.781494140625, avg loss = 5.781494140625
client 5, data condensation 3400, total loss = 7.266998291015625, avg loss = 7.266998291015625
client 5, data condensation 3600, total loss = 10.02911376953125, avg loss = 10.02911376953125
client 5, data condensation 3800, total loss = 21.629913330078125, avg loss = 21.629913330078125
client 5, data condensation 4000, total loss = 6.099853515625, avg loss = 6.099853515625
client 5, data condensation 4200, total loss = 8.413909912109375, avg loss = 8.413909912109375
client 5, data condensation 4400, total loss = 12.989501953125, avg loss = 12.989501953125
client 5, data condensation 4600, total loss = 6.74737548828125, avg loss = 6.74737548828125
client 5, data condensation 4800, total loss = 14.728668212890625, avg loss = 14.728668212890625
client 5, data condensation 5000, total loss = 8.4998779296875, avg loss = 8.4998779296875
Round 7, client 5 condense time: 265.6915748119354
client 5, class 8 have 12151 samples
total 24576.0MB, used 3061.06MB, free 21514.94MB
total 24576.0MB, used 3061.06MB, free 21514.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [6705  432  257  209  166  185  163  175  281 1621], bin edged: [8.45462745e-05 9.00309712e-05 9.55156679e-05 1.01000365e-04
 1.06485061e-04 1.11969758e-04 1.17454455e-04 1.22939151e-04
 1.28423848e-04 1.33908545e-04 1.39393241e-04]
client 6, data condensation 0, total loss = 266.0968017578125, avg loss = 266.0968017578125
client 6, data condensation 200, total loss = 5.276611328125, avg loss = 5.276611328125
client 6, data condensation 400, total loss = 26.92767333984375, avg loss = 26.92767333984375
client 6, data condensation 600, total loss = 11.90069580078125, avg loss = 11.90069580078125
client 6, data condensation 800, total loss = 9.59564208984375, avg loss = 9.59564208984375
client 6, data condensation 1000, total loss = 15.613525390625, avg loss = 15.613525390625
client 6, data condensation 1200, total loss = 16.28582763671875, avg loss = 16.28582763671875
client 6, data condensation 1400, total loss = 18.94866943359375, avg loss = 18.94866943359375
client 6, data condensation 1600, total loss = 18.03167724609375, avg loss = 18.03167724609375
client 6, data condensation 1800, total loss = 15.40093994140625, avg loss = 15.40093994140625
client 6, data condensation 2000, total loss = 16.69537353515625, avg loss = 16.69537353515625
client 6, data condensation 2200, total loss = 36.56024169921875, avg loss = 36.56024169921875
client 6, data condensation 2400, total loss = 125.10906982421875, avg loss = 125.10906982421875
client 6, data condensation 2600, total loss = 16.27545166015625, avg loss = 16.27545166015625
client 6, data condensation 2800, total loss = 5.76055908203125, avg loss = 5.76055908203125
client 6, data condensation 3000, total loss = 24.506591796875, avg loss = 24.506591796875
client 6, data condensation 3200, total loss = 14.77362060546875, avg loss = 14.77362060546875
client 6, data condensation 3400, total loss = 11.0125732421875, avg loss = 11.0125732421875
client 6, data condensation 3600, total loss = 8.6982421875, avg loss = 8.6982421875
client 6, data condensation 3800, total loss = 18.19091796875, avg loss = 18.19091796875
client 6, data condensation 4000, total loss = 69.755126953125, avg loss = 69.755126953125
client 6, data condensation 4200, total loss = 17.19525146484375, avg loss = 17.19525146484375
client 6, data condensation 4400, total loss = 132.50152587890625, avg loss = 132.50152587890625
client 6, data condensation 4600, total loss = 14.03240966796875, avg loss = 14.03240966796875
client 6, data condensation 4800, total loss = 13.2962646484375, avg loss = 13.2962646484375
client 6, data condensation 5000, total loss = 15.27392578125, avg loss = 15.27392578125
Round 7, client 6 condense time: 230.87743616104126
client 6, class 3 have 10194 samples
total 24576.0MB, used 3065.06MB, free 21510.94MB
total 24576.0MB, used 3065.06MB, free 21510.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [6743  218  116   90   68   75   77   95  150 1480], bin edged: [9.64325645e-05 1.02688345e-04 1.08944126e-04 1.15199907e-04
 1.21455688e-04 1.27711469e-04 1.33967250e-04 1.40223030e-04
 1.46478811e-04 1.52734592e-04 1.58990373e-04]
client 7, data condensation 0, total loss = 146.906982421875, avg loss = 146.906982421875
client 7, data condensation 200, total loss = 9.68505859375, avg loss = 9.68505859375
client 7, data condensation 400, total loss = 7.73126220703125, avg loss = 7.73126220703125
client 7, data condensation 600, total loss = 12.9268798828125, avg loss = 12.9268798828125
client 7, data condensation 800, total loss = 13.38555908203125, avg loss = 13.38555908203125
client 7, data condensation 1000, total loss = 6.06341552734375, avg loss = 6.06341552734375
client 7, data condensation 1200, total loss = 10.37786865234375, avg loss = 10.37786865234375
client 7, data condensation 1400, total loss = 10.157745361328125, avg loss = 10.157745361328125
client 7, data condensation 1600, total loss = 37.7337646484375, avg loss = 37.7337646484375
client 7, data condensation 1800, total loss = 68.25579833984375, avg loss = 68.25579833984375
client 7, data condensation 2000, total loss = 14.9510498046875, avg loss = 14.9510498046875
client 7, data condensation 2200, total loss = 11.7445068359375, avg loss = 11.7445068359375
client 7, data condensation 2400, total loss = 6.91522216796875, avg loss = 6.91522216796875
client 7, data condensation 2600, total loss = 7.26409912109375, avg loss = 7.26409912109375
client 7, data condensation 2800, total loss = 26.59234619140625, avg loss = 26.59234619140625
client 7, data condensation 3000, total loss = 29.49615478515625, avg loss = 29.49615478515625
client 7, data condensation 3200, total loss = 22.36102294921875, avg loss = 22.36102294921875
client 7, data condensation 3400, total loss = 12.31695556640625, avg loss = 12.31695556640625
client 7, data condensation 3600, total loss = 38.67144775390625, avg loss = 38.67144775390625
client 7, data condensation 3800, total loss = 5.88543701171875, avg loss = 5.88543701171875
client 7, data condensation 4000, total loss = 21.4561767578125, avg loss = 21.4561767578125
client 7, data condensation 4200, total loss = 50.35009765625, avg loss = 50.35009765625
client 7, data condensation 4400, total loss = 77.1611328125, avg loss = 77.1611328125
client 7, data condensation 4600, total loss = 16.61395263671875, avg loss = 16.61395263671875
client 7, data condensation 4800, total loss = 7.14288330078125, avg loss = 7.14288330078125
client 7, data condensation 5000, total loss = 11.57293701171875, avg loss = 11.57293701171875
Round 7, client 7 condense time: 200.9177770614624
client 7, class 1 have 9112 samples
total 24576.0MB, used 3063.06MB, free 21512.94MB
total 24576.0MB, used 3063.06MB, free 21512.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [186   4   3   2   2   0   1   1   3   4], bin edged: [0.00468211 0.00498585 0.00528959 0.00559333 0.00589707 0.00620081
 0.00650455 0.00680828 0.00711202 0.00741576 0.0077195 ]
class 4 have 1179 samples, histogram: [811  41  39  23  27  16  20  19  27 156], bin edged: [0.00074239 0.00079055 0.00083872 0.00088688 0.00093504 0.0009832
 0.00103136 0.00107952 0.00112768 0.00117584 0.001224  ]
class 8 have 734 samples, histogram: [377  40  22  20  18  22  14  17  33 171], bin edged: [0.00110642 0.0011782  0.00124998 0.00132175 0.00139353 0.0014653
 0.00153708 0.00160886 0.00168063 0.00175241 0.00182419]
client 8, data condensation 0, total loss = 217.903564453125, avg loss = 72.634521484375
client 8, data condensation 200, total loss = 84.4420166015625, avg loss = 28.1473388671875
client 8, data condensation 400, total loss = 70.13430786132812, avg loss = 23.378102620442707
client 8, data condensation 600, total loss = 61.7137451171875, avg loss = 20.571248372395832
client 8, data condensation 800, total loss = 394.42120361328125, avg loss = 131.4737345377604
client 8, data condensation 1000, total loss = 197.8138427734375, avg loss = 65.93794759114583
client 8, data condensation 1200, total loss = 96.8048095703125, avg loss = 32.268269856770836
client 8, data condensation 1400, total loss = 50.333953857421875, avg loss = 16.777984619140625
client 8, data condensation 1600, total loss = 50.097686767578125, avg loss = 16.699228922526043
client 8, data condensation 1800, total loss = 172.5111083984375, avg loss = 57.503702799479164
client 8, data condensation 2000, total loss = 84.31613159179688, avg loss = 28.105377197265625
client 8, data condensation 2200, total loss = 91.14544677734375, avg loss = 30.381815592447918
client 8, data condensation 2400, total loss = 172.927001953125, avg loss = 57.642333984375
client 8, data condensation 2600, total loss = 67.22509765625, avg loss = 22.408365885416668
client 8, data condensation 2800, total loss = 63.65301513671875, avg loss = 21.217671712239582
client 8, data condensation 3000, total loss = 106.2408447265625, avg loss = 35.413614908854164
client 8, data condensation 3200, total loss = 65.10809326171875, avg loss = 21.70269775390625
client 8, data condensation 3400, total loss = 53.7762451171875, avg loss = 17.9254150390625
client 8, data condensation 3600, total loss = 76.466064453125, avg loss = 25.488688151041668
client 8, data condensation 3800, total loss = 131.32110595703125, avg loss = 43.773701985677086
client 8, data condensation 4000, total loss = 74.42626953125, avg loss = 24.808756510416668
client 8, data condensation 4200, total loss = 98.6749267578125, avg loss = 32.891642252604164
client 8, data condensation 4400, total loss = 63.50634765625, avg loss = 21.168782552083332
client 8, data condensation 4600, total loss = 43.989959716796875, avg loss = 14.663319905598959
client 8, data condensation 4800, total loss = 81.188232421875, avg loss = 27.062744140625
client 8, data condensation 5000, total loss = 86.07009887695312, avg loss = 28.690032958984375
Round 7, client 8 condense time: 362.72461009025574
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3701.06MB, free 20874.94MB
total 24576.0MB, used 3701.06MB, free 20874.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2324  358  218  180  174  186  224  250  388 6014], bin edged: [6.72570329e-05 7.16201397e-05 7.59832465e-05 8.03463533e-05
 8.47094600e-05 8.90725668e-05 9.34356736e-05 9.77987804e-05
 1.02161887e-04 1.06524994e-04 1.10888101e-04]
client 9, data condensation 0, total loss = 82.9888916015625, avg loss = 82.9888916015625
client 9, data condensation 200, total loss = 38.150970458984375, avg loss = 38.150970458984375
client 9, data condensation 400, total loss = 10.271453857421875, avg loss = 10.271453857421875
client 9, data condensation 600, total loss = 9.053466796875, avg loss = 9.053466796875
client 9, data condensation 800, total loss = 21.10443115234375, avg loss = 21.10443115234375
client 9, data condensation 1000, total loss = 9.12322998046875, avg loss = 9.12322998046875
client 9, data condensation 1200, total loss = 9.278778076171875, avg loss = 9.278778076171875
client 9, data condensation 1400, total loss = 14.058319091796875, avg loss = 14.058319091796875
client 9, data condensation 1600, total loss = 6.796966552734375, avg loss = 6.796966552734375
client 9, data condensation 1800, total loss = 10.62158203125, avg loss = 10.62158203125
client 9, data condensation 2000, total loss = 5.988128662109375, avg loss = 5.988128662109375
client 9, data condensation 2200, total loss = 4.16900634765625, avg loss = 4.16900634765625
client 9, data condensation 2400, total loss = 26.509552001953125, avg loss = 26.509552001953125
client 9, data condensation 2600, total loss = 11.050567626953125, avg loss = 11.050567626953125
client 9, data condensation 2800, total loss = 3.773223876953125, avg loss = 3.773223876953125
client 9, data condensation 3000, total loss = 15.258056640625, avg loss = 15.258056640625
client 9, data condensation 3200, total loss = 6.170928955078125, avg loss = 6.170928955078125
client 9, data condensation 3400, total loss = 30.8896484375, avg loss = 30.8896484375
client 9, data condensation 3600, total loss = 14.855194091796875, avg loss = 14.855194091796875
client 9, data condensation 3800, total loss = 19.16693115234375, avg loss = 19.16693115234375
client 9, data condensation 4000, total loss = 9.05792236328125, avg loss = 9.05792236328125
client 9, data condensation 4200, total loss = 18.705322265625, avg loss = 18.705322265625
client 9, data condensation 4400, total loss = 11.07269287109375, avg loss = 11.07269287109375
client 9, data condensation 4600, total loss = 7.46978759765625, avg loss = 7.46978759765625
client 9, data condensation 4800, total loss = 7.731170654296875, avg loss = 7.731170654296875
client 9, data condensation 5000, total loss = 8.46990966796875, avg loss = 8.46990966796875
Round 7, client 9 condense time: 256.75368332862854
client 9, class 2 have 10316 samples
total 24576.0MB, used 3065.06MB, free 21510.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 24.3290,   4.8145,  -5.8448, -16.5646,   8.7472,   5.9842,   1.0435,
          -8.9661, -13.2608],
        [  1.5023,  15.2361,   1.1550,  -6.1836,  -0.4402,   4.4916,  -7.4530,
          -1.9818,  -5.2840],
        [-10.7710,  -9.3185,   9.7028,  -3.2243,   0.0517,   7.2494,   1.0202,
           5.7999,   0.2665],
        [-13.1409,  -9.0045,  -2.2056,  19.1259,   0.7042,  -2.9022,   8.3688,
           1.8262,  -2.4292],
        [ -2.1897,  -5.0861,  -4.0678,  -2.8701,  12.3987,  -1.2796,   8.0349,
          -0.8594,  -3.6769],
        [ -7.6067,  -6.6930,   4.0052,  -7.3886,   1.2168,  13.1406,  -0.4129,
           4.2111,   0.3884],
        [ -7.9274,  -9.4669,  -1.5458,   2.1262,   4.3032,  -2.0270,  14.3273,
          -0.6024,   0.9466],
        [-12.3155, -10.9019,   2.3460,   1.0601,   2.2093,   4.6575,   2.4999,
          10.5191,   0.8360],
        [-12.0473,  -8.3514,  -0.9488,  -0.7644,   0.6785,   1.8484,  10.4101,
          -0.8394,  10.5201]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 0, 2, 4],
        [2, 5, 7, 6, 8],
        [3, 6, 7, 4, 2],
        [4, 6, 7, 5, 0],
        [5, 7, 2, 4, 8],
        [6, 4, 3, 8, 7],
        [7, 5, 6, 2, 4],
        [8, 6, 5, 4, 3]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
8
Round 7: # synthetic sample: 7296
total 24576.0MB, used 3065.06MB, free 21510.94MB
{0: {0: 1330, 1: 4, 2: 0, 3: 0, 4: 1, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 828, 2: 11, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 208, 3: 10, 4: 0, 5: 60, 6: 13, 7: 44, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 571, 4: 1, 5: 1, 6: 57, 7: 0, 8: 4}, 4: {0: 53, 1: 51, 2: 0, 3: 1, 4: 889, 5: 6, 6: 32, 7: 2, 8: 1}, 5: {0: 2, 1: 44, 2: 45, 3: 44, 4: 1, 5: 208, 6: 12, 7: 225, 8: 11}, 6: {0: 20, 1: 2, 2: 0, 3: 14, 4: 25, 5: 6, 6: 669, 7: 0, 8: 5}, 7: {0: 2, 1: 10, 2: 8, 3: 95, 4: 18, 5: 74, 6: 61, 7: 133, 8: 20}, 8: {0: 16, 1: 1, 2: 0, 3: 57, 4: 9, 5: 43, 6: 381, 7: 32, 8: 694}}
round 7 evaluation: test acc is 0.7702, test loss = 1.372532
{0: {0: 1304, 1: 5, 2: 0, 3: 0, 4: 0, 5: 3, 6: 26, 7: 0, 8: 0}, 1: {0: 6, 1: 790, 2: 13, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 35}, 2: {0: 0, 1: 0, 2: 305, 3: 5, 4: 0, 5: 24, 6: 3, 7: 2, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 547, 4: 0, 5: 0, 6: 86, 7: 0, 8: 1}, 4: {0: 48, 1: 47, 2: 6, 3: 22, 4: 399, 5: 44, 6: 467, 7: 2, 8: 0}, 5: {0: 0, 1: 26, 2: 352, 3: 8, 4: 0, 5: 132, 6: 14, 7: 58, 8: 2}, 6: {0: 10, 1: 0, 2: 7, 3: 5, 4: 0, 5: 1, 6: 716, 7: 0, 8: 2}, 7: {0: 1, 1: 0, 2: 88, 3: 96, 4: 9, 5: 83, 6: 121, 7: 22, 8: 1}, 8: {0: 8, 1: 3, 2: 63, 3: 62, 4: 0, 5: 22, 6: 855, 7: 3, 8: 217}}
epoch 0, train loss avg now = 0.098260, train contrast loss now = 1.565249, test acc now = 0.6173, test loss now = 2.678087
{0: {0: 1334, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 9, 1: 835, 2: 0, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 23, 1: 0, 2: 129, 3: 3, 4: 0, 5: 112, 6: 3, 7: 68, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 549, 4: 1, 5: 1, 6: 79, 7: 0, 8: 4}, 4: {0: 63, 1: 59, 2: 0, 3: 3, 4: 873, 5: 4, 6: 30, 7: 3, 8: 0}, 5: {0: 3, 1: 53, 2: 65, 3: 24, 4: 1, 5: 187, 6: 15, 7: 237, 8: 7}, 6: {0: 26, 1: 1, 2: 0, 3: 5, 4: 21, 5: 6, 6: 677, 7: 0, 8: 5}, 7: {0: 2, 1: 4, 2: 9, 3: 60, 4: 22, 5: 81, 6: 57, 7: 172, 8: 14}, 8: {0: 23, 1: 3, 2: 2, 3: 64, 4: 7, 5: 44, 6: 392, 7: 36, 8: 662}}
epoch 100, train loss avg now = 0.029497, train contrast loss now = 1.267844, test acc now = 0.7546, test loss now = 1.462568
{0: {0: 1328, 1: 2, 2: 0, 3: 0, 4: 2, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 5, 1: 842, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 282, 3: 7, 4: 0, 5: 27, 6: 0, 7: 21, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 605, 4: 1, 5: 0, 6: 23, 7: 0, 8: 5}, 4: {0: 51, 1: 56, 2: 0, 3: 12, 4: 874, 5: 7, 6: 27, 7: 3, 8: 5}, 5: {0: 0, 1: 59, 2: 143, 3: 42, 4: 1, 5: 114, 6: 5, 7: 204, 8: 24}, 6: {0: 12, 1: 2, 2: 5, 3: 35, 4: 16, 5: 5, 6: 647, 7: 3, 8: 16}, 7: {0: 0, 1: 20, 2: 24, 3: 100, 4: 11, 5: 38, 6: 31, 7: 158, 8: 39}, 8: {0: 8, 1: 6, 2: 7, 3: 71, 4: 6, 5: 15, 6: 215, 7: 45, 8: 860}}
epoch 200, train loss avg now = 0.023352, train contrast loss now = 1.262249, test acc now = 0.7953, test loss now = 1.236605
{0: {0: 1329, 1: 5, 2: 0, 3: 0, 4: 1, 5: 0, 6: 3, 7: 0, 8: 0}, 1: {0: 10, 1: 837, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 127, 3: 2, 4: 0, 5: 121, 6: 30, 7: 58, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 469, 4: 0, 5: 0, 6: 164, 7: 0, 8: 0}, 4: {0: 63, 1: 40, 2: 0, 3: 0, 4: 881, 5: 2, 6: 47, 7: 2, 8: 0}, 5: {0: 1, 1: 54, 2: 23, 3: 12, 4: 0, 5: 203, 6: 44, 7: 248, 8: 7}, 6: {0: 20, 1: 1, 2: 0, 3: 2, 4: 22, 5: 7, 6: 689, 7: 0, 8: 0}, 7: {0: 2, 1: 11, 2: 2, 3: 35, 4: 12, 5: 58, 6: 99, 7: 192, 8: 10}, 8: {0: 16, 1: 7, 2: 0, 3: 19, 4: 7, 5: 37, 6: 670, 7: 32, 8: 445}}
epoch 300, train loss avg now = 0.035865, train contrast loss now = 1.260233, test acc now = 0.7203, test loss now = 1.733829
{0: {0: 1325, 1: 4, 2: 0, 3: 0, 4: 0, 5: 0, 6: 9, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 196, 3: 11, 4: 0, 5: 54, 6: 0, 7: 78, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 591, 4: 0, 5: 0, 6: 39, 7: 0, 8: 4}, 4: {0: 32, 1: 24, 2: 0, 3: 26, 4: 856, 5: 2, 6: 87, 7: 6, 8: 2}, 5: {0: 55, 1: 5, 2: 68, 3: 36, 4: 0, 5: 138, 6: 11, 7: 260, 8: 19}, 6: {0: 14, 1: 0, 2: 0, 3: 18, 4: 7, 5: 2, 6: 688, 7: 2, 8: 10}, 7: {0: 1, 1: 6, 2: 17, 3: 79, 4: 3, 5: 43, 6: 37, 7: 208, 8: 27}, 8: {0: 13, 1: 1, 2: 4, 3: 69, 4: 0, 5: 9, 6: 275, 7: 40, 8: 822}}
epoch 400, train loss avg now = 0.026810, train contrast loss now = 1.257466, test acc now = 0.7883, test loss now = 1.331696
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1329, 1: 3, 2: 0, 3: 0, 4: 1, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 10, 1: 837, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 11, 1: 0, 2: 150, 3: 5, 4: 0, 5: 121, 6: 6, 7: 46, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 565, 4: 0, 5: 0, 6: 65, 7: 0, 8: 4}, 4: {0: 39, 1: 67, 2: 0, 3: 1, 4: 853, 5: 20, 6: 52, 7: 3, 8: 0}, 5: {0: 55, 1: 12, 2: 70, 3: 32, 4: 1, 5: 195, 6: 11, 7: 202, 8: 14}, 6: {0: 18, 1: 1, 2: 0, 3: 6, 4: 13, 5: 5, 6: 691, 7: 0, 8: 7}, 7: {0: 2, 1: 17, 2: 14, 3: 69, 4: 12, 5: 57, 6: 68, 7: 159, 8: 23}, 8: {0: 15, 1: 1, 2: 3, 3: 44, 4: 4, 5: 22, 6: 421, 7: 21, 8: 702}}
epoch 500, train loss avg now = 0.025617, train contrast loss now = 1.254815, test acc now = 0.7634, test loss now = 1.366028
{0: {0: 1331, 1: 1, 2: 0, 3: 0, 4: 1, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 16, 1: 831, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 158, 3: 9, 4: 0, 5: 104, 6: 2, 7: 66, 8: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 576, 4: 1, 5: 0, 6: 52, 7: 0, 8: 5}, 4: {0: 41, 1: 60, 2: 0, 3: 4, 4: 877, 5: 10, 6: 39, 7: 2, 8: 2}, 5: {0: 38, 1: 21, 2: 50, 3: 35, 4: 1, 5: 176, 6: 13, 7: 232, 8: 26}, 6: {0: 15, 1: 1, 2: 0, 3: 11, 4: 13, 5: 7, 6: 684, 7: 0, 8: 10}, 7: {0: 2, 1: 14, 2: 9, 3: 94, 4: 14, 5: 50, 6: 59, 7: 153, 8: 26}, 8: {0: 10, 1: 2, 2: 3, 3: 53, 4: 4, 5: 27, 6: 356, 7: 30, 8: 748}}
epoch 600, train loss avg now = 0.009212, train contrast loss now = 1.258894, test acc now = 0.7708, test loss now = 1.273160
{0: {0: 1330, 1: 3, 2: 0, 3: 0, 4: 0, 5: 1, 6: 4, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 171, 3: 7, 4: 0, 5: 85, 6: 2, 7: 73, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 572, 4: 1, 5: 0, 6: 56, 7: 0, 8: 5}, 4: {0: 46, 1: 70, 2: 0, 3: 4, 4: 867, 5: 11, 6: 33, 7: 2, 8: 2}, 5: {0: 43, 1: 16, 2: 50, 3: 32, 4: 1, 5: 177, 6: 10, 7: 244, 8: 19}, 6: {0: 16, 1: 1, 2: 0, 3: 11, 4: 15, 5: 6, 6: 684, 7: 0, 8: 8}, 7: {0: 2, 1: 13, 2: 10, 3: 80, 4: 15, 5: 55, 6: 56, 7: 166, 8: 24}, 8: {0: 14, 1: 1, 2: 2, 3: 49, 4: 5, 5: 27, 6: 357, 7: 28, 8: 750}}
epoch 700, train loss avg now = 0.008831, train contrast loss now = 1.258109, test acc now = 0.7734, test loss now = 1.292402
{0: {0: 1331, 1: 3, 2: 0, 3: 0, 4: 0, 5: 1, 6: 3, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 176, 3: 5, 4: 0, 5: 83, 6: 4, 7: 70, 8: 1}, 3: {0: 0, 1: 0, 2: 0, 3: 569, 4: 1, 5: 0, 6: 58, 7: 0, 8: 6}, 4: {0: 41, 1: 50, 2: 0, 3: 5, 4: 887, 5: 9, 6: 38, 7: 3, 8: 2}, 5: {0: 36, 1: 14, 2: 66, 3: 30, 4: 1, 5: 189, 6: 11, 7: 228, 8: 17}, 6: {0: 18, 1: 1, 2: 0, 3: 10, 4: 17, 5: 7, 6: 679, 7: 0, 8: 9}, 7: {0: 2, 1: 13, 2: 9, 3: 75, 4: 15, 5: 55, 6: 60, 7: 165, 8: 27}, 8: {0: 14, 1: 1, 2: 2, 3: 44, 4: 5, 5: 29, 6: 344, 7: 24, 8: 770}}
epoch 800, train loss avg now = 0.006875, train contrast loss now = 1.257039, test acc now = 0.7802, test loss now = 1.267107
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 181, 3: 8, 4: 0, 5: 79, 6: 2, 7: 68, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 575, 4: 1, 5: 0, 6: 52, 7: 0, 8: 5}, 4: {0: 55, 1: 43, 2: 0, 3: 4, 4: 886, 5: 6, 6: 38, 7: 2, 8: 1}, 5: {0: 24, 1: 30, 2: 83, 3: 35, 4: 1, 5: 191, 6: 8, 7: 203, 8: 17}, 6: {0: 19, 1: 1, 2: 0, 3: 12, 4: 16, 5: 9, 6: 676, 7: 0, 8: 8}, 7: {0: 2, 1: 15, 2: 13, 3: 80, 4: 14, 5: 58, 6: 56, 7: 158, 8: 25}, 8: {0: 14, 1: 2, 2: 2, 3: 51, 4: 5, 5: 35, 6: 362, 7: 23, 8: 739}}
epoch 900, train loss avg now = 0.006142, train contrast loss now = 1.255977, test acc now = 0.7765, test loss now = 1.316430
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 16, 1: 825, 2: 5, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 173, 3: 13, 4: 0, 5: 88, 6: 1, 7: 64, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 40, 7: 0, 8: 5}, 4: {0: 58, 1: 36, 2: 0, 3: 19, 4: 868, 5: 8, 6: 40, 7: 4, 8: 2}, 5: {0: 25, 1: 22, 2: 81, 3: 51, 4: 1, 5: 199, 6: 8, 7: 193, 8: 12}, 6: {0: 22, 1: 1, 2: 1, 3: 20, 4: 16, 5: 6, 6: 665, 7: 0, 8: 10}, 7: {0: 2, 1: 10, 2: 13, 3: 104, 4: 12, 5: 59, 6: 52, 7: 145, 8: 24}, 8: {0: 18, 1: 1, 2: 3, 3: 72, 4: 3, 5: 34, 6: 336, 7: 23, 8: 743}}
epoch 1000, train loss avg now = 0.008730, train contrast loss now = 1.258600, test acc now = 0.7713, test loss now = 1.335741
epoch avg loss = 8.729760208281508e-06, total time = 6899.222979545593
total 24576.0MB, used 3615.06MB, free 20960.94MB
Round 7 finish, update the prev_syn_proto
torch.Size([768, 3, 28, 28])
torch.Size([776, 3, 28, 28])
torch.Size([832, 3, 28, 28])
torch.Size([856, 3, 28, 28])
torch.Size([648, 3, 28, 28])
torch.Size([976, 3, 28, 28])
torch.Size([648, 3, 28, 28])
torch.Size([752, 3, 28, 28])
torch.Size([1040, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 16, 1: 825, 2: 5, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 173, 3: 13, 4: 0, 5: 88, 6: 1, 7: 64, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 40, 7: 0, 8: 5}, 4: {0: 58, 1: 36, 2: 0, 3: 19, 4: 868, 5: 8, 6: 40, 7: 4, 8: 2}, 5: {0: 25, 1: 22, 2: 81, 3: 51, 4: 1, 5: 199, 6: 8, 7: 193, 8: 12}, 6: {0: 22, 1: 1, 2: 1, 3: 20, 4: 16, 5: 6, 6: 665, 7: 0, 8: 10}, 7: {0: 2, 1: 10, 2: 13, 3: 104, 4: 12, 5: 59, 6: 52, 7: 145, 8: 24}, 8: {0: 18, 1: 1, 2: 3, 3: 72, 4: 3, 5: 34, 6: 336, 7: 23, 8: 743}}
round 7 evaluation: test acc is 0.7713, test loss = 1.335741
 ====== round 8 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3615.06MB, free 20960.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [3004  322  231  182  134  160  159  181  211 1193], bin edged: [0.00014198 0.00015119 0.0001604  0.00016961 0.00017882 0.00018803
 0.00019724 0.00020645 0.00021566 0.00022487 0.00023409]
class 7 have 9330 samples, histogram: [4886  487  303  262  202  209  219  253  325 2184], bin edged: [8.73135133e-05 9.29777262e-05 9.86419391e-05 1.04306152e-04
 1.09970365e-04 1.15634578e-04 1.21298791e-04 1.26963004e-04
 1.32627217e-04 1.38291429e-04 1.43955642e-04]
client 0, data condensation 0, total loss = 80.77902221679688, avg loss = 40.38951110839844
client 0, data condensation 200, total loss = 92.73309326171875, avg loss = 46.366546630859375
client 0, data condensation 400, total loss = 18.952850341796875, avg loss = 9.476425170898438
client 0, data condensation 600, total loss = 223.25244140625, avg loss = 111.626220703125
client 0, data condensation 800, total loss = 17.114715576171875, avg loss = 8.557357788085938
client 0, data condensation 1000, total loss = 22.1578369140625, avg loss = 11.07891845703125
client 0, data condensation 1200, total loss = 99.48385620117188, avg loss = 49.74192810058594
client 0, data condensation 1400, total loss = 892.1631469726562, avg loss = 446.0815734863281
client 0, data condensation 1600, total loss = 99.66680908203125, avg loss = 49.833404541015625
client 0, data condensation 1800, total loss = 12.77825927734375, avg loss = 6.389129638671875
client 0, data condensation 2000, total loss = 34.50640869140625, avg loss = 17.253204345703125
client 0, data condensation 2200, total loss = 12.3140869140625, avg loss = 6.15704345703125
client 0, data condensation 2400, total loss = 19.00860595703125, avg loss = 9.504302978515625
client 0, data condensation 2600, total loss = 14.38873291015625, avg loss = 7.194366455078125
client 0, data condensation 2800, total loss = 50.63818359375, avg loss = 25.319091796875
client 0, data condensation 3000, total loss = 7.46527099609375, avg loss = 3.732635498046875
client 0, data condensation 3200, total loss = 14.49188232421875, avg loss = 7.245941162109375
client 0, data condensation 3400, total loss = 55.30413818359375, avg loss = 27.652069091796875
client 0, data condensation 3600, total loss = 224.875732421875, avg loss = 112.4378662109375
client 0, data condensation 3800, total loss = 15.242431640625, avg loss = 7.6212158203125
client 0, data condensation 4000, total loss = 143.23590087890625, avg loss = 71.61795043945312
client 0, data condensation 4200, total loss = 14.85394287109375, avg loss = 7.426971435546875
client 0, data condensation 4400, total loss = 18.996063232421875, avg loss = 9.498031616210938
client 0, data condensation 4600, total loss = 23.179229736328125, avg loss = 11.589614868164062
client 0, data condensation 4800, total loss = 32.28216552734375, avg loss = 16.141082763671875
client 0, data condensation 5000, total loss = 20.51824951171875, avg loss = 10.259124755859375
Round 8, client 0 condense time: 407.3883671760559
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3403.06MB, free 21172.94MB
total 24576.0MB, used 3403.06MB, free 21172.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [6876  250  156  130  124  115  125  113  176  957], bin edged: [9.97474410e-05 1.06218270e-04 1.12689098e-04 1.19159927e-04
 1.25630756e-04 1.32101584e-04 1.38572413e-04 1.45043242e-04
 1.51514070e-04 1.57984899e-04 1.64455728e-04]
client 1, data condensation 0, total loss = 317.89654541015625, avg loss = 317.89654541015625
client 1, data condensation 200, total loss = 8.01031494140625, avg loss = 8.01031494140625
client 1, data condensation 400, total loss = 27.0552978515625, avg loss = 27.0552978515625
client 1, data condensation 600, total loss = 7.91534423828125, avg loss = 7.91534423828125
client 1, data condensation 800, total loss = 31.766845703125, avg loss = 31.766845703125
client 1, data condensation 1000, total loss = 31.33642578125, avg loss = 31.33642578125
client 1, data condensation 1200, total loss = 43.79266357421875, avg loss = 43.79266357421875
client 1, data condensation 1400, total loss = 55.09613037109375, avg loss = 55.09613037109375
client 1, data condensation 1600, total loss = 29.7264404296875, avg loss = 29.7264404296875
client 1, data condensation 1800, total loss = 45.39080810546875, avg loss = 45.39080810546875
client 1, data condensation 2000, total loss = 11.018798828125, avg loss = 11.018798828125
client 1, data condensation 2200, total loss = 32.54388427734375, avg loss = 32.54388427734375
client 1, data condensation 2400, total loss = 8.493896484375, avg loss = 8.493896484375
client 1, data condensation 2600, total loss = 18.47869873046875, avg loss = 18.47869873046875
client 1, data condensation 2800, total loss = 26.34478759765625, avg loss = 26.34478759765625
client 1, data condensation 3000, total loss = 42.42681884765625, avg loss = 42.42681884765625
client 1, data condensation 3200, total loss = 24.999755859375, avg loss = 24.999755859375
client 1, data condensation 3400, total loss = 19.9925537109375, avg loss = 19.9925537109375
client 1, data condensation 3600, total loss = 12.2730712890625, avg loss = 12.2730712890625
client 1, data condensation 3800, total loss = 26.37945556640625, avg loss = 26.37945556640625
client 1, data condensation 4000, total loss = 14.8709716796875, avg loss = 14.8709716796875
client 1, data condensation 4200, total loss = 15.90435791015625, avg loss = 15.90435791015625
client 1, data condensation 4400, total loss = 38.13226318359375, avg loss = 38.13226318359375
client 1, data condensation 4600, total loss = 21.785400390625, avg loss = 21.785400390625
client 1, data condensation 4800, total loss = 12.01019287109375, avg loss = 12.01019287109375
client 1, data condensation 5000, total loss = 27.78253173828125, avg loss = 27.78253173828125
Round 8, client 1 condense time: 239.32542848587036
client 1, class 0 have 9022 samples
total 24576.0MB, used 3019.06MB, free 21556.94MB
total 24576.0MB, used 3019.06MB, free 21556.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [273   6   5   3   4   1   3   4   7  21], bin edged: [0.00284499 0.00302955 0.00321411 0.00339867 0.00358323 0.0037678
 0.00395236 0.00413692 0.00432148 0.00450604 0.0046906 ]
class 5 have 12176 samples, histogram: [5241  550  403  301  275  286  306  349  500 3965], bin edged: [6.37003083e-05 6.78326828e-05 7.19650573e-05 7.60974317e-05
 8.02298062e-05 8.43621807e-05 8.84945552e-05 9.26269297e-05
 9.67593042e-05 1.00891679e-04 1.05024053e-04]
client 2, data condensation 0, total loss = 192.26055908203125, avg loss = 96.13027954101562
client 2, data condensation 200, total loss = 123.91897583007812, avg loss = 61.95948791503906
client 2, data condensation 400, total loss = 81.97271728515625, avg loss = 40.986358642578125
client 2, data condensation 600, total loss = 114.87896728515625, avg loss = 57.439483642578125
client 2, data condensation 800, total loss = 151.602294921875, avg loss = 75.8011474609375
client 2, data condensation 1000, total loss = 60.283935546875, avg loss = 30.1419677734375
client 2, data condensation 1200, total loss = 78.3214111328125, avg loss = 39.16070556640625
client 2, data condensation 1400, total loss = 46.19464111328125, avg loss = 23.097320556640625
client 2, data condensation 1600, total loss = 40.22918701171875, avg loss = 20.114593505859375
client 2, data condensation 1800, total loss = 87.08380126953125, avg loss = 43.541900634765625
client 2, data condensation 2000, total loss = 280.8067626953125, avg loss = 140.40338134765625
client 2, data condensation 2200, total loss = 93.16558837890625, avg loss = 46.582794189453125
client 2, data condensation 2400, total loss = 37.44317626953125, avg loss = 18.721588134765625
client 2, data condensation 2600, total loss = 58.26934814453125, avg loss = 29.134674072265625
client 2, data condensation 2800, total loss = 106.12420654296875, avg loss = 53.062103271484375
client 2, data condensation 3000, total loss = 61.4168701171875, avg loss = 30.70843505859375
client 2, data condensation 3200, total loss = 50.5606689453125, avg loss = 25.28033447265625
client 2, data condensation 3400, total loss = 43.42999267578125, avg loss = 21.714996337890625
client 2, data condensation 3600, total loss = 120.25543212890625, avg loss = 60.127716064453125
client 2, data condensation 3800, total loss = 283.60479736328125, avg loss = 141.80239868164062
client 2, data condensation 4000, total loss = 42.14324951171875, avg loss = 21.071624755859375
client 2, data condensation 4200, total loss = 59.622802734375, avg loss = 29.8114013671875
client 2, data condensation 4400, total loss = 59.3612060546875, avg loss = 29.68060302734375
client 2, data condensation 4600, total loss = 51.6363525390625, avg loss = 25.81817626953125
client 2, data condensation 4800, total loss = 138.24169921875, avg loss = 69.120849609375
client 2, data condensation 5000, total loss = 73.04608154296875, avg loss = 36.523040771484375
Round 8, client 2 condense time: 377.14738631248474
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3403.06MB, free 21172.94MB
total 24576.0MB, used 3403.06MB, free 21172.94MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [157  16  15  12  10   8   8   6  12  69], bin edged: [0.00260559 0.00277462 0.00294365 0.00311267 0.0032817  0.00345073
 0.00361976 0.00378879 0.00395782 0.00412685 0.00429587]
client 3, data condensation 0, total loss = 89.4945068359375, avg loss = 89.4945068359375
client 3, data condensation 200, total loss = 49.3531494140625, avg loss = 49.3531494140625
client 3, data condensation 400, total loss = 41.05560302734375, avg loss = 41.05560302734375
client 3, data condensation 600, total loss = 37.05889892578125, avg loss = 37.05889892578125
client 3, data condensation 800, total loss = 67.131103515625, avg loss = 67.131103515625
client 3, data condensation 1000, total loss = 51.795166015625, avg loss = 51.795166015625
client 3, data condensation 1200, total loss = 41.012725830078125, avg loss = 41.012725830078125
client 3, data condensation 1400, total loss = 42.1324462890625, avg loss = 42.1324462890625
client 3, data condensation 1600, total loss = 310.37261962890625, avg loss = 310.37261962890625
client 3, data condensation 1800, total loss = 63.71356201171875, avg loss = 63.71356201171875
client 3, data condensation 2000, total loss = 49.28460693359375, avg loss = 49.28460693359375
client 3, data condensation 2200, total loss = 91.91802978515625, avg loss = 91.91802978515625
client 3, data condensation 2400, total loss = 29.3873291015625, avg loss = 29.3873291015625
client 3, data condensation 2600, total loss = 40.4576416015625, avg loss = 40.4576416015625
client 3, data condensation 2800, total loss = 85.095947265625, avg loss = 85.095947265625
client 3, data condensation 3000, total loss = 47.37261962890625, avg loss = 47.37261962890625
client 3, data condensation 3200, total loss = 62.92724609375, avg loss = 62.92724609375
client 3, data condensation 3400, total loss = 464.02813720703125, avg loss = 464.02813720703125
client 3, data condensation 3600, total loss = 49.0162353515625, avg loss = 49.0162353515625
client 3, data condensation 3800, total loss = 73.31475830078125, avg loss = 73.31475830078125
client 3, data condensation 4000, total loss = 51.7105712890625, avg loss = 51.7105712890625
client 3, data condensation 4200, total loss = 120.28680419921875, avg loss = 120.28680419921875
client 3, data condensation 4400, total loss = 34.46826171875, avg loss = 34.46826171875
client 3, data condensation 4600, total loss = 44.06634521484375, avg loss = 44.06634521484375
client 3, data condensation 4800, total loss = 44.5340576171875, avg loss = 44.5340576171875
client 3, data condensation 5000, total loss = 46.4205322265625, avg loss = 46.4205322265625
Round 8, client 3 condense time: 142.76623439788818
client 3, class 6 have 313 samples
total 24576.0MB, used 3017.06MB, free 21558.94MB
total 24576.0MB, used 3017.06MB, free 21558.94MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [309   7   1   0   2   1   4   3   2  32], bin edged: [0.00257515 0.00274221 0.00290926 0.00307632 0.00324337 0.00341043
 0.00357748 0.00374454 0.0039116  0.00407865 0.00424571]
class 4 have 1048 samples, histogram: [551  68  39  29  27  25  37  35  29 208], bin edged: [0.00078701 0.00083807 0.00088912 0.00094018 0.00099123 0.00104229
 0.00109334 0.0011444  0.00119545 0.00124651 0.00129756]
class 6 have 7572 samples, histogram: [5344  342  216  165  132  140  121  139  212  761], bin edged: [0.00011739 0.00012501 0.00013262 0.00014024 0.00014785 0.00015547
 0.00016308 0.0001707  0.00017832 0.00018593 0.00019355]
client 4, data condensation 0, total loss = 133.82208251953125, avg loss = 44.60736083984375
client 4, data condensation 200, total loss = 66.52001953125, avg loss = 22.17333984375
client 4, data condensation 400, total loss = 55.59893798828125, avg loss = 18.532979329427082
client 4, data condensation 600, total loss = 600.6436767578125, avg loss = 200.21455891927084
client 4, data condensation 800, total loss = 431.87469482421875, avg loss = 143.9582316080729
client 4, data condensation 1000, total loss = 63.478179931640625, avg loss = 21.159393310546875
client 4, data condensation 1200, total loss = 83.5413818359375, avg loss = 27.847127278645832
client 4, data condensation 1400, total loss = 68.421630859375, avg loss = 22.807210286458332
client 4, data condensation 1600, total loss = 46.28912353515625, avg loss = 15.429707845052084
client 4, data condensation 1800, total loss = 117.87042236328125, avg loss = 39.290140787760414
client 4, data condensation 2000, total loss = 53.5218505859375, avg loss = 17.840616861979168
client 4, data condensation 2200, total loss = 62.052032470703125, avg loss = 20.684010823567707
client 4, data condensation 2400, total loss = 62.73760986328125, avg loss = 20.91253662109375
client 4, data condensation 2600, total loss = 280.253173828125, avg loss = 93.417724609375
client 4, data condensation 2800, total loss = 123.83355712890625, avg loss = 41.277852376302086
client 4, data condensation 3000, total loss = 48.9560546875, avg loss = 16.318684895833332
client 4, data condensation 3200, total loss = 71.25787353515625, avg loss = 23.75262451171875
client 4, data condensation 3400, total loss = 61.573486328125, avg loss = 20.524495442708332
client 4, data condensation 3600, total loss = 39.76483154296875, avg loss = 13.25494384765625
client 4, data condensation 3800, total loss = 58.245849609375, avg loss = 19.415283203125
client 4, data condensation 4000, total loss = 67.38739013671875, avg loss = 22.46246337890625
client 4, data condensation 4200, total loss = 57.52276611328125, avg loss = 19.17425537109375
client 4, data condensation 4400, total loss = 99.4449462890625, avg loss = 33.1483154296875
client 4, data condensation 4600, total loss = 54.8953857421875, avg loss = 18.2984619140625
client 4, data condensation 4800, total loss = 97.9130859375, avg loss = 32.6376953125
client 4, data condensation 5000, total loss = 124.21014404296875, avg loss = 41.40338134765625
Round 8, client 4 condense time: 420.2759966850281
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 3659.06MB, free 20916.94MB
total 24576.0MB, used 3659.06MB, free 20916.94MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [3668  526  371  281  280  290  289  360  575 5511], bin edged: [5.98990532e-05 6.37848322e-05 6.76706112e-05 7.15563902e-05
 7.54421692e-05 7.93279482e-05 8.32137272e-05 8.70995062e-05
 9.09852852e-05 9.48710641e-05 9.87568431e-05]
client 5, data condensation 0, total loss = 66.40875244140625, avg loss = 66.40875244140625
client 5, data condensation 200, total loss = 7.1871337890625, avg loss = 7.1871337890625
client 5, data condensation 400, total loss = 9.43389892578125, avg loss = 9.43389892578125
client 5, data condensation 600, total loss = 6.25885009765625, avg loss = 6.25885009765625
client 5, data condensation 800, total loss = 18.26727294921875, avg loss = 18.26727294921875
client 5, data condensation 1000, total loss = 17.292236328125, avg loss = 17.292236328125
client 5, data condensation 1200, total loss = 13.677154541015625, avg loss = 13.677154541015625
client 5, data condensation 1400, total loss = 5.3385009765625, avg loss = 5.3385009765625
client 5, data condensation 1600, total loss = 7.609619140625, avg loss = 7.609619140625
client 5, data condensation 1800, total loss = 73.191650390625, avg loss = 73.191650390625
client 5, data condensation 2000, total loss = 7.93170166015625, avg loss = 7.93170166015625
client 5, data condensation 2200, total loss = 4.87542724609375, avg loss = 4.87542724609375
client 5, data condensation 2400, total loss = 116.40902709960938, avg loss = 116.40902709960938
client 5, data condensation 2600, total loss = 8.53973388671875, avg loss = 8.53973388671875
client 5, data condensation 2800, total loss = 39.592041015625, avg loss = 39.592041015625
client 5, data condensation 3000, total loss = 8.15472412109375, avg loss = 8.15472412109375
client 5, data condensation 3200, total loss = 24.17608642578125, avg loss = 24.17608642578125
client 5, data condensation 3400, total loss = 18.4954833984375, avg loss = 18.4954833984375
client 5, data condensation 3600, total loss = 6.22906494140625, avg loss = 6.22906494140625
client 5, data condensation 3800, total loss = 15.60888671875, avg loss = 15.60888671875
client 5, data condensation 4000, total loss = 15.2855224609375, avg loss = 15.2855224609375
client 5, data condensation 4200, total loss = 53.82745361328125, avg loss = 53.82745361328125
client 5, data condensation 4400, total loss = 58.762664794921875, avg loss = 58.762664794921875
client 5, data condensation 4600, total loss = 7.1102294921875, avg loss = 7.1102294921875
client 5, data condensation 4800, total loss = 12.55889892578125, avg loss = 12.55889892578125
client 5, data condensation 5000, total loss = 581.7066650390625, avg loss = 581.7066650390625
Round 8, client 5 condense time: 257.31288170814514
client 5, class 8 have 12151 samples
total 24576.0MB, used 3021.06MB, free 21554.94MB
total 24576.0MB, used 3021.06MB, free 21554.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [8011  287  158  131  100  110  101  103  158 1035], bin edged: [8.91827158e-05 9.49681883e-05 1.00753661e-04 1.06539133e-04
 1.12324606e-04 1.18110078e-04 1.23895551e-04 1.29681023e-04
 1.35466496e-04 1.41251968e-04 1.47037441e-04]
client 6, data condensation 0, total loss = 165.0013427734375, avg loss = 165.0013427734375
client 6, data condensation 200, total loss = 36.32537841796875, avg loss = 36.32537841796875
client 6, data condensation 400, total loss = 14.5377197265625, avg loss = 14.5377197265625
client 6, data condensation 600, total loss = 309.23101806640625, avg loss = 309.23101806640625
client 6, data condensation 800, total loss = 6.2459716796875, avg loss = 6.2459716796875
client 6, data condensation 1000, total loss = 45.47027587890625, avg loss = 45.47027587890625
client 6, data condensation 1200, total loss = 29.36138916015625, avg loss = 29.36138916015625
client 6, data condensation 1400, total loss = 9.448486328125, avg loss = 9.448486328125
client 6, data condensation 1600, total loss = 214.59368896484375, avg loss = 214.59368896484375
client 6, data condensation 1800, total loss = 13.8018798828125, avg loss = 13.8018798828125
client 6, data condensation 2000, total loss = 14.72900390625, avg loss = 14.72900390625
client 6, data condensation 2200, total loss = 171.50152587890625, avg loss = 171.50152587890625
client 6, data condensation 2400, total loss = 586.9091186523438, avg loss = 586.9091186523438
client 6, data condensation 2600, total loss = 7.526123046875, avg loss = 7.526123046875
client 6, data condensation 2800, total loss = 7.69744873046875, avg loss = 7.69744873046875
client 6, data condensation 3000, total loss = 9.59130859375, avg loss = 9.59130859375
client 6, data condensation 3200, total loss = 18.1123046875, avg loss = 18.1123046875
client 6, data condensation 3400, total loss = 12.91497802734375, avg loss = 12.91497802734375
client 6, data condensation 3600, total loss = 22.88214111328125, avg loss = 22.88214111328125
client 6, data condensation 3800, total loss = 23.60845947265625, avg loss = 23.60845947265625
client 6, data condensation 4000, total loss = 15.73486328125, avg loss = 15.73486328125
client 6, data condensation 4200, total loss = 9.30322265625, avg loss = 9.30322265625
client 6, data condensation 4400, total loss = 10.45654296875, avg loss = 10.45654296875
client 6, data condensation 4600, total loss = 12.66876220703125, avg loss = 12.66876220703125
client 6, data condensation 4800, total loss = 533.0379638671875, avg loss = 533.0379638671875
client 6, data condensation 5000, total loss = 30.384033203125, avg loss = 30.384033203125
Round 8, client 6 condense time: 218.6741795539856
client 6, class 3 have 10194 samples
total 24576.0MB, used 3023.06MB, free 21552.94MB
total 24576.0MB, used 3023.06MB, free 21552.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [6502  280  128   86   69   60   60   77  112 1738], bin edged: [9.50016714e-05 1.01164620e-04 1.07327569e-04 1.13490518e-04
 1.19653467e-04 1.25816416e-04 1.31979364e-04 1.38142313e-04
 1.44305262e-04 1.50468211e-04 1.56631160e-04]
client 7, data condensation 0, total loss = 134.52557373046875, avg loss = 134.52557373046875
client 7, data condensation 200, total loss = 10.9246826171875, avg loss = 10.9246826171875
client 7, data condensation 400, total loss = 36.909912109375, avg loss = 36.909912109375
client 7, data condensation 600, total loss = 14.34210205078125, avg loss = 14.34210205078125
client 7, data condensation 800, total loss = 20.25213623046875, avg loss = 20.25213623046875
client 7, data condensation 1000, total loss = 20.33905029296875, avg loss = 20.33905029296875
client 7, data condensation 1200, total loss = 14.98980712890625, avg loss = 14.98980712890625
client 7, data condensation 1400, total loss = 17.02825927734375, avg loss = 17.02825927734375
client 7, data condensation 1600, total loss = 5.0819091796875, avg loss = 5.0819091796875
client 7, data condensation 1800, total loss = 8.22943115234375, avg loss = 8.22943115234375
client 7, data condensation 2000, total loss = 8.67669677734375, avg loss = 8.67669677734375
client 7, data condensation 2200, total loss = 25.08251953125, avg loss = 25.08251953125
client 7, data condensation 2400, total loss = 16.7645263671875, avg loss = 16.7645263671875
client 7, data condensation 2600, total loss = 4.42584228515625, avg loss = 4.42584228515625
client 7, data condensation 2800, total loss = 8.14361572265625, avg loss = 8.14361572265625
client 7, data condensation 3000, total loss = 10.70965576171875, avg loss = 10.70965576171875
client 7, data condensation 3200, total loss = 18.184814453125, avg loss = 18.184814453125
client 7, data condensation 3400, total loss = 3.95654296875, avg loss = 3.95654296875
client 7, data condensation 3600, total loss = 7.11773681640625, avg loss = 7.11773681640625
client 7, data condensation 3800, total loss = 18.20343017578125, avg loss = 18.20343017578125
client 7, data condensation 4000, total loss = 14.194091796875, avg loss = 14.194091796875
client 7, data condensation 4200, total loss = 14.11419677734375, avg loss = 14.11419677734375
client 7, data condensation 4400, total loss = 6.46124267578125, avg loss = 6.46124267578125
client 7, data condensation 4600, total loss = 22.102783203125, avg loss = 22.102783203125
client 7, data condensation 4800, total loss = 540.6404418945312, avg loss = 540.6404418945312
client 7, data condensation 5000, total loss = 18.61761474609375, avg loss = 18.61761474609375
Round 8, client 7 condense time: 200.84770941734314
client 7, class 1 have 9112 samples
total 24576.0MB, used 3023.06MB, free 21552.94MB
total 24576.0MB, used 3023.06MB, free 21552.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [195   3   0   3   0   0   0   0   1   4], bin edged: [0.00475707 0.00506567 0.00537427 0.00568287 0.00599147 0.00630007
 0.00660867 0.00691727 0.00722587 0.00753447 0.00784307]
class 4 have 1179 samples, histogram: [783  52  30  29  18  21  23  18  31 174], bin edged: [0.00073523 0.00078293 0.00083062 0.00087832 0.00092602 0.00097371
 0.00102141 0.00106911 0.0011168  0.0011645  0.00121219]
class 8 have 734 samples, histogram: [410  36  21  16  16   6  19  18  30 162], bin edged: [0.00112127 0.00119401 0.00126675 0.00133949 0.00141223 0.00148497
 0.00155771 0.00163045 0.00170319 0.00177592 0.00184866]
client 8, data condensation 0, total loss = 190.49176025390625, avg loss = 63.49725341796875
client 8, data condensation 200, total loss = 266.36590576171875, avg loss = 88.78863525390625
client 8, data condensation 400, total loss = 156.07623291015625, avg loss = 52.025410970052086
client 8, data condensation 600, total loss = 67.2364501953125, avg loss = 22.412150065104168
client 8, data condensation 800, total loss = 99.2081298828125, avg loss = 33.069376627604164
client 8, data condensation 1000, total loss = 362.07501220703125, avg loss = 120.69167073567708
client 8, data condensation 1200, total loss = 80.246337890625, avg loss = 26.748779296875
client 8, data condensation 1400, total loss = 439.5080261230469, avg loss = 146.50267537434897
client 8, data condensation 1600, total loss = 76.90277099609375, avg loss = 25.634256998697918
client 8, data condensation 1800, total loss = 97.59014892578125, avg loss = 32.530049641927086
client 8, data condensation 2000, total loss = 494.27557373046875, avg loss = 164.7585245768229
client 8, data condensation 2200, total loss = 122.07452392578125, avg loss = 40.691507975260414
client 8, data condensation 2400, total loss = 57.401885986328125, avg loss = 19.133961995442707
client 8, data condensation 2600, total loss = 64.36181640625, avg loss = 21.453938802083332
client 8, data condensation 2800, total loss = 57.103790283203125, avg loss = 19.034596761067707
client 8, data condensation 3000, total loss = 182.6959228515625, avg loss = 60.898640950520836
client 8, data condensation 3200, total loss = 122.1871337890625, avg loss = 40.729044596354164
client 8, data condensation 3400, total loss = 47.81365966796875, avg loss = 15.937886555989584
client 8, data condensation 3600, total loss = 70.43551635742188, avg loss = 23.478505452473957
client 8, data condensation 3800, total loss = 67.05230712890625, avg loss = 22.35076904296875
client 8, data condensation 4000, total loss = 57.08477783203125, avg loss = 19.02825927734375
client 8, data condensation 4200, total loss = 109.65353393554688, avg loss = 36.551177978515625
client 8, data condensation 4400, total loss = 53.29608154296875, avg loss = 17.765360514322918
client 8, data condensation 4600, total loss = 64.1146240234375, avg loss = 21.371541341145832
client 8, data condensation 4800, total loss = 53.244140625, avg loss = 17.748046875
client 8, data condensation 5000, total loss = 97.97760009765625, avg loss = 32.659200032552086
Round 8, client 8 condense time: 400.5549433231354
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3661.06MB, free 20914.94MB
total 24576.0MB, used 3661.06MB, free 20914.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2459  337  212  192  170  172  199  235  358 5982], bin edged: [6.75282988e-05 7.19090032e-05 7.62897075e-05 8.06704119e-05
 8.50511163e-05 8.94318207e-05 9.38125251e-05 9.81932294e-05
 1.02573934e-04 1.06954638e-04 1.11335343e-04]
client 9, data condensation 0, total loss = 66.33905029296875, avg loss = 66.33905029296875
client 9, data condensation 200, total loss = 7.5623779296875, avg loss = 7.5623779296875
client 9, data condensation 400, total loss = 2.511993408203125, avg loss = 2.511993408203125
client 9, data condensation 600, total loss = 14.467041015625, avg loss = 14.467041015625
client 9, data condensation 800, total loss = 5.808837890625, avg loss = 5.808837890625
client 9, data condensation 1000, total loss = 5.04400634765625, avg loss = 5.04400634765625
client 9, data condensation 1200, total loss = 7.290191650390625, avg loss = 7.290191650390625
client 9, data condensation 1400, total loss = 3.06427001953125, avg loss = 3.06427001953125
client 9, data condensation 1600, total loss = 3.494598388671875, avg loss = 3.494598388671875
client 9, data condensation 1800, total loss = 12.29425048828125, avg loss = 12.29425048828125
client 9, data condensation 2000, total loss = 4.55059814453125, avg loss = 4.55059814453125
client 9, data condensation 2200, total loss = 22.487060546875, avg loss = 22.487060546875
client 9, data condensation 2400, total loss = 8.68035888671875, avg loss = 8.68035888671875
client 9, data condensation 2600, total loss = 9.7086181640625, avg loss = 9.7086181640625
client 9, data condensation 2800, total loss = 10.368316650390625, avg loss = 10.368316650390625
client 9, data condensation 3000, total loss = 13.706298828125, avg loss = 13.706298828125
client 9, data condensation 3200, total loss = 9.2335205078125, avg loss = 9.2335205078125
client 9, data condensation 3400, total loss = 20.280181884765625, avg loss = 20.280181884765625
client 9, data condensation 3600, total loss = 232.056396484375, avg loss = 232.056396484375
client 9, data condensation 3800, total loss = 40.4249267578125, avg loss = 40.4249267578125
client 9, data condensation 4000, total loss = 14.968109130859375, avg loss = 14.968109130859375
client 9, data condensation 4200, total loss = 29.167755126953125, avg loss = 29.167755126953125
client 9, data condensation 4400, total loss = 132.3033447265625, avg loss = 132.3033447265625
client 9, data condensation 4600, total loss = 9.243133544921875, avg loss = 9.243133544921875
client 9, data condensation 4800, total loss = 5.3896484375, avg loss = 5.3896484375
client 9, data condensation 5000, total loss = 3.84564208984375, avg loss = 3.84564208984375
Round 8, client 9 condense time: 219.3478980064392
client 9, class 2 have 10316 samples
total 24576.0MB, used 3025.06MB, free 21550.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 2.5446e+01,  4.0816e+00, -5.3130e+00, -1.6379e+01,  8.5716e+00,
          6.0964e+00,  1.1131e+00, -9.0387e+00, -1.4244e+01],
        [ 1.4530e+00,  1.4791e+01,  2.0535e+00, -5.6496e+00, -4.3862e-01,
          3.6689e+00, -7.5243e+00, -1.6622e+00, -5.6246e+00],
        [-1.1821e+01, -1.0403e+01,  1.0891e+01, -2.5632e+00, -1.4913e+00,
          8.0988e+00,  8.7349e-01,  6.6707e+00,  3.7775e-01],
        [-1.3925e+01, -1.1740e+01, -1.8759e+00,  2.2211e+01,  2.5340e-03,
         -3.0766e+00,  9.1990e+00,  1.6915e+00, -2.5358e+00],
        [-3.0912e+00, -6.4942e+00, -3.6166e+00, -1.6560e+00,  1.1965e+01,
         -1.6674e+00,  8.1571e+00,  1.9098e-01, -3.5328e+00],
        [-8.5835e+00, -7.1396e+00,  5.0785e+00, -7.2945e+00, -1.3373e-01,
          1.4213e+01, -1.1803e+00,  5.1557e+00,  6.1177e-01],
        [-8.7339e+00, -1.1003e+01, -1.0868e+00,  3.7575e+00,  3.2932e+00,
         -2.6241e+00,  1.4929e+01,  5.5412e-02,  1.3579e+00],
        [-1.3523e+01, -1.2213e+01,  3.4840e+00,  1.7151e+00,  9.9909e-01,
          5.1544e+00,  2.3673e+00,  1.1945e+01,  9.0227e-01],
        [-1.3609e+01, -1.0034e+01,  1.3566e-01,  4.9644e-01, -9.8220e-01,
          1.4231e+00,  1.1476e+01, -6.9412e-01,  1.2113e+01]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 2, 0, 4],
        [2, 5, 7, 6, 8],
        [3, 6, 7, 4, 2],
        [4, 6, 7, 3, 5],
        [5, 7, 2, 8, 4],
        [6, 3, 4, 8, 7],
        [7, 5, 2, 6, 3],
        [8, 6, 5, 3, 2]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
9
Round 8: # synthetic sample: 8208
total 24576.0MB, used 3025.06MB, free 21550.94MB
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 16, 1: 825, 2: 5, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 173, 3: 13, 4: 0, 5: 88, 6: 1, 7: 64, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 40, 7: 0, 8: 5}, 4: {0: 58, 1: 36, 2: 0, 3: 19, 4: 868, 5: 8, 6: 40, 7: 4, 8: 2}, 5: {0: 25, 1: 22, 2: 81, 3: 51, 4: 1, 5: 199, 6: 8, 7: 193, 8: 12}, 6: {0: 22, 1: 1, 2: 1, 3: 20, 4: 16, 5: 6, 6: 665, 7: 0, 8: 10}, 7: {0: 2, 1: 10, 2: 13, 3: 104, 4: 12, 5: 59, 6: 52, 7: 145, 8: 24}, 8: {0: 18, 1: 1, 2: 3, 3: 72, 4: 3, 5: 34, 6: 336, 7: 23, 8: 743}}
round 8 evaluation: test acc is 0.7713, test loss = 1.335741
{0: {0: 1262, 1: 20, 2: 0, 3: 0, 4: 14, 5: 42, 6: 0, 7: 0, 8: 0}, 1: {0: 5, 1: 842, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 186, 3: 8, 4: 0, 5: 130, 6: 0, 7: 12, 8: 1}, 3: {0: 0, 1: 0, 2: 2, 3: 611, 4: 3, 5: 5, 6: 9, 7: 0, 8: 4}, 4: {0: 13, 1: 199, 2: 1, 3: 43, 4: 749, 5: 12, 6: 14, 7: 2, 8: 2}, 5: {0: 0, 1: 53, 2: 231, 3: 23, 4: 0, 5: 223, 6: 0, 7: 44, 8: 18}, 6: {0: 2, 1: 8, 2: 34, 3: 56, 4: 61, 5: 72, 6: 466, 7: 0, 8: 42}, 7: {0: 2, 1: 32, 2: 55, 3: 118, 4: 10, 5: 52, 6: 8, 7: 98, 8: 46}, 8: {0: 3, 1: 19, 2: 39, 3: 89, 4: 10, 5: 99, 6: 82, 7: 15, 8: 877}}
epoch 0, train loss avg now = 0.060247, train contrast loss now = 1.580513, test acc now = 0.7401, test loss now = 1.570787
{0: {0: 1326, 1: 4, 2: 0, 3: 0, 4: 0, 5: 5, 6: 3, 7: 0, 8: 0}, 1: {0: 20, 1: 827, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 79, 3: 9, 4: 0, 5: 163, 6: 0, 7: 76, 8: 10}, 3: {0: 0, 1: 0, 2: 0, 3: 569, 4: 1, 5: 0, 6: 57, 7: 0, 8: 7}, 4: {0: 44, 1: 73, 2: 0, 3: 13, 4: 814, 5: 9, 6: 77, 7: 1, 8: 4}, 5: {0: 10, 1: 49, 2: 23, 3: 55, 4: 0, 5: 218, 6: 18, 7: 152, 8: 67}, 6: {0: 15, 1: 1, 2: 0, 3: 6, 4: 4, 5: 20, 6: 639, 7: 0, 8: 56}, 7: {0: 3, 1: 24, 2: 10, 3: 119, 4: 4, 5: 49, 6: 60, 7: 83, 8: 69}, 8: {0: 11, 1: 3, 2: 0, 3: 39, 4: 0, 5: 23, 6: 182, 7: 9, 8: 966}}
epoch 100, train loss avg now = 0.062171, train contrast loss now = 1.314635, test acc now = 0.7689, test loss now = 1.593360
{0: {0: 1277, 1: 6, 2: 0, 3: 0, 4: 53, 5: 1, 6: 1, 7: 0, 8: 0}, 1: {0: 11, 1: 830, 2: 6, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 249, 3: 10, 4: 1, 5: 22, 6: 0, 7: 51, 8: 6}, 3: {0: 1, 1: 0, 2: 0, 3: 607, 4: 7, 5: 0, 6: 9, 7: 0, 8: 10}, 4: {0: 9, 1: 49, 2: 0, 3: 1, 4: 969, 5: 3, 6: 0, 7: 2, 8: 2}, 5: {0: 25, 1: 31, 2: 71, 3: 39, 4: 4, 5: 170, 6: 0, 7: 188, 8: 64}, 6: {0: 6, 1: 0, 2: 1, 3: 43, 4: 203, 5: 4, 6: 394, 7: 3, 8: 87}, 7: {0: 2, 1: 9, 2: 13, 3: 78, 4: 36, 5: 53, 6: 10, 7: 144, 8: 76}, 8: {0: 7, 1: 6, 2: 1, 3: 68, 4: 37, 5: 11, 6: 48, 7: 35, 8: 1020}}
epoch 200, train loss avg now = 0.033628, train contrast loss now = 1.286921, test acc now = 0.7883, test loss now = 1.232840
{0: {0: 1318, 1: 5, 2: 0, 3: 0, 4: 0, 5: 4, 6: 11, 7: 0, 8: 0}, 1: {0: 6, 1: 834, 2: 7, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 141, 3: 24, 4: 0, 5: 103, 6: 0, 7: 36, 8: 34}, 3: {0: 0, 1: 0, 2: 0, 3: 607, 4: 0, 5: 0, 6: 22, 7: 0, 8: 5}, 4: {0: 49, 1: 167, 2: 0, 3: 35, 4: 590, 5: 45, 6: 142, 7: 3, 8: 4}, 5: {0: 3, 1: 54, 2: 33, 3: 76, 4: 0, 5: 198, 6: 10, 7: 102, 8: 116}, 6: {0: 15, 1: 1, 2: 0, 3: 37, 4: 5, 5: 3, 6: 617, 7: 0, 8: 63}, 7: {0: 2, 1: 15, 2: 10, 3: 123, 4: 4, 5: 54, 6: 26, 7: 72, 8: 115}, 8: {0: 4, 1: 3, 2: 0, 3: 72, 4: 1, 5: 7, 6: 125, 7: 7, 8: 1014}}
epoch 300, train loss avg now = 0.046093, train contrast loss now = 1.283984, test acc now = 0.7508, test loss now = 1.579423
{0: {0: 1326, 1: 5, 2: 0, 3: 0, 4: 2, 5: 3, 6: 2, 7: 0, 8: 0}, 1: {0: 40, 1: 799, 2: 5, 3: 0, 4: 0, 5: 3, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 218, 3: 1, 4: 0, 5: 28, 6: 0, 7: 91, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 573, 4: 1, 5: 1, 6: 54, 7: 1, 8: 3}, 4: {0: 56, 1: 88, 2: 0, 3: 6, 4: 816, 5: 18, 6: 23, 7: 26, 8: 2}, 5: {0: 40, 1: 16, 2: 41, 3: 8, 4: 1, 5: 199, 6: 0, 7: 284, 8: 3}, 6: {0: 21, 1: 1, 2: 2, 3: 13, 4: 39, 5: 14, 6: 630, 7: 11, 8: 10}, 7: {0: 2, 1: 3, 2: 9, 3: 34, 4: 7, 5: 61, 6: 19, 7: 275, 8: 11}, 8: {0: 13, 1: 5, 2: 8, 3: 48, 4: 10, 5: 81, 6: 233, 7: 110, 8: 725}}
epoch 400, train loss avg now = 0.016455, train contrast loss now = 1.279916, test acc now = 0.7745, test loss now = 1.333375
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1333, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 3, 7: 0, 8: 0}, 1: {0: 21, 1: 803, 2: 22, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 221, 3: 4, 4: 0, 5: 54, 6: 0, 7: 58, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 582, 4: 1, 5: 0, 6: 46, 7: 1, 8: 3}, 4: {0: 54, 1: 44, 2: 0, 3: 3, 4: 903, 5: 5, 6: 21, 7: 3, 8: 2}, 5: {0: 29, 1: 28, 2: 104, 3: 31, 4: 1, 5: 216, 6: 6, 7: 167, 8: 10}, 6: {0: 18, 1: 1, 2: 0, 3: 19, 4: 16, 5: 10, 6: 662, 7: 3, 8: 12}, 7: {0: 4, 1: 12, 2: 16, 3: 71, 4: 24, 5: 61, 6: 36, 7: 175, 8: 22}, 8: {0: 16, 1: 5, 2: 3, 3: 52, 4: 7, 5: 36, 6: 278, 7: 41, 8: 795}}
epoch 500, train loss avg now = 0.024405, train contrast loss now = 1.277936, test acc now = 0.7925, test loss now = 1.248743
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 15, 1: 819, 2: 13, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 230, 3: 8, 4: 0, 5: 23, 6: 2, 7: 74, 8: 1}, 3: {0: 3, 1: 0, 2: 0, 3: 579, 4: 2, 5: 0, 6: 46, 7: 0, 8: 4}, 4: {0: 66, 1: 60, 2: 0, 3: 1, 4: 883, 5: 5, 6: 18, 7: 1, 8: 1}, 5: {0: 34, 1: 30, 2: 67, 3: 40, 4: 1, 5: 168, 6: 13, 7: 215, 8: 24}, 6: {0: 28, 1: 1, 2: 0, 3: 19, 4: 27, 5: 7, 6: 645, 7: 1, 8: 13}, 7: {0: 7, 1: 18, 2: 13, 3: 75, 4: 29, 5: 38, 6: 47, 7: 162, 8: 32}, 8: {0: 16, 1: 4, 2: 3, 3: 47, 4: 11, 5: 20, 6: 288, 7: 29, 8: 815}}
epoch 600, train loss avg now = 0.011734, train contrast loss now = 1.276719, test acc now = 0.7847, test loss now = 1.387465
{0: {0: 1331, 1: 3, 2: 0, 3: 0, 4: 0, 5: 2, 6: 2, 7: 0, 8: 0}, 1: {0: 20, 1: 820, 2: 5, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 0, 2: 236, 3: 4, 4: 0, 5: 23, 6: 1, 7: 73, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 571, 4: 1, 5: 0, 6: 57, 7: 1, 8: 3}, 4: {0: 49, 1: 37, 2: 0, 3: 1, 4: 911, 5: 3, 6: 30, 7: 3, 8: 1}, 5: {0: 52, 1: 8, 2: 68, 3: 21, 4: 1, 5: 179, 6: 9, 7: 239, 8: 15}, 6: {0: 20, 1: 1, 2: 0, 3: 17, 4: 23, 5: 11, 6: 650, 7: 5, 8: 14}, 7: {0: 6, 1: 8, 2: 9, 3: 62, 4: 21, 5: 47, 6: 37, 7: 202, 8: 29}, 8: {0: 12, 1: 2, 2: 4, 3: 40, 4: 9, 5: 29, 6: 262, 7: 56, 8: 819}}
epoch 700, train loss avg now = 0.009111, train contrast loss now = 1.276043, test acc now = 0.7965, test loss now = 1.221696
{0: {0: 1333, 1: 3, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 14, 1: 820, 2: 13, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 217, 3: 10, 4: 0, 5: 43, 6: 2, 7: 67, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 583, 4: 1, 5: 0, 6: 47, 7: 0, 8: 2}, 4: {0: 65, 1: 65, 2: 0, 3: 1, 4: 863, 5: 8, 6: 29, 7: 3, 8: 1}, 5: {0: 30, 1: 29, 2: 59, 3: 30, 4: 1, 5: 199, 6: 9, 7: 224, 8: 11}, 6: {0: 24, 1: 1, 2: 0, 3: 20, 4: 19, 5: 8, 6: 658, 7: 1, 8: 10}, 7: {0: 2, 1: 10, 2: 11, 3: 84, 4: 21, 5: 61, 6: 47, 7: 163, 8: 22}, 8: {0: 16, 1: 4, 2: 3, 3: 55, 4: 7, 5: 25, 6: 327, 7: 38, 8: 758}}
epoch 800, train loss avg now = 0.006617, train contrast loss now = 1.278938, test acc now = 0.7791, test loss now = 1.315715
{0: {0: 1330, 1: 3, 2: 0, 3: 0, 4: 0, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 15, 1: 830, 2: 2, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 225, 3: 9, 4: 0, 5: 35, 6: 1, 7: 69, 8: 0}, 3: {0: 1, 1: 0, 2: 0, 3: 571, 4: 1, 5: 0, 6: 58, 7: 1, 8: 2}, 4: {0: 55, 1: 58, 2: 0, 3: 1, 4: 884, 5: 6, 6: 27, 7: 3, 8: 1}, 5: {0: 50, 1: 9, 2: 46, 3: 27, 4: 1, 5: 191, 6: 11, 7: 239, 8: 18}, 6: {0: 19, 1: 1, 2: 0, 3: 14, 4: 17, 5: 9, 6: 668, 7: 1, 8: 12}, 7: {0: 2, 1: 12, 2: 8, 3: 76, 4: 22, 5: 55, 6: 40, 7: 183, 8: 23}, 8: {0: 12, 1: 2, 2: 4, 3: 46, 4: 8, 5: 32, 6: 302, 7: 45, 8: 782}}
epoch 900, train loss avg now = 0.008166, train contrast loss now = 1.277808, test acc now = 0.7889, test loss now = 1.226205
{0: {0: 1328, 1: 4, 2: 0, 3: 0, 4: 0, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 225, 3: 6, 4: 0, 5: 39, 6: 1, 7: 67, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 570, 4: 1, 5: 0, 6: 56, 7: 1, 8: 5}, 4: {0: 50, 1: 66, 2: 0, 3: 2, 4: 873, 5: 5, 6: 34, 7: 3, 8: 2}, 5: {0: 43, 1: 23, 2: 59, 3: 30, 4: 1, 5: 181, 6: 7, 7: 229, 8: 19}, 6: {0: 19, 1: 1, 2: 0, 3: 16, 4: 16, 5: 9, 6: 660, 7: 1, 8: 19}, 7: {0: 2, 1: 18, 2: 9, 3: 73, 4: 15, 5: 50, 6: 33, 7: 188, 8: 33}, 8: {0: 11, 1: 3, 2: 2, 3: 46, 4: 7, 5: 24, 6: 224, 7: 35, 8: 881}}
epoch 1000, train loss avg now = 0.007858, train contrast loss now = 1.278838, test acc now = 0.7997, test loss now = 1.184037
epoch avg loss = 7.857737924765658e-06, total time = 7439.344517230988
total 24576.0MB, used 3653.06MB, free 20922.94MB
Round 8 finish, update the prev_syn_proto
torch.Size([864, 3, 28, 28])
torch.Size([873, 3, 28, 28])
torch.Size([936, 3, 28, 28])
torch.Size([963, 3, 28, 28])
torch.Size([729, 3, 28, 28])
torch.Size([1098, 3, 28, 28])
torch.Size([729, 3, 28, 28])
torch.Size([846, 3, 28, 28])
torch.Size([1170, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1328, 1: 4, 2: 0, 3: 0, 4: 0, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 225, 3: 6, 4: 0, 5: 39, 6: 1, 7: 67, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 570, 4: 1, 5: 0, 6: 56, 7: 1, 8: 5}, 4: {0: 50, 1: 66, 2: 0, 3: 2, 4: 873, 5: 5, 6: 34, 7: 3, 8: 2}, 5: {0: 43, 1: 23, 2: 59, 3: 30, 4: 1, 5: 181, 6: 7, 7: 229, 8: 19}, 6: {0: 19, 1: 1, 2: 0, 3: 16, 4: 16, 5: 9, 6: 660, 7: 1, 8: 19}, 7: {0: 2, 1: 18, 2: 9, 3: 73, 4: 15, 5: 50, 6: 33, 7: 188, 8: 33}, 8: {0: 11, 1: 3, 2: 2, 3: 46, 4: 7, 5: 24, 6: 224, 7: 35, 8: 881}}
round 8 evaluation: test acc is 0.7997, test loss = 1.184037
 ====== round 9 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3653.06MB, free 20922.94MB
initialized by random noise
client 0 have real samples [5777, 9330]
client 0 will condense {4: 58, 7: 94} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 4 have 5777 samples, histogram: [3701  278  175  148  113  127  112  120  150  853], bin edged: [0.00014915 0.00015883 0.0001685  0.00017818 0.00018785 0.00019753
 0.00020721 0.00021688 0.00022656 0.00023623 0.00024591]
class 7 have 9330 samples, histogram: [5013  509  363  240  212  201  224  226  355 1987], bin edged: [8.81782325e-05 9.38985407e-05 9.96188490e-05 1.05339157e-04
 1.11059465e-04 1.16779774e-04 1.22500082e-04 1.28220390e-04
 1.33940698e-04 1.39661006e-04 1.45381315e-04]
client 0, data condensation 0, total loss = 74.0224609375, avg loss = 37.01123046875
client 0, data condensation 200, total loss = 19.794921875, avg loss = 9.8974609375
client 0, data condensation 400, total loss = 23.092437744140625, avg loss = 11.546218872070312
client 0, data condensation 600, total loss = 13.7747802734375, avg loss = 6.88739013671875
client 0, data condensation 800, total loss = 24.8692626953125, avg loss = 12.43463134765625
client 0, data condensation 1000, total loss = 105.80523681640625, avg loss = 52.902618408203125
client 0, data condensation 1200, total loss = 24.24652099609375, avg loss = 12.123260498046875
client 0, data condensation 1400, total loss = 28.2010498046875, avg loss = 14.10052490234375
client 0, data condensation 1600, total loss = 18.85223388671875, avg loss = 9.426116943359375
client 0, data condensation 1800, total loss = 9.93975830078125, avg loss = 4.969879150390625
client 0, data condensation 2000, total loss = 25.481658935546875, avg loss = 12.740829467773438
client 0, data condensation 2200, total loss = 38.0938720703125, avg loss = 19.04693603515625
client 0, data condensation 2400, total loss = 13.63543701171875, avg loss = 6.817718505859375
client 0, data condensation 2600, total loss = 123.4573974609375, avg loss = 61.72869873046875
client 0, data condensation 2800, total loss = 18.7708740234375, avg loss = 9.38543701171875
client 0, data condensation 3000, total loss = 25.44378662109375, avg loss = 12.721893310546875
client 0, data condensation 3200, total loss = 306.0350341796875, avg loss = 153.01751708984375
client 0, data condensation 3400, total loss = 13.450408935546875, avg loss = 6.7252044677734375
client 0, data condensation 3600, total loss = 13.51239013671875, avg loss = 6.756195068359375
client 0, data condensation 3800, total loss = 17.931884765625, avg loss = 8.9659423828125
client 0, data condensation 4000, total loss = 10.4808349609375, avg loss = 5.24041748046875
client 0, data condensation 4200, total loss = 31.96820068359375, avg loss = 15.984100341796875
client 0, data condensation 4400, total loss = 25.31451416015625, avg loss = 12.657257080078125
client 0, data condensation 4600, total loss = 157.96548461914062, avg loss = 78.98274230957031
client 0, data condensation 4800, total loss = 26.4608154296875, avg loss = 13.23040771484375
client 0, data condensation 5000, total loss = 128.21401977539062, avg loss = 64.10700988769531
Round 9, client 0 condense time: 434.62920665740967
client 0, class 4 have 5777 samples
client 0, class 7 have 9330 samples
total 24576.0MB, used 3427.06MB, free 21148.94MB
total 24576.0MB, used 3427.06MB, free 21148.94MB
initialized by random noise
client 1 have real samples [9022]
client 1 will condense {0: 91} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 9022 samples, histogram: [6133  429  219  178  145  148  131  156  233 1250], bin edged: [9.67964499e-05 1.03075841e-04 1.09355233e-04 1.15634625e-04
 1.21914016e-04 1.28193408e-04 1.34472799e-04 1.40752191e-04
 1.47031583e-04 1.53310974e-04 1.59590366e-04]
client 1, data condensation 0, total loss = 212.18328857421875, avg loss = 212.18328857421875
client 1, data condensation 200, total loss = 9.597900390625, avg loss = 9.597900390625
client 1, data condensation 400, total loss = 22.2767333984375, avg loss = 22.2767333984375
client 1, data condensation 600, total loss = 19.9189453125, avg loss = 19.9189453125
client 1, data condensation 800, total loss = 133.81622314453125, avg loss = 133.81622314453125
client 1, data condensation 1000, total loss = 27.6136474609375, avg loss = 27.6136474609375
client 1, data condensation 1200, total loss = 28.64276123046875, avg loss = 28.64276123046875
client 1, data condensation 1400, total loss = 31.0618896484375, avg loss = 31.0618896484375
client 1, data condensation 1600, total loss = 176.46044921875, avg loss = 176.46044921875
client 1, data condensation 1800, total loss = 36.354248046875, avg loss = 36.354248046875
client 1, data condensation 2000, total loss = 24.75616455078125, avg loss = 24.75616455078125
client 1, data condensation 2200, total loss = 119.8568115234375, avg loss = 119.8568115234375
client 1, data condensation 2400, total loss = 13.57537841796875, avg loss = 13.57537841796875
client 1, data condensation 2600, total loss = 148.37890625, avg loss = 148.37890625
client 1, data condensation 2800, total loss = 30.91278076171875, avg loss = 30.91278076171875
client 1, data condensation 3000, total loss = 29.13592529296875, avg loss = 29.13592529296875
client 1, data condensation 3200, total loss = 66.8106689453125, avg loss = 66.8106689453125
client 1, data condensation 3400, total loss = 22.2139892578125, avg loss = 22.2139892578125
client 1, data condensation 3600, total loss = 13.9349365234375, avg loss = 13.9349365234375
client 1, data condensation 3800, total loss = 244.60528564453125, avg loss = 244.60528564453125
client 1, data condensation 4000, total loss = 29.8634033203125, avg loss = 29.8634033203125
client 1, data condensation 4200, total loss = 187.42425537109375, avg loss = 187.42425537109375
client 1, data condensation 4400, total loss = 10.8367919921875, avg loss = 10.8367919921875
client 1, data condensation 4600, total loss = 9.04119873046875, avg loss = 9.04119873046875
client 1, data condensation 4800, total loss = 11.996337890625, avg loss = 11.996337890625
client 1, data condensation 5000, total loss = 32.61053466796875, avg loss = 32.61053466796875
Round 9, client 1 condense time: 260.9778952598572
client 1, class 0 have 9022 samples
total 24576.0MB, used 3043.06MB, free 21532.94MB
total 24576.0MB, used 3043.06MB, free 21532.94MB
initialized by random noise
client 2 have real samples [327, 12176]
client 2 will condense {0: 5, 5: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 327 samples, histogram: [264   8   3   2   2   3   1   6   4  34], bin edged: [0.00278757 0.00296841 0.00314924 0.00333008 0.00351092 0.00369175
 0.00387259 0.00405342 0.00423426 0.00441509 0.00459593]
class 5 have 12176 samples, histogram: [5196  570  393  336  290  285  320  394  515 3877], bin edged: [6.37455612e-05 6.78808713e-05 7.20161814e-05 7.61514916e-05
 8.02868017e-05 8.44221119e-05 8.85574220e-05 9.26927322e-05
 9.68280423e-05 1.00963352e-04 1.05098663e-04]
client 2, data condensation 0, total loss = 251.1561279296875, avg loss = 125.57806396484375
client 2, data condensation 200, total loss = 44.8487548828125, avg loss = 22.42437744140625
client 2, data condensation 400, total loss = 143.72723388671875, avg loss = 71.86361694335938
client 2, data condensation 600, total loss = 88.5406494140625, avg loss = 44.27032470703125
client 2, data condensation 800, total loss = 42.4659423828125, avg loss = 21.23297119140625
client 2, data condensation 1000, total loss = 287.08160400390625, avg loss = 143.54080200195312
client 2, data condensation 1200, total loss = 70.470703125, avg loss = 35.2353515625
client 2, data condensation 1400, total loss = 68.80072021484375, avg loss = 34.400360107421875
client 2, data condensation 1600, total loss = 74.72259521484375, avg loss = 37.361297607421875
client 2, data condensation 1800, total loss = 96.2894287109375, avg loss = 48.14471435546875
client 2, data condensation 2000, total loss = 90.6741943359375, avg loss = 45.33709716796875
client 2, data condensation 2200, total loss = 39.08941650390625, avg loss = 19.544708251953125
client 2, data condensation 2400, total loss = 59.63336181640625, avg loss = 29.816680908203125
client 2, data condensation 2600, total loss = 36.79351806640625, avg loss = 18.396759033203125
client 2, data condensation 2800, total loss = 49.05963134765625, avg loss = 24.529815673828125
client 2, data condensation 3000, total loss = 23.9454345703125, avg loss = 11.97271728515625
client 2, data condensation 3200, total loss = 77.09185791015625, avg loss = 38.545928955078125
client 2, data condensation 3400, total loss = 354.323974609375, avg loss = 177.1619873046875
client 2, data condensation 3600, total loss = 95.10089111328125, avg loss = 47.550445556640625
client 2, data condensation 3800, total loss = 64.707275390625, avg loss = 32.3536376953125
client 2, data condensation 4000, total loss = 85.7908935546875, avg loss = 42.89544677734375
client 2, data condensation 4200, total loss = 164.93853759765625, avg loss = 82.46926879882812
client 2, data condensation 4400, total loss = 58.97772216796875, avg loss = 29.488861083984375
client 2, data condensation 4600, total loss = 74.3470458984375, avg loss = 37.17352294921875
client 2, data condensation 4800, total loss = 64.16009521484375, avg loss = 32.080047607421875
client 2, data condensation 5000, total loss = 69.5155029296875, avg loss = 34.75775146484375
Round 9, client 2 condense time: 433.759090423584
client 2, class 0 have 327 samples
client 2, class 5 have 12176 samples
total 24576.0MB, used 3686.0MB, free 20890.0MB
total 24576.0MB, used 3686.0MB, free 20890.0MB
initialized by random noise
client 3 have real samples [313]
client 3 will condense {6: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 6 have 313 samples, histogram: [125  20  12   5  17  10  10  10  18  86], bin edged: [0.00248677 0.00264809 0.00280941 0.00297073 0.00313205 0.00329337
 0.00345468 0.003616   0.00377732 0.00393864 0.00409996]
client 3, data condensation 0, total loss = 90.9520263671875, avg loss = 90.9520263671875
client 3, data condensation 200, total loss = 27.66583251953125, avg loss = 27.66583251953125
client 3, data condensation 400, total loss = 42.5, avg loss = 42.5
client 3, data condensation 600, total loss = 64.036865234375, avg loss = 64.036865234375
client 3, data condensation 800, total loss = 62.5003662109375, avg loss = 62.5003662109375
client 3, data condensation 1000, total loss = 42.966796875, avg loss = 42.966796875
client 3, data condensation 1200, total loss = 36.9970703125, avg loss = 36.9970703125
client 3, data condensation 1400, total loss = 42.56536865234375, avg loss = 42.56536865234375
client 3, data condensation 1600, total loss = 33.0760498046875, avg loss = 33.0760498046875
client 3, data condensation 1800, total loss = 54.741455078125, avg loss = 54.741455078125
client 3, data condensation 2000, total loss = 567.4143676757812, avg loss = 567.4143676757812
client 3, data condensation 2200, total loss = 36.709228515625, avg loss = 36.709228515625
client 3, data condensation 2400, total loss = 54.482666015625, avg loss = 54.482666015625
client 3, data condensation 2600, total loss = 45.2578125, avg loss = 45.2578125
client 3, data condensation 2800, total loss = 52.4146728515625, avg loss = 52.4146728515625
client 3, data condensation 3000, total loss = 83.2789306640625, avg loss = 83.2789306640625
client 3, data condensation 3200, total loss = 81.33209228515625, avg loss = 81.33209228515625
client 3, data condensation 3400, total loss = 139.2288818359375, avg loss = 139.2288818359375
client 3, data condensation 3600, total loss = 34.67816162109375, avg loss = 34.67816162109375
client 3, data condensation 3800, total loss = 45.3116455078125, avg loss = 45.3116455078125
client 3, data condensation 4000, total loss = 45.85076904296875, avg loss = 45.85076904296875
client 3, data condensation 4200, total loss = 37.18212890625, avg loss = 37.18212890625
client 3, data condensation 4400, total loss = 73.0758056640625, avg loss = 73.0758056640625
client 3, data condensation 4600, total loss = 95.25372314453125, avg loss = 95.25372314453125
client 3, data condensation 4800, total loss = 55.0823974609375, avg loss = 55.0823974609375
client 3, data condensation 5000, total loss = 28.5272216796875, avg loss = 28.5272216796875
Round 9, client 3 condense time: 158.2179605960846
client 3, class 6 have 313 samples
total 24576.0MB, used 3296.0MB, free 21280.0MB
total 24576.0MB, used 3296.0MB, free 21280.0MB
initialized by random noise
client 4 have real samples [361, 1048, 7572]
client 4 will condense {1: 5, 4: 11, 6: 76} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 361 samples, histogram: [314   1   1   2   0   2   2   3   2  34], bin edged: [0.00257549 0.00274257 0.00290964 0.00307672 0.0032438  0.00341088
 0.00357795 0.00374503 0.00391211 0.00407918 0.00424626]
class 4 have 1048 samples, histogram: [637  51  46  24  22  18  20  36  37 157], bin edged: [0.00081275 0.00086547 0.00091819 0.00097092 0.00102364 0.00107637
 0.00112909 0.00118182 0.00123454 0.00128727 0.00133999]
class 6 have 7572 samples, histogram: [4987  401  227  184  131  146  168  153  217  958], bin edged: [0.00011507 0.00012254 0.00013    0.00013747 0.00014493 0.0001524
 0.00015986 0.00016733 0.00017479 0.00018226 0.00018972]
client 4, data condensation 0, total loss = 203.8502197265625, avg loss = 67.9500732421875
client 4, data condensation 200, total loss = 835.9896850585938, avg loss = 278.66322835286456
client 4, data condensation 400, total loss = 42.541015625, avg loss = 14.180338541666666
client 4, data condensation 600, total loss = 78.06463623046875, avg loss = 26.02154541015625
client 4, data condensation 800, total loss = 103.6990966796875, avg loss = 34.566365559895836
client 4, data condensation 1000, total loss = 422.87762451171875, avg loss = 140.9592081705729
client 4, data condensation 1200, total loss = 176.0064697265625, avg loss = 58.6688232421875
client 4, data condensation 1400, total loss = 80.2899169921875, avg loss = 26.7633056640625
client 4, data condensation 1600, total loss = 45.3367919921875, avg loss = 15.112263997395834
client 4, data condensation 1800, total loss = 57.25738525390625, avg loss = 19.085795084635418
client 4, data condensation 2000, total loss = 65.4703369140625, avg loss = 21.823445638020832
client 4, data condensation 2200, total loss = 170.20306396484375, avg loss = 56.734354654947914
client 4, data condensation 2400, total loss = 181.0518798828125, avg loss = 60.350626627604164
client 4, data condensation 2600, total loss = 53.9788818359375, avg loss = 17.992960611979168
client 4, data condensation 2800, total loss = 232.20449829101562, avg loss = 77.40149943033855
client 4, data condensation 3000, total loss = 76.6361083984375, avg loss = 25.545369466145832
client 4, data condensation 3200, total loss = 300.8927001953125, avg loss = 100.29756673177083
client 4, data condensation 3400, total loss = 111.15631103515625, avg loss = 37.052103678385414
client 4, data condensation 3600, total loss = 52.869384765625, avg loss = 17.623128255208332
client 4, data condensation 3800, total loss = 118.45468139648438, avg loss = 39.484893798828125
client 4, data condensation 4000, total loss = 112.157958984375, avg loss = 37.385986328125
client 4, data condensation 4200, total loss = 57.77130126953125, avg loss = 19.257100423177082
client 4, data condensation 4400, total loss = 218.22537231445312, avg loss = 72.74179077148438
client 4, data condensation 4600, total loss = 91.89019775390625, avg loss = 30.63006591796875
client 4, data condensation 4800, total loss = 73.46142578125, avg loss = 24.487141927083332
client 4, data condensation 5000, total loss = 186.27093505859375, avg loss = 62.090311686197914
Round 9, client 4 condense time: 508.09153604507446
client 4, class 1 have 361 samples
client 4, class 4 have 1048 samples
client 4, class 6 have 7572 samples
total 24576.0MB, used 24032.0MB, free 544.0MB
total 24576.0MB, used 24032.0MB, free 544.0MB
initialized by random noise
client 5 have real samples [12151]
client 5 will condense {8: 122} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 12151 samples, histogram: [4161  564  393  277  279  290  309  415  546 4917], bin edged: [6.12147833e-05 6.51859165e-05 6.91570497e-05 7.31281828e-05
 7.70993160e-05 8.10704492e-05 8.50415824e-05 8.90127156e-05
 9.29838488e-05 9.69549820e-05 1.00926115e-04]
client 5, data condensation 0, total loss = 100.48748779296875, avg loss = 100.48748779296875
client 5, data condensation 200, total loss = 5.4031982421875, avg loss = 5.4031982421875
client 5, data condensation 400, total loss = 57.4339599609375, avg loss = 57.4339599609375
client 5, data condensation 600, total loss = 195.9317626953125, avg loss = 195.9317626953125
client 5, data condensation 800, total loss = 28.1903076171875, avg loss = 28.1903076171875
client 5, data condensation 1000, total loss = 15.33270263671875, avg loss = 15.33270263671875
client 5, data condensation 1200, total loss = 26.2322998046875, avg loss = 26.2322998046875
client 5, data condensation 1400, total loss = 11.21014404296875, avg loss = 11.21014404296875
client 5, data condensation 1600, total loss = 11.30914306640625, avg loss = 11.30914306640625
client 5, data condensation 1800, total loss = 25.95416259765625, avg loss = 25.95416259765625
client 5, data condensation 2000, total loss = 16.15264892578125, avg loss = 16.15264892578125
client 5, data condensation 2200, total loss = 10.93450927734375, avg loss = 10.93450927734375
client 5, data condensation 2400, total loss = 20.60577392578125, avg loss = 20.60577392578125
client 5, data condensation 2600, total loss = 15.899932861328125, avg loss = 15.899932861328125
client 5, data condensation 2800, total loss = 29.899749755859375, avg loss = 29.899749755859375
client 5, data condensation 3000, total loss = 13.53839111328125, avg loss = 13.53839111328125
client 5, data condensation 3200, total loss = 12.96783447265625, avg loss = 12.96783447265625
client 5, data condensation 3400, total loss = 12.29095458984375, avg loss = 12.29095458984375
client 5, data condensation 3600, total loss = 7.29278564453125, avg loss = 7.29278564453125
client 5, data condensation 3800, total loss = 4.9635009765625, avg loss = 4.9635009765625
client 5, data condensation 4000, total loss = 7.0150146484375, avg loss = 7.0150146484375
client 5, data condensation 4200, total loss = 8.3255615234375, avg loss = 8.3255615234375
client 5, data condensation 4400, total loss = 4.93707275390625, avg loss = 4.93707275390625
client 5, data condensation 4600, total loss = 7.05523681640625, avg loss = 7.05523681640625
client 5, data condensation 4800, total loss = 5.1871337890625, avg loss = 5.1871337890625
client 5, data condensation 5000, total loss = 5.5958251953125, avg loss = 5.5958251953125
Round 9, client 5 condense time: 276.8987021446228
client 5, class 8 have 12151 samples
total 24576.0MB, used 3043.06MB, free 21532.94MB
total 24576.0MB, used 3043.06MB, free 21532.94MB
initialized by random noise
client 6 have real samples [10194]
client 6 will condense {3: 102} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 10194 samples, histogram: [7414  339  180  146  136  130  131  154  197 1367], bin edged: [8.68241248e-05 9.24565904e-05 9.80890561e-05 1.03721522e-04
 1.09353987e-04 1.14986453e-04 1.20618919e-04 1.26251384e-04
 1.31883850e-04 1.37516316e-04 1.43148781e-04]
client 6, data condensation 0, total loss = 219.94158935546875, avg loss = 219.94158935546875
client 6, data condensation 200, total loss = 17.91754150390625, avg loss = 17.91754150390625
client 6, data condensation 400, total loss = 114.51641845703125, avg loss = 114.51641845703125
client 6, data condensation 600, total loss = 15.861083984375, avg loss = 15.861083984375
client 6, data condensation 800, total loss = 552.6600341796875, avg loss = 552.6600341796875
client 6, data condensation 1000, total loss = 12.18212890625, avg loss = 12.18212890625
client 6, data condensation 1200, total loss = 105.1702880859375, avg loss = 105.1702880859375
client 6, data condensation 1400, total loss = 10.22113037109375, avg loss = 10.22113037109375
client 6, data condensation 1600, total loss = 647.87548828125, avg loss = 647.87548828125
client 6, data condensation 1800, total loss = 11.75604248046875, avg loss = 11.75604248046875
client 6, data condensation 2000, total loss = 11.19805908203125, avg loss = 11.19805908203125
client 6, data condensation 2200, total loss = 530.33154296875, avg loss = 530.33154296875
client 6, data condensation 2400, total loss = 625.9968872070312, avg loss = 625.9968872070312
client 6, data condensation 2600, total loss = 4.714599609375, avg loss = 4.714599609375
client 6, data condensation 2800, total loss = 150.07965087890625, avg loss = 150.07965087890625
client 6, data condensation 3000, total loss = 28.9090576171875, avg loss = 28.9090576171875
client 6, data condensation 3200, total loss = 269.76617431640625, avg loss = 269.76617431640625
client 6, data condensation 3400, total loss = 5.528076171875, avg loss = 5.528076171875
client 6, data condensation 3600, total loss = 7.505859375, avg loss = 7.505859375
client 6, data condensation 3800, total loss = 10.210205078125, avg loss = 10.210205078125
client 6, data condensation 4000, total loss = 30.580322265625, avg loss = 30.580322265625
client 6, data condensation 4200, total loss = 48.5648193359375, avg loss = 48.5648193359375
client 6, data condensation 4400, total loss = 20.42822265625, avg loss = 20.42822265625
client 6, data condensation 4600, total loss = 27.7529296875, avg loss = 27.7529296875
client 6, data condensation 4800, total loss = 19.70648193359375, avg loss = 19.70648193359375
client 6, data condensation 5000, total loss = 26.797119140625, avg loss = 26.797119140625
Round 9, client 6 condense time: 267.95302295684814
client 6, class 3 have 10194 samples
total 24576.0MB, used 3045.06MB, free 21530.94MB
total 24576.0MB, used 3045.06MB, free 21530.94MB
initialized by random noise
client 7 have real samples [9112]
client 7 will condense {1: 92} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 9112 samples, histogram: [6240  415  184  115   75   67   97   62  111 1746], bin edged: [9.44601121e-05 1.00587930e-04 1.06715748e-04 1.12843565e-04
 1.18971383e-04 1.25099201e-04 1.31227018e-04 1.37354836e-04
 1.43482654e-04 1.49610472e-04 1.55738289e-04]
client 7, data condensation 0, total loss = 152.372802734375, avg loss = 152.372802734375
client 7, data condensation 200, total loss = 11.0504150390625, avg loss = 11.0504150390625
client 7, data condensation 400, total loss = 11.406494140625, avg loss = 11.406494140625
client 7, data condensation 600, total loss = 37.93603515625, avg loss = 37.93603515625
client 7, data condensation 800, total loss = 11.6759033203125, avg loss = 11.6759033203125
client 7, data condensation 1000, total loss = 50.6629638671875, avg loss = 50.6629638671875
client 7, data condensation 1200, total loss = 26.93536376953125, avg loss = 26.93536376953125
client 7, data condensation 1400, total loss = 11.1602783203125, avg loss = 11.1602783203125
client 7, data condensation 1600, total loss = 20.0010986328125, avg loss = 20.0010986328125
client 7, data condensation 1800, total loss = 22.0048828125, avg loss = 22.0048828125
client 7, data condensation 2000, total loss = 10.192138671875, avg loss = 10.192138671875
client 7, data condensation 2200, total loss = 217.2265625, avg loss = 217.2265625
client 7, data condensation 2400, total loss = 17.23883056640625, avg loss = 17.23883056640625
client 7, data condensation 2600, total loss = 23.1357421875, avg loss = 23.1357421875
client 7, data condensation 2800, total loss = 174.78680419921875, avg loss = 174.78680419921875
client 7, data condensation 3000, total loss = 18.181884765625, avg loss = 18.181884765625
client 7, data condensation 3200, total loss = 7.08282470703125, avg loss = 7.08282470703125
client 7, data condensation 3400, total loss = 22.14752197265625, avg loss = 22.14752197265625
client 7, data condensation 3600, total loss = 18.843017578125, avg loss = 18.843017578125
client 7, data condensation 3800, total loss = 5.27862548828125, avg loss = 5.27862548828125
client 7, data condensation 4000, total loss = 24.923095703125, avg loss = 24.923095703125
client 7, data condensation 4200, total loss = 232.36761474609375, avg loss = 232.36761474609375
client 7, data condensation 4400, total loss = 18.78887939453125, avg loss = 18.78887939453125
client 7, data condensation 4600, total loss = 29.59991455078125, avg loss = 29.59991455078125
client 7, data condensation 4800, total loss = 3.70904541015625, avg loss = 3.70904541015625
client 7, data condensation 5000, total loss = 13.33624267578125, avg loss = 13.33624267578125
Round 9, client 7 condense time: 268.1401541233063
client 7, class 1 have 9112 samples
total 24576.0MB, used 3045.06MB, free 21530.94MB
total 24576.0MB, used 3045.06MB, free 21530.94MB
initialized by random noise
client 8 have real samples [206, 1179, 734]
client 8 will condense {3: 5, 4: 12, 8: 8} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 3 have 206 samples, histogram: [192   2   1   1   3   0   0   0   2   5], bin edged: [0.00471202 0.00501769 0.00532337 0.00562905 0.00593473 0.00624041
 0.00654609 0.00685176 0.00715744 0.00746312 0.0077688 ]
class 4 have 1179 samples, histogram: [770  50  37  24  20  17  20  26  40 175], bin edged: [0.00073093 0.00077834 0.00082576 0.00087318 0.00092059 0.00096801
 0.00101543 0.00106284 0.00111026 0.00115768 0.00120509]
class 8 have 734 samples, histogram: [452  28  15  23  18   9  14  12  26 137], bin edged: [0.00115063 0.00122528 0.00129992 0.00137457 0.00144921 0.00152385
 0.0015985  0.00167314 0.00174779 0.00182243 0.00189707]
client 8, data condensation 0, total loss = 230.11614990234375, avg loss = 76.70538330078125
client 8, data condensation 200, total loss = 167.975830078125, avg loss = 55.991943359375
client 8, data condensation 400, total loss = 57.61798095703125, avg loss = 19.20599365234375
client 8, data condensation 600, total loss = 157.16455078125, avg loss = 52.38818359375
client 8, data condensation 800, total loss = 54.70269775390625, avg loss = 18.234232584635418
client 8, data condensation 1000, total loss = 73.99954223632812, avg loss = 24.666514078776043
client 8, data condensation 1200, total loss = 82.7362060546875, avg loss = 27.5787353515625
client 8, data condensation 1400, total loss = 153.77764892578125, avg loss = 51.25921630859375
client 8, data condensation 1600, total loss = 140.9498291015625, avg loss = 46.9832763671875
client 8, data condensation 1800, total loss = 167.33251953125, avg loss = 55.777506510416664
client 8, data condensation 2000, total loss = 793.5408935546875, avg loss = 264.5136311848958
client 8, data condensation 2200, total loss = 76.84991455078125, avg loss = 25.61663818359375
client 8, data condensation 2400, total loss = 52.446685791015625, avg loss = 17.482228597005207
client 8, data condensation 2600, total loss = 50.5257568359375, avg loss = 16.8419189453125
client 8, data condensation 2800, total loss = 58.07012939453125, avg loss = 19.356709798177082
client 8, data condensation 3000, total loss = 61.9366455078125, avg loss = 20.645548502604168
client 8, data condensation 3200, total loss = 51.41278076171875, avg loss = 17.137593587239582
client 8, data condensation 3400, total loss = 95.9727783203125, avg loss = 31.990926106770832
client 8, data condensation 3600, total loss = 249.0880126953125, avg loss = 83.02933756510417
client 8, data condensation 3800, total loss = 74.65753173828125, avg loss = 24.885843912760418
client 8, data condensation 4000, total loss = 159.91168212890625, avg loss = 53.30389404296875
client 8, data condensation 4200, total loss = 55.2557373046875, avg loss = 18.4185791015625
client 8, data condensation 4400, total loss = 69.81564331054688, avg loss = 23.271881103515625
client 8, data condensation 4600, total loss = 102.86492919921875, avg loss = 34.288309733072914
client 8, data condensation 4800, total loss = 55.04339599609375, avg loss = 18.347798665364582
client 8, data condensation 5000, total loss = 43.8446044921875, avg loss = 14.6148681640625
Round 9, client 8 condense time: 440.9027826786041
client 8, class 3 have 206 samples
client 8, class 4 have 1179 samples
client 8, class 8 have 734 samples
total 24576.0MB, used 3683.06MB, free 20892.94MB
total 24576.0MB, used 3683.06MB, free 20892.94MB
initialized by random noise
client 9 have real samples [10316]
client 9 will condense {2: 104} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 10316 samples, histogram: [2388  363  237  191  173  196  220  276  421 5851], bin edged: [6.75683832e-05 7.19516879e-05 7.63349927e-05 8.07182974e-05
 8.51016021e-05 8.94849069e-05 9.38682116e-05 9.82515164e-05
 1.02634821e-04 1.07018126e-04 1.11401431e-04]
client 9, data condensation 0, total loss = 93.0455322265625, avg loss = 93.0455322265625
client 9, data condensation 200, total loss = 16.426910400390625, avg loss = 16.426910400390625
client 9, data condensation 400, total loss = 17.617431640625, avg loss = 17.617431640625
client 9, data condensation 600, total loss = 22.080963134765625, avg loss = 22.080963134765625
client 9, data condensation 800, total loss = 224.71875, avg loss = 224.71875
client 9, data condensation 1000, total loss = 8.66802978515625, avg loss = 8.66802978515625
client 9, data condensation 1200, total loss = 5.532958984375, avg loss = 5.532958984375
client 9, data condensation 1400, total loss = 146.81683349609375, avg loss = 146.81683349609375
client 9, data condensation 1600, total loss = 44.634765625, avg loss = 44.634765625
client 9, data condensation 1800, total loss = 119.22186279296875, avg loss = 119.22186279296875
client 9, data condensation 2000, total loss = 2.9837646484375, avg loss = 2.9837646484375
client 9, data condensation 2200, total loss = 16.0213623046875, avg loss = 16.0213623046875
client 9, data condensation 2400, total loss = 3.93798828125, avg loss = 3.93798828125
client 9, data condensation 2600, total loss = 29.899200439453125, avg loss = 29.899200439453125
client 9, data condensation 2800, total loss = 24.99090576171875, avg loss = 24.99090576171875
client 9, data condensation 3000, total loss = 5.12371826171875, avg loss = 5.12371826171875
client 9, data condensation 3200, total loss = 29.18109130859375, avg loss = 29.18109130859375
client 9, data condensation 3400, total loss = 19.324127197265625, avg loss = 19.324127197265625
client 9, data condensation 3600, total loss = 8.9461669921875, avg loss = 8.9461669921875
client 9, data condensation 3800, total loss = 12.14105224609375, avg loss = 12.14105224609375
client 9, data condensation 4000, total loss = 13.90606689453125, avg loss = 13.90606689453125
client 9, data condensation 4200, total loss = 5.983184814453125, avg loss = 5.983184814453125
client 9, data condensation 4400, total loss = 103.2701416015625, avg loss = 103.2701416015625
client 9, data condensation 4600, total loss = 18.40765380859375, avg loss = 18.40765380859375
client 9, data condensation 4800, total loss = 16.017303466796875, avg loss = 16.017303466796875
client 9, data condensation 5000, total loss = 10.64044189453125, avg loss = 10.64044189453125
Round 9, client 9 condense time: 241.70302605628967
client 9, class 2 have 10316 samples
total 24576.0MB, used 3047.06MB, free 21528.94MB
server receives {0: 96, 1: 97, 2: 104, 3: 107, 4: 81, 5: 122, 6: 81, 7: 94, 8: 130} condensed samples for each class
logit_proto before softmax: tensor([[ 22.7944,   3.3414,  -5.7699, -16.7684,   8.5163,   5.9141,   1.4218,
          -6.2610, -12.7883],
        [  0.9338,  15.2575,   1.8493,  -5.9605,  -0.4630,   4.1250,  -7.5063,
          -1.1923,  -6.2122],
        [-12.4870,  -9.6013,  11.2412,  -4.4630,  -0.9698,   8.4746,  -0.2137,
           7.3208,   1.2811],
        [-14.2310, -12.1881,  -0.4878,  20.2890,   1.5251,  -4.2117,   8.9805,
           2.3537,  -2.1923],
        [ -3.6454,  -5.5138,  -3.7511,  -3.0597,  12.0111,  -1.4074,   7.7595,
           0.3940,  -2.5670],
        [ -9.1865,  -6.4790,   5.3506,  -9.2123,   0.3472,  14.7217,  -1.8596,
           5.5604,   1.3245],
        [ -9.0087, -10.4490,  -0.6698,   1.6173,   3.9439,  -2.4650,  14.2918,
           0.3313,   2.3058],
        [-13.5699, -11.3814,   4.1360,  -0.4830,   1.5111,   5.1406,   1.3340,
          12.0864,   1.9771],
        [-14.0119,  -9.2443,   1.1251,  -1.9989,  -0.3414,   1.7658,  10.5911,
          -0.7562,  13.1651]], device='cuda:4')
shape of prototypes in tensor: torch.Size([9, 2048])
shape of logit prototypes in tensor: torch.Size([9, 9])
relation tensor: tensor([[0, 4, 5, 1, 6],
        [1, 5, 2, 0, 4],
        [2, 5, 7, 8, 6],
        [3, 6, 7, 4, 2],
        [4, 6, 7, 5, 8],
        [5, 7, 2, 8, 4],
        [6, 4, 8, 3, 7],
        [7, 5, 2, 8, 4],
        [8, 6, 5, 2, 4]], device='cuda:4')
---------- update global model ----------
912
preserve threshold: 10
10
Round 9: # synthetic sample: 9120
total 24576.0MB, used 3047.06MB, free 21528.94MB
{0: {0: 1328, 1: 4, 2: 0, 3: 0, 4: 0, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 11, 1: 836, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 225, 3: 6, 4: 0, 5: 39, 6: 1, 7: 67, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 570, 4: 1, 5: 0, 6: 56, 7: 1, 8: 5}, 4: {0: 50, 1: 66, 2: 0, 3: 2, 4: 873, 5: 5, 6: 34, 7: 3, 8: 2}, 5: {0: 43, 1: 23, 2: 59, 3: 30, 4: 1, 5: 181, 6: 7, 7: 229, 8: 19}, 6: {0: 19, 1: 1, 2: 0, 3: 16, 4: 16, 5: 9, 6: 660, 7: 1, 8: 19}, 7: {0: 2, 1: 18, 2: 9, 3: 73, 4: 15, 5: 50, 6: 33, 7: 188, 8: 33}, 8: {0: 11, 1: 3, 2: 2, 3: 46, 4: 7, 5: 24, 6: 224, 7: 35, 8: 881}}
round 9 evaluation: test acc is 0.7997, test loss = 1.184037
{0: {0: 1328, 1: 4, 2: 0, 3: 0, 4: 0, 5: 2, 6: 4, 7: 0, 8: 0}, 1: {0: 11, 1: 834, 2: 0, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 157, 3: 9, 4: 0, 5: 83, 6: 0, 7: 82, 8: 7}, 3: {0: 0, 1: 0, 2: 0, 3: 594, 4: 0, 5: 0, 6: 34, 7: 1, 8: 5}, 4: {0: 54, 1: 54, 2: 0, 3: 13, 4: 783, 5: 12, 6: 73, 7: 42, 8: 4}, 5: {0: 5, 1: 55, 2: 26, 3: 32, 4: 0, 5: 140, 6: 4, 7: 280, 8: 50}, 6: {0: 15, 1: 1, 2: 0, 3: 37, 4: 12, 5: 5, 6: 619, 7: 5, 8: 47}, 7: {0: 2, 1: 8, 2: 8, 3: 76, 4: 2, 5: 42, 6: 18, 7: 220, 8: 45}, 8: {0: 8, 1: 2, 2: 0, 3: 74, 4: 1, 5: 12, 6: 132, 7: 42, 8: 962}}
epoch 0, train loss avg now = 0.055401, train contrast loss now = 1.584870, test acc now = 0.7851, test loss now = 1.333402
{0: {0: 1331, 1: 5, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 827, 2: 1, 3: 0, 4: 0, 5: 12, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 3, 2: 195, 3: 10, 4: 0, 5: 41, 6: 0, 7: 78, 8: 12}, 3: {0: 1, 1: 0, 2: 0, 3: 571, 4: 1, 5: 0, 6: 60, 7: 0, 8: 1}, 4: {0: 67, 1: 87, 2: 0, 3: 4, 4: 852, 5: 2, 6: 18, 7: 4, 8: 1}, 5: {0: 0, 1: 68, 2: 32, 3: 42, 4: 1, 5: 166, 6: 12, 7: 232, 8: 39}, 6: {0: 28, 1: 1, 2: 0, 3: 18, 4: 23, 5: 3, 6: 651, 7: 0, 8: 17}, 7: {0: 2, 1: 25, 2: 10, 3: 73, 4: 10, 5: 35, 6: 37, 7: 177, 8: 52}, 8: {0: 21, 1: 7, 2: 0, 3: 57, 4: 10, 5: 12, 6: 217, 7: 27, 8: 882}}
epoch 100, train loss avg now = 0.020528, train contrast loss now = 1.402367, test acc now = 0.7872, test loss now = 1.412997
{0: {0: 1329, 1: 2, 2: 0, 3: 0, 4: 0, 5: 4, 6: 3, 7: 0, 8: 0}, 1: {0: 7, 1: 812, 2: 26, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 241, 3: 9, 4: 0, 5: 20, 6: 0, 7: 64, 8: 4}, 3: {0: 0, 1: 0, 2: 0, 3: 562, 4: 0, 5: 0, 6: 72, 7: 0, 8: 0}, 4: {0: 49, 1: 26, 2: 0, 3: 5, 4: 916, 5: 7, 6: 30, 7: 2, 8: 0}, 5: {0: 1, 1: 55, 2: 104, 3: 39, 4: 0, 5: 193, 6: 12, 7: 171, 8: 17}, 6: {0: 19, 1: 1, 2: 1, 3: 14, 4: 11, 5: 15, 6: 667, 7: 0, 8: 13}, 7: {0: 2, 1: 6, 2: 21, 3: 94, 4: 12, 5: 52, 6: 39, 7: 152, 8: 43}, 8: {0: 10, 1: 3, 2: 14, 3: 67, 4: 4, 5: 27, 6: 246, 7: 27, 8: 835}}
epoch 200, train loss avg now = 0.027399, train contrast loss now = 1.342713, test acc now = 0.7948, test loss now = 1.357665
{0: {0: 1332, 1: 4, 2: 0, 3: 0, 4: 0, 5: 0, 6: 2, 7: 0, 8: 0}, 1: {0: 7, 1: 840, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}, 2: {0: 1, 1: 1, 2: 235, 3: 8, 4: 0, 5: 31, 6: 1, 7: 60, 8: 2}, 3: {0: 3, 1: 0, 2: 0, 3: 564, 4: 1, 5: 0, 6: 63, 7: 0, 8: 3}, 4: {0: 95, 1: 91, 2: 0, 3: 7, 4: 801, 5: 7, 6: 32, 7: 2, 8: 0}, 5: {0: 23, 1: 45, 2: 72, 3: 26, 4: 0, 5: 224, 6: 12, 7: 182, 8: 8}, 6: {0: 46, 1: 1, 2: 0, 3: 7, 4: 10, 5: 10, 6: 652, 7: 0, 8: 15}, 7: {0: 5, 1: 18, 2: 16, 3: 85, 4: 17, 5: 51, 6: 51, 7: 144, 8: 34}, 8: {0: 25, 1: 3, 2: 3, 3: 50, 4: 5, 5: 31, 6: 260, 7: 22, 8: 834}}
epoch 300, train loss avg now = 0.013957, train contrast loss now = 1.327019, test acc now = 0.7836, test loss now = 1.504354
{0: {0: 1332, 1: 2, 2: 0, 3: 0, 4: 0, 5: 2, 6: 2, 7: 0, 8: 0}, 1: {0: 16, 1: 830, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 2, 2: 241, 3: 7, 4: 0, 5: 47, 6: 0, 7: 41, 8: 1}, 3: {0: 2, 1: 0, 2: 0, 3: 571, 4: 0, 5: 0, 6: 61, 7: 0, 8: 0}, 4: {0: 82, 1: 72, 2: 0, 3: 2, 4: 837, 5: 9, 6: 31, 7: 2, 8: 0}, 5: {0: 1, 1: 55, 2: 88, 3: 22, 4: 0, 5: 284, 6: 9, 7: 125, 8: 8}, 6: {0: 24, 1: 1, 2: 0, 3: 9, 4: 12, 5: 15, 6: 673, 7: 0, 8: 7}, 7: {0: 2, 1: 17, 2: 18, 3: 76, 4: 10, 5: 77, 6: 51, 7: 147, 8: 23}, 8: {0: 16, 1: 3, 2: 17, 3: 59, 4: 5, 5: 64, 6: 391, 7: 20, 8: 658}}
epoch 400, train loss avg now = 0.014005, train contrast loss now = 1.315059, test acc now = 0.7762, test loss now = 1.528024
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 1334, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 11, 1: 825, 2: 0, 3: 0, 4: 0, 5: 11, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 1, 2: 234, 3: 10, 4: 0, 5: 22, 6: 0, 7: 61, 8: 11}, 3: {0: 1, 1: 0, 2: 0, 3: 579, 4: 1, 5: 0, 6: 53, 7: 0, 8: 0}, 4: {0: 66, 1: 68, 2: 0, 3: 5, 4: 843, 5: 12, 6: 36, 7: 4, 8: 1}, 5: {0: 3, 1: 56, 2: 74, 3: 41, 4: 0, 5: 179, 6: 14, 7: 191, 8: 34}, 6: {0: 21, 1: 1, 2: 1, 3: 14, 4: 19, 5: 2, 6: 662, 7: 0, 8: 21}, 7: {0: 2, 1: 15, 2: 11, 3: 94, 4: 10, 5: 46, 6: 37, 7: 149, 8: 57}, 8: {0: 13, 1: 2, 2: 4, 3: 64, 4: 6, 5: 13, 6: 243, 7: 22, 8: 866}}
epoch 500, train loss avg now = 0.009562, train contrast loss now = 1.306546, test acc now = 0.7898, test loss now = 1.378627
{0: {0: 1333, 1: 2, 2: 0, 3: 0, 4: 0, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 17, 1: 813, 2: 15, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 236, 3: 10, 4: 0, 5: 24, 6: 0, 7: 64, 8: 5}, 3: {0: 1, 1: 0, 2: 0, 3: 575, 4: 1, 5: 0, 6: 57, 7: 0, 8: 0}, 4: {0: 66, 1: 51, 2: 0, 3: 4, 4: 881, 5: 5, 6: 26, 7: 2, 8: 0}, 5: {0: 7, 1: 52, 2: 74, 3: 40, 4: 0, 5: 174, 6: 13, 7: 208, 8: 24}, 6: {0: 26, 1: 1, 2: 0, 3: 14, 4: 18, 5: 6, 6: 659, 7: 0, 8: 17}, 7: {0: 2, 1: 12, 2: 10, 3: 90, 4: 13, 5: 49, 6: 40, 7: 162, 8: 43}, 8: {0: 19, 1: 2, 2: 6, 3: 63, 4: 7, 5: 16, 6: 283, 7: 25, 8: 812}}
epoch 600, train loss avg now = 0.010643, train contrast loss now = 1.309034, test acc now = 0.7862, test loss now = 1.388449
{0: {0: 1334, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 18, 1: 811, 2: 16, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 229, 3: 10, 4: 0, 5: 29, 6: 0, 7: 67, 8: 4}, 3: {0: 1, 1: 0, 2: 0, 3: 575, 4: 1, 5: 0, 6: 57, 7: 0, 8: 0}, 4: {0: 63, 1: 55, 2: 0, 3: 2, 4: 882, 5: 8, 6: 23, 7: 2, 8: 0}, 5: {0: 7, 1: 52, 2: 62, 3: 32, 4: 0, 5: 188, 6: 14, 7: 219, 8: 18}, 6: {0: 25, 1: 1, 2: 0, 3: 16, 4: 23, 5: 9, 6: 652, 7: 0, 8: 15}, 7: {0: 2, 1: 7, 2: 10, 3: 91, 4: 15, 5: 52, 6: 37, 7: 170, 8: 37}, 8: {0: 14, 1: 3, 2: 4, 3: 62, 4: 8, 5: 26, 6: 264, 7: 28, 8: 824}}
epoch 700, train loss avg now = 0.006688, train contrast loss now = 1.308568, test acc now = 0.7890, test loss now = 1.397157
{0: {0: 1332, 1: 3, 2: 0, 3: 0, 4: 0, 5: 1, 6: 2, 7: 0, 8: 0}, 1: {0: 14, 1: 814, 2: 17, 3: 0, 4: 0, 5: 2, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 233, 3: 10, 4: 0, 5: 29, 6: 0, 7: 64, 8: 3}, 3: {0: 2, 1: 0, 2: 0, 3: 577, 4: 1, 5: 0, 6: 54, 7: 0, 8: 0}, 4: {0: 72, 1: 56, 2: 0, 3: 4, 4: 865, 5: 9, 6: 25, 7: 4, 8: 0}, 5: {0: 6, 1: 53, 2: 89, 3: 30, 4: 0, 5: 195, 6: 12, 7: 195, 8: 12}, 6: {0: 28, 1: 1, 2: 0, 3: 16, 4: 19, 5: 8, 6: 654, 7: 0, 8: 15}, 7: {0: 2, 1: 11, 2: 13, 3: 84, 4: 13, 5: 57, 6: 36, 7: 166, 8: 39}, 8: {0: 19, 1: 3, 2: 5, 3: 65, 4: 7, 5: 18, 6: 275, 7: 27, 8: 814}}
epoch 800, train loss avg now = 0.006082, train contrast loss now = 1.306440, test acc now = 0.7869, test loss now = 1.438202
{0: {0: 1331, 1: 2, 2: 0, 3: 0, 4: 0, 5: 2, 6: 3, 7: 0, 8: 0}, 1: {0: 19, 1: 806, 2: 7, 3: 0, 4: 0, 5: 15, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 235, 3: 10, 4: 0, 5: 29, 6: 0, 7: 63, 8: 2}, 3: {0: 1, 1: 0, 2: 0, 3: 577, 4: 1, 5: 0, 6: 54, 7: 0, 8: 1}, 4: {0: 50, 1: 66, 2: 0, 3: 2, 4: 876, 5: 12, 6: 26, 7: 2, 8: 1}, 5: {0: 7, 1: 52, 2: 66, 3: 38, 4: 0, 5: 181, 6: 13, 7: 211, 8: 24}, 6: {0: 17, 1: 1, 2: 0, 3: 17, 4: 18, 5: 9, 6: 659, 7: 1, 8: 19}, 7: {0: 2, 1: 8, 2: 12, 3: 96, 4: 9, 5: 49, 6: 36, 7: 168, 8: 41}, 8: {0: 11, 1: 3, 2: 4, 3: 63, 4: 7, 5: 15, 6: 254, 7: 32, 8: 844}}
epoch 900, train loss avg now = 0.007683, train contrast loss now = 1.308293, test acc now = 0.7907, test loss now = 1.337097
{0: {0: 1333, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 3, 7: 0, 8: 0}, 1: {0: 14, 1: 822, 2: 6, 3: 0, 4: 0, 5: 5, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 232, 3: 12, 4: 0, 5: 32, 6: 0, 7: 62, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 45, 7: 0, 8: 0}, 4: {0: 52, 1: 55, 2: 0, 3: 8, 4: 878, 5: 5, 6: 34, 7: 3, 8: 0}, 5: {0: 7, 1: 51, 2: 69, 3: 41, 4: 0, 5: 194, 6: 10, 7: 202, 8: 18}, 6: {0: 20, 1: 1, 2: 0, 3: 26, 4: 17, 5: 6, 6: 652, 7: 1, 8: 18}, 7: {0: 2, 1: 6, 2: 11, 3: 106, 4: 6, 5: 51, 6: 34, 7: 167, 8: 38}, 8: {0: 12, 1: 3, 2: 6, 3: 81, 4: 4, 5: 14, 6: 264, 7: 31, 8: 818}}
epoch 1000, train loss avg now = 0.006546, train contrast loss now = 1.305977, test acc now = 0.7915, test loss now = 1.361675
epoch avg loss = 6.546099366326081e-06, total time = 7861.059057474136
total 24576.0MB, used 3675.06MB, free 20900.94MB
Round 9 finish, update the prev_syn_proto
torch.Size([960, 3, 28, 28])
torch.Size([970, 3, 28, 28])
torch.Size([1040, 3, 28, 28])
torch.Size([1070, 3, 28, 28])
torch.Size([810, 3, 28, 28])
torch.Size([1220, 3, 28, 28])
torch.Size([810, 3, 28, 28])
torch.Size([940, 3, 28, 28])
torch.Size([1300, 3, 28, 28])
shape of prev_syn_proto: torch.Size([9, 2048])
{0: {0: 1333, 1: 1, 2: 0, 3: 0, 4: 0, 5: 1, 6: 3, 7: 0, 8: 0}, 1: {0: 14, 1: 822, 2: 6, 3: 0, 4: 0, 5: 5, 6: 0, 7: 0, 8: 0}, 2: {0: 0, 1: 0, 2: 232, 3: 12, 4: 0, 5: 32, 6: 0, 7: 62, 8: 1}, 3: {0: 1, 1: 0, 2: 0, 3: 587, 4: 1, 5: 0, 6: 45, 7: 0, 8: 0}, 4: {0: 52, 1: 55, 2: 0, 3: 8, 4: 878, 5: 5, 6: 34, 7: 3, 8: 0}, 5: {0: 7, 1: 51, 2: 69, 3: 41, 4: 0, 5: 194, 6: 10, 7: 202, 8: 18}, 6: {0: 20, 1: 1, 2: 0, 3: 26, 4: 17, 5: 6, 6: 652, 7: 1, 8: 18}, 7: {0: 2, 1: 6, 2: 11, 3: 106, 4: 6, 5: 51, 6: 34, 7: 167, 8: 38}, 8: {0: 12, 1: 3, 2: 6, 3: 81, 4: 4, 5: 14, 6: 264, 7: 31, 8: 818}}
round 9 evaluation: test acc is 0.7915, test loss = 1.361675
[0.5181058495821727, 0.5564066852367688, 0.6986072423398328, 0.7033426183844012, 0.7422005571030641, 0.7798050139275766, 0.7701949860724234, 0.771309192200557, 0.7997214484679666, 0.7915041782729805]
{0: [163.67632161102296, 26.04178628540039, 30.16126121520996, 29.3332391998291, 26.959859298706053, 26.481419534301757, 26.227013897705078, 26.032387966918947, 27.217112313842772, 27.639857583618163], 1: [226.32622278442383, 41.86360113525391, 38.95801905517578, 37.03389207763672, 34.21702935791016, 34.24122108764649, 33.940003955078126, 34.74743507080078, 36.2213544128418, 36.14441013793945], 2: [137.46981076354982, 37.660608114624026, 37.61136990661621, 41.75882422790527, 38.6803421875, 41.59355844726562, 41.855225765991214, 44.6411049407959, 44.32492562866211, 44.39625548095703], 3: [118.77429128112793, 53.20971354370117, 74.12300626831055, 74.63338354492187, 74.14337030029297, 79.70261857910157, 79.06463164672851, 88.08010186157226, 96.34747550048829, 93.03940170898437], 4: [78.02574600931803, 25.495516880289713, 26.25416292521159, 28.667743731689452, 28.569581921386717, 32.409290519205726, 33.43165658365886, 34.45951934204101, 36.467958227539064, 38.039759181722005], 5: [187.6443207092285, 33.56307151489258, 35.75055842895508, 33.13215026855469, 29.953204986572267, 30.189987408447266, 29.832905682373045, 28.904128393554686, 28.95649867553711, 28.830662463378907], 6: [371.68029181518557, 58.19879025878906, 63.72404753417969, 57.59690928344727, 51.21839129638672, 52.195185864257816, 49.399097088623044, 48.7587897277832, 51.78292889404297, 50.006303149414066], 7: [118.91434909667969, 34.180072576904294, 28.6171190246582, 31.605734619140627, 32.04067642822265, 35.118160552978516, 33.605375610351565, 38.2730751159668, 37.36546493530273, 34.743498510742185], 8: [93.03717774759929, 28.542794018554687, 35.880333766682945, 33.79742048746745, 37.212628091430666, 40.05578114420573, 39.4860138671875, 41.65088501993815, 42.510779038492835, 42.48372919108073], 9: [217.97547112121583, 25.151239602661132, 32.86224512939453, 28.15596266479492, 25.4233907409668, 26.848405078125, 24.47617445678711, 24.431396472167968, 24.107831158447265, 24.050797705078125]}
{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}
{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}

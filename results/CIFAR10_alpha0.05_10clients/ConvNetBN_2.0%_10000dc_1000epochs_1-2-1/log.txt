Data statistics: {0: {2: 3593, 5: 1, 7: 4999}, 1: {2: 175, 4: 4958}, 2: {1: 29, 2: 9, 4: 41, 6: 9, 9: 242}, 3: {0: 847, 1: 1, 2: 1094, 6: 1, 9: 30}, 4: {0: 4152, 5: 307}, 5: {2: 8, 8: 4999}, 6: {2: 1, 5: 4365, 6: 3914}, 7: {1: 4605, 3: 4999}, 8: {1: 364, 3: 1, 5: 135, 6: 1, 9: 4727}, 9: {0: 1, 1: 1, 2: 120, 4: 1, 5: 192, 6: 1075, 7: 1, 8: 1, 9: 1}}
client classes: [[2, 7], [2, 4], [9], [0, 2], [0, 5], [8], [5, 6], [1, 3], [1, 5, 9], [2, 5, 6]]
ConvNet(
  (net_act): ReLU(inplace=True)
  (net_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=2048, out_features=10, bias=True)
)
320010
{'seed': 19260817, 'device': 'cuda:2', 'dataset_root': './dataset/torchvision', 'split_file': '/home/ygc/dcfl/cr/dataset/split_file/CIFAR10_client_num=10_alpha=0.05.json', 'dataset': 'CIFAR10', 'client_num': 10, 'partition': 'dirichlet', 'alpha': 0.05, 'num_classes_per_client': 2, 'model': 'ConvNetBN', 'communication_rounds': 10, 'join_ratio': 1.0, 'lr_server': 0.005, 'momentum_server': 0.9, 'weight_decay_server': 1e-06, 'batch_size': 256, 'model_epochs': 1000, 'local_ep': 40, 'ipc': 10, 'compression_ratio': 0.02, 'dc_iterations': 10000, 'dc_batch_size': 256, 'image_lr': 1.0, 'image_momentum': 0.5, 'image_weight_decay': 0, 'init': 'random_noise', 'clip_norm': 30, 'weighted_matching': False, 'weighted_sample': False, 'weighted_mmd': True, 'contrastive_way': 'supcon_asym_syn', 'con_beta': 0.1, 'con_temp': 0.1, 'topk': 5, 'lr_head': 0.01, 'momentum_head': 0.9, 'weight_decay_head': 0, 'gamma': 1.0, 'lamda': 0.5, 'b': 0.3, 'kernel': 'linear', 'lr': 0.01, 'momentum': 0.5, 'weight_decay': 0, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'preserve_all': False, 'eval_gap': 1, 'tag': '1-2-1', 'save_root_path': '/home/ygc/dcfl/cr/results/CIFAR10_alpha0.05_10clients/ConvNetBN_2.0%_10000dc_1000epochs_1-2-1', 'dsa_param': <src.utils.ParamDiffAug object at 0x7f95e4883f10>, 'dsa': True}
 ====== round 0 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 1799.06MB, free 22776.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [   3   10   90  488 1171 1079  546  161   36    9], bin edged: [0.00027677 0.00027707 0.00027738 0.00027768 0.00027799 0.00027829
 0.0002786  0.0002789  0.00027921 0.00027951 0.00027982]
class 7 have 4999 samples, histogram: [  48  555 1547 1528  836  352  100   30    2    1], bin edged: [0.00019935 0.00019955 0.00019976 0.00019997 0.00020018 0.00020039
 0.00020059 0.0002008  0.00020101 0.00020122 0.00020142]
client 0, data condensation 0, total loss = 642.142578125, avg loss = 321.0712890625
client 0, data condensation 200, total loss = 156.56539916992188, avg loss = 78.28269958496094
client 0, data condensation 400, total loss = 87.47116088867188, avg loss = 43.73558044433594
client 0, data condensation 600, total loss = 307.850341796875, avg loss = 153.9251708984375
client 0, data condensation 800, total loss = 650.9638061523438, avg loss = 325.4819030761719
client 0, data condensation 1000, total loss = 40.883819580078125, avg loss = 20.441909790039062
client 0, data condensation 1200, total loss = 24.173553466796875, avg loss = 12.086776733398438
client 0, data condensation 1400, total loss = 27.333160400390625, avg loss = 13.666580200195312
client 0, data condensation 1600, total loss = 29.892242431640625, avg loss = 14.946121215820312
client 0, data condensation 1800, total loss = 13.533782958984375, avg loss = 6.7668914794921875
client 0, data condensation 2000, total loss = 11.027435302734375, avg loss = 5.5137176513671875
client 0, data condensation 2200, total loss = 14.61199951171875, avg loss = 7.305999755859375
client 0, data condensation 2400, total loss = 17.979248046875, avg loss = 8.9896240234375
client 0, data condensation 2600, total loss = 10.86383056640625, avg loss = 5.431915283203125
client 0, data condensation 2800, total loss = 27.87481689453125, avg loss = 13.937408447265625
client 0, data condensation 3000, total loss = 25.33447265625, avg loss = 12.667236328125
client 0, data condensation 3200, total loss = 8.91802978515625, avg loss = 4.459014892578125
client 0, data condensation 3400, total loss = 125.39077758789062, avg loss = 62.69538879394531
client 0, data condensation 3600, total loss = 9.1778564453125, avg loss = 4.58892822265625
client 0, data condensation 3800, total loss = 6.240631103515625, avg loss = 3.1203155517578125
client 0, data condensation 4000, total loss = 7.44866943359375, avg loss = 3.724334716796875
client 0, data condensation 4200, total loss = 4.814361572265625, avg loss = 2.4071807861328125
client 0, data condensation 4400, total loss = 24.959121704101562, avg loss = 12.479560852050781
client 0, data condensation 4600, total loss = 11.992828369140625, avg loss = 5.9964141845703125
client 0, data condensation 4800, total loss = 31.0740966796875, avg loss = 15.53704833984375
client 0, data condensation 5000, total loss = 5.28460693359375, avg loss = 2.642303466796875
client 0, data condensation 5200, total loss = 13.82659912109375, avg loss = 6.913299560546875
client 0, data condensation 5400, total loss = 33.07806396484375, avg loss = 16.539031982421875
client 0, data condensation 5600, total loss = 12.623687744140625, avg loss = 6.3118438720703125
client 0, data condensation 5800, total loss = 3.70880126953125, avg loss = 1.854400634765625
client 0, data condensation 6000, total loss = 7.69268798828125, avg loss = 3.846343994140625
client 0, data condensation 6200, total loss = 8.932769775390625, avg loss = 4.4663848876953125
client 0, data condensation 6400, total loss = 22.989105224609375, avg loss = 11.494552612304688
client 0, data condensation 6600, total loss = 8.22857666015625, avg loss = 4.114288330078125
client 0, data condensation 6800, total loss = 7.70233154296875, avg loss = 3.851165771484375
client 0, data condensation 7000, total loss = 118.35832214355469, avg loss = 59.179161071777344
client 0, data condensation 7200, total loss = 3.882232666015625, avg loss = 1.9411163330078125
client 0, data condensation 7400, total loss = 48.1937255859375, avg loss = 24.09686279296875
client 0, data condensation 7600, total loss = 11.115203857421875, avg loss = 5.5576019287109375
client 0, data condensation 7800, total loss = 19.881942749023438, avg loss = 9.940971374511719
client 0, data condensation 8000, total loss = 10.23883056640625, avg loss = 5.119415283203125
client 0, data condensation 8200, total loss = 23.278900146484375, avg loss = 11.639450073242188
client 0, data condensation 8400, total loss = 4.42315673828125, avg loss = 2.211578369140625
client 0, data condensation 8600, total loss = 59.40345764160156, avg loss = 29.70172882080078
client 0, data condensation 8800, total loss = 4.793426513671875, avg loss = 2.3967132568359375
client 0, data condensation 9000, total loss = 4.2100830078125, avg loss = 2.10504150390625
client 0, data condensation 9200, total loss = 8.22802734375, avg loss = 4.114013671875
client 0, data condensation 9400, total loss = 3.8648681640625, avg loss = 1.93243408203125
client 0, data condensation 9600, total loss = 20.426971435546875, avg loss = 10.213485717773438
client 0, data condensation 9800, total loss = 64.53126525878906, avg loss = 32.26563262939453
client 0, data condensation 10000, total loss = 14.05810546875, avg loss = 7.029052734375
Round 0, client 0 condense time: 924.8750641345978
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2911.06MB, free 21664.94MB
total 24576.0MB, used 2911.06MB, free 21664.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [ 8 15 47 33 29 19 11  8  2  3], bin edged: [0.00569985 0.00570361 0.00570738 0.00571114 0.0057149  0.00571867
 0.00572243 0.0057262  0.00572996 0.00573373 0.00573749]
class 4 have 4958 samples, histogram: [  15   67  206  525 1015 1307 1118  512  162   31], bin edged: [0.00020046 0.00020069 0.00020091 0.00020114 0.00020137 0.00020159
 0.00020182 0.00020205 0.00020227 0.0002025  0.00020273]
client 1, data condensation 0, total loss = 667.3159790039062, avg loss = 333.6579895019531
client 1, data condensation 200, total loss = 94.37126159667969, avg loss = 47.185630798339844
client 1, data condensation 400, total loss = 79.19107055664062, avg loss = 39.59553527832031
client 1, data condensation 600, total loss = 53.369110107421875, avg loss = 26.684555053710938
client 1, data condensation 800, total loss = 20.40106201171875, avg loss = 10.200531005859375
client 1, data condensation 1000, total loss = 23.44580078125, avg loss = 11.722900390625
client 1, data condensation 1200, total loss = 16.215362548828125, avg loss = 8.107681274414062
client 1, data condensation 1400, total loss = 14.3914794921875, avg loss = 7.19573974609375
client 1, data condensation 1600, total loss = 7.361053466796875, avg loss = 3.6805267333984375
client 1, data condensation 1800, total loss = 19.129791259765625, avg loss = 9.564895629882812
client 1, data condensation 2000, total loss = 7.359375, avg loss = 3.6796875
client 1, data condensation 2200, total loss = 47.00103759765625, avg loss = 23.500518798828125
client 1, data condensation 2400, total loss = 25.93829345703125, avg loss = 12.969146728515625
client 1, data condensation 2600, total loss = 17.38604736328125, avg loss = 8.693023681640625
client 1, data condensation 2800, total loss = 7.49346923828125, avg loss = 3.746734619140625
client 1, data condensation 3000, total loss = 16.22064208984375, avg loss = 8.110321044921875
client 1, data condensation 3200, total loss = 6.93426513671875, avg loss = 3.467132568359375
client 1, data condensation 3400, total loss = 9.887481689453125, avg loss = 4.9437408447265625
client 1, data condensation 3600, total loss = 17.19915771484375, avg loss = 8.599578857421875
client 1, data condensation 3800, total loss = 7.91680908203125, avg loss = 3.958404541015625
client 1, data condensation 4000, total loss = 6.5194091796875, avg loss = 3.25970458984375
client 1, data condensation 4200, total loss = 8.047760009765625, avg loss = 4.0238800048828125
client 1, data condensation 4400, total loss = 8.853759765625, avg loss = 4.4268798828125
client 1, data condensation 4600, total loss = 5.8955078125, avg loss = 2.94775390625
client 1, data condensation 4800, total loss = 9.591339111328125, avg loss = 4.7956695556640625
client 1, data condensation 5000, total loss = 18.605682373046875, avg loss = 9.302841186523438
client 1, data condensation 5200, total loss = 19.790679931640625, avg loss = 9.895339965820312
client 1, data condensation 5400, total loss = 15.192840576171875, avg loss = 7.5964202880859375
client 1, data condensation 5600, total loss = 22.43621826171875, avg loss = 11.218109130859375
client 1, data condensation 5800, total loss = 303.9700622558594, avg loss = 151.9850311279297
client 1, data condensation 6000, total loss = 8.20654296875, avg loss = 4.103271484375
client 1, data condensation 6200, total loss = 7.825958251953125, avg loss = 3.9129791259765625
client 1, data condensation 6400, total loss = 11.031494140625, avg loss = 5.5157470703125
client 1, data condensation 6600, total loss = 43.063690185546875, avg loss = 21.531845092773438
client 1, data condensation 6800, total loss = 8.86785888671875, avg loss = 4.433929443359375
client 1, data condensation 7000, total loss = 11.4844970703125, avg loss = 5.74224853515625
client 1, data condensation 7200, total loss = 6.813934326171875, avg loss = 3.4069671630859375
client 1, data condensation 7400, total loss = 25.9466552734375, avg loss = 12.97332763671875
client 1, data condensation 7600, total loss = 30.902099609375, avg loss = 15.4510498046875
client 1, data condensation 7800, total loss = 72.52334594726562, avg loss = 36.26167297363281
client 1, data condensation 8000, total loss = 6.2020263671875, avg loss = 3.10101318359375
client 1, data condensation 8200, total loss = 3.23040771484375, avg loss = 1.615203857421875
client 1, data condensation 8400, total loss = 7.952972412109375, avg loss = 3.9764862060546875
client 1, data condensation 8600, total loss = 8.055908203125, avg loss = 4.0279541015625
client 1, data condensation 8800, total loss = 15.55218505859375, avg loss = 7.776092529296875
client 1, data condensation 9000, total loss = 16.45941162109375, avg loss = 8.229705810546875
client 1, data condensation 9200, total loss = 6.022552490234375, avg loss = 3.0112762451171875
client 1, data condensation 9400, total loss = 4.937774658203125, avg loss = 2.4688873291015625
client 1, data condensation 9600, total loss = 12.7176513671875, avg loss = 6.35882568359375
client 1, data condensation 9800, total loss = 49.0594482421875, avg loss = 24.52972412109375
client 1, data condensation 10000, total loss = 2.89599609375, avg loss = 1.447998046875
Round 0, client 1 condense time: 858.4169912338257
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 9776.0MB, free 14800.0MB
total 24576.0MB, used 9776.0MB, free 14800.0MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [ 7 20 35 66 51 40 13  5  3  2], bin edged: [0.00411917 0.00412244 0.00412572 0.00412899 0.00413226 0.00413553
 0.00413881 0.00414208 0.00414535 0.00414862 0.0041519 ]
client 2, data condensation 0, total loss = 364.8060607910156, avg loss = 364.8060607910156
client 2, data condensation 200, total loss = 17.740814208984375, avg loss = 17.740814208984375
client 2, data condensation 400, total loss = 16.463897705078125, avg loss = 16.463897705078125
client 2, data condensation 600, total loss = 6.111175537109375, avg loss = 6.111175537109375
client 2, data condensation 800, total loss = 7.48565673828125, avg loss = 7.48565673828125
client 2, data condensation 1000, total loss = 7.752716064453125, avg loss = 7.752716064453125
client 2, data condensation 1200, total loss = 8.692413330078125, avg loss = 8.692413330078125
client 2, data condensation 1400, total loss = 40.41644287109375, avg loss = 40.41644287109375
client 2, data condensation 1600, total loss = 6.500457763671875, avg loss = 6.500457763671875
client 2, data condensation 1800, total loss = 17.27874755859375, avg loss = 17.27874755859375
client 2, data condensation 2000, total loss = 14.055328369140625, avg loss = 14.055328369140625
client 2, data condensation 2200, total loss = 8.40850830078125, avg loss = 8.40850830078125
client 2, data condensation 2400, total loss = 6.326812744140625, avg loss = 6.326812744140625
client 2, data condensation 2600, total loss = 5.416290283203125, avg loss = 5.416290283203125
client 2, data condensation 2800, total loss = 40.661376953125, avg loss = 40.661376953125
client 2, data condensation 3000, total loss = 21.869140625, avg loss = 21.869140625
client 2, data condensation 3200, total loss = 6.2508544921875, avg loss = 6.2508544921875
client 2, data condensation 3400, total loss = 6.180938720703125, avg loss = 6.180938720703125
client 2, data condensation 3600, total loss = 11.8966064453125, avg loss = 11.8966064453125
client 2, data condensation 3800, total loss = 30.03857421875, avg loss = 30.03857421875
client 2, data condensation 4000, total loss = 7.709625244140625, avg loss = 7.709625244140625
client 2, data condensation 4200, total loss = 8.79486083984375, avg loss = 8.79486083984375
client 2, data condensation 4400, total loss = 4.00286865234375, avg loss = 4.00286865234375
client 2, data condensation 4600, total loss = 13.229522705078125, avg loss = 13.229522705078125
client 2, data condensation 4800, total loss = 25.621185302734375, avg loss = 25.621185302734375
client 2, data condensation 5000, total loss = 3.212799072265625, avg loss = 3.212799072265625
client 2, data condensation 5200, total loss = 45.207855224609375, avg loss = 45.207855224609375
client 2, data condensation 5400, total loss = 2.90814208984375, avg loss = 2.90814208984375
client 2, data condensation 5600, total loss = 41.833953857421875, avg loss = 41.833953857421875
client 2, data condensation 5800, total loss = 7.077178955078125, avg loss = 7.077178955078125
client 2, data condensation 6000, total loss = 8.67083740234375, avg loss = 8.67083740234375
client 2, data condensation 6200, total loss = 16.0906982421875, avg loss = 16.0906982421875
client 2, data condensation 6400, total loss = 8.664703369140625, avg loss = 8.664703369140625
client 2, data condensation 6600, total loss = 8.898406982421875, avg loss = 8.898406982421875
client 2, data condensation 6800, total loss = 86.6942138671875, avg loss = 86.6942138671875
client 2, data condensation 7000, total loss = 9.337432861328125, avg loss = 9.337432861328125
client 2, data condensation 7200, total loss = 6.78076171875, avg loss = 6.78076171875
client 2, data condensation 7400, total loss = 8.315673828125, avg loss = 8.315673828125
client 2, data condensation 7600, total loss = 69.727294921875, avg loss = 69.727294921875
client 2, data condensation 7800, total loss = 3.356719970703125, avg loss = 3.356719970703125
client 2, data condensation 8000, total loss = 5.684783935546875, avg loss = 5.684783935546875
client 2, data condensation 8200, total loss = 12.437225341796875, avg loss = 12.437225341796875
client 2, data condensation 8400, total loss = 10.52740478515625, avg loss = 10.52740478515625
client 2, data condensation 8600, total loss = 10.564453125, avg loss = 10.564453125
client 2, data condensation 8800, total loss = 9.28912353515625, avg loss = 9.28912353515625
client 2, data condensation 9000, total loss = 10.070404052734375, avg loss = 10.070404052734375
client 2, data condensation 9200, total loss = 2.81298828125, avg loss = 2.81298828125
client 2, data condensation 9400, total loss = 49.501251220703125, avg loss = 49.501251220703125
client 2, data condensation 9600, total loss = 20.55816650390625, avg loss = 20.55816650390625
client 2, data condensation 9800, total loss = 20.114776611328125, avg loss = 20.114776611328125
client 2, data condensation 10000, total loss = 10.311676025390625, avg loss = 10.311676025390625
Round 0, client 2 condense time: 330.58790493011475
client 2, class 9 have 242 samples
total 24576.0MB, used 9520.0MB, free 15056.0MB
total 24576.0MB, used 9520.0MB, free 15056.0MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [ 17  69 165 235 190 117  35  13   4   2], bin edged: [0.00117721 0.0011781  0.001179   0.0011799  0.0011808  0.0011817
 0.0011826  0.0011835  0.0011844  0.0011853  0.0011862 ]
class 2 have 1094 samples, histogram: [  4  22 119 302 339 199  80  21   5   3], bin edged: [0.00091049 0.00091132 0.00091214 0.00091297 0.00091379 0.00091462
 0.00091544 0.00091627 0.0009171  0.00091792 0.00091875]
client 3, data condensation 0, total loss = 715.4329833984375, avg loss = 357.71649169921875
client 3, data condensation 200, total loss = 57.29510498046875, avg loss = 28.647552490234375
client 3, data condensation 400, total loss = 28.63543701171875, avg loss = 14.317718505859375
client 3, data condensation 600, total loss = 39.53575134277344, avg loss = 19.76787567138672
client 3, data condensation 800, total loss = 13.158447265625, avg loss = 6.5792236328125
client 3, data condensation 1000, total loss = 13.12457275390625, avg loss = 6.562286376953125
client 3, data condensation 1200, total loss = 69.40362548828125, avg loss = 34.701812744140625
client 3, data condensation 1400, total loss = 14.511871337890625, avg loss = 7.2559356689453125
client 3, data condensation 1600, total loss = 9.2196044921875, avg loss = 4.60980224609375
client 3, data condensation 1800, total loss = 27.6951904296875, avg loss = 13.84759521484375
client 3, data condensation 2000, total loss = 11.9539794921875, avg loss = 5.97698974609375
client 3, data condensation 2200, total loss = 6.1922607421875, avg loss = 3.09613037109375
client 3, data condensation 2400, total loss = 5.41485595703125, avg loss = 2.707427978515625
client 3, data condensation 2600, total loss = 66.88424682617188, avg loss = 33.44212341308594
client 3, data condensation 2800, total loss = 13.5357666015625, avg loss = 6.76788330078125
client 3, data condensation 3000, total loss = 13.20123291015625, avg loss = 6.600616455078125
client 3, data condensation 3200, total loss = 3.72674560546875, avg loss = 1.863372802734375
client 3, data condensation 3400, total loss = 6.4810791015625, avg loss = 3.24053955078125
client 3, data condensation 3600, total loss = 5.1517333984375, avg loss = 2.57586669921875
client 3, data condensation 3800, total loss = 7.5228271484375, avg loss = 3.76141357421875
client 3, data condensation 4000, total loss = 7.668060302734375, avg loss = 3.8340301513671875
client 3, data condensation 4200, total loss = 3.115966796875, avg loss = 1.5579833984375
client 3, data condensation 4400, total loss = 16.994384765625, avg loss = 8.4971923828125
client 3, data condensation 4600, total loss = 5.534912109375, avg loss = 2.7674560546875
client 3, data condensation 4800, total loss = 4.3966064453125, avg loss = 2.19830322265625
client 3, data condensation 5000, total loss = 3.084381103515625, avg loss = 1.5421905517578125
client 3, data condensation 5200, total loss = 14.055084228515625, avg loss = 7.0275421142578125
client 3, data condensation 5400, total loss = 7.0299072265625, avg loss = 3.51495361328125
client 3, data condensation 5600, total loss = 7.747100830078125, avg loss = 3.8735504150390625
client 3, data condensation 5800, total loss = 15.554840087890625, avg loss = 7.7774200439453125
client 3, data condensation 6000, total loss = 11.645904541015625, avg loss = 5.8229522705078125
client 3, data condensation 6200, total loss = 6.46551513671875, avg loss = 3.232757568359375
client 3, data condensation 6400, total loss = 2.837005615234375, avg loss = 1.4185028076171875
client 3, data condensation 6600, total loss = 3.6131591796875, avg loss = 1.80657958984375
client 3, data condensation 6800, total loss = 6.242218017578125, avg loss = 3.1211090087890625
client 3, data condensation 7000, total loss = 15.819427490234375, avg loss = 7.9097137451171875
client 3, data condensation 7200, total loss = 17.99481201171875, avg loss = 8.997406005859375
client 3, data condensation 7400, total loss = 69.95440673828125, avg loss = 34.977203369140625
client 3, data condensation 7600, total loss = 31.32275390625, avg loss = 15.661376953125
client 3, data condensation 7800, total loss = 4.027618408203125, avg loss = 2.0138092041015625
client 3, data condensation 8000, total loss = 4.930450439453125, avg loss = 2.4652252197265625
client 3, data condensation 8200, total loss = 4.73699951171875, avg loss = 2.368499755859375
client 3, data condensation 8400, total loss = 4.4468994140625, avg loss = 2.22344970703125
client 3, data condensation 8600, total loss = 7.77105712890625, avg loss = 3.885528564453125
client 3, data condensation 8800, total loss = 1.83734130859375, avg loss = 0.918670654296875
client 3, data condensation 9000, total loss = 18.5184326171875, avg loss = 9.25921630859375
client 3, data condensation 9200, total loss = 17.123016357421875, avg loss = 8.561508178710938
client 3, data condensation 9400, total loss = 6.56787109375, avg loss = 3.283935546875
client 3, data condensation 9600, total loss = 2.89453125, avg loss = 1.447265625
client 3, data condensation 9800, total loss = 45.984832763671875, avg loss = 22.992416381835938
client 3, data condensation 10000, total loss = 6.56097412109375, avg loss = 3.280487060546875
Round 0, client 3 condense time: 777.3521301746368
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2663.06MB, free 21912.94MB
total 24576.0MB, used 2663.06MB, free 21912.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [  16   51  160  358  686 1012 1051  724   91    3], bin edged: [0.00023897 0.0002393  0.00023964 0.00023997 0.0002403  0.00024063
 0.00024097 0.0002413  0.00024163 0.00024197 0.0002423 ]
class 5 have 307 samples, histogram: [ 7 12 24 33 56 66 61 27 18  3], bin edged: [0.00324742 0.00324934 0.00325125 0.00325316 0.00325507 0.00325699
 0.0032589  0.00326081 0.00326272 0.00326463 0.00326655]
client 4, data condensation 0, total loss = 668.8508911132812, avg loss = 334.4254455566406
client 4, data condensation 200, total loss = 190.360595703125, avg loss = 95.1802978515625
client 4, data condensation 400, total loss = 77.193603515625, avg loss = 38.5968017578125
client 4, data condensation 600, total loss = 71.31588745117188, avg loss = 35.65794372558594
client 4, data condensation 800, total loss = 36.618865966796875, avg loss = 18.309432983398438
client 4, data condensation 1000, total loss = 43.40582275390625, avg loss = 21.702911376953125
client 4, data condensation 1200, total loss = 29.575897216796875, avg loss = 14.787948608398438
client 4, data condensation 1400, total loss = 23.837799072265625, avg loss = 11.918899536132812
client 4, data condensation 1600, total loss = 53.479034423828125, avg loss = 26.739517211914062
client 4, data condensation 1800, total loss = 22.321533203125, avg loss = 11.1607666015625
client 4, data condensation 2000, total loss = 41.619659423828125, avg loss = 20.809829711914062
client 4, data condensation 2200, total loss = 19.963592529296875, avg loss = 9.981796264648438
client 4, data condensation 2400, total loss = 18.206817626953125, avg loss = 9.103408813476562
client 4, data condensation 2600, total loss = 23.115875244140625, avg loss = 11.557937622070312
client 4, data condensation 2800, total loss = 13.7261962890625, avg loss = 6.86309814453125
client 4, data condensation 3000, total loss = 8.704803466796875, avg loss = 4.3524017333984375
client 4, data condensation 3200, total loss = 5.765960693359375, avg loss = 2.8829803466796875
client 4, data condensation 3400, total loss = 7.709625244140625, avg loss = 3.8548126220703125
client 4, data condensation 3600, total loss = 13.61700439453125, avg loss = 6.808502197265625
client 4, data condensation 3800, total loss = 5.199737548828125, avg loss = 2.5998687744140625
client 4, data condensation 4000, total loss = 8.887664794921875, avg loss = 4.4438323974609375
client 4, data condensation 4200, total loss = 8.540008544921875, avg loss = 4.2700042724609375
client 4, data condensation 4400, total loss = 9.43829345703125, avg loss = 4.719146728515625
client 4, data condensation 4600, total loss = 63.810546875, avg loss = 31.9052734375
client 4, data condensation 4800, total loss = 22.6485595703125, avg loss = 11.32427978515625
client 4, data condensation 5000, total loss = 6.167724609375, avg loss = 3.0838623046875
client 4, data condensation 5200, total loss = 27.031402587890625, avg loss = 13.515701293945312
client 4, data condensation 5400, total loss = 3.11492919921875, avg loss = 1.557464599609375
client 4, data condensation 5600, total loss = 66.39935302734375, avg loss = 33.199676513671875
client 4, data condensation 5800, total loss = 27.604339599609375, avg loss = 13.802169799804688
client 4, data condensation 6000, total loss = 12.49322509765625, avg loss = 6.246612548828125
client 4, data condensation 6200, total loss = 18.044708251953125, avg loss = 9.022354125976562
client 4, data condensation 6400, total loss = 7.1995849609375, avg loss = 3.59979248046875
client 4, data condensation 6600, total loss = 6.2696533203125, avg loss = 3.13482666015625
client 4, data condensation 6800, total loss = 6.033721923828125, avg loss = 3.0168609619140625
client 4, data condensation 7000, total loss = 22.437042236328125, avg loss = 11.218521118164062
client 4, data condensation 7200, total loss = 9.12591552734375, avg loss = 4.562957763671875
client 4, data condensation 7400, total loss = 12.4200439453125, avg loss = 6.21002197265625
client 4, data condensation 7600, total loss = 8.6226806640625, avg loss = 4.31134033203125
client 4, data condensation 7800, total loss = 5.816436767578125, avg loss = 2.9082183837890625
client 4, data condensation 8000, total loss = 26.689910888671875, avg loss = 13.344955444335938
client 4, data condensation 8200, total loss = 7.895721435546875, avg loss = 3.9478607177734375
client 4, data condensation 8400, total loss = 26.519805908203125, avg loss = 13.259902954101562
client 4, data condensation 8600, total loss = 23.98016357421875, avg loss = 11.990081787109375
client 4, data condensation 8800, total loss = 6.750213623046875, avg loss = 3.3751068115234375
client 4, data condensation 9000, total loss = 10.00115966796875, avg loss = 5.000579833984375
client 4, data condensation 9200, total loss = 24.0091552734375, avg loss = 12.00457763671875
client 4, data condensation 9400, total loss = 13.3414306640625, avg loss = 6.67071533203125
client 4, data condensation 9600, total loss = 6.824859619140625, avg loss = 3.4124298095703125
client 4, data condensation 9800, total loss = 8.597015380859375, avg loss = 4.2985076904296875
client 4, data condensation 10000, total loss = 9.035919189453125, avg loss = 4.5179595947265625
Round 0, client 4 condense time: 742.2024049758911
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 10596.0MB, free 13980.0MB
total 24576.0MB, used 10596.0MB, free 13980.0MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [   3   18   80  246  557 1094 1490 1125  340   46], bin edged: [0.00019836 0.00019863 0.0001989  0.00019917 0.00019944 0.00019971
 0.00019998 0.00020025 0.00020052 0.00020079 0.00020106]
client 5, data condensation 0, total loss = 352.01104736328125, avg loss = 352.01104736328125
client 5, data condensation 200, total loss = 124.74667358398438, avg loss = 124.74667358398438
client 5, data condensation 400, total loss = 75.75621032714844, avg loss = 75.75621032714844
client 5, data condensation 600, total loss = 46.620849609375, avg loss = 46.620849609375
client 5, data condensation 800, total loss = 172.4469757080078, avg loss = 172.4469757080078
client 5, data condensation 1000, total loss = 25.086456298828125, avg loss = 25.086456298828125
client 5, data condensation 1200, total loss = 31.476776123046875, avg loss = 31.476776123046875
client 5, data condensation 1400, total loss = 21.958221435546875, avg loss = 21.958221435546875
client 5, data condensation 1600, total loss = 10.5570068359375, avg loss = 10.5570068359375
client 5, data condensation 1800, total loss = 26.024383544921875, avg loss = 26.024383544921875
client 5, data condensation 2000, total loss = 119.2393798828125, avg loss = 119.2393798828125
client 5, data condensation 2200, total loss = 9.424102783203125, avg loss = 9.424102783203125
client 5, data condensation 2400, total loss = 20.122589111328125, avg loss = 20.122589111328125
client 5, data condensation 2600, total loss = 10.043975830078125, avg loss = 10.043975830078125
client 5, data condensation 2800, total loss = 4.83624267578125, avg loss = 4.83624267578125
client 5, data condensation 3000, total loss = 9.158203125, avg loss = 9.158203125
client 5, data condensation 3200, total loss = 10.386199951171875, avg loss = 10.386199951171875
client 5, data condensation 3400, total loss = 4.465728759765625, avg loss = 4.465728759765625
client 5, data condensation 3600, total loss = 14.297515869140625, avg loss = 14.297515869140625
client 5, data condensation 3800, total loss = 3.043121337890625, avg loss = 3.043121337890625
client 5, data condensation 4000, total loss = 10.54083251953125, avg loss = 10.54083251953125
client 5, data condensation 4200, total loss = 79.322509765625, avg loss = 79.322509765625
client 5, data condensation 4400, total loss = 18.65576171875, avg loss = 18.65576171875
client 5, data condensation 4600, total loss = 2.0650634765625, avg loss = 2.0650634765625
client 5, data condensation 4800, total loss = 49.30010986328125, avg loss = 49.30010986328125
client 5, data condensation 5000, total loss = 6.47314453125, avg loss = 6.47314453125
client 5, data condensation 5200, total loss = 1.5960693359375, avg loss = 1.5960693359375
client 5, data condensation 5400, total loss = 96.13307189941406, avg loss = 96.13307189941406
client 5, data condensation 5600, total loss = 67.2764892578125, avg loss = 67.2764892578125
client 5, data condensation 5800, total loss = 3.23248291015625, avg loss = 3.23248291015625
client 5, data condensation 6000, total loss = 1.6673583984375, avg loss = 1.6673583984375
client 5, data condensation 6200, total loss = 4.22808837890625, avg loss = 4.22808837890625
client 5, data condensation 6400, total loss = 2.10858154296875, avg loss = 2.10858154296875
client 5, data condensation 6600, total loss = 24.21527099609375, avg loss = 24.21527099609375
client 5, data condensation 6800, total loss = 4.16925048828125, avg loss = 4.16925048828125
client 5, data condensation 7000, total loss = 4.628936767578125, avg loss = 4.628936767578125
client 5, data condensation 7200, total loss = 88.75656127929688, avg loss = 88.75656127929688
client 5, data condensation 7400, total loss = 4.264129638671875, avg loss = 4.264129638671875
client 5, data condensation 7600, total loss = 5.486846923828125, avg loss = 5.486846923828125
client 5, data condensation 7800, total loss = 4.131683349609375, avg loss = 4.131683349609375
client 5, data condensation 8000, total loss = 11.23028564453125, avg loss = 11.23028564453125
client 5, data condensation 8200, total loss = 25.602203369140625, avg loss = 25.602203369140625
client 5, data condensation 8400, total loss = 6.791595458984375, avg loss = 6.791595458984375
client 5, data condensation 8600, total loss = 2.19317626953125, avg loss = 2.19317626953125
client 5, data condensation 8800, total loss = 52.37628173828125, avg loss = 52.37628173828125
client 5, data condensation 9000, total loss = 1.721588134765625, avg loss = 1.721588134765625
client 5, data condensation 9200, total loss = 3.7431640625, avg loss = 3.7431640625
client 5, data condensation 9400, total loss = 13.534149169921875, avg loss = 13.534149169921875
client 5, data condensation 9600, total loss = 2.2098388671875, avg loss = 2.2098388671875
client 5, data condensation 9800, total loss = 4.81103515625, avg loss = 4.81103515625
client 5, data condensation 10000, total loss = 71.33660888671875, avg loss = 71.33660888671875
Round 0, client 5 condense time: 483.67138743400574
client 5, class 8 have 4999 samples
total 24576.0MB, used 2413.06MB, free 22162.94MB
total 24576.0MB, used 2413.06MB, free 22162.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [   4    7   60  285  816 1250 1286  546   95   16], bin edged: [0.00022774 0.00022798 0.00022821 0.00022845 0.00022868 0.00022892
 0.00022915 0.00022939 0.00022962 0.00022986 0.00023009]
class 6 have 3914 samples, histogram: [  40  293 1169 1602  542  159   79   24    4    2], bin edged: [0.00025443 0.00025475 0.00025507 0.00025539 0.0002557  0.00025602
 0.00025634 0.00025666 0.00025698 0.0002573  0.00025761]
client 6, data condensation 0, total loss = 647.5028076171875, avg loss = 323.75140380859375
client 6, data condensation 200, total loss = 164.15478515625, avg loss = 82.077392578125
client 6, data condensation 400, total loss = 85.74433898925781, avg loss = 42.872169494628906
client 6, data condensation 600, total loss = 91.41819763183594, avg loss = 45.70909881591797
client 6, data condensation 800, total loss = 49.502593994140625, avg loss = 24.751296997070312
client 6, data condensation 1000, total loss = 55.879364013671875, avg loss = 27.939682006835938
client 6, data condensation 1200, total loss = 30.156494140625, avg loss = 15.0782470703125
client 6, data condensation 1400, total loss = 27.0855712890625, avg loss = 13.54278564453125
client 6, data condensation 1600, total loss = 28.687911987304688, avg loss = 14.343955993652344
client 6, data condensation 1800, total loss = 13.3968505859375, avg loss = 6.69842529296875
client 6, data condensation 2000, total loss = 13.727447509765625, avg loss = 6.8637237548828125
client 6, data condensation 2200, total loss = 19.144775390625, avg loss = 9.5723876953125
client 6, data condensation 2400, total loss = 10.350173950195312, avg loss = 5.175086975097656
client 6, data condensation 2600, total loss = 37.26263427734375, avg loss = 18.631317138671875
client 6, data condensation 2800, total loss = 11.197265625, avg loss = 5.5986328125
client 6, data condensation 3000, total loss = 19.845550537109375, avg loss = 9.922775268554688
client 6, data condensation 3200, total loss = 6.060760498046875, avg loss = 3.0303802490234375
client 6, data condensation 3400, total loss = 8.87860107421875, avg loss = 4.439300537109375
client 6, data condensation 3600, total loss = 15.063629150390625, avg loss = 7.5318145751953125
client 6, data condensation 3800, total loss = 13.524566650390625, avg loss = 6.7622833251953125
client 6, data condensation 4000, total loss = 7.79168701171875, avg loss = 3.895843505859375
client 6, data condensation 4200, total loss = 6.90716552734375, avg loss = 3.453582763671875
client 6, data condensation 4400, total loss = 10.760894775390625, avg loss = 5.3804473876953125
client 6, data condensation 4600, total loss = 6.736724853515625, avg loss = 3.3683624267578125
client 6, data condensation 4800, total loss = 4.847198486328125, avg loss = 2.4235992431640625
client 6, data condensation 5000, total loss = 23.43255615234375, avg loss = 11.716278076171875
client 6, data condensation 5200, total loss = 11.125823974609375, avg loss = 5.5629119873046875
client 6, data condensation 5400, total loss = 10.28741455078125, avg loss = 5.143707275390625
client 6, data condensation 5600, total loss = 6.00164794921875, avg loss = 3.000823974609375
client 6, data condensation 5800, total loss = 3.877777099609375, avg loss = 1.9388885498046875
client 6, data condensation 6000, total loss = 4.376617431640625, avg loss = 2.1883087158203125
client 6, data condensation 6200, total loss = 4.633544921875, avg loss = 2.3167724609375
client 6, data condensation 6400, total loss = 12.295379638671875, avg loss = 6.1476898193359375
client 6, data condensation 6600, total loss = 26.40496826171875, avg loss = 13.202484130859375
client 6, data condensation 6800, total loss = 6.02484130859375, avg loss = 3.012420654296875
client 6, data condensation 7000, total loss = 37.18426513671875, avg loss = 18.592132568359375
client 6, data condensation 7200, total loss = 19.092864990234375, avg loss = 9.546432495117188
client 6, data condensation 7400, total loss = 5.188140869140625, avg loss = 2.5940704345703125
client 6, data condensation 7600, total loss = 5.67431640625, avg loss = 2.837158203125
client 6, data condensation 7800, total loss = 6.97174072265625, avg loss = 3.485870361328125
client 6, data condensation 8000, total loss = 7.383941650390625, avg loss = 3.6919708251953125
client 6, data condensation 8200, total loss = 7.744476318359375, avg loss = 3.8722381591796875
client 6, data condensation 8400, total loss = 17.752593994140625, avg loss = 8.876296997070312
client 6, data condensation 8600, total loss = 7.601318359375, avg loss = 3.8006591796875
client 6, data condensation 8800, total loss = 6.169189453125, avg loss = 3.0845947265625
client 6, data condensation 9000, total loss = 5.5281982421875, avg loss = 2.76409912109375
client 6, data condensation 9200, total loss = 5.60980224609375, avg loss = 2.804901123046875
client 6, data condensation 9400, total loss = 6.4798583984375, avg loss = 3.23992919921875
client 6, data condensation 9600, total loss = 4.357269287109375, avg loss = 2.1786346435546875
client 6, data condensation 9800, total loss = 5.51141357421875, avg loss = 2.755706787109375
client 6, data condensation 10000, total loss = 4.717742919921875, avg loss = 2.3588714599609375
Round 0, client 6 condense time: 921.001547574997
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2929.06MB, free 21646.94MB
total 24576.0MB, used 2929.06MB, free 21646.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [   8   28   88  134  330  872 1649 1121  336   39], bin edged: [0.00021517 0.00021548 0.00021579 0.00021611 0.00021642 0.00021673
 0.00021704 0.00021735 0.00021766 0.00021797 0.00021828]
class 3 have 4999 samples, histogram: [   6   73  326  891 1333 1281  705  301   67   16], bin edged: [0.00019904 0.00019924 0.00019944 0.00019965 0.00019985 0.00020005
 0.00020026 0.00020046 0.00020066 0.00020087 0.00020107]
client 7, data condensation 0, total loss = 663.7119140625, avg loss = 331.85595703125
client 7, data condensation 200, total loss = 176.122802734375, avg loss = 88.0614013671875
client 7, data condensation 400, total loss = 118.50323486328125, avg loss = 59.251617431640625
client 7, data condensation 600, total loss = 78.31045532226562, avg loss = 39.15522766113281
client 7, data condensation 800, total loss = 69.90653991699219, avg loss = 34.953269958496094
client 7, data condensation 1000, total loss = 41.0531005859375, avg loss = 20.52655029296875
client 7, data condensation 1200, total loss = 54.142913818359375, avg loss = 27.071456909179688
client 7, data condensation 1400, total loss = 49.93556213378906, avg loss = 24.96778106689453
client 7, data condensation 1600, total loss = 201.0494384765625, avg loss = 100.52471923828125
client 7, data condensation 1800, total loss = 55.113861083984375, avg loss = 27.556930541992188
client 7, data condensation 2000, total loss = 24.394134521484375, avg loss = 12.197067260742188
client 7, data condensation 2200, total loss = 18.915618896484375, avg loss = 9.457809448242188
client 7, data condensation 2400, total loss = 25.2574462890625, avg loss = 12.62872314453125
client 7, data condensation 2600, total loss = 22.9176025390625, avg loss = 11.45880126953125
client 7, data condensation 2800, total loss = 18.16082763671875, avg loss = 9.080413818359375
client 7, data condensation 3000, total loss = 16.356658935546875, avg loss = 8.178329467773438
client 7, data condensation 3200, total loss = 9.31463623046875, avg loss = 4.657318115234375
client 7, data condensation 3400, total loss = 15.93084716796875, avg loss = 7.965423583984375
client 7, data condensation 3600, total loss = 21.367095947265625, avg loss = 10.683547973632812
client 7, data condensation 3800, total loss = 140.81112670898438, avg loss = 70.40556335449219
client 7, data condensation 4000, total loss = 11.88531494140625, avg loss = 5.942657470703125
client 7, data condensation 4200, total loss = 7.95062255859375, avg loss = 3.975311279296875
client 7, data condensation 4400, total loss = 8.336334228515625, avg loss = 4.1681671142578125
client 7, data condensation 4600, total loss = 22.8905029296875, avg loss = 11.44525146484375
client 7, data condensation 4800, total loss = 16.284393310546875, avg loss = 8.142196655273438
client 7, data condensation 5000, total loss = 34.74310302734375, avg loss = 17.371551513671875
client 7, data condensation 5200, total loss = 7.579376220703125, avg loss = 3.7896881103515625
client 7, data condensation 5400, total loss = 6.588287353515625, avg loss = 3.2941436767578125
client 7, data condensation 5600, total loss = 15.173858642578125, avg loss = 7.5869293212890625
client 7, data condensation 5800, total loss = 43.90592956542969, avg loss = 21.952964782714844
client 7, data condensation 6000, total loss = 12.647613525390625, avg loss = 6.3238067626953125
client 7, data condensation 6200, total loss = 4.854766845703125, avg loss = 2.4273834228515625
client 7, data condensation 6400, total loss = 15.830718994140625, avg loss = 7.9153594970703125
client 7, data condensation 6600, total loss = 7.904754638671875, avg loss = 3.9523773193359375
client 7, data condensation 6800, total loss = 12.592376708984375, avg loss = 6.2961883544921875
client 7, data condensation 7000, total loss = 7.09222412109375, avg loss = 3.546112060546875
client 7, data condensation 7200, total loss = 13.069732666015625, avg loss = 6.5348663330078125
client 7, data condensation 7400, total loss = 9.0367431640625, avg loss = 4.51837158203125
client 7, data condensation 7600, total loss = 21.250213623046875, avg loss = 10.625106811523438
client 7, data condensation 7800, total loss = 5.02423095703125, avg loss = 2.512115478515625
client 7, data condensation 8000, total loss = 118.35159301757812, avg loss = 59.17579650878906
client 7, data condensation 8200, total loss = 4.796295166015625, avg loss = 2.3981475830078125
client 7, data condensation 8400, total loss = 6.684722900390625, avg loss = 3.3423614501953125
client 7, data condensation 8600, total loss = 65.18255615234375, avg loss = 32.591278076171875
client 7, data condensation 8800, total loss = 3.685638427734375, avg loss = 1.8428192138671875
client 7, data condensation 9000, total loss = 5.244598388671875, avg loss = 2.6222991943359375
client 7, data condensation 9200, total loss = 41.1920166015625, avg loss = 20.59600830078125
client 7, data condensation 9400, total loss = 9.069000244140625, avg loss = 4.5345001220703125
client 7, data condensation 9600, total loss = 6.232391357421875, avg loss = 3.1161956787109375
client 7, data condensation 9800, total loss = 8.26983642578125, avg loss = 4.134918212890625
client 7, data condensation 10000, total loss = 4.85888671875, avg loss = 2.429443359375
Round 0, client 7 condense time: 975.9202990531921
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2933.06MB, free 21642.94MB
total 24576.0MB, used 2933.06MB, free 21642.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [ 2  4 10 29 25 79 85 79 34 17], bin edged: [0.00273119 0.00273376 0.00273633 0.0027389  0.00274148 0.00274405
 0.00274662 0.00274919 0.00275177 0.00275434 0.00275691]
class 5 have 135 samples, histogram: [ 1  0  3  1  4 19 25 37 30 15], bin edged: [0.00734863 0.00735684 0.00736504 0.00737325 0.00738145 0.00738966
 0.00739786 0.00740607 0.00741427 0.00742248 0.00743068]
class 9 have 4727 samples, histogram: [  24  129  441  760 1020 1031  772  401  126   23], bin edged: [0.00021043 0.00021065 0.00021088 0.0002111  0.00021133 0.00021156
 0.00021178 0.00021201 0.00021223 0.00021246 0.00021269]
client 8, data condensation 0, total loss = 1013.3392333984375, avg loss = 337.7797444661458
client 8, data condensation 200, total loss = 135.77215576171875, avg loss = 45.25738525390625
client 8, data condensation 400, total loss = 75.16622924804688, avg loss = 25.055409749348957
client 8, data condensation 600, total loss = 65.39248657226562, avg loss = 21.797495524088543
client 8, data condensation 800, total loss = 68.41775512695312, avg loss = 22.805918375651043
client 8, data condensation 1000, total loss = 36.066436767578125, avg loss = 12.022145589192709
client 8, data condensation 1200, total loss = 22.22381591796875, avg loss = 7.407938639322917
client 8, data condensation 1400, total loss = 32.065826416015625, avg loss = 10.688608805338541
client 8, data condensation 1600, total loss = 30.756988525390625, avg loss = 10.252329508463541
client 8, data condensation 1800, total loss = 23.613983154296875, avg loss = 7.871327718098958
client 8, data condensation 2000, total loss = 19.69390869140625, avg loss = 6.56463623046875
client 8, data condensation 2200, total loss = 22.697265625, avg loss = 7.565755208333333
client 8, data condensation 2400, total loss = 27.210601806640625, avg loss = 9.070200602213541
client 8, data condensation 2600, total loss = 19.18231201171875, avg loss = 6.39410400390625
client 8, data condensation 2800, total loss = 16.307769775390625, avg loss = 5.435923258463542
client 8, data condensation 3000, total loss = 18.0272216796875, avg loss = 6.009073893229167
client 8, data condensation 3200, total loss = 23.893646240234375, avg loss = 7.964548746744792
client 8, data condensation 3400, total loss = 10.964874267578125, avg loss = 3.6549580891927085
client 8, data condensation 3600, total loss = 8.495269775390625, avg loss = 2.831756591796875
client 8, data condensation 3800, total loss = 14.56500244140625, avg loss = 4.855000813802083
client 8, data condensation 4000, total loss = 16.277435302734375, avg loss = 5.425811767578125
client 8, data condensation 4200, total loss = 12.57196044921875, avg loss = 4.190653483072917
client 8, data condensation 4400, total loss = 10.9932861328125, avg loss = 3.6644287109375
client 8, data condensation 4600, total loss = 32.40411376953125, avg loss = 10.801371256510416
client 8, data condensation 4800, total loss = 17.772003173828125, avg loss = 5.924001057942708
client 8, data condensation 5000, total loss = 22.721221923828125, avg loss = 7.573740641276042
client 8, data condensation 5200, total loss = 19.07379150390625, avg loss = 6.357930501302083
client 8, data condensation 5400, total loss = 17.865478515625, avg loss = 5.955159505208333
client 8, data condensation 5600, total loss = 4.91180419921875, avg loss = 1.63726806640625
client 8, data condensation 5800, total loss = 13.406646728515625, avg loss = 4.468882242838542
client 8, data condensation 6000, total loss = 14.11492919921875, avg loss = 4.704976399739583
client 8, data condensation 6200, total loss = 9.914093017578125, avg loss = 3.3046976725260415
client 8, data condensation 6400, total loss = 22.248321533203125, avg loss = 7.416107177734375
client 8, data condensation 6600, total loss = 72.44036865234375, avg loss = 24.14678955078125
client 8, data condensation 6800, total loss = 19.063385009765625, avg loss = 6.354461669921875
client 8, data condensation 7000, total loss = 45.029052734375, avg loss = 15.009684244791666
client 8, data condensation 7200, total loss = 54.1424560546875, avg loss = 18.0474853515625
client 8, data condensation 7400, total loss = 8.438232421875, avg loss = 2.812744140625
client 8, data condensation 7600, total loss = 4.937408447265625, avg loss = 1.6458028157552083
client 8, data condensation 7800, total loss = 73.19522094726562, avg loss = 24.398406982421875
client 8, data condensation 8000, total loss = 32.063079833984375, avg loss = 10.687693277994791
client 8, data condensation 8200, total loss = 4.599151611328125, avg loss = 1.533050537109375
client 8, data condensation 8400, total loss = 27.419281005859375, avg loss = 9.139760335286459
client 8, data condensation 8600, total loss = 7.899688720703125, avg loss = 2.6332295735677085
client 8, data condensation 8800, total loss = 8.032867431640625, avg loss = 2.6776224772135415
client 8, data condensation 9000, total loss = 6.427337646484375, avg loss = 2.1424458821614585
client 8, data condensation 9200, total loss = 8.185028076171875, avg loss = 2.7283426920572915
client 8, data condensation 9400, total loss = 40.637420654296875, avg loss = 13.545806884765625
client 8, data condensation 9600, total loss = 8.225616455078125, avg loss = 2.7418721516927085
client 8, data condensation 9800, total loss = 8.902587890625, avg loss = 2.967529296875
client 8, data condensation 10000, total loss = 16.5738525390625, avg loss = 5.524617513020833
Round 0, client 8 condense time: 1107.9033179283142
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 2933.06MB, free 21642.94MB
total 24576.0MB, used 2933.06MB, free 21642.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [ 1  1  1  1  7 13 21 28 29 18], bin edged: [0.00829056 0.00829639 0.00830222 0.00830804 0.00831387 0.0083197
 0.00832553 0.00833135 0.00833718 0.00834301 0.00834884]
class 5 have 192 samples, histogram: [ 2  4  4 30 30 49 38 18 12  5], bin edged: [0.00518982 0.0051932  0.00519659 0.00519997 0.00520336 0.00520674
 0.00521013 0.00521351 0.0052169  0.00522028 0.00522367]
class 6 have 1075 samples, histogram: [ 17 229 420 221  95  49  24  13   3   4], bin edged: [0.00092714 0.00092818 0.00092922 0.00093026 0.0009313  0.00093233
 0.00093337 0.00093441 0.00093545 0.00093649 0.00093753]
client 9, data condensation 0, total loss = 993.570068359375, avg loss = 331.1900227864583
client 9, data condensation 200, total loss = 44.05963134765625, avg loss = 14.686543782552084
client 9, data condensation 400, total loss = 52.43487548828125, avg loss = 17.478291829427082
client 9, data condensation 600, total loss = 81.30702209472656, avg loss = 27.102340698242188
client 9, data condensation 800, total loss = 43.22685241699219, avg loss = 14.408950805664062
client 9, data condensation 1000, total loss = 20.762542724609375, avg loss = 6.920847574869792
client 9, data condensation 1200, total loss = 11.890869140625, avg loss = 3.963623046875
client 9, data condensation 1400, total loss = 57.1884765625, avg loss = 19.062825520833332
client 9, data condensation 1600, total loss = 33.873046875, avg loss = 11.291015625
client 9, data condensation 1800, total loss = 13.354522705078125, avg loss = 4.451507568359375
client 9, data condensation 2000, total loss = 16.915313720703125, avg loss = 5.638437906901042
client 9, data condensation 2200, total loss = 20.266510009765625, avg loss = 6.755503336588542
client 9, data condensation 2400, total loss = 19.181610107421875, avg loss = 6.393870035807292
client 9, data condensation 2600, total loss = 29.9078369140625, avg loss = 9.969278971354166
client 9, data condensation 2800, total loss = 24.916748046875, avg loss = 8.305582682291666
client 9, data condensation 3000, total loss = 14.5413818359375, avg loss = 4.847127278645833
client 9, data condensation 3200, total loss = 10.513458251953125, avg loss = 3.504486083984375
client 9, data condensation 3400, total loss = 7.784698486328125, avg loss = 2.5948994954427085
client 9, data condensation 3600, total loss = 16.966094970703125, avg loss = 5.655364990234375
client 9, data condensation 3800, total loss = 16.456756591796875, avg loss = 5.485585530598958
client 9, data condensation 4000, total loss = 21.46240234375, avg loss = 7.154134114583333
client 9, data condensation 4200, total loss = 9.8695068359375, avg loss = 3.2898356119791665
client 9, data condensation 4400, total loss = 11.901519775390625, avg loss = 3.9671732584635415
client 9, data condensation 4600, total loss = 13.08734130859375, avg loss = 4.362447102864583
client 9, data condensation 4800, total loss = 21.194610595703125, avg loss = 7.064870198567708
client 9, data condensation 5000, total loss = 8.929718017578125, avg loss = 2.9765726725260415
client 9, data condensation 5200, total loss = 23.18536376953125, avg loss = 7.72845458984375
client 9, data condensation 5400, total loss = 14.91082763671875, avg loss = 4.97027587890625
client 9, data condensation 5600, total loss = 12.631683349609375, avg loss = 4.210561116536458
client 9, data condensation 5800, total loss = 14.917999267578125, avg loss = 4.972666422526042
client 9, data condensation 6000, total loss = 10.860687255859375, avg loss = 3.6202290852864585
client 9, data condensation 6200, total loss = 8.978118896484375, avg loss = 2.992706298828125
client 9, data condensation 6400, total loss = 19.817901611328125, avg loss = 6.605967203776042
client 9, data condensation 6600, total loss = 8.7940673828125, avg loss = 2.9313557942708335
client 9, data condensation 6800, total loss = 11.70159912109375, avg loss = 3.9005330403645835
client 9, data condensation 7000, total loss = 63.25994873046875, avg loss = 21.086649576822918
client 9, data condensation 7200, total loss = 11.81341552734375, avg loss = 3.93780517578125
client 9, data condensation 7400, total loss = 23.9412841796875, avg loss = 7.980428059895833
client 9, data condensation 7600, total loss = 12.37335205078125, avg loss = 4.12445068359375
client 9, data condensation 7800, total loss = 14.420867919921875, avg loss = 4.806955973307292
client 9, data condensation 8000, total loss = 13.85687255859375, avg loss = 4.61895751953125
client 9, data condensation 8200, total loss = 60.754150390625, avg loss = 20.251383463541668
client 9, data condensation 8400, total loss = 37.753997802734375, avg loss = 12.584665934244791
client 9, data condensation 8600, total loss = 11.414825439453125, avg loss = 3.8049418131510415
client 9, data condensation 8800, total loss = 40.760589599609375, avg loss = 13.586863199869791
client 9, data condensation 9000, total loss = 33.173797607421875, avg loss = 11.057932535807291
client 9, data condensation 9200, total loss = 12.463409423828125, avg loss = 4.154469807942708
client 9, data condensation 9400, total loss = 27.07666015625, avg loss = 9.025553385416666
client 9, data condensation 9600, total loss = 4.67681884765625, avg loss = 1.5589396158854167
client 9, data condensation 9800, total loss = 27.499755859375, avg loss = 9.166585286458334
client 9, data condensation 10000, total loss = 25.09796142578125, avg loss = 8.365987141927084
Round 0, client 9 condense time: 836.5751781463623
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 2935.06MB, free 21640.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[-0.0040, -0.0196, -0.0024,  0.0111,  0.0454, -0.0034,  0.0143, -0.0439,
          0.0130,  0.0035],
        [-0.0074,  0.0280, -0.0165, -0.0021,  0.0132, -0.0043,  0.0004, -0.0059,
         -0.0117,  0.0043],
        [-0.0396, -0.0018, -0.0341, -0.0027,  0.0035,  0.0384, -0.0241, -0.0233,
         -0.0091,  0.0137],
        [ 0.0040,  0.0381, -0.0225, -0.0017,  0.0172, -0.0073,  0.0064, -0.0039,
         -0.0112, -0.0027],
        [ 0.0386, -0.0561, -0.0280,  0.0494,  0.0390,  0.0202, -0.0019,  0.0156,
          0.0092, -0.0435],
        [ 0.0103, -0.0339,  0.0089,  0.0226,  0.0103, -0.0010,  0.0141,  0.0172,
          0.0145, -0.0021],
        [ 0.0083, -0.0200,  0.0081,  0.0199,  0.0135, -0.0008,  0.0020,  0.0216,
          0.0096, -0.0004],
        [-0.0483,  0.0095, -0.0371, -0.0076, -0.0017,  0.0412, -0.0311, -0.0352,
         -0.0173,  0.0203],
        [ 0.0127, -0.0062,  0.0012,  0.0516,  0.0455, -0.0142,  0.0044, -0.0207,
         -0.0254, -0.0572],
        [ 0.0294, -0.0251, -0.0296, -0.0477, -0.0084,  0.0381, -0.0440,  0.0420,
         -0.0031,  0.0588]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[4, 6, 8, 3, 9],
        [1, 4, 9, 6, 3],
        [5, 9, 4, 1, 3],
        [1, 4, 6, 0, 3],
        [3, 4, 0, 5, 7],
        [3, 7, 8, 6, 4],
        [7, 3, 4, 8, 0],
        [5, 9, 1, 4, 3],
        [3, 4, 0, 6, 2],
        [9, 7, 5, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
1
Round 0: # synthetic sample: 1012
total 24576.0MB, used 2937.06MB, free 21638.94MB
{0: {0: 68, 1: 1, 2: 2, 3: 1, 4: 450, 5: 478, 6: 0, 7: 0, 8: 0, 9: 0}, 1: {0: 31, 1: 0, 2: 4, 3: 2, 4: 480, 5: 480, 6: 3, 7: 0, 8: 0, 9: 0}, 2: {0: 12, 1: 0, 2: 11, 3: 3, 4: 603, 5: 368, 6: 0, 7: 0, 8: 0, 9: 3}, 3: {0: 11, 1: 1, 2: 21, 3: 1, 4: 534, 5: 429, 6: 0, 7: 0, 8: 0, 9: 3}, 4: {0: 20, 1: 0, 2: 8, 3: 4, 4: 631, 5: 332, 6: 2, 7: 0, 8: 0, 9: 3}, 5: {0: 13, 1: 0, 2: 37, 3: 1, 4: 418, 5: 529, 6: 1, 7: 0, 8: 0, 9: 1}, 6: {0: 5, 1: 1, 2: 12, 3: 1, 4: 632, 5: 348, 6: 0, 7: 0, 8: 0, 9: 1}, 7: {0: 39, 1: 0, 2: 9, 3: 8, 4: 506, 5: 430, 6: 1, 7: 0, 8: 0, 9: 7}, 8: {0: 115, 1: 0, 2: 21, 3: 1, 4: 440, 5: 423, 6: 0, 7: 0, 8: 0, 9: 0}, 9: {0: 44, 1: 0, 2: 1, 3: 7, 4: 531, 5: 404, 6: 3, 7: 0, 8: 0, 9: 10}}
round 0 evaluation: test acc is 0.1250, test loss = 2.300052
{0: {0: 649, 1: 4, 2: 19, 3: 2, 4: 14, 5: 4, 6: 34, 7: 0, 8: 254, 9: 20}, 1: {0: 403, 1: 15, 2: 54, 3: 3, 4: 45, 5: 15, 6: 145, 7: 2, 8: 244, 9: 74}, 2: {0: 312, 1: 14, 2: 39, 3: 9, 4: 53, 5: 33, 6: 307, 7: 0, 8: 209, 9: 24}, 3: {0: 341, 1: 14, 2: 67, 3: 12, 4: 74, 5: 41, 6: 241, 7: 0, 8: 160, 9: 50}, 4: {0: 180, 1: 12, 2: 37, 3: 9, 4: 63, 5: 45, 6: 447, 7: 0, 8: 183, 9: 24}, 5: {0: 406, 1: 15, 2: 68, 3: 5, 4: 48, 5: 56, 6: 186, 7: 0, 8: 179, 9: 37}, 6: {0: 166, 1: 19, 2: 27, 3: 5, 4: 27, 5: 44, 6: 533, 7: 0, 8: 155, 9: 24}, 7: {0: 353, 1: 11, 2: 37, 3: 3, 4: 40, 5: 23, 6: 188, 7: 1, 8: 289, 9: 55}, 8: {0: 497, 1: 3, 2: 33, 3: 0, 4: 19, 5: 11, 6: 29, 7: 0, 8: 383, 9: 25}, 9: {0: 336, 1: 8, 2: 13, 3: 4, 4: 30, 5: 3, 6: 93, 7: 1, 8: 409, 9: 103}}
epoch 0, train loss avg now = 2.284285, train contrast loss now = 0.000000, test acc now = 0.1854, test loss now = 2.273162
{0: {0: 588, 1: 16, 2: 55, 3: 7, 4: 29, 5: 9, 6: 20, 7: 7, 8: 249, 9: 20}, 1: {0: 155, 1: 244, 2: 59, 3: 1, 4: 29, 5: 1, 6: 21, 7: 15, 8: 372, 9: 103}, 2: {0: 185, 1: 17, 2: 322, 3: 27, 4: 163, 5: 43, 6: 97, 7: 43, 8: 90, 9: 13}, 3: {0: 147, 1: 40, 2: 112, 3: 120, 4: 54, 5: 128, 6: 173, 7: 72, 8: 80, 9: 74}, 4: {0: 121, 1: 12, 2: 214, 3: 20, 4: 258, 5: 37, 6: 157, 7: 77, 8: 75, 9: 29}, 5: {0: 88, 1: 20, 2: 157, 3: 95, 4: 69, 5: 234, 6: 134, 7: 85, 8: 78, 9: 40}, 6: {0: 56, 1: 9, 2: 168, 3: 31, 4: 171, 5: 21, 6: 428, 7: 29, 8: 49, 9: 38}, 7: {0: 115, 1: 16, 2: 115, 3: 20, 4: 119, 5: 87, 6: 91, 7: 328, 8: 47, 9: 62}, 8: {0: 247, 1: 23, 2: 23, 3: 5, 4: 9, 5: 2, 6: 14, 7: 0, 8: 629, 9: 48}, 9: {0: 184, 1: 128, 2: 52, 3: 6, 4: 26, 5: 3, 6: 18, 7: 19, 8: 326, 9: 238}}
epoch 100, train loss avg now = 0.240831, train contrast loss now = 0.000000, test acc now = 0.3389, test loss now = 4.106620
{0: {0: 546, 1: 78, 2: 63, 3: 39, 4: 28, 5: 3, 6: 24, 7: 5, 8: 156, 9: 58}, 1: {0: 71, 1: 467, 2: 76, 3: 2, 4: 9, 5: 3, 6: 39, 7: 13, 8: 156, 9: 164}, 2: {0: 101, 1: 31, 2: 368, 3: 113, 4: 67, 5: 31, 6: 165, 7: 31, 8: 48, 9: 45}, 3: {0: 43, 1: 59, 2: 115, 3: 274, 4: 27, 5: 105, 6: 204, 7: 50, 8: 43, 9: 80}, 4: {0: 78, 1: 14, 2: 265, 3: 50, 4: 126, 5: 22, 6: 275, 7: 74, 8: 50, 9: 46}, 5: {0: 24, 1: 30, 2: 173, 3: 278, 4: 42, 5: 145, 6: 155, 7: 61, 8: 43, 9: 49}, 6: {0: 20, 1: 25, 2: 156, 3: 80, 4: 38, 5: 11, 6: 577, 7: 24, 8: 23, 9: 46}, 7: {0: 53, 1: 35, 2: 129, 3: 66, 4: 63, 5: 72, 6: 123, 7: 337, 8: 19, 9: 103}, 8: {0: 275, 1: 84, 2: 36, 3: 19, 4: 7, 5: 0, 6: 11, 7: 1, 8: 471, 9: 96}, 9: {0: 90, 1: 220, 2: 57, 3: 17, 4: 5, 5: 4, 6: 47, 7: 10, 8: 174, 9: 376}}
epoch 200, train loss avg now = 0.044471, train contrast loss now = 0.000000, test acc now = 0.3687, test loss now = 4.485288
{0: {0: 440, 1: 97, 2: 71, 3: 10, 4: 19, 5: 5, 6: 40, 7: 10, 8: 251, 9: 57}, 1: {0: 32, 1: 506, 2: 60, 3: 2, 4: 16, 5: 2, 6: 36, 7: 19, 8: 216, 9: 111}, 2: {0: 112, 1: 39, 2: 358, 3: 42, 4: 79, 5: 36, 6: 177, 7: 40, 8: 50, 9: 67}, 3: {0: 50, 1: 73, 2: 123, 3: 138, 4: 26, 5: 124, 6: 241, 7: 63, 8: 44, 9: 118}, 4: {0: 78, 1: 23, 2: 242, 3: 27, 4: 152, 5: 21, 6: 267, 7: 88, 8: 45, 9: 57}, 5: {0: 32, 1: 36, 2: 166, 3: 117, 4: 41, 5: 221, 6: 205, 7: 71, 8: 43, 9: 68}, 6: {0: 23, 1: 30, 2: 133, 3: 32, 4: 49, 5: 8, 6: 619, 7: 26, 8: 23, 9: 57}, 7: {0: 45, 1: 34, 2: 106, 3: 21, 4: 56, 5: 79, 6: 146, 7: 390, 8: 29, 9: 94}, 8: {0: 173, 1: 83, 2: 29, 3: 4, 4: 3, 5: 1, 6: 17, 7: 2, 8: 614, 9: 74}, 9: {0: 58, 1: 277, 2: 58, 3: 7, 4: 10, 5: 2, 6: 41, 7: 24, 8: 236, 9: 287}}
epoch 300, train loss avg now = 0.018843, train contrast loss now = 0.000000, test acc now = 0.3725, test loss now = 4.732888
{0: {0: 328, 1: 146, 2: 47, 3: 52, 4: 40, 5: 12, 6: 41, 7: 5, 8: 300, 9: 29}, 1: {0: 10, 1: 542, 2: 13, 3: 37, 4: 48, 5: 10, 6: 51, 7: 2, 8: 263, 9: 24}, 2: {0: 74, 1: 34, 2: 155, 3: 168, 4: 174, 5: 55, 6: 230, 7: 10, 8: 85, 9: 15}, 3: {0: 16, 1: 60, 2: 22, 3: 379, 4: 52, 5: 148, 6: 226, 7: 9, 8: 63, 9: 25}, 4: {0: 44, 1: 18, 2: 102, 3: 95, 4: 300, 5: 52, 6: 283, 7: 22, 8: 68, 9: 16}, 5: {0: 13, 1: 18, 2: 45, 3: 351, 4: 80, 5: 231, 6: 185, 7: 12, 8: 55, 9: 10}, 6: {0: 7, 1: 31, 2: 43, 3: 135, 4: 97, 5: 37, 6: 605, 7: 2, 8: 37, 9: 6}, 7: {0: 17, 1: 36, 2: 33, 3: 161, 4: 167, 5: 159, 6: 182, 7: 179, 8: 44, 9: 22}, 8: {0: 70, 1: 114, 2: 13, 3: 39, 4: 13, 5: 10, 6: 23, 7: 0, 8: 682, 9: 36}, 9: {0: 24, 1: 403, 2: 18, 3: 68, 4: 37, 5: 16, 6: 49, 7: 3, 8: 298, 9: 84}}
epoch 400, train loss avg now = 0.033662, train contrast loss now = 0.000000, test acc now = 0.3485, test loss now = 5.220669
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 505, 1: 83, 2: 76, 3: 23, 4: 23, 5: 1, 6: 53, 7: 11, 8: 168, 9: 57}, 1: {0: 88, 1: 486, 2: 71, 3: 2, 4: 17, 5: 1, 6: 57, 7: 22, 8: 109, 9: 147}, 2: {0: 114, 1: 33, 2: 316, 3: 85, 4: 100, 5: 34, 6: 199, 7: 37, 8: 40, 9: 42}, 3: {0: 38, 1: 53, 2: 102, 3: 216, 4: 33, 5: 105, 6: 275, 7: 51, 8: 38, 9: 89}, 4: {0: 84, 1: 17, 2: 196, 3: 37, 4: 180, 5: 34, 6: 302, 7: 69, 8: 35, 9: 46}, 5: {0: 29, 1: 26, 2: 140, 3: 197, 4: 60, 5: 178, 6: 226, 7: 64, 8: 37, 9: 43}, 6: {0: 31, 1: 16, 2: 99, 3: 41, 4: 52, 5: 14, 6: 664, 7: 21, 8: 16, 9: 46}, 7: {0: 52, 1: 25, 2: 105, 3: 43, 4: 68, 5: 72, 6: 181, 7: 353, 8: 16, 9: 85}, 8: {0: 280, 1: 74, 2: 30, 3: 15, 4: 7, 5: 0, 6: 26, 7: 2, 8: 494, 9: 72}, 9: {0: 120, 1: 237, 2: 64, 3: 12, 4: 10, 5: 7, 6: 75, 7: 20, 8: 135, 9: 320}}
epoch 500, train loss avg now = 0.106222, train contrast loss now = 0.000000, test acc now = 0.3712, test loss now = 4.964602
{0: {0: 500, 1: 79, 2: 57, 3: 21, 4: 28, 5: 5, 6: 37, 7: 7, 8: 206, 9: 60}, 1: {0: 60, 1: 496, 2: 51, 3: 4, 4: 22, 5: 2, 6: 35, 7: 17, 8: 188, 9: 125}, 2: {0: 117, 1: 34, 2: 308, 3: 80, 4: 117, 5: 33, 6: 173, 7: 30, 8: 45, 9: 63}, 3: {0: 47, 1: 68, 2: 81, 3: 223, 4: 50, 5: 109, 6: 243, 7: 39, 8: 39, 9: 101}, 4: {0: 85, 1: 20, 2: 200, 3: 40, 4: 212, 5: 23, 6: 267, 7: 57, 8: 46, 9: 50}, 5: {0: 37, 1: 30, 2: 132, 3: 206, 4: 68, 5: 194, 6: 193, 7: 47, 8: 40, 9: 53}, 6: {0: 26, 1: 26, 2: 125, 3: 54, 4: 63, 5: 12, 6: 600, 7: 19, 8: 26, 9: 49}, 7: {0: 54, 1: 32, 2: 91, 3: 48, 4: 89, 5: 74, 6: 161, 7: 335, 8: 20, 9: 96}, 8: {0: 231, 1: 72, 2: 28, 3: 8, 4: 7, 5: 0, 6: 17, 7: 1, 8: 557, 9: 79}, 9: {0: 90, 1: 249, 2: 51, 3: 11, 4: 13, 5: 3, 6: 46, 7: 18, 8: 208, 9: 311}}
epoch 600, train loss avg now = 0.005693, train contrast loss now = 0.000000, test acc now = 0.3736, test loss now = 4.846542
{0: {0: 483, 1: 86, 2: 55, 3: 21, 4: 32, 5: 5, 6: 37, 7: 7, 8: 212, 9: 62}, 1: {0: 58, 1: 489, 2: 50, 3: 2, 4: 18, 5: 2, 6: 39, 7: 17, 8: 189, 9: 136}, 2: {0: 110, 1: 38, 2: 296, 3: 76, 4: 120, 5: 35, 6: 183, 7: 30, 8: 47, 9: 65}, 3: {0: 48, 1: 71, 2: 82, 3: 197, 4: 48, 5: 120, 6: 251, 7: 38, 8: 39, 9: 106}, 4: {0: 81, 1: 20, 2: 189, 3: 35, 4: 200, 5: 28, 6: 287, 7: 59, 8: 47, 9: 54}, 5: {0: 34, 1: 32, 2: 132, 3: 192, 4: 70, 5: 195, 6: 198, 7: 51, 8: 39, 9: 57}, 6: {0: 24, 1: 26, 2: 119, 3: 49, 4: 56, 5: 13, 6: 619, 7: 17, 8: 27, 9: 50}, 7: {0: 55, 1: 34, 2: 82, 3: 38, 4: 83, 5: 80, 6: 171, 7: 333, 8: 20, 9: 104}, 8: {0: 216, 1: 77, 2: 27, 3: 8, 4: 6, 5: 0, 6: 19, 7: 1, 8: 566, 9: 80}, 9: {0: 85, 1: 244, 2: 45, 3: 10, 4: 13, 5: 2, 6: 51, 7: 19, 8: 210, 9: 321}}
epoch 700, train loss avg now = 0.009112, train contrast loss now = 0.000000, test acc now = 0.3699, test loss now = 4.974350
{0: {0: 483, 1: 83, 2: 55, 3: 20, 4: 31, 5: 5, 6: 38, 7: 7, 8: 216, 9: 62}, 1: {0: 51, 1: 484, 2: 54, 3: 3, 4: 20, 5: 2, 6: 37, 7: 21, 8: 192, 9: 136}, 2: {0: 111, 1: 34, 2: 318, 3: 72, 4: 105, 5: 36, 6: 183, 7: 32, 8: 45, 9: 64}, 3: {0: 47, 1: 65, 2: 87, 3: 202, 4: 48, 5: 115, 6: 249, 7: 44, 8: 38, 9: 105}, 4: {0: 81, 1: 20, 2: 200, 3: 34, 4: 189, 5: 26, 6: 290, 7: 62, 8: 47, 9: 51}, 5: {0: 33, 1: 29, 2: 131, 3: 194, 4: 69, 5: 197, 6: 199, 7: 53, 8: 38, 9: 57}, 6: {0: 26, 1: 23, 2: 123, 3: 47, 4: 52, 5: 14, 6: 618, 7: 19, 8: 24, 9: 54}, 7: {0: 55, 1: 30, 2: 87, 3: 40, 4: 80, 5: 76, 6: 164, 7: 353, 8: 18, 9: 97}, 8: {0: 213, 1: 75, 2: 28, 3: 8, 4: 6, 5: 0, 6: 20, 7: 1, 8: 572, 9: 77}, 9: {0: 86, 1: 240, 2: 49, 3: 10, 4: 13, 5: 2, 6: 48, 7: 21, 8: 209, 9: 322}}
epoch 800, train loss avg now = 0.015857, train contrast loss now = 0.000000, test acc now = 0.3738, test loss now = 4.947924
{0: {0: 507, 1: 70, 2: 57, 3: 17, 4: 29, 5: 4, 6: 38, 7: 6, 8: 206, 9: 66}, 1: {0: 60, 1: 495, 2: 51, 3: 3, 4: 20, 5: 2, 6: 35, 7: 21, 8: 181, 9: 132}, 2: {0: 113, 1: 33, 2: 309, 3: 73, 4: 113, 5: 35, 6: 185, 7: 29, 8: 46, 9: 64}, 3: {0: 53, 1: 72, 2: 87, 3: 195, 4: 46, 5: 121, 6: 245, 7: 38, 8: 37, 9: 106}, 4: {0: 88, 1: 21, 2: 190, 3: 36, 4: 200, 5: 26, 6: 287, 7: 57, 8: 42, 9: 53}, 5: {0: 36, 1: 33, 2: 138, 3: 193, 4: 71, 5: 192, 6: 200, 7: 45, 8: 36, 9: 56}, 6: {0: 26, 1: 24, 2: 126, 3: 49, 4: 55, 5: 11, 6: 611, 7: 18, 8: 25, 9: 55}, 7: {0: 55, 1: 31, 2: 85, 3: 39, 4: 76, 5: 75, 6: 166, 7: 352, 8: 20, 9: 101}, 8: {0: 240, 1: 72, 2: 29, 3: 7, 4: 5, 5: 0, 6: 19, 7: 2, 8: 552, 9: 74}, 9: {0: 89, 1: 250, 2: 52, 3: 11, 4: 14, 5: 2, 6: 45, 7: 18, 8: 200, 9: 319}}
epoch 900, train loss avg now = 0.004210, train contrast loss now = 0.000000, test acc now = 0.3732, test loss now = 5.002957
{0: {0: 479, 1: 84, 2: 55, 3: 19, 4: 30, 5: 5, 6: 38, 7: 8, 8: 214, 9: 68}, 1: {0: 51, 1: 501, 2: 51, 3: 3, 4: 19, 5: 2, 6: 37, 7: 22, 8: 178, 9: 136}, 2: {0: 108, 1: 36, 2: 304, 3: 77, 4: 109, 5: 32, 6: 188, 7: 32, 8: 47, 9: 67}, 3: {0: 48, 1: 70, 2: 85, 3: 201, 4: 46, 5: 109, 6: 255, 7: 41, 8: 35, 9: 110}, 4: {0: 80, 1: 21, 2: 191, 3: 36, 4: 192, 5: 23, 6: 298, 7: 62, 8: 41, 9: 56}, 5: {0: 35, 1: 31, 2: 132, 3: 197, 4: 69, 5: 179, 6: 213, 7: 51, 8: 34, 9: 59}, 6: {0: 24, 1: 26, 2: 121, 3: 49, 4: 54, 5: 12, 6: 619, 7: 19, 8: 21, 9: 55}, 7: {0: 53, 1: 33, 2: 84, 3: 36, 4: 77, 5: 74, 6: 170, 7: 355, 8: 17, 9: 101}, 8: {0: 214, 1: 78, 2: 26, 3: 9, 4: 4, 5: 0, 6: 20, 7: 1, 8: 571, 9: 77}, 9: {0: 84, 1: 251, 2: 46, 3: 12, 4: 14, 5: 2, 6: 48, 7: 21, 8: 205, 9: 317}}
epoch 1000, train loss avg now = 0.004597, train contrast loss now = 0.000000, test acc now = 0.3718, test loss now = 5.008571
epoch avg loss = 4.597178005094349e-06, total time = 8564.227386713028
total 24576.0MB, used 3325.06MB, free 21250.94MB
Round 0 finish, update the prev_syn_proto
torch.Size([101, 3, 32, 32])
torch.Size([101, 3, 32, 32])
torch.Size([104, 3, 32, 32])
torch.Size([100, 3, 32, 32])
torch.Size([100, 3, 32, 32])
torch.Size([105, 3, 32, 32])
torch.Size([101, 3, 32, 32])
torch.Size([100, 3, 32, 32])
torch.Size([100, 3, 32, 32])
torch.Size([100, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 479, 1: 84, 2: 55, 3: 19, 4: 30, 5: 5, 6: 38, 7: 8, 8: 214, 9: 68}, 1: {0: 51, 1: 501, 2: 51, 3: 3, 4: 19, 5: 2, 6: 37, 7: 22, 8: 178, 9: 136}, 2: {0: 108, 1: 36, 2: 304, 3: 77, 4: 109, 5: 32, 6: 188, 7: 32, 8: 47, 9: 67}, 3: {0: 48, 1: 70, 2: 85, 3: 201, 4: 46, 5: 109, 6: 255, 7: 41, 8: 35, 9: 110}, 4: {0: 80, 1: 21, 2: 191, 3: 36, 4: 192, 5: 23, 6: 298, 7: 62, 8: 41, 9: 56}, 5: {0: 35, 1: 31, 2: 132, 3: 197, 4: 69, 5: 179, 6: 213, 7: 51, 8: 34, 9: 59}, 6: {0: 24, 1: 26, 2: 121, 3: 49, 4: 54, 5: 12, 6: 619, 7: 19, 8: 21, 9: 55}, 7: {0: 53, 1: 33, 2: 84, 3: 36, 4: 77, 5: 74, 6: 170, 7: 355, 8: 17, 9: 101}, 8: {0: 214, 1: 78, 2: 26, 3: 9, 4: 4, 5: 0, 6: 20, 7: 1, 8: 571, 9: 77}, 9: {0: 84, 1: 251, 2: 46, 3: 12, 4: 14, 5: 2, 6: 48, 7: 21, 8: 205, 9: 317}}
round 0 evaluation: test acc is 0.3718, test loss = 5.008571
 ====== round 1 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3325.06MB, free 21250.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [ 670  203  149  119  156  132  156  174  300 1534], bin edged: [0.00018711 0.00020163 0.00021615 0.00023068 0.0002452  0.00025972
 0.00027424 0.00028877 0.00030329 0.00031781 0.00033233]
class 7 have 4999 samples, histogram: [1123  280  236  189  180  190  221  265  428 1887], bin edged: [0.00013742 0.00014809 0.00015875 0.00016942 0.00018009 0.00019075
 0.00020142 0.00021208 0.00022275 0.00023341 0.00024408]
client 0, data condensation 0, total loss = 269.795654296875, avg loss = 134.8978271484375
client 0, data condensation 200, total loss = 13.762847900390625, avg loss = 6.8814239501953125
client 0, data condensation 400, total loss = 14.36639404296875, avg loss = 7.183197021484375
client 0, data condensation 600, total loss = 6.518157958984375, avg loss = 3.2590789794921875
client 0, data condensation 800, total loss = 4.3345947265625, avg loss = 2.16729736328125
client 0, data condensation 1000, total loss = 5.290863037109375, avg loss = 2.6454315185546875
client 0, data condensation 1200, total loss = 6.55780029296875, avg loss = 3.278900146484375
client 0, data condensation 1400, total loss = 3.186492919921875, avg loss = 1.5932464599609375
client 0, data condensation 1600, total loss = 13.843109130859375, avg loss = 6.9215545654296875
client 0, data condensation 1800, total loss = 5.43634033203125, avg loss = 2.718170166015625
client 0, data condensation 2000, total loss = 7.278472900390625, avg loss = 3.6392364501953125
client 0, data condensation 2200, total loss = 8.92706298828125, avg loss = 4.463531494140625
client 0, data condensation 2400, total loss = 8.555572509765625, avg loss = 4.2777862548828125
client 0, data condensation 2600, total loss = 3.87103271484375, avg loss = 1.935516357421875
client 0, data condensation 2800, total loss = 19.3265380859375, avg loss = 9.66326904296875
client 0, data condensation 3000, total loss = 3.3072509765625, avg loss = 1.65362548828125
client 0, data condensation 3200, total loss = 14.019439697265625, avg loss = 7.0097198486328125
client 0, data condensation 3400, total loss = 6.51751708984375, avg loss = 3.258758544921875
client 0, data condensation 3600, total loss = 4.924346923828125, avg loss = 2.4621734619140625
client 0, data condensation 3800, total loss = 3.25750732421875, avg loss = 1.628753662109375
client 0, data condensation 4000, total loss = 17.0849609375, avg loss = 8.54248046875
client 0, data condensation 4200, total loss = 13.92010498046875, avg loss = 6.960052490234375
client 0, data condensation 4400, total loss = 34.4820556640625, avg loss = 17.24102783203125
client 0, data condensation 4600, total loss = 6.95831298828125, avg loss = 3.479156494140625
client 0, data condensation 4800, total loss = 22.40045166015625, avg loss = 11.200225830078125
client 0, data condensation 5000, total loss = 2.970733642578125, avg loss = 1.4853668212890625
client 0, data condensation 5200, total loss = 4.163116455078125, avg loss = 2.0815582275390625
client 0, data condensation 5400, total loss = 3.86474609375, avg loss = 1.932373046875
client 0, data condensation 5600, total loss = 3.26715087890625, avg loss = 1.633575439453125
client 0, data condensation 5800, total loss = 5.216552734375, avg loss = 2.6082763671875
client 0, data condensation 6000, total loss = 7.203277587890625, avg loss = 3.6016387939453125
client 0, data condensation 6200, total loss = 5.350372314453125, avg loss = 2.6751861572265625
client 0, data condensation 6400, total loss = 3.7027587890625, avg loss = 1.85137939453125
client 0, data condensation 6600, total loss = 2.5430908203125, avg loss = 1.27154541015625
client 0, data condensation 6800, total loss = 5.3333740234375, avg loss = 2.66668701171875
client 0, data condensation 7000, total loss = 4.476531982421875, avg loss = 2.2382659912109375
client 0, data condensation 7200, total loss = 8.07568359375, avg loss = 4.037841796875
client 0, data condensation 7400, total loss = 15.22802734375, avg loss = 7.614013671875
client 0, data condensation 7600, total loss = 3.5789794921875, avg loss = 1.78948974609375
client 0, data condensation 7800, total loss = 7.730316162109375, avg loss = 3.8651580810546875
client 0, data condensation 8000, total loss = 10.390594482421875, avg loss = 5.1952972412109375
client 0, data condensation 8200, total loss = 3.871826171875, avg loss = 1.9359130859375
client 0, data condensation 8400, total loss = 4.38677978515625, avg loss = 2.193389892578125
client 0, data condensation 8600, total loss = 4.846710205078125, avg loss = 2.4233551025390625
client 0, data condensation 8800, total loss = 3.31634521484375, avg loss = 1.658172607421875
client 0, data condensation 9000, total loss = 2.6875, avg loss = 1.34375
client 0, data condensation 9200, total loss = 2.761505126953125, avg loss = 1.3807525634765625
client 0, data condensation 9400, total loss = 5.34375, avg loss = 2.671875
client 0, data condensation 9600, total loss = 5.828155517578125, avg loss = 2.9140777587890625
client 0, data condensation 9800, total loss = 5.06964111328125, avg loss = 2.534820556640625
client 0, data condensation 10000, total loss = 7.068939208984375, avg loss = 3.5344696044921875
Round 1, client 0 condense time: 890.2338485717773
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 3201.06MB, free 21374.94MB
total 24576.0MB, used 3201.06MB, free 21374.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [38  8  5  7  7  7  9  6  9 79], bin edged: [0.00386212 0.00416186 0.0044616  0.00476134 0.00506107 0.00536081
 0.00566055 0.00596029 0.00626003 0.00655977 0.00685951]
class 4 have 4958 samples, histogram: [ 567  255  191  184  201  185  241  321  525 2288], bin edged: [0.0001311  0.00014128 0.00015145 0.00016162 0.0001718  0.00018197
 0.00019214 0.00020232 0.00021249 0.00022267 0.00023284]
client 1, data condensation 0, total loss = 286.5753479003906, avg loss = 143.2876739501953
client 1, data condensation 200, total loss = 24.427001953125, avg loss = 12.2135009765625
client 1, data condensation 400, total loss = 12.3321533203125, avg loss = 6.16607666015625
client 1, data condensation 600, total loss = 25.173492431640625, avg loss = 12.586746215820312
client 1, data condensation 800, total loss = 15.722930908203125, avg loss = 7.8614654541015625
client 1, data condensation 1000, total loss = 13.82122802734375, avg loss = 6.910614013671875
client 1, data condensation 1200, total loss = 35.897735595703125, avg loss = 17.948867797851562
client 1, data condensation 1400, total loss = 13.331512451171875, avg loss = 6.6657562255859375
client 1, data condensation 1600, total loss = 10.4949951171875, avg loss = 5.24749755859375
client 1, data condensation 1800, total loss = 28.237884521484375, avg loss = 14.118942260742188
client 1, data condensation 2000, total loss = 16.0772705078125, avg loss = 8.03863525390625
client 1, data condensation 2200, total loss = 13.429534912109375, avg loss = 6.7147674560546875
client 1, data condensation 2400, total loss = 16.080596923828125, avg loss = 8.040298461914062
client 1, data condensation 2600, total loss = 9.1768798828125, avg loss = 4.58843994140625
client 1, data condensation 2800, total loss = 29.158447265625, avg loss = 14.5792236328125
client 1, data condensation 3000, total loss = 7.575836181640625, avg loss = 3.7879180908203125
client 1, data condensation 3200, total loss = 14.84503173828125, avg loss = 7.422515869140625
client 1, data condensation 3400, total loss = 39.908905029296875, avg loss = 19.954452514648438
client 1, data condensation 3600, total loss = 20.140533447265625, avg loss = 10.070266723632812
client 1, data condensation 3800, total loss = 13.3082275390625, avg loss = 6.65411376953125
client 1, data condensation 4000, total loss = 12.86181640625, avg loss = 6.430908203125
client 1, data condensation 4200, total loss = 19.00469970703125, avg loss = 9.502349853515625
client 1, data condensation 4400, total loss = 20.687255859375, avg loss = 10.3436279296875
client 1, data condensation 4600, total loss = 8.296661376953125, avg loss = 4.1483306884765625
client 1, data condensation 4800, total loss = 10.4669189453125, avg loss = 5.23345947265625
client 1, data condensation 5000, total loss = 7.332000732421875, avg loss = 3.6660003662109375
client 1, data condensation 5200, total loss = 21.3616943359375, avg loss = 10.68084716796875
client 1, data condensation 5400, total loss = 9.2974853515625, avg loss = 4.64874267578125
client 1, data condensation 5600, total loss = 17.968505859375, avg loss = 8.9842529296875
client 1, data condensation 5800, total loss = 34.358245849609375, avg loss = 17.179122924804688
client 1, data condensation 6000, total loss = 7.9744873046875, avg loss = 3.98724365234375
client 1, data condensation 6200, total loss = 12.70709228515625, avg loss = 6.353546142578125
client 1, data condensation 6400, total loss = 13.4384765625, avg loss = 6.71923828125
client 1, data condensation 6600, total loss = 15.74359130859375, avg loss = 7.871795654296875
client 1, data condensation 6800, total loss = 9.092803955078125, avg loss = 4.5464019775390625
client 1, data condensation 7000, total loss = 19.194061279296875, avg loss = 9.597030639648438
client 1, data condensation 7200, total loss = 11.056915283203125, avg loss = 5.5284576416015625
client 1, data condensation 7400, total loss = 11.87969970703125, avg loss = 5.939849853515625
client 1, data condensation 7600, total loss = 7.32037353515625, avg loss = 3.660186767578125
client 1, data condensation 7800, total loss = 17.331817626953125, avg loss = 8.665908813476562
client 1, data condensation 8000, total loss = 9.399993896484375, avg loss = 4.6999969482421875
client 1, data condensation 8200, total loss = 5.95703125, avg loss = 2.978515625
client 1, data condensation 8400, total loss = 14.287445068359375, avg loss = 7.1437225341796875
client 1, data condensation 8600, total loss = 25.48651123046875, avg loss = 12.743255615234375
client 1, data condensation 8800, total loss = 14.13519287109375, avg loss = 7.067596435546875
client 1, data condensation 9000, total loss = 12.070526123046875, avg loss = 6.0352630615234375
client 1, data condensation 9200, total loss = 13.6312255859375, avg loss = 6.81561279296875
client 1, data condensation 9400, total loss = 19.74896240234375, avg loss = 9.874481201171875
client 1, data condensation 9600, total loss = 14.704315185546875, avg loss = 7.3521575927734375
client 1, data condensation 9800, total loss = 9.853302001953125, avg loss = 4.9266510009765625
client 1, data condensation 10000, total loss = 11.420074462890625, avg loss = 5.7100372314453125
Round 1, client 1 condense time: 844.4866058826447
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 3201.06MB, free 21374.94MB
total 24576.0MB, used 3201.06MB, free 21374.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [46 21 10 15 19 15 11 22 27 56], bin edged: [0.00291516 0.00314129 0.00336743 0.00359357 0.00381971 0.00404584
 0.00427198 0.00449812 0.00472425 0.00495039 0.00517653]
client 2, data condensation 0, total loss = 180.65496826171875, avg loss = 180.65496826171875
client 2, data condensation 200, total loss = 19.496124267578125, avg loss = 19.496124267578125
client 2, data condensation 400, total loss = 20.680450439453125, avg loss = 20.680450439453125
client 2, data condensation 600, total loss = 22.1243896484375, avg loss = 22.1243896484375
client 2, data condensation 800, total loss = 9.010162353515625, avg loss = 9.010162353515625
client 2, data condensation 1000, total loss = 32.87225341796875, avg loss = 32.87225341796875
client 2, data condensation 1200, total loss = 26.03277587890625, avg loss = 26.03277587890625
client 2, data condensation 1400, total loss = 12.1396484375, avg loss = 12.1396484375
client 2, data condensation 1600, total loss = 19.52301025390625, avg loss = 19.52301025390625
client 2, data condensation 1800, total loss = 18.362213134765625, avg loss = 18.362213134765625
client 2, data condensation 2000, total loss = 18.030059814453125, avg loss = 18.030059814453125
client 2, data condensation 2200, total loss = 9.579864501953125, avg loss = 9.579864501953125
client 2, data condensation 2400, total loss = 4.696380615234375, avg loss = 4.696380615234375
client 2, data condensation 2600, total loss = 52.612823486328125, avg loss = 52.612823486328125
client 2, data condensation 2800, total loss = 11.370025634765625, avg loss = 11.370025634765625
client 2, data condensation 3000, total loss = 9.02532958984375, avg loss = 9.02532958984375
client 2, data condensation 3200, total loss = 17.77862548828125, avg loss = 17.77862548828125
client 2, data condensation 3400, total loss = 7.579498291015625, avg loss = 7.579498291015625
client 2, data condensation 3600, total loss = 21.763580322265625, avg loss = 21.763580322265625
client 2, data condensation 3800, total loss = 22.2056884765625, avg loss = 22.2056884765625
client 2, data condensation 4000, total loss = 21.766143798828125, avg loss = 21.766143798828125
client 2, data condensation 4200, total loss = 12.489990234375, avg loss = 12.489990234375
client 2, data condensation 4400, total loss = 8.139434814453125, avg loss = 8.139434814453125
client 2, data condensation 4600, total loss = 44.10162353515625, avg loss = 44.10162353515625
client 2, data condensation 4800, total loss = 48.11993408203125, avg loss = 48.11993408203125
client 2, data condensation 5000, total loss = 14.97711181640625, avg loss = 14.97711181640625
client 2, data condensation 5200, total loss = 7.00140380859375, avg loss = 7.00140380859375
client 2, data condensation 5400, total loss = 16.27044677734375, avg loss = 16.27044677734375
client 2, data condensation 5600, total loss = 23.1502685546875, avg loss = 23.1502685546875
client 2, data condensation 5800, total loss = 12.883514404296875, avg loss = 12.883514404296875
client 2, data condensation 6000, total loss = 7.67205810546875, avg loss = 7.67205810546875
client 2, data condensation 6200, total loss = 8.37493896484375, avg loss = 8.37493896484375
client 2, data condensation 6400, total loss = 6.37872314453125, avg loss = 6.37872314453125
client 2, data condensation 6600, total loss = 14.7401123046875, avg loss = 14.7401123046875
client 2, data condensation 6800, total loss = 22.671051025390625, avg loss = 22.671051025390625
client 2, data condensation 7000, total loss = 39.56280517578125, avg loss = 39.56280517578125
client 2, data condensation 7200, total loss = 12.89410400390625, avg loss = 12.89410400390625
client 2, data condensation 7400, total loss = 21.27642822265625, avg loss = 21.27642822265625
client 2, data condensation 7600, total loss = 15.648040771484375, avg loss = 15.648040771484375
client 2, data condensation 7800, total loss = 8.28436279296875, avg loss = 8.28436279296875
client 2, data condensation 8000, total loss = 34.955596923828125, avg loss = 34.955596923828125
client 2, data condensation 8200, total loss = 9.568328857421875, avg loss = 9.568328857421875
client 2, data condensation 8400, total loss = 10.288116455078125, avg loss = 10.288116455078125
client 2, data condensation 8600, total loss = 11.590667724609375, avg loss = 11.590667724609375
client 2, data condensation 8800, total loss = 16.699432373046875, avg loss = 16.699432373046875
client 2, data condensation 9000, total loss = 14.400787353515625, avg loss = 14.400787353515625
client 2, data condensation 9200, total loss = 5.70904541015625, avg loss = 5.70904541015625
client 2, data condensation 9400, total loss = 15.36669921875, avg loss = 15.36669921875
client 2, data condensation 9600, total loss = 14.095245361328125, avg loss = 14.095245361328125
client 2, data condensation 9800, total loss = 10.217376708984375, avg loss = 10.217376708984375
client 2, data condensation 10000, total loss = 14.211639404296875, avg loss = 14.211639404296875
Round 1, client 2 condense time: 314.8341438770294
client 2, class 9 have 242 samples
total 24576.0MB, used 2813.06MB, free 21762.94MB
total 24576.0MB, used 2813.06MB, free 21762.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [255  56  48  50  44  38  32  50  71 203], bin edged: [0.00086278 0.00092974 0.00099671 0.00106367 0.00113063 0.00119759
 0.00126456 0.00133152 0.00139848 0.00146544 0.00153241]
class 2 have 1094 samples, histogram: [205  57  42  29  37  46  54  44  80 500], bin edged: [0.00060986 0.00065719 0.00070452 0.00075186 0.00079919 0.00084652
 0.00089386 0.00094119 0.00098852 0.00103586 0.00108319]
client 3, data condensation 0, total loss = 139.2894287109375, avg loss = 69.64471435546875
client 3, data condensation 200, total loss = 10.4134521484375, avg loss = 5.20672607421875
client 3, data condensation 400, total loss = 8.284210205078125, avg loss = 4.1421051025390625
client 3, data condensation 600, total loss = 15.794158935546875, avg loss = 7.8970794677734375
client 3, data condensation 800, total loss = 6.748870849609375, avg loss = 3.3744354248046875
client 3, data condensation 1000, total loss = 7.71405029296875, avg loss = 3.857025146484375
client 3, data condensation 1200, total loss = 8.4822998046875, avg loss = 4.24114990234375
client 3, data condensation 1400, total loss = 3.9774169921875, avg loss = 1.98870849609375
client 3, data condensation 1600, total loss = 19.353179931640625, avg loss = 9.676589965820312
client 3, data condensation 1800, total loss = 6.034210205078125, avg loss = 3.0171051025390625
client 3, data condensation 2000, total loss = 3.735382080078125, avg loss = 1.8676910400390625
client 3, data condensation 2200, total loss = 2.658294677734375, avg loss = 1.3291473388671875
client 3, data condensation 2400, total loss = 36.811431884765625, avg loss = 18.405715942382812
client 3, data condensation 2600, total loss = 9.363006591796875, avg loss = 4.6815032958984375
client 3, data condensation 2800, total loss = 4.916412353515625, avg loss = 2.4582061767578125
client 3, data condensation 3000, total loss = 4.4267578125, avg loss = 2.21337890625
client 3, data condensation 3200, total loss = 7.858856201171875, avg loss = 3.9294281005859375
client 3, data condensation 3400, total loss = 7.260650634765625, avg loss = 3.6303253173828125
client 3, data condensation 3600, total loss = 8.81982421875, avg loss = 4.409912109375
client 3, data condensation 3800, total loss = 4.366241455078125, avg loss = 2.1831207275390625
client 3, data condensation 4000, total loss = 8.40875244140625, avg loss = 4.204376220703125
client 3, data condensation 4200, total loss = 5.939239501953125, avg loss = 2.9696197509765625
client 3, data condensation 4400, total loss = 3.407989501953125, avg loss = 1.7039947509765625
client 3, data condensation 4600, total loss = 7.99493408203125, avg loss = 3.997467041015625
client 3, data condensation 4800, total loss = 5.191680908203125, avg loss = 2.5958404541015625
client 3, data condensation 5000, total loss = 5.54107666015625, avg loss = 2.770538330078125
client 3, data condensation 5200, total loss = 4.780426025390625, avg loss = 2.3902130126953125
client 3, data condensation 5400, total loss = 55.63818359375, avg loss = 27.819091796875
client 3, data condensation 5600, total loss = 7.904022216796875, avg loss = 3.9520111083984375
client 3, data condensation 5800, total loss = 8.004913330078125, avg loss = 4.0024566650390625
client 3, data condensation 6000, total loss = 10.72540283203125, avg loss = 5.362701416015625
client 3, data condensation 6200, total loss = 3.501190185546875, avg loss = 1.7505950927734375
client 3, data condensation 6400, total loss = 58.117034912109375, avg loss = 29.058517456054688
client 3, data condensation 6600, total loss = 4.279815673828125, avg loss = 2.1399078369140625
client 3, data condensation 6800, total loss = 3.908050537109375, avg loss = 1.9540252685546875
client 3, data condensation 7000, total loss = 17.7669677734375, avg loss = 8.88348388671875
client 3, data condensation 7200, total loss = 2.914520263671875, avg loss = 1.4572601318359375
client 3, data condensation 7400, total loss = 7.60882568359375, avg loss = 3.804412841796875
client 3, data condensation 7600, total loss = 8.629638671875, avg loss = 4.3148193359375
client 3, data condensation 7800, total loss = 11.111175537109375, avg loss = 5.5555877685546875
client 3, data condensation 8000, total loss = 29.24658203125, avg loss = 14.623291015625
client 3, data condensation 8200, total loss = 5.831634521484375, avg loss = 2.9158172607421875
client 3, data condensation 8400, total loss = 2.799713134765625, avg loss = 1.3998565673828125
client 3, data condensation 8600, total loss = 17.197265625, avg loss = 8.5986328125
client 3, data condensation 8800, total loss = 11.724334716796875, avg loss = 5.8621673583984375
client 3, data condensation 9000, total loss = 8.802703857421875, avg loss = 4.4013519287109375
client 3, data condensation 9200, total loss = 3.798797607421875, avg loss = 1.8993988037109375
client 3, data condensation 9400, total loss = 4.72442626953125, avg loss = 2.362213134765625
client 3, data condensation 9600, total loss = 3.691741943359375, avg loss = 1.8458709716796875
client 3, data condensation 9800, total loss = 8.77490234375, avg loss = 4.387451171875
client 3, data condensation 10000, total loss = 2.6279296875, avg loss = 1.31396484375
Round 1, client 3 condense time: 662.2473568916321
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 3197.06MB, free 21378.94MB
total 24576.0MB, used 3197.06MB, free 21378.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [1400  398  259  224  173  171  184  199  314  830], bin edged: [0.00018141 0.00019549 0.00020957 0.00022365 0.00023773 0.00025181
 0.00026589 0.00027996 0.00029404 0.00030812 0.0003222 ]
class 5 have 307 samples, histogram: [ 32  16  16  15  10  17  16  13  35 137], bin edged: [0.00212527 0.00228972 0.00245418 0.00261863 0.00278309 0.00294754
 0.003112   0.00327645 0.00344091 0.00360536 0.00376982]
client 4, data condensation 0, total loss = 157.32171630859375, avg loss = 78.66085815429688
client 4, data condensation 200, total loss = 11.670806884765625, avg loss = 5.8354034423828125
client 4, data condensation 400, total loss = 16.53759765625, avg loss = 8.268798828125
client 4, data condensation 600, total loss = 19.328094482421875, avg loss = 9.664047241210938
client 4, data condensation 800, total loss = 18.366119384765625, avg loss = 9.183059692382812
client 4, data condensation 1000, total loss = 16.344268798828125, avg loss = 8.172134399414062
client 4, data condensation 1200, total loss = 6.623992919921875, avg loss = 3.3119964599609375
client 4, data condensation 1400, total loss = 12.58709716796875, avg loss = 6.293548583984375
client 4, data condensation 1600, total loss = 12.3182373046875, avg loss = 6.15911865234375
client 4, data condensation 1800, total loss = 10.566925048828125, avg loss = 5.2834625244140625
client 4, data condensation 2000, total loss = 8.592620849609375, avg loss = 4.2963104248046875
client 4, data condensation 2200, total loss = 12.38616943359375, avg loss = 6.193084716796875
client 4, data condensation 2400, total loss = 26.36004638671875, avg loss = 13.180023193359375
client 4, data condensation 2600, total loss = 34.46392822265625, avg loss = 17.231964111328125
client 4, data condensation 2800, total loss = 15.9144287109375, avg loss = 7.95721435546875
client 4, data condensation 3000, total loss = 5.384765625, avg loss = 2.6923828125
client 4, data condensation 3200, total loss = 23.958282470703125, avg loss = 11.979141235351562
client 4, data condensation 3400, total loss = 18.3125, avg loss = 9.15625
client 4, data condensation 3600, total loss = 14.8831787109375, avg loss = 7.44158935546875
client 4, data condensation 3800, total loss = 17.3282470703125, avg loss = 8.66412353515625
client 4, data condensation 4000, total loss = 13.244873046875, avg loss = 6.6224365234375
client 4, data condensation 4200, total loss = 8.506805419921875, avg loss = 4.2534027099609375
client 4, data condensation 4400, total loss = 10.178466796875, avg loss = 5.0892333984375
client 4, data condensation 4600, total loss = 8.32086181640625, avg loss = 4.160430908203125
client 4, data condensation 4800, total loss = 22.68963623046875, avg loss = 11.344818115234375
client 4, data condensation 5000, total loss = 30.868072509765625, avg loss = 15.434036254882812
client 4, data condensation 5200, total loss = 8.187957763671875, avg loss = 4.0939788818359375
client 4, data condensation 5400, total loss = 8.960205078125, avg loss = 4.4801025390625
client 4, data condensation 5600, total loss = 14.75079345703125, avg loss = 7.375396728515625
client 4, data condensation 5800, total loss = 20.56396484375, avg loss = 10.281982421875
client 4, data condensation 6000, total loss = 8.605438232421875, avg loss = 4.3027191162109375
client 4, data condensation 6200, total loss = 33.378265380859375, avg loss = 16.689132690429688
client 4, data condensation 6400, total loss = 27.87109375, avg loss = 13.935546875
client 4, data condensation 6600, total loss = 4.239471435546875, avg loss = 2.1197357177734375
client 4, data condensation 6800, total loss = 21.109161376953125, avg loss = 10.554580688476562
client 4, data condensation 7000, total loss = 14.700927734375, avg loss = 7.3504638671875
client 4, data condensation 7200, total loss = 10.373748779296875, avg loss = 5.1868743896484375
client 4, data condensation 7400, total loss = 13.871673583984375, avg loss = 6.9358367919921875
client 4, data condensation 7600, total loss = 7.92303466796875, avg loss = 3.961517333984375
client 4, data condensation 7800, total loss = 13.41949462890625, avg loss = 6.709747314453125
client 4, data condensation 8000, total loss = 21.56109619140625, avg loss = 10.780548095703125
client 4, data condensation 8200, total loss = 15.33660888671875, avg loss = 7.668304443359375
client 4, data condensation 8400, total loss = 5.81689453125, avg loss = 2.908447265625
client 4, data condensation 8600, total loss = 10.93804931640625, avg loss = 5.469024658203125
client 4, data condensation 8800, total loss = 91.1556396484375, avg loss = 45.57781982421875
client 4, data condensation 9000, total loss = 9.396148681640625, avg loss = 4.6980743408203125
client 4, data condensation 9200, total loss = 7.0760498046875, avg loss = 3.53802490234375
client 4, data condensation 9400, total loss = 15.26861572265625, avg loss = 7.634307861328125
client 4, data condensation 9600, total loss = 4.63165283203125, avg loss = 2.315826416015625
client 4, data condensation 9800, total loss = 8.56396484375, avg loss = 4.281982421875
client 4, data condensation 10000, total loss = 7.354095458984375, avg loss = 3.6770477294921875
Round 1, client 4 condense time: 714.1159093379974
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 3201.06MB, free 21374.94MB
total 24576.0MB, used 3201.06MB, free 21374.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [1899  580  374  266  257  224  198  225  273  703], bin edged: [0.0001567  0.00016886 0.00018102 0.00019318 0.00020534 0.0002175
 0.00022966 0.00024182 0.00025398 0.00026614 0.0002783 ]
client 5, data condensation 0, total loss = 152.62435913085938, avg loss = 152.62435913085938
client 5, data condensation 200, total loss = 24.64239501953125, avg loss = 24.64239501953125
client 5, data condensation 400, total loss = 6.66900634765625, avg loss = 6.66900634765625
client 5, data condensation 600, total loss = 3.3995361328125, avg loss = 3.3995361328125
client 5, data condensation 800, total loss = 5.17083740234375, avg loss = 5.17083740234375
client 5, data condensation 1000, total loss = 11.4266357421875, avg loss = 11.4266357421875
client 5, data condensation 1200, total loss = 2.996978759765625, avg loss = 2.996978759765625
client 5, data condensation 1400, total loss = 5.11553955078125, avg loss = 5.11553955078125
client 5, data condensation 1600, total loss = 2.9403076171875, avg loss = 2.9403076171875
client 5, data condensation 1800, total loss = 10.15203857421875, avg loss = 10.15203857421875
client 5, data condensation 2000, total loss = 3.653961181640625, avg loss = 3.653961181640625
client 5, data condensation 2200, total loss = 6.484283447265625, avg loss = 6.484283447265625
client 5, data condensation 2400, total loss = 109.0294189453125, avg loss = 109.0294189453125
client 5, data condensation 2600, total loss = 7.37322998046875, avg loss = 7.37322998046875
client 5, data condensation 2800, total loss = 9.388458251953125, avg loss = 9.388458251953125
client 5, data condensation 3000, total loss = 2.3529052734375, avg loss = 2.3529052734375
client 5, data condensation 3200, total loss = 30.41143798828125, avg loss = 30.41143798828125
client 5, data condensation 3400, total loss = 3.233428955078125, avg loss = 3.233428955078125
client 5, data condensation 3600, total loss = 3.30841064453125, avg loss = 3.30841064453125
client 5, data condensation 3800, total loss = 3.496337890625, avg loss = 3.496337890625
client 5, data condensation 4000, total loss = 15.04046630859375, avg loss = 15.04046630859375
client 5, data condensation 4200, total loss = 7.964996337890625, avg loss = 7.964996337890625
client 5, data condensation 4400, total loss = 1.22119140625, avg loss = 1.22119140625
client 5, data condensation 4600, total loss = 2.61090087890625, avg loss = 2.61090087890625
client 5, data condensation 4800, total loss = 22.006256103515625, avg loss = 22.006256103515625
client 5, data condensation 5000, total loss = 4.2957763671875, avg loss = 4.2957763671875
client 5, data condensation 5200, total loss = 7.365081787109375, avg loss = 7.365081787109375
client 5, data condensation 5400, total loss = 3.831451416015625, avg loss = 3.831451416015625
client 5, data condensation 5600, total loss = 4.64361572265625, avg loss = 4.64361572265625
client 5, data condensation 5800, total loss = 2.8306884765625, avg loss = 2.8306884765625
client 5, data condensation 6000, total loss = 1.920745849609375, avg loss = 1.920745849609375
client 5, data condensation 6200, total loss = 3.921905517578125, avg loss = 3.921905517578125
client 5, data condensation 6400, total loss = 2.77301025390625, avg loss = 2.77301025390625
client 5, data condensation 6600, total loss = 8.91851806640625, avg loss = 8.91851806640625
client 5, data condensation 6800, total loss = 3.274017333984375, avg loss = 3.274017333984375
client 5, data condensation 7000, total loss = 2.56011962890625, avg loss = 2.56011962890625
client 5, data condensation 7200, total loss = 3.965911865234375, avg loss = 3.965911865234375
client 5, data condensation 7400, total loss = 3.230133056640625, avg loss = 3.230133056640625
client 5, data condensation 7600, total loss = 3.197021484375, avg loss = 3.197021484375
client 5, data condensation 7800, total loss = 14.78070068359375, avg loss = 14.78070068359375
client 5, data condensation 8000, total loss = 4.351226806640625, avg loss = 4.351226806640625
client 5, data condensation 8200, total loss = 4.11676025390625, avg loss = 4.11676025390625
client 5, data condensation 8400, total loss = 2.7830810546875, avg loss = 2.7830810546875
client 5, data condensation 8600, total loss = 7.452484130859375, avg loss = 7.452484130859375
client 5, data condensation 8800, total loss = 5.911102294921875, avg loss = 5.911102294921875
client 5, data condensation 9000, total loss = 4.13250732421875, avg loss = 4.13250732421875
client 5, data condensation 9200, total loss = 3.05181884765625, avg loss = 3.05181884765625
client 5, data condensation 9400, total loss = 8.87255859375, avg loss = 8.87255859375
client 5, data condensation 9600, total loss = 4.11517333984375, avg loss = 4.11517333984375
client 5, data condensation 9800, total loss = 8.621551513671875, avg loss = 8.621551513671875
client 5, data condensation 10000, total loss = 4.368316650390625, avg loss = 4.368316650390625
Round 1, client 5 condense time: 461.2886574268341
client 5, class 8 have 4999 samples
total 24576.0MB, used 2817.06MB, free 21758.94MB
total 24576.0MB, used 2817.06MB, free 21758.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [ 340  193  184  156  174  197  219  262  387 2253], bin edged: [0.00014589 0.0001572  0.00016852 0.00017983 0.00019115 0.00020246
 0.00021378 0.00022509 0.00023641 0.00024772 0.00025904]
class 6 have 3914 samples, histogram: [1899  334  151  135  137  122  104  129  177  726], bin edged: [0.0002024  0.00021811 0.00023381 0.00024952 0.00026523 0.00028094
 0.00029665 0.00031236 0.00032807 0.00034378 0.00035948]
client 6, data condensation 0, total loss = 260.559814453125, avg loss = 130.2799072265625
client 6, data condensation 200, total loss = 13.51025390625, avg loss = 6.755126953125
client 6, data condensation 400, total loss = 6.471923828125, avg loss = 3.2359619140625
client 6, data condensation 600, total loss = 14.8214111328125, avg loss = 7.41070556640625
client 6, data condensation 800, total loss = 16.0301513671875, avg loss = 8.01507568359375
client 6, data condensation 1000, total loss = 3.783660888671875, avg loss = 1.8918304443359375
client 6, data condensation 1200, total loss = 22.479034423828125, avg loss = 11.239517211914062
client 6, data condensation 1400, total loss = 14.038360595703125, avg loss = 7.0191802978515625
client 6, data condensation 1600, total loss = 5.40924072265625, avg loss = 2.704620361328125
client 6, data condensation 1800, total loss = 5.963226318359375, avg loss = 2.9816131591796875
client 6, data condensation 2000, total loss = 3.280517578125, avg loss = 1.6402587890625
client 6, data condensation 2200, total loss = 3.50921630859375, avg loss = 1.754608154296875
client 6, data condensation 2400, total loss = 16.072265625, avg loss = 8.0361328125
client 6, data condensation 2600, total loss = 11.93109130859375, avg loss = 5.965545654296875
client 6, data condensation 2800, total loss = 22.28326416015625, avg loss = 11.141632080078125
client 6, data condensation 3000, total loss = 8.464996337890625, avg loss = 4.2324981689453125
client 6, data condensation 3200, total loss = 3.071136474609375, avg loss = 1.5355682373046875
client 6, data condensation 3400, total loss = 4.217041015625, avg loss = 2.1085205078125
client 6, data condensation 3600, total loss = 21.377655029296875, avg loss = 10.688827514648438
client 6, data condensation 3800, total loss = 12.3148193359375, avg loss = 6.15740966796875
client 6, data condensation 4000, total loss = 9.41168212890625, avg loss = 4.705841064453125
client 6, data condensation 4200, total loss = 4.679779052734375, avg loss = 2.3398895263671875
client 6, data condensation 4400, total loss = 3.877838134765625, avg loss = 1.9389190673828125
client 6, data condensation 4600, total loss = 2.50738525390625, avg loss = 1.253692626953125
client 6, data condensation 4800, total loss = 6.7490234375, avg loss = 3.37451171875
client 6, data condensation 5000, total loss = 10.86578369140625, avg loss = 5.432891845703125
client 6, data condensation 5200, total loss = 8.307159423828125, avg loss = 4.1535797119140625
client 6, data condensation 5400, total loss = 27.91839599609375, avg loss = 13.959197998046875
client 6, data condensation 5600, total loss = 4.79974365234375, avg loss = 2.399871826171875
client 6, data condensation 5800, total loss = 14.202606201171875, avg loss = 7.1013031005859375
client 6, data condensation 6000, total loss = 5.70074462890625, avg loss = 2.850372314453125
client 6, data condensation 6200, total loss = 10.91082763671875, avg loss = 5.455413818359375
client 6, data condensation 6400, total loss = 6.024871826171875, avg loss = 3.0124359130859375
client 6, data condensation 6600, total loss = 12.845916748046875, avg loss = 6.4229583740234375
client 6, data condensation 6800, total loss = 3.96441650390625, avg loss = 1.982208251953125
client 6, data condensation 7000, total loss = 4.2127685546875, avg loss = 2.10638427734375
client 6, data condensation 7200, total loss = 10.342926025390625, avg loss = 5.1714630126953125
client 6, data condensation 7400, total loss = 2.721923828125, avg loss = 1.3609619140625
client 6, data condensation 7600, total loss = 4.171051025390625, avg loss = 2.0855255126953125
client 6, data condensation 7800, total loss = 7.628265380859375, avg loss = 3.8141326904296875
client 6, data condensation 8000, total loss = 4.912872314453125, avg loss = 2.4564361572265625
client 6, data condensation 8200, total loss = 6.70269775390625, avg loss = 3.351348876953125
client 6, data condensation 8400, total loss = 32.592041015625, avg loss = 16.2960205078125
client 6, data condensation 8600, total loss = 16.759521484375, avg loss = 8.3797607421875
client 6, data condensation 8800, total loss = 3.749542236328125, avg loss = 1.8747711181640625
client 6, data condensation 9000, total loss = 5.271881103515625, avg loss = 2.6359405517578125
client 6, data condensation 9200, total loss = 2.39007568359375, avg loss = 1.195037841796875
client 6, data condensation 9400, total loss = 14.779541015625, avg loss = 7.3897705078125
client 6, data condensation 9600, total loss = 3.395050048828125, avg loss = 1.6975250244140625
client 6, data condensation 9800, total loss = 7.854278564453125, avg loss = 3.9271392822265625
client 6, data condensation 10000, total loss = 4.99530029296875, avg loss = 2.497650146484375
Round 1, client 6 condense time: 819.5112760066986
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 3201.06MB, free 21374.94MB
total 24576.0MB, used 3201.06MB, free 21374.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [1558  550  362  242  240  198  185  193  280  797], bin edged: [0.00016654 0.00017946 0.00019239 0.00020531 0.00021824 0.00023116
 0.00024409 0.00025701 0.00026994 0.00028286 0.00029579]
class 3 have 4999 samples, histogram: [ 383  264  234  217  211  244  275  379  493 2299], bin edged: [0.00012902 0.00013903 0.00014903 0.00015903 0.00016903 0.00017904
 0.00018904 0.00019904 0.00020904 0.00021904 0.00022905]
client 7, data condensation 0, total loss = 268.9311218261719, avg loss = 134.46556091308594
client 7, data condensation 200, total loss = 53.2860107421875, avg loss = 26.64300537109375
client 7, data condensation 400, total loss = 45.58612060546875, avg loss = 22.793060302734375
client 7, data condensation 600, total loss = 11.562408447265625, avg loss = 5.7812042236328125
client 7, data condensation 800, total loss = 5.6412353515625, avg loss = 2.82061767578125
client 7, data condensation 1000, total loss = 9.232391357421875, avg loss = 4.6161956787109375
client 7, data condensation 1200, total loss = 12.187744140625, avg loss = 6.0938720703125
client 7, data condensation 1400, total loss = 25.045928955078125, avg loss = 12.522964477539062
client 7, data condensation 1600, total loss = 7.9710693359375, avg loss = 3.98553466796875
client 7, data condensation 1800, total loss = 21.65948486328125, avg loss = 10.829742431640625
client 7, data condensation 2000, total loss = 4.5894775390625, avg loss = 2.29473876953125
client 7, data condensation 2200, total loss = 9.4730224609375, avg loss = 4.73651123046875
client 7, data condensation 2400, total loss = 6.267669677734375, avg loss = 3.1338348388671875
client 7, data condensation 2600, total loss = 10.571014404296875, avg loss = 5.2855072021484375
client 7, data condensation 2800, total loss = 8.9530029296875, avg loss = 4.47650146484375
client 7, data condensation 3000, total loss = 5.2420654296875, avg loss = 2.62103271484375
client 7, data condensation 3200, total loss = 18.201904296875, avg loss = 9.1009521484375
client 7, data condensation 3400, total loss = 4.078765869140625, avg loss = 2.0393829345703125
client 7, data condensation 3600, total loss = 6.295745849609375, avg loss = 3.1478729248046875
client 7, data condensation 3800, total loss = 6.230255126953125, avg loss = 3.1151275634765625
client 7, data condensation 4000, total loss = 11.791748046875, avg loss = 5.8958740234375
client 7, data condensation 4200, total loss = 13.040191650390625, avg loss = 6.5200958251953125
client 7, data condensation 4400, total loss = 14.43475341796875, avg loss = 7.217376708984375
client 7, data condensation 4600, total loss = 9.651123046875, avg loss = 4.8255615234375
client 7, data condensation 4800, total loss = 26.59210205078125, avg loss = 13.296051025390625
client 7, data condensation 5000, total loss = 4.4342041015625, avg loss = 2.21710205078125
client 7, data condensation 5200, total loss = 10.816314697265625, avg loss = 5.4081573486328125
client 7, data condensation 5400, total loss = 6.565673828125, avg loss = 3.2828369140625
client 7, data condensation 5600, total loss = 7.1544189453125, avg loss = 3.57720947265625
client 7, data condensation 5800, total loss = 8.46240234375, avg loss = 4.231201171875
client 7, data condensation 6000, total loss = 23.03070068359375, avg loss = 11.515350341796875
client 7, data condensation 6200, total loss = 22.01702880859375, avg loss = 11.008514404296875
client 7, data condensation 6400, total loss = 17.00531005859375, avg loss = 8.502655029296875
client 7, data condensation 6600, total loss = 4.666168212890625, avg loss = 2.3330841064453125
client 7, data condensation 6800, total loss = 3.11468505859375, avg loss = 1.557342529296875
client 7, data condensation 7000, total loss = 7.37255859375, avg loss = 3.686279296875
client 7, data condensation 7200, total loss = 3.41851806640625, avg loss = 1.709259033203125
client 7, data condensation 7400, total loss = 3.347991943359375, avg loss = 1.6739959716796875
client 7, data condensation 7600, total loss = 10.826446533203125, avg loss = 5.4132232666015625
client 7, data condensation 7800, total loss = 8.625213623046875, avg loss = 4.3126068115234375
client 7, data condensation 8000, total loss = 7.934051513671875, avg loss = 3.9670257568359375
client 7, data condensation 8200, total loss = 13.77496337890625, avg loss = 6.887481689453125
client 7, data condensation 8400, total loss = 20.39300537109375, avg loss = 10.196502685546875
client 7, data condensation 8600, total loss = 35.17083740234375, avg loss = 17.585418701171875
client 7, data condensation 8800, total loss = 5.450042724609375, avg loss = 2.7250213623046875
client 7, data condensation 9000, total loss = 3.48052978515625, avg loss = 1.740264892578125
client 7, data condensation 9200, total loss = 3.628662109375, avg loss = 1.8143310546875
client 7, data condensation 9400, total loss = 12.53826904296875, avg loss = 6.269134521484375
client 7, data condensation 9600, total loss = 10.8017578125, avg loss = 5.40087890625
client 7, data condensation 9800, total loss = 24.499298095703125, avg loss = 12.249649047851562
client 7, data condensation 10000, total loss = 4.79541015625, avg loss = 2.397705078125
Round 1, client 7 condense time: 929.2600977420807
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 3203.06MB, free 21372.94MB
total 24576.0MB, used 3203.06MB, free 21372.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [116  35  30  23  20  15  18  22  15  70], bin edged: [0.00207533 0.00223624 0.00239714 0.00255805 0.00271896 0.00287987
 0.00304078 0.00320169 0.0033626  0.00352351 0.00368442]
class 5 have 135 samples, histogram: [10  8 10  7  4  5 10  4 17 60], bin edged: [0.00481173 0.0051824  0.00555308 0.00592375 0.00629442 0.0066651
 0.00703577 0.00740644 0.00777712 0.00814779 0.00851847]
class 9 have 4727 samples, histogram: [ 814  434  374  274  286  288  317  342  434 1164], bin edged: [0.0001495  0.0001611  0.0001727  0.00018431 0.00019591 0.00020751
 0.00021911 0.00023072 0.00024232 0.00025392 0.00026553]
client 8, data condensation 0, total loss = 275.05419921875, avg loss = 91.68473307291667
client 8, data condensation 200, total loss = 72.447998046875, avg loss = 24.149332682291668
client 8, data condensation 400, total loss = 30.328765869140625, avg loss = 10.109588623046875
client 8, data condensation 600, total loss = 24.512908935546875, avg loss = 8.170969645182291
client 8, data condensation 800, total loss = 18.89215087890625, avg loss = 6.297383626302083
client 8, data condensation 1000, total loss = 32.00921630859375, avg loss = 10.66973876953125
client 8, data condensation 1200, total loss = 31.603271484375, avg loss = 10.534423828125
client 8, data condensation 1400, total loss = 28.735504150390625, avg loss = 9.578501383463541
client 8, data condensation 1600, total loss = 33.10467529296875, avg loss = 11.034891764322916
client 8, data condensation 1800, total loss = 7.553070068359375, avg loss = 2.5176900227864585
client 8, data condensation 2000, total loss = 18.2779541015625, avg loss = 6.0926513671875
client 8, data condensation 2200, total loss = 44.57501220703125, avg loss = 14.85833740234375
client 8, data condensation 2400, total loss = 12.7796630859375, avg loss = 4.2598876953125
client 8, data condensation 2600, total loss = 19.575439453125, avg loss = 6.525146484375
client 8, data condensation 2800, total loss = 16.380401611328125, avg loss = 5.460133870442708
client 8, data condensation 3000, total loss = 33.826416015625, avg loss = 11.275472005208334
client 8, data condensation 3200, total loss = 29.64111328125, avg loss = 9.88037109375
client 8, data condensation 3400, total loss = 31.667205810546875, avg loss = 10.555735270182291
client 8, data condensation 3600, total loss = 35.21929931640625, avg loss = 11.739766438802084
client 8, data condensation 3800, total loss = 18.387451171875, avg loss = 6.129150390625
client 8, data condensation 4000, total loss = 28.362884521484375, avg loss = 9.454294840494791
client 8, data condensation 4200, total loss = 22.369110107421875, avg loss = 7.456370035807292
client 8, data condensation 4400, total loss = 14.31414794921875, avg loss = 4.771382649739583
client 8, data condensation 4600, total loss = 23.764373779296875, avg loss = 7.921457926432292
client 8, data condensation 4800, total loss = 33.48101806640625, avg loss = 11.16033935546875
client 8, data condensation 5000, total loss = 25.693450927734375, avg loss = 8.564483642578125
client 8, data condensation 5200, total loss = 29.462799072265625, avg loss = 9.820933024088541
client 8, data condensation 5400, total loss = 42.910247802734375, avg loss = 14.303415934244791
client 8, data condensation 5600, total loss = 14.660308837890625, avg loss = 4.886769612630208
client 8, data condensation 5800, total loss = 30.27545166015625, avg loss = 10.091817220052084
client 8, data condensation 6000, total loss = 34.550323486328125, avg loss = 11.516774495442709
client 8, data condensation 6200, total loss = 15.4520263671875, avg loss = 5.150675455729167
client 8, data condensation 6400, total loss = 20.52325439453125, avg loss = 6.841084798177083
client 8, data condensation 6600, total loss = 16.509521484375, avg loss = 5.503173828125
client 8, data condensation 6800, total loss = 20.680206298828125, avg loss = 6.893402099609375
client 8, data condensation 7000, total loss = 16.561492919921875, avg loss = 5.520497639973958
client 8, data condensation 7200, total loss = 17.381866455078125, avg loss = 5.793955485026042
client 8, data condensation 7400, total loss = 20.513824462890625, avg loss = 6.837941487630208
client 8, data condensation 7600, total loss = 16.463287353515625, avg loss = 5.487762451171875
client 8, data condensation 7800, total loss = 16.9962158203125, avg loss = 5.6654052734375
client 8, data condensation 8000, total loss = 20.766845703125, avg loss = 6.922281901041667
client 8, data condensation 8200, total loss = 24.040069580078125, avg loss = 8.013356526692709
client 8, data condensation 8400, total loss = 27.517669677734375, avg loss = 9.172556559244791
client 8, data condensation 8600, total loss = 28.025482177734375, avg loss = 9.341827392578125
client 8, data condensation 8800, total loss = 25.67901611328125, avg loss = 8.559672037760416
client 8, data condensation 9000, total loss = 21.16461181640625, avg loss = 7.05487060546875
client 8, data condensation 9200, total loss = 20.3480224609375, avg loss = 6.782674153645833
client 8, data condensation 9400, total loss = 14.042388916015625, avg loss = 4.680796305338542
client 8, data condensation 9600, total loss = 18.82330322265625, avg loss = 6.274434407552083
client 8, data condensation 9800, total loss = 17.493499755859375, avg loss = 5.831166585286458
client 8, data condensation 10000, total loss = 22.092376708984375, avg loss = 7.364125569661458
Round 1, client 8 condense time: 1063.5120258331299
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3457.06MB, free 21118.94MB
total 24576.0MB, used 3457.06MB, free 21118.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [31  6  6  4  3  4  3  7  8 48], bin edged: [0.00575559 0.00620214 0.00664868 0.00709523 0.00754178 0.00798833
 0.00843488 0.00888142 0.00932797 0.00977452 0.01022107]
class 5 have 192 samples, histogram: [18 12  9  8  6  5 12 11 12 99], bin edged: [0.003365   0.00362401 0.00388302 0.00414203 0.00440104 0.00466005
 0.00491906 0.00517807 0.00543708 0.00569609 0.0059551 ]
class 6 have 1075 samples, histogram: [531  97  45  32  39  38  34  33  46 180], bin edged: [0.00074373 0.00080145 0.00085918 0.0009169  0.00097462 0.00103235
 0.00109007 0.00114779 0.00120552 0.00126324 0.00132096]
client 9, data condensation 0, total loss = 248.47299194335938, avg loss = 82.82433064778645
client 9, data condensation 200, total loss = 35.519683837890625, avg loss = 11.839894612630209
client 9, data condensation 400, total loss = 31.879791259765625, avg loss = 10.626597086588541
client 9, data condensation 600, total loss = 23.887603759765625, avg loss = 7.962534586588542
client 9, data condensation 800, total loss = 43.840087890625, avg loss = 14.613362630208334
client 9, data condensation 1000, total loss = 31.441253662109375, avg loss = 10.480417887369791
client 9, data condensation 1200, total loss = 25.34613037109375, avg loss = 8.448710123697916
client 9, data condensation 1400, total loss = 24.390045166015625, avg loss = 8.130015055338541
client 9, data condensation 1600, total loss = 40.7882080078125, avg loss = 13.5960693359375
client 9, data condensation 1800, total loss = 23.97998046875, avg loss = 7.993326822916667
client 9, data condensation 2000, total loss = 22.725189208984375, avg loss = 7.575063069661458
client 9, data condensation 2200, total loss = 39.469970703125, avg loss = 13.156656901041666
client 9, data condensation 2400, total loss = 28.62384033203125, avg loss = 9.541280110677084
client 9, data condensation 2600, total loss = 49.99951171875, avg loss = 16.66650390625
client 9, data condensation 2800, total loss = 21.3216552734375, avg loss = 7.107218424479167
client 9, data condensation 3000, total loss = 13.173248291015625, avg loss = 4.391082763671875
client 9, data condensation 3200, total loss = 33.4248046875, avg loss = 11.1416015625
client 9, data condensation 3400, total loss = 27.91925048828125, avg loss = 9.306416829427084
client 9, data condensation 3600, total loss = 25.828643798828125, avg loss = 8.609547932942709
client 9, data condensation 3800, total loss = 27.517486572265625, avg loss = 9.172495524088541
client 9, data condensation 4000, total loss = 41.366455078125, avg loss = 13.788818359375
client 9, data condensation 4200, total loss = 19.499359130859375, avg loss = 6.499786376953125
client 9, data condensation 4400, total loss = 22.57745361328125, avg loss = 7.52581787109375
client 9, data condensation 4600, total loss = 38.479644775390625, avg loss = 12.826548258463541
client 9, data condensation 4800, total loss = 33.083404541015625, avg loss = 11.027801513671875
client 9, data condensation 5000, total loss = 45.2506103515625, avg loss = 15.083536783854166
client 9, data condensation 5200, total loss = 26.621490478515625, avg loss = 8.873830159505209
client 9, data condensation 5400, total loss = 33.225341796875, avg loss = 11.075113932291666
client 9, data condensation 5600, total loss = 57.790679931640625, avg loss = 19.263559977213543
client 9, data condensation 5800, total loss = 24.466400146484375, avg loss = 8.155466715494791
client 9, data condensation 6000, total loss = 24.495452880859375, avg loss = 8.165150960286459
client 9, data condensation 6200, total loss = 24.05206298828125, avg loss = 8.017354329427084
client 9, data condensation 6400, total loss = 27.648681640625, avg loss = 9.216227213541666
client 9, data condensation 6600, total loss = 33.05517578125, avg loss = 11.018391927083334
client 9, data condensation 6800, total loss = 22.458587646484375, avg loss = 7.486195882161458
client 9, data condensation 7000, total loss = 18.6092529296875, avg loss = 6.203084309895833
client 9, data condensation 7200, total loss = 19.976165771484375, avg loss = 6.658721923828125
client 9, data condensation 7400, total loss = 38.133758544921875, avg loss = 12.711252848307291
client 9, data condensation 7600, total loss = 10.633880615234375, avg loss = 3.5446268717447915
client 9, data condensation 7800, total loss = 19.035308837890625, avg loss = 6.345102945963542
client 9, data condensation 8000, total loss = 27.1495361328125, avg loss = 9.049845377604166
client 9, data condensation 8200, total loss = 45.86553955078125, avg loss = 15.28851318359375
client 9, data condensation 8400, total loss = 48.169189453125, avg loss = 16.056396484375
client 9, data condensation 8600, total loss = 36.420135498046875, avg loss = 12.140045166015625
client 9, data condensation 8800, total loss = 25.432525634765625, avg loss = 8.477508544921875
client 9, data condensation 9000, total loss = 34.67877197265625, avg loss = 11.559590657552084
client 9, data condensation 9200, total loss = 34.41815185546875, avg loss = 11.47271728515625
client 9, data condensation 9400, total loss = 31.178619384765625, avg loss = 10.392873128255209
client 9, data condensation 9600, total loss = 20.596405029296875, avg loss = 6.865468343098958
client 9, data condensation 9800, total loss = 24.931915283203125, avg loss = 8.310638427734375
client 9, data condensation 10000, total loss = 67.8714599609375, avg loss = 22.623819986979168
Round 1, client 9 condense time: 890.3254389762878
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3455.06MB, free 21120.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[11.1276,  3.2111,  2.9592, -3.1762, -0.3206, -8.7437, -7.2671, -6.9273,
          7.5184,  1.1294],
        [ 3.0138,  9.0403, -1.6687, -4.7282, -1.4824, -6.9891, -3.4494, -3.4475,
          5.1572,  5.3267],
        [ 0.0498, -3.7548,  7.3850, -1.4777,  5.3599, -3.8181,  3.9474, -1.0758,
         -4.1208, -0.9465],
        [-2.1808, -1.2071, -0.8899,  3.8813,  1.8351,  1.5932,  3.5552, -0.9781,
         -5.3249,  0.7392],
        [-0.9353, -5.5910,  5.8347, -3.0225,  7.7173, -3.3924,  7.0142,  1.6591,
         -5.4088, -1.8424],
        [-3.1602, -2.8887,  0.2947,  3.3613,  3.2128,  3.2743,  3.3698,  0.9683,
         -6.6800, -0.6329],
        [-2.3092, -4.0547,  3.0195, -0.3491,  5.6961, -2.3520, 12.1097, -1.3468,
         -6.5202, -1.5188],
        [-1.8244, -3.9858,  2.2294, -2.0346,  3.8155, -1.5175,  4.0235,  6.6484,
         -5.4218, -0.1107],
        [10.4793,  5.0714,  0.8178, -4.4299, -2.3611, -8.7473, -9.4710, -8.2705,
         12.9762,  3.3153],
        [ 2.4679,  5.7733, -1.0741, -3.9491, -1.7985, -6.2612, -3.4456, -2.3144,
          4.4973,  6.8644]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 1, 2, 9],
        [1, 9, 8, 0, 4],
        [2, 4, 6, 0, 9],
        [3, 6, 4, 5, 9],
        [4, 6, 2, 7, 0],
        [6, 3, 5, 4, 7],
        [6, 4, 2, 3, 7],
        [7, 6, 4, 2, 9],
        [8, 0, 1, 9, 2],
        [9, 1, 8, 0, 2]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
2
Round 1: # synthetic sample: 2024
total 24576.0MB, used 3455.06MB, free 21120.94MB
{0: {0: 479, 1: 84, 2: 55, 3: 19, 4: 30, 5: 5, 6: 38, 7: 8, 8: 214, 9: 68}, 1: {0: 51, 1: 501, 2: 51, 3: 3, 4: 19, 5: 2, 6: 37, 7: 22, 8: 178, 9: 136}, 2: {0: 108, 1: 36, 2: 304, 3: 77, 4: 109, 5: 32, 6: 188, 7: 32, 8: 47, 9: 67}, 3: {0: 48, 1: 70, 2: 85, 3: 201, 4: 46, 5: 109, 6: 255, 7: 41, 8: 35, 9: 110}, 4: {0: 80, 1: 21, 2: 191, 3: 36, 4: 192, 5: 23, 6: 298, 7: 62, 8: 41, 9: 56}, 5: {0: 35, 1: 31, 2: 132, 3: 197, 4: 69, 5: 179, 6: 213, 7: 51, 8: 34, 9: 59}, 6: {0: 24, 1: 26, 2: 121, 3: 49, 4: 54, 5: 12, 6: 619, 7: 19, 8: 21, 9: 55}, 7: {0: 53, 1: 33, 2: 84, 3: 36, 4: 77, 5: 74, 6: 170, 7: 355, 8: 17, 9: 101}, 8: {0: 214, 1: 78, 2: 26, 3: 9, 4: 4, 5: 0, 6: 20, 7: 1, 8: 571, 9: 77}, 9: {0: 84, 1: 251, 2: 46, 3: 12, 4: 14, 5: 2, 6: 48, 7: 21, 8: 205, 9: 317}}
round 1 evaluation: test acc is 0.3718, test loss = 5.008571
{0: {0: 433, 1: 97, 2: 6, 3: 37, 4: 8, 5: 83, 6: 7, 7: 2, 8: 218, 9: 109}, 1: {0: 15, 1: 612, 2: 1, 3: 29, 4: 8, 5: 45, 6: 4, 7: 3, 8: 59, 9: 224}, 2: {0: 102, 1: 30, 2: 71, 3: 206, 4: 107, 5: 239, 6: 84, 7: 14, 8: 58, 9: 89}, 3: {0: 34, 1: 42, 2: 6, 3: 396, 4: 18, 5: 362, 6: 30, 7: 7, 8: 10, 9: 95}, 4: {0: 49, 1: 33, 2: 42, 3: 186, 4: 180, 5: 207, 6: 140, 7: 46, 8: 46, 9: 71}, 5: {0: 15, 1: 15, 2: 5, 3: 291, 4: 39, 5: 552, 6: 16, 7: 5, 8: 7, 9: 55}, 6: {0: 17, 1: 53, 2: 30, 3: 346, 4: 77, 5: 136, 6: 247, 7: 5, 8: 16, 9: 73}, 7: {0: 31, 1: 39, 2: 10, 3: 172, 4: 38, 5: 336, 6: 26, 7: 177, 8: 14, 9: 157}, 8: {0: 131, 1: 148, 2: 0, 3: 10, 4: 1, 5: 67, 6: 3, 7: 0, 8: 467, 9: 173}, 9: {0: 36, 1: 256, 2: 2, 3: 42, 4: 3, 5: 49, 6: 7, 7: 0, 8: 96, 9: 509}}
epoch 0, train loss avg now = 1.604156, train contrast loss now = 1.266672, test acc now = 0.3644, test loss now = 4.421206
{0: {0: 509, 1: 108, 2: 26, 3: 30, 4: 71, 5: 13, 6: 18, 7: 19, 8: 67, 9: 139}, 1: {0: 17, 1: 689, 2: 2, 3: 18, 4: 32, 5: 7, 6: 11, 7: 13, 8: 5, 9: 206}, 2: {0: 101, 1: 44, 2: 167, 3: 111, 4: 262, 5: 89, 6: 89, 7: 61, 8: 25, 9: 51}, 3: {0: 19, 1: 34, 2: 21, 3: 324, 4: 133, 5: 189, 6: 104, 7: 57, 8: 12, 9: 107}, 4: {0: 38, 1: 27, 2: 33, 3: 57, 4: 552, 5: 44, 6: 98, 7: 95, 8: 14, 9: 42}, 5: {0: 12, 1: 18, 2: 23, 3: 210, 4: 155, 5: 393, 6: 58, 7: 81, 8: 10, 9: 40}, 6: {0: 10, 1: 33, 2: 14, 3: 65, 4: 125, 5: 23, 6: 666, 7: 17, 8: 5, 9: 42}, 7: {0: 10, 1: 29, 2: 8, 3: 70, 4: 155, 5: 73, 6: 16, 7: 534, 8: 2, 9: 103}, 8: {0: 181, 1: 190, 2: 8, 3: 28, 4: 30, 5: 1, 6: 3, 7: 6, 8: 322, 9: 231}, 9: {0: 20, 1: 166, 2: 1, 3: 18, 4: 41, 5: 8, 6: 13, 7: 17, 8: 10, 9: 706}}
epoch 100, train loss avg now = 0.118876, train contrast loss now = 0.146363, test acc now = 0.4862, test loss now = 2.465347
{0: {0: 668, 1: 50, 2: 73, 3: 13, 4: 5, 5: 6, 6: 2, 7: 13, 8: 43, 9: 127}, 1: {0: 56, 1: 570, 2: 10, 3: 7, 4: 4, 5: 6, 6: 1, 7: 13, 8: 4, 9: 329}, 2: {0: 133, 1: 36, 2: 455, 3: 53, 4: 60, 5: 71, 6: 39, 7: 72, 8: 12, 9: 69}, 3: {0: 50, 1: 50, 2: 118, 3: 242, 4: 28, 5: 192, 6: 46, 7: 52, 8: 9, 9: 213}, 4: {0: 93, 1: 48, 2: 222, 3: 38, 4: 238, 5: 56, 6: 53, 7: 165, 8: 4, 9: 83}, 5: {0: 35, 1: 23, 2: 147, 3: 146, 4: 28, 5: 398, 6: 26, 7: 100, 8: 11, 9: 86}, 6: {0: 40, 1: 41, 2: 143, 3: 56, 4: 33, 5: 40, 6: 478, 7: 26, 8: 3, 9: 140}, 7: {0: 42, 1: 25, 2: 64, 3: 37, 4: 23, 5: 76, 6: 4, 7: 552, 8: 0, 9: 177}, 8: {0: 344, 1: 124, 2: 26, 3: 15, 4: 2, 5: 1, 6: 1, 7: 5, 8: 222, 9: 260}, 9: {0: 52, 1: 95, 2: 11, 3: 3, 4: 2, 5: 3, 6: 4, 7: 10, 8: 6, 9: 814}}
epoch 200, train loss avg now = 0.044540, train contrast loss now = 0.077871, test acc now = 0.4637, test loss now = 2.909625
{0: {0: 676, 1: 24, 2: 61, 3: 26, 4: 7, 5: 14, 6: 21, 7: 14, 8: 111, 9: 46}, 1: {0: 95, 1: 507, 2: 10, 3: 30, 4: 5, 5: 28, 6: 50, 7: 36, 8: 23, 9: 216}, 2: {0: 138, 1: 8, 2: 383, 3: 106, 4: 63, 5: 86, 6: 117, 7: 58, 8: 24, 9: 17}, 3: {0: 55, 1: 6, 2: 95, 3: 355, 4: 17, 5: 217, 6: 137, 7: 60, 8: 15, 9: 43}, 4: {0: 87, 1: 4, 2: 165, 3: 90, 4: 231, 5: 73, 6: 180, 7: 144, 8: 15, 9: 11}, 5: {0: 36, 1: 4, 2: 105, 3: 250, 4: 22, 5: 392, 6: 72, 7: 91, 8: 15, 9: 13}, 6: {0: 26, 1: 5, 2: 70, 3: 99, 4: 20, 5: 43, 6: 708, 7: 13, 8: 5, 9: 11}, 7: {0: 56, 1: 6, 2: 64, 3: 73, 4: 17, 5: 105, 6: 44, 7: 593, 8: 6, 9: 36}, 8: {0: 300, 1: 63, 2: 21, 3: 31, 4: 5, 5: 12, 6: 15, 7: 7, 8: 447, 9: 99}, 9: {0: 75, 1: 73, 2: 21, 3: 22, 4: 7, 5: 30, 6: 38, 7: 34, 8: 37, 9: 663}}
epoch 300, train loss avg now = 0.040958, train contrast loss now = 0.071940, test acc now = 0.4955, test loss now = 2.692128
{0: {0: 616, 1: 55, 2: 58, 3: 27, 4: 25, 5: 18, 6: 27, 7: 17, 8: 73, 9: 84}, 1: {0: 26, 1: 666, 2: 8, 3: 22, 4: 20, 5: 11, 6: 21, 7: 26, 8: 20, 9: 180}, 2: {0: 114, 1: 22, 2: 319, 3: 85, 4: 172, 5: 91, 6: 97, 7: 58, 8: 12, 9: 30}, 3: {0: 28, 1: 23, 2: 72, 3: 303, 4: 71, 5: 231, 6: 153, 7: 48, 8: 12, 9: 59}, 4: {0: 48, 1: 17, 2: 97, 3: 64, 4: 450, 5: 48, 6: 126, 7: 111, 8: 12, 9: 27}, 5: {0: 16, 1: 10, 2: 75, 3: 185, 4: 74, 5: 478, 6: 70, 7: 63, 8: 7, 9: 22}, 6: {0: 11, 1: 7, 2: 47, 3: 74, 4: 67, 5: 27, 6: 732, 7: 15, 8: 3, 9: 17}, 7: {0: 21, 1: 17, 2: 51, 3: 69, 4: 86, 5: 113, 6: 35, 7: 549, 8: 4, 9: 55}, 8: {0: 233, 1: 133, 2: 21, 3: 27, 4: 14, 5: 11, 6: 14, 7: 8, 8: 368, 9: 171}, 9: {0: 31, 1: 137, 2: 14, 3: 15, 4: 26, 5: 20, 6: 31, 7: 25, 8: 27, 9: 674}}
epoch 400, train loss avg now = 0.024764, train contrast loss now = 0.059347, test acc now = 0.5155, test loss now = 2.599088
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 651, 1: 45, 2: 50, 3: 28, 4: 18, 5: 13, 6: 14, 7: 20, 8: 67, 9: 94}, 1: {0: 43, 1: 667, 2: 5, 3: 32, 4: 14, 5: 11, 6: 14, 7: 20, 8: 5, 9: 189}, 2: {0: 120, 1: 23, 2: 337, 3: 93, 4: 152, 5: 73, 6: 89, 7: 72, 8: 13, 9: 28}, 3: {0: 44, 1: 22, 2: 81, 3: 390, 4: 61, 5: 157, 6: 118, 7: 56, 8: 6, 9: 65}, 4: {0: 68, 1: 20, 2: 96, 3: 73, 4: 409, 5: 41, 6: 135, 7: 131, 8: 9, 9: 18}, 5: {0: 27, 1: 10, 2: 87, 3: 246, 4: 60, 5: 406, 6: 56, 7: 84, 8: 5, 9: 19}, 6: {0: 27, 1: 13, 2: 56, 3: 81, 4: 54, 5: 23, 6: 700, 7: 19, 8: 2, 9: 25}, 7: {0: 35, 1: 16, 2: 50, 3: 96, 4: 76, 5: 73, 6: 21, 7: 564, 8: 1, 9: 68}, 8: {0: 250, 1: 126, 2: 20, 3: 33, 4: 15, 5: 2, 6: 7, 7: 6, 8: 362, 9: 179}, 9: {0: 51, 1: 136, 2: 11, 3: 25, 4: 18, 5: 9, 6: 15, 7: 22, 8: 16, 9: 697}}
epoch 500, train loss avg now = 0.012658, train contrast loss now = 0.054881, test acc now = 0.5183, test loss now = 2.692363
{0: {0: 673, 1: 53, 2: 46, 3: 25, 4: 14, 5: 10, 6: 11, 7: 17, 8: 50, 9: 101}, 1: {0: 31, 1: 695, 2: 6, 3: 28, 4: 9, 5: 15, 6: 8, 7: 18, 8: 4, 9: 186}, 2: {0: 136, 1: 28, 2: 340, 3: 110, 4: 125, 5: 76, 6: 75, 7: 71, 8: 10, 9: 29}, 3: {0: 47, 1: 31, 2: 72, 3: 392, 4: 51, 5: 181, 6: 91, 7: 51, 8: 8, 9: 76}, 4: {0: 76, 1: 29, 2: 101, 3: 82, 4: 384, 5: 48, 6: 119, 7: 127, 8: 6, 9: 28}, 5: {0: 29, 1: 12, 2: 86, 3: 252, 4: 53, 5: 411, 6: 44, 7: 82, 8: 4, 9: 27}, 6: {0: 27, 1: 17, 2: 54, 3: 110, 4: 53, 5: 30, 6: 664, 7: 17, 8: 1, 9: 27}, 7: {0: 34, 1: 22, 2: 50, 3: 88, 4: 60, 5: 82, 6: 16, 7: 567, 8: 1, 9: 80}, 8: {0: 281, 1: 138, 2: 22, 3: 34, 4: 12, 5: 2, 6: 6, 7: 7, 8: 299, 9: 199}, 9: {0: 47, 1: 139, 2: 6, 3: 17, 4: 13, 5: 10, 6: 13, 7: 23, 8: 8, 9: 724}}
epoch 600, train loss avg now = 0.010604, train contrast loss now = 0.051942, test acc now = 0.5149, test loss now = 2.693184
{0: {0: 666, 1: 57, 2: 48, 3: 33, 4: 14, 5: 12, 6: 9, 7: 18, 8: 50, 9: 93}, 1: {0: 34, 1: 703, 2: 5, 3: 28, 4: 13, 5: 15, 6: 8, 7: 19, 8: 5, 9: 170}, 2: {0: 130, 1: 32, 2: 326, 3: 114, 4: 132, 5: 83, 6: 70, 7: 75, 8: 11, 9: 27}, 3: {0: 48, 1: 29, 2: 75, 3: 382, 4: 51, 5: 192, 6: 94, 7: 50, 8: 6, 9: 73}, 4: {0: 69, 1: 26, 2: 102, 3: 88, 4: 401, 5: 45, 6: 116, 7: 125, 8: 7, 9: 21}, 5: {0: 27, 1: 13, 2: 86, 3: 246, 4: 52, 5: 414, 6: 46, 7: 93, 8: 4, 9: 19}, 6: {0: 26, 1: 22, 2: 59, 3: 118, 4: 60, 5: 29, 6: 641, 7: 18, 8: 2, 9: 25}, 7: {0: 37, 1: 21, 2: 53, 3: 89, 4: 60, 5: 85, 6: 9, 7: 579, 8: 0, 9: 67}, 8: {0: 285, 1: 143, 2: 24, 3: 31, 4: 12, 5: 4, 6: 6, 7: 7, 8: 299, 9: 189}, 9: {0: 48, 1: 151, 2: 10, 3: 19, 4: 15, 5: 10, 6: 13, 7: 28, 8: 10, 9: 696}}
epoch 700, train loss avg now = 0.011232, train contrast loss now = 0.051745, test acc now = 0.5107, test loss now = 2.797395
{0: {0: 664, 1: 57, 2: 48, 3: 30, 4: 15, 5: 9, 6: 10, 7: 18, 8: 51, 9: 98}, 1: {0: 32, 1: 700, 2: 5, 3: 27, 4: 12, 5: 15, 6: 8, 7: 19, 8: 4, 9: 178}, 2: {0: 136, 1: 34, 2: 322, 3: 117, 4: 130, 5: 74, 6: 78, 7: 73, 8: 7, 9: 29}, 3: {0: 42, 1: 33, 2: 73, 3: 398, 4: 50, 5: 169, 6: 103, 7: 48, 8: 7, 9: 77}, 4: {0: 73, 1: 31, 2: 98, 3: 87, 4: 401, 5: 41, 6: 119, 7: 116, 8: 7, 9: 27}, 5: {0: 29, 1: 17, 2: 86, 3: 249, 4: 55, 5: 405, 6: 50, 7: 84, 8: 4, 9: 21}, 6: {0: 29, 1: 19, 2: 58, 3: 116, 4: 58, 5: 24, 6: 652, 7: 16, 8: 2, 9: 26}, 7: {0: 37, 1: 21, 2: 48, 3: 94, 4: 63, 5: 82, 6: 12, 7: 566, 8: 0, 9: 77}, 8: {0: 283, 1: 144, 2: 21, 3: 29, 4: 12, 5: 3, 6: 6, 7: 7, 8: 296, 9: 199}, 9: {0: 46, 1: 148, 2: 6, 3: 18, 4: 17, 5: 7, 6: 14, 7: 25, 8: 9, 9: 710}}
epoch 800, train loss avg now = 0.007700, train contrast loss now = 0.051759, test acc now = 0.5114, test loss now = 2.803851
{0: {0: 658, 1: 63, 2: 44, 3: 32, 4: 14, 5: 12, 6: 9, 7: 20, 8: 51, 9: 97}, 1: {0: 33, 1: 702, 2: 5, 3: 28, 4: 10, 5: 15, 6: 9, 7: 19, 8: 4, 9: 175}, 2: {0: 133, 1: 34, 2: 304, 3: 122, 4: 146, 5: 79, 6: 75, 7: 69, 8: 9, 9: 29}, 3: {0: 41, 1: 32, 2: 64, 3: 400, 4: 52, 5: 191, 6: 91, 7: 46, 8: 6, 9: 77}, 4: {0: 73, 1: 34, 2: 86, 3: 97, 4: 418, 5: 47, 6: 100, 7: 112, 8: 6, 9: 27}, 5: {0: 26, 1: 15, 2: 84, 3: 250, 4: 57, 5: 416, 6: 40, 7: 84, 8: 4, 9: 24}, 6: {0: 25, 1: 21, 2: 52, 3: 135, 4: 60, 5: 30, 6: 632, 7: 17, 8: 1, 9: 27}, 7: {0: 32, 1: 23, 2: 45, 3: 90, 4: 66, 5: 88, 6: 10, 7: 567, 8: 1, 9: 78}, 8: {0: 293, 1: 154, 2: 22, 3: 30, 4: 11, 5: 5, 6: 7, 7: 6, 8: 280, 9: 192}, 9: {0: 45, 1: 148, 2: 6, 3: 19, 4: 15, 5: 10, 6: 14, 7: 24, 8: 9, 9: 710}}
epoch 900, train loss avg now = 0.010288, train contrast loss now = 0.051763, test acc now = 0.5087, test loss now = 2.840600
{0: {0: 663, 1: 56, 2: 38, 3: 28, 4: 16, 5: 11, 6: 12, 7: 17, 8: 59, 9: 100}, 1: {0: 37, 1: 695, 2: 5, 3: 28, 4: 11, 5: 13, 6: 10, 7: 19, 8: 4, 9: 178}, 2: {0: 138, 1: 32, 2: 319, 3: 117, 4: 140, 5: 72, 6: 74, 7: 70, 8: 10, 9: 28}, 3: {0: 49, 1: 30, 2: 74, 3: 384, 4: 53, 5: 180, 6: 94, 7: 53, 8: 8, 9: 75}, 4: {0: 72, 1: 28, 2: 98, 3: 85, 4: 411, 5: 40, 6: 106, 7: 125, 8: 7, 9: 28}, 5: {0: 28, 1: 15, 2: 83, 3: 257, 4: 60, 5: 392, 6: 48, 7: 85, 8: 7, 9: 25}, 6: {0: 28, 1: 20, 2: 58, 3: 119, 4: 61, 5: 27, 6: 640, 7: 18, 8: 2, 9: 27}, 7: {0: 38, 1: 20, 2: 48, 3: 87, 4: 64, 5: 82, 6: 13, 7: 572, 8: 1, 9: 75}, 8: {0: 292, 1: 136, 2: 20, 3: 32, 4: 12, 5: 3, 6: 6, 7: 6, 8: 295, 9: 198}, 9: {0: 50, 1: 143, 2: 7, 3: 18, 4: 15, 5: 6, 6: 15, 7: 26, 8: 10, 9: 710}}
epoch 1000, train loss avg now = 0.006617, train contrast loss now = 0.051925, test acc now = 0.5081, test loss now = 2.854106
epoch avg loss = 6.617357018919065e-06, total time = 8863.727587223053
total 24576.0MB, used 3459.06MB, free 21116.94MB
Round 1 finish, update the prev_syn_proto
torch.Size([202, 3, 32, 32])
torch.Size([202, 3, 32, 32])
torch.Size([208, 3, 32, 32])
torch.Size([200, 3, 32, 32])
torch.Size([200, 3, 32, 32])
torch.Size([210, 3, 32, 32])
torch.Size([202, 3, 32, 32])
torch.Size([200, 3, 32, 32])
torch.Size([200, 3, 32, 32])
torch.Size([200, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 663, 1: 56, 2: 38, 3: 28, 4: 16, 5: 11, 6: 12, 7: 17, 8: 59, 9: 100}, 1: {0: 37, 1: 695, 2: 5, 3: 28, 4: 11, 5: 13, 6: 10, 7: 19, 8: 4, 9: 178}, 2: {0: 138, 1: 32, 2: 319, 3: 117, 4: 140, 5: 72, 6: 74, 7: 70, 8: 10, 9: 28}, 3: {0: 49, 1: 30, 2: 74, 3: 384, 4: 53, 5: 180, 6: 94, 7: 53, 8: 8, 9: 75}, 4: {0: 72, 1: 28, 2: 98, 3: 85, 4: 411, 5: 40, 6: 106, 7: 125, 8: 7, 9: 28}, 5: {0: 28, 1: 15, 2: 83, 3: 257, 4: 60, 5: 392, 6: 48, 7: 85, 8: 7, 9: 25}, 6: {0: 28, 1: 20, 2: 58, 3: 119, 4: 61, 5: 27, 6: 640, 7: 18, 8: 2, 9: 27}, 7: {0: 38, 1: 20, 2: 48, 3: 87, 4: 64, 5: 82, 6: 13, 7: 572, 8: 1, 9: 75}, 8: {0: 292, 1: 136, 2: 20, 3: 32, 4: 12, 5: 3, 6: 6, 7: 6, 8: 295, 9: 198}, 9: {0: 50, 1: 143, 2: 7, 3: 18, 4: 15, 5: 6, 6: 15, 7: 26, 8: 10, 9: 710}}
round 1 evaluation: test acc is 0.5081, test loss = 2.854106
 ====== round 2 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3459.06MB, free 21116.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [ 861  184  142  113  122   97  117  146  224 1587], bin edged: [0.00018959 0.0002043  0.00021902 0.00023373 0.00024845 0.00026316
 0.00027788 0.00029259 0.0003073  0.00032202 0.00033673]
class 7 have 4999 samples, histogram: [2018  240  158  129  119  131  144  159  256 1645], bin edged: [0.000147   0.00015841 0.00016981 0.00018122 0.00019263 0.00020404
 0.00021545 0.00022686 0.00023827 0.00024968 0.00026109]
client 0, data condensation 0, total loss = 82.77801513671875, avg loss = 41.389007568359375
client 0, data condensation 200, total loss = 5.8851318359375, avg loss = 2.94256591796875
client 0, data condensation 400, total loss = 14.84649658203125, avg loss = 7.423248291015625
client 0, data condensation 600, total loss = 16.921539306640625, avg loss = 8.460769653320312
client 0, data condensation 800, total loss = 5.888519287109375, avg loss = 2.9442596435546875
client 0, data condensation 1000, total loss = 8.165283203125, avg loss = 4.0826416015625
client 0, data condensation 1200, total loss = 33.6175537109375, avg loss = 16.80877685546875
client 0, data condensation 1400, total loss = 4.130950927734375, avg loss = 2.0654754638671875
client 0, data condensation 1600, total loss = 4.061492919921875, avg loss = 2.0307464599609375
client 0, data condensation 1800, total loss = 8.01910400390625, avg loss = 4.009552001953125
client 0, data condensation 2000, total loss = 20.636383056640625, avg loss = 10.318191528320312
client 0, data condensation 2200, total loss = 5.886627197265625, avg loss = 2.9433135986328125
client 0, data condensation 2400, total loss = 4.657196044921875, avg loss = 2.3285980224609375
client 0, data condensation 2600, total loss = 18.798828125, avg loss = 9.3994140625
client 0, data condensation 2800, total loss = 6.79119873046875, avg loss = 3.395599365234375
client 0, data condensation 3000, total loss = 5.282012939453125, avg loss = 2.6410064697265625
client 0, data condensation 3200, total loss = 18.83111572265625, avg loss = 9.415557861328125
client 0, data condensation 3400, total loss = 31.638275146484375, avg loss = 15.819137573242188
client 0, data condensation 3600, total loss = 5.605438232421875, avg loss = 2.8027191162109375
client 0, data condensation 3800, total loss = 4.37957763671875, avg loss = 2.189788818359375
client 0, data condensation 4000, total loss = 5.80670166015625, avg loss = 2.903350830078125
client 0, data condensation 4200, total loss = 8.98211669921875, avg loss = 4.491058349609375
client 0, data condensation 4400, total loss = 29.336151123046875, avg loss = 14.668075561523438
client 0, data condensation 4600, total loss = 6.500701904296875, avg loss = 3.2503509521484375
client 0, data condensation 4800, total loss = 4.9415283203125, avg loss = 2.47076416015625
client 0, data condensation 5000, total loss = 7.03985595703125, avg loss = 3.519927978515625
client 0, data condensation 5200, total loss = 7.586029052734375, avg loss = 3.7930145263671875
client 0, data condensation 5400, total loss = 7.33367919921875, avg loss = 3.666839599609375
client 0, data condensation 5600, total loss = 5.40191650390625, avg loss = 2.700958251953125
client 0, data condensation 5800, total loss = 3.05877685546875, avg loss = 1.529388427734375
client 0, data condensation 6000, total loss = 4.182098388671875, avg loss = 2.0910491943359375
client 0, data condensation 6200, total loss = 6.15869140625, avg loss = 3.079345703125
client 0, data condensation 6400, total loss = 8.26397705078125, avg loss = 4.131988525390625
client 0, data condensation 6600, total loss = 3.54254150390625, avg loss = 1.771270751953125
client 0, data condensation 6800, total loss = 9.179656982421875, avg loss = 4.5898284912109375
client 0, data condensation 7000, total loss = 3.76458740234375, avg loss = 1.882293701171875
client 0, data condensation 7200, total loss = 4.00225830078125, avg loss = 2.001129150390625
client 0, data condensation 7400, total loss = 19.769622802734375, avg loss = 9.884811401367188
client 0, data condensation 7600, total loss = 4.0416259765625, avg loss = 2.02081298828125
client 0, data condensation 7800, total loss = 5.42694091796875, avg loss = 2.713470458984375
client 0, data condensation 8000, total loss = 10.18377685546875, avg loss = 5.091888427734375
client 0, data condensation 8200, total loss = 6.755401611328125, avg loss = 3.3777008056640625
client 0, data condensation 8400, total loss = 27.751678466796875, avg loss = 13.875839233398438
client 0, data condensation 8600, total loss = 7.79156494140625, avg loss = 3.895782470703125
client 0, data condensation 8800, total loss = 20.5262451171875, avg loss = 10.26312255859375
client 0, data condensation 9000, total loss = 30.291534423828125, avg loss = 15.145767211914062
client 0, data condensation 9200, total loss = 5.36572265625, avg loss = 2.682861328125
client 0, data condensation 9400, total loss = 3.7794189453125, avg loss = 1.88970947265625
client 0, data condensation 9600, total loss = 2.634674072265625, avg loss = 1.3173370361328125
client 0, data condensation 9800, total loss = 10.696990966796875, avg loss = 5.3484954833984375
client 0, data condensation 10000, total loss = 6.32342529296875, avg loss = 3.161712646484375
Round 2, client 0 condense time: 789.3285984992981
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [46  4  6  6  2  5  4  5 11 86], bin edged: [0.00385114 0.00415004 0.00444893 0.00474783 0.00504672 0.00534562
 0.00564451 0.00594341 0.0062423  0.0065412  0.00684009]
class 4 have 4958 samples, histogram: [ 910  211  149  139  129  152  159  213  376 2520], bin edged: [0.00013235 0.00014262 0.00015289 0.00016317 0.00017344 0.00018371
 0.00019398 0.00020425 0.00021453 0.0002248  0.00023507]
client 1, data condensation 0, total loss = 140.72219848632812, avg loss = 70.36109924316406
client 1, data condensation 200, total loss = 56.4620361328125, avg loss = 28.23101806640625
client 1, data condensation 400, total loss = 36.300567626953125, avg loss = 18.150283813476562
client 1, data condensation 600, total loss = 48.2135009765625, avg loss = 24.10675048828125
client 1, data condensation 800, total loss = 10.486968994140625, avg loss = 5.2434844970703125
client 1, data condensation 1000, total loss = 205.63568115234375, avg loss = 102.81784057617188
client 1, data condensation 1200, total loss = 111.6268310546875, avg loss = 55.81341552734375
client 1, data condensation 1400, total loss = 35.388427734375, avg loss = 17.6942138671875
client 1, data condensation 1600, total loss = 24.65460205078125, avg loss = 12.327301025390625
client 1, data condensation 1800, total loss = 34.763427734375, avg loss = 17.3817138671875
client 1, data condensation 2000, total loss = 48.5028076171875, avg loss = 24.25140380859375
client 1, data condensation 2200, total loss = 24.91845703125, avg loss = 12.459228515625
client 1, data condensation 2400, total loss = 40.390899658203125, avg loss = 20.195449829101562
client 1, data condensation 2600, total loss = 13.738861083984375, avg loss = 6.8694305419921875
client 1, data condensation 2800, total loss = 79.435302734375, avg loss = 39.7176513671875
client 1, data condensation 3000, total loss = 12.37139892578125, avg loss = 6.185699462890625
client 1, data condensation 3200, total loss = 41.51434326171875, avg loss = 20.757171630859375
client 1, data condensation 3400, total loss = 17.96478271484375, avg loss = 8.982391357421875
client 1, data condensation 3600, total loss = 19.82769775390625, avg loss = 9.913848876953125
client 1, data condensation 3800, total loss = 19.594573974609375, avg loss = 9.797286987304688
client 1, data condensation 4000, total loss = 27.18023681640625, avg loss = 13.590118408203125
client 1, data condensation 4200, total loss = 32.341033935546875, avg loss = 16.170516967773438
client 1, data condensation 4400, total loss = 29.559539794921875, avg loss = 14.779769897460938
client 1, data condensation 4600, total loss = 31.392333984375, avg loss = 15.6961669921875
client 1, data condensation 4800, total loss = 55.3740234375, avg loss = 27.68701171875
client 1, data condensation 5000, total loss = 17.229522705078125, avg loss = 8.614761352539062
client 1, data condensation 5200, total loss = 16.74749755859375, avg loss = 8.373748779296875
client 1, data condensation 5400, total loss = 14.464813232421875, avg loss = 7.2324066162109375
client 1, data condensation 5600, total loss = 39.543609619140625, avg loss = 19.771804809570312
client 1, data condensation 5800, total loss = 32.016021728515625, avg loss = 16.008010864257812
client 1, data condensation 6000, total loss = 44.907928466796875, avg loss = 22.453964233398438
client 1, data condensation 6200, total loss = 43.447601318359375, avg loss = 21.723800659179688
client 1, data condensation 6400, total loss = 64.27859497070312, avg loss = 32.13929748535156
client 1, data condensation 6600, total loss = 9.0264892578125, avg loss = 4.51324462890625
client 1, data condensation 6800, total loss = 32.35186767578125, avg loss = 16.175933837890625
client 1, data condensation 7000, total loss = 17.088226318359375, avg loss = 8.544113159179688
client 1, data condensation 7200, total loss = 27.473785400390625, avg loss = 13.736892700195312
client 1, data condensation 7400, total loss = 21.341156005859375, avg loss = 10.670578002929688
client 1, data condensation 7600, total loss = 54.044158935546875, avg loss = 27.022079467773438
client 1, data condensation 7800, total loss = 24.88580322265625, avg loss = 12.442901611328125
client 1, data condensation 8000, total loss = 28.726043701171875, avg loss = 14.363021850585938
client 1, data condensation 8200, total loss = 13.098968505859375, avg loss = 6.5494842529296875
client 1, data condensation 8400, total loss = 41.224517822265625, avg loss = 20.612258911132812
client 1, data condensation 8600, total loss = 18.236602783203125, avg loss = 9.118301391601562
client 1, data condensation 8800, total loss = 19.9705810546875, avg loss = 9.98529052734375
client 1, data condensation 9000, total loss = 24.0155029296875, avg loss = 12.00775146484375
client 1, data condensation 9200, total loss = 20.716278076171875, avg loss = 10.358139038085938
client 1, data condensation 9400, total loss = 13.256561279296875, avg loss = 6.6282806396484375
client 1, data condensation 9600, total loss = 29.818756103515625, avg loss = 14.909378051757812
client 1, data condensation 9800, total loss = 38.1337890625, avg loss = 19.06689453125
client 1, data condensation 10000, total loss = 25.414947509765625, avg loss = 12.707473754882812
Round 2, client 1 condense time: 689.7003428936005
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2823.06MB, free 21752.94MB
total 24576.0MB, used 2823.06MB, free 21752.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [113  20  13   5   3   7   4  15  10  52], bin edged: [0.00320907 0.00345814 0.0037072  0.00395627 0.00420534 0.00445441
 0.00470348 0.00495255 0.00520161 0.00545068 0.00569975]
client 2, data condensation 0, total loss = 71.561279296875, avg loss = 71.561279296875
client 2, data condensation 200, total loss = 15.6842041015625, avg loss = 15.6842041015625
client 2, data condensation 400, total loss = 13.511871337890625, avg loss = 13.511871337890625
client 2, data condensation 600, total loss = 19.2530517578125, avg loss = 19.2530517578125
client 2, data condensation 800, total loss = 29.202880859375, avg loss = 29.202880859375
client 2, data condensation 1000, total loss = 17.49053955078125, avg loss = 17.49053955078125
client 2, data condensation 1200, total loss = 17.09014892578125, avg loss = 17.09014892578125
client 2, data condensation 1400, total loss = 43.3153076171875, avg loss = 43.3153076171875
client 2, data condensation 1600, total loss = 16.07080078125, avg loss = 16.07080078125
client 2, data condensation 1800, total loss = 12.44757080078125, avg loss = 12.44757080078125
client 2, data condensation 2000, total loss = 10.8519287109375, avg loss = 10.8519287109375
client 2, data condensation 2200, total loss = 17.8365478515625, avg loss = 17.8365478515625
client 2, data condensation 2400, total loss = 21.95166015625, avg loss = 21.95166015625
client 2, data condensation 2600, total loss = 10.006103515625, avg loss = 10.006103515625
client 2, data condensation 2800, total loss = 26.617919921875, avg loss = 26.617919921875
client 2, data condensation 3000, total loss = 24.6129150390625, avg loss = 24.6129150390625
client 2, data condensation 3200, total loss = 24.120391845703125, avg loss = 24.120391845703125
client 2, data condensation 3400, total loss = 27.52276611328125, avg loss = 27.52276611328125
client 2, data condensation 3600, total loss = 12.679412841796875, avg loss = 12.679412841796875
client 2, data condensation 3800, total loss = 27.64984130859375, avg loss = 27.64984130859375
client 2, data condensation 4000, total loss = 19.68572998046875, avg loss = 19.68572998046875
client 2, data condensation 4200, total loss = 3.9403076171875, avg loss = 3.9403076171875
client 2, data condensation 4400, total loss = 8.12127685546875, avg loss = 8.12127685546875
client 2, data condensation 4600, total loss = 26.52313232421875, avg loss = 26.52313232421875
client 2, data condensation 4800, total loss = 13.66302490234375, avg loss = 13.66302490234375
client 2, data condensation 5000, total loss = 16.8953857421875, avg loss = 16.8953857421875
client 2, data condensation 5200, total loss = 8.81097412109375, avg loss = 8.81097412109375
client 2, data condensation 5400, total loss = 29.43048095703125, avg loss = 29.43048095703125
client 2, data condensation 5600, total loss = 16.793121337890625, avg loss = 16.793121337890625
client 2, data condensation 5800, total loss = 10.5704345703125, avg loss = 10.5704345703125
client 2, data condensation 6000, total loss = 20.15606689453125, avg loss = 20.15606689453125
client 2, data condensation 6200, total loss = 14.56512451171875, avg loss = 14.56512451171875
client 2, data condensation 6400, total loss = 19.13433837890625, avg loss = 19.13433837890625
client 2, data condensation 6600, total loss = 936.7842407226562, avg loss = 936.7842407226562
client 2, data condensation 6800, total loss = 15.039520263671875, avg loss = 15.039520263671875
client 2, data condensation 7000, total loss = 22.72113037109375, avg loss = 22.72113037109375
client 2, data condensation 7200, total loss = 14.98687744140625, avg loss = 14.98687744140625
client 2, data condensation 7400, total loss = 8.5849609375, avg loss = 8.5849609375
client 2, data condensation 7600, total loss = 12.97113037109375, avg loss = 12.97113037109375
client 2, data condensation 7800, total loss = 25.38787841796875, avg loss = 25.38787841796875
client 2, data condensation 8000, total loss = 82.18563842773438, avg loss = 82.18563842773438
client 2, data condensation 8200, total loss = 27.546417236328125, avg loss = 27.546417236328125
client 2, data condensation 8400, total loss = 24.09283447265625, avg loss = 24.09283447265625
client 2, data condensation 8600, total loss = 19.725341796875, avg loss = 19.725341796875
client 2, data condensation 8800, total loss = 36.89056396484375, avg loss = 36.89056396484375
client 2, data condensation 9000, total loss = 19.54522705078125, avg loss = 19.54522705078125
client 2, data condensation 9200, total loss = 60.87640380859375, avg loss = 60.87640380859375
client 2, data condensation 9400, total loss = 9.68511962890625, avg loss = 9.68511962890625
client 2, data condensation 9600, total loss = 5.79620361328125, avg loss = 5.79620361328125
client 2, data condensation 9800, total loss = 12.03289794921875, avg loss = 12.03289794921875
client 2, data condensation 10000, total loss = 14.610107421875, avg loss = 14.610107421875
Round 2, client 2 condense time: 334.4900267124176
client 2, class 9 have 242 samples
total 24576.0MB, used 2437.06MB, free 22138.94MB
total 24576.0MB, used 2437.06MB, free 22138.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [428  50  31  27  19  16  26  17  37 196], bin edged: [0.00092445 0.0009962  0.00106795 0.0011397  0.00121145 0.0012832
 0.00135495 0.0014267  0.00149845 0.00157021 0.00164196]
class 2 have 1094 samples, histogram: [293  68  33  24  31  41  25  47  64 468], bin edged: [0.00062962 0.00067849 0.00072735 0.00077622 0.00082509 0.00087396
 0.00092282 0.00097169 0.00102056 0.00106942 0.00111829]
client 3, data condensation 0, total loss = 88.89520263671875, avg loss = 44.447601318359375
client 3, data condensation 200, total loss = 7.10205078125, avg loss = 3.551025390625
client 3, data condensation 400, total loss = 8.83380126953125, avg loss = 4.416900634765625
client 3, data condensation 600, total loss = 11.310699462890625, avg loss = 5.6553497314453125
client 3, data condensation 800, total loss = 10.1722412109375, avg loss = 5.08612060546875
client 3, data condensation 1000, total loss = 4.161224365234375, avg loss = 2.0806121826171875
client 3, data condensation 1200, total loss = 6.408782958984375, avg loss = 3.2043914794921875
client 3, data condensation 1400, total loss = 12.73236083984375, avg loss = 6.366180419921875
client 3, data condensation 1600, total loss = 11.939361572265625, avg loss = 5.9696807861328125
client 3, data condensation 1800, total loss = 14.04693603515625, avg loss = 7.023468017578125
client 3, data condensation 2000, total loss = 14.618438720703125, avg loss = 7.3092193603515625
client 3, data condensation 2200, total loss = 12.63458251953125, avg loss = 6.317291259765625
client 3, data condensation 2400, total loss = 8.507904052734375, avg loss = 4.2539520263671875
client 3, data condensation 2600, total loss = 10.936370849609375, avg loss = 5.4681854248046875
client 3, data condensation 2800, total loss = 9.145294189453125, avg loss = 4.5726470947265625
client 3, data condensation 3000, total loss = 4.4808349609375, avg loss = 2.24041748046875
client 3, data condensation 3200, total loss = 7.39666748046875, avg loss = 3.698333740234375
client 3, data condensation 3400, total loss = 7.63702392578125, avg loss = 3.818511962890625
client 3, data condensation 3600, total loss = 5.67791748046875, avg loss = 2.838958740234375
client 3, data condensation 3800, total loss = 7.97625732421875, avg loss = 3.988128662109375
client 3, data condensation 4000, total loss = 8.633758544921875, avg loss = 4.3168792724609375
client 3, data condensation 4200, total loss = 8.64605712890625, avg loss = 4.323028564453125
client 3, data condensation 4400, total loss = 7.296661376953125, avg loss = 3.6483306884765625
client 3, data condensation 4600, total loss = 21.001678466796875, avg loss = 10.500839233398438
client 3, data condensation 4800, total loss = 5.13934326171875, avg loss = 2.569671630859375
client 3, data condensation 5000, total loss = 7.614471435546875, avg loss = 3.8072357177734375
client 3, data condensation 5200, total loss = 6.55072021484375, avg loss = 3.275360107421875
client 3, data condensation 5400, total loss = 4.238555908203125, avg loss = 2.1192779541015625
client 3, data condensation 5600, total loss = 27.66680908203125, avg loss = 13.833404541015625
client 3, data condensation 5800, total loss = 8.53558349609375, avg loss = 4.267791748046875
client 3, data condensation 6000, total loss = 7.69671630859375, avg loss = 3.848358154296875
client 3, data condensation 6200, total loss = 8.16754150390625, avg loss = 4.083770751953125
client 3, data condensation 6400, total loss = 10.64422607421875, avg loss = 5.322113037109375
client 3, data condensation 6600, total loss = 26.80401611328125, avg loss = 13.402008056640625
client 3, data condensation 6800, total loss = 9.86077880859375, avg loss = 4.930389404296875
client 3, data condensation 7000, total loss = 6.1395263671875, avg loss = 3.06976318359375
client 3, data condensation 7200, total loss = 8.246337890625, avg loss = 4.1231689453125
client 3, data condensation 7400, total loss = 4.1839599609375, avg loss = 2.09197998046875
client 3, data condensation 7600, total loss = 4.653594970703125, avg loss = 2.3267974853515625
client 3, data condensation 7800, total loss = 16.860015869140625, avg loss = 8.430007934570312
client 3, data condensation 8000, total loss = 10.18634033203125, avg loss = 5.093170166015625
client 3, data condensation 8200, total loss = 5.1251220703125, avg loss = 2.56256103515625
client 3, data condensation 8400, total loss = 6.231201171875, avg loss = 3.1156005859375
client 3, data condensation 8600, total loss = 8.787017822265625, avg loss = 4.3935089111328125
client 3, data condensation 8800, total loss = 20.220794677734375, avg loss = 10.110397338867188
client 3, data condensation 9000, total loss = 4.317474365234375, avg loss = 2.1587371826171875
client 3, data condensation 9200, total loss = 7.45489501953125, avg loss = 3.727447509765625
client 3, data condensation 9400, total loss = 12.72698974609375, avg loss = 6.363494873046875
client 3, data condensation 9600, total loss = 10.0357666015625, avg loss = 5.01788330078125
client 3, data condensation 9800, total loss = 53.721710205078125, avg loss = 26.860855102539062
client 3, data condensation 10000, total loss = 5.67193603515625, avg loss = 2.835968017578125
Round 2, client 3 condense time: 709.6682004928589
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2819.06MB, free 21756.94MB
total 24576.0MB, used 2819.06MB, free 21756.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2170  222  158  104  103  115  118  109  173  880], bin edged: [0.00019051 0.0002053  0.00022009 0.00023487 0.00024966 0.00026445
 0.00027923 0.00029402 0.00030881 0.00032359 0.00033838]
class 5 have 307 samples, histogram: [109  12  10   6  10  16   5  17  16 106], bin edged: [0.00233513 0.00251636 0.0026976  0.00287884 0.00306008 0.00324132
 0.00342256 0.00360379 0.00378503 0.00396627 0.00414751]
client 4, data condensation 0, total loss = 101.60943603515625, avg loss = 50.804718017578125
client 4, data condensation 200, total loss = 16.353759765625, avg loss = 8.1768798828125
client 4, data condensation 400, total loss = 19.203216552734375, avg loss = 9.601608276367188
client 4, data condensation 600, total loss = 13.73211669921875, avg loss = 6.866058349609375
client 4, data condensation 800, total loss = 19.7828369140625, avg loss = 9.89141845703125
client 4, data condensation 1000, total loss = 13.687896728515625, avg loss = 6.8439483642578125
client 4, data condensation 1200, total loss = 52.6099853515625, avg loss = 26.30499267578125
client 4, data condensation 1400, total loss = 35.92742919921875, avg loss = 17.963714599609375
client 4, data condensation 1600, total loss = 38.039215087890625, avg loss = 19.019607543945312
client 4, data condensation 1800, total loss = 13.41156005859375, avg loss = 6.705780029296875
client 4, data condensation 2000, total loss = 18.99102783203125, avg loss = 9.495513916015625
client 4, data condensation 2200, total loss = 16.70751953125, avg loss = 8.353759765625
client 4, data condensation 2400, total loss = 23.308349609375, avg loss = 11.6541748046875
client 4, data condensation 2600, total loss = 439.9586181640625, avg loss = 219.97930908203125
client 4, data condensation 2800, total loss = 19.291412353515625, avg loss = 9.645706176757812
client 4, data condensation 3000, total loss = 12.574462890625, avg loss = 6.2872314453125
client 4, data condensation 3200, total loss = 11.5106201171875, avg loss = 5.75531005859375
client 4, data condensation 3400, total loss = 9.61065673828125, avg loss = 4.805328369140625
client 4, data condensation 3600, total loss = 14.49676513671875, avg loss = 7.248382568359375
client 4, data condensation 3800, total loss = 12.82855224609375, avg loss = 6.414276123046875
client 4, data condensation 4000, total loss = 9.8394775390625, avg loss = 4.91973876953125
client 4, data condensation 4200, total loss = 16.08538818359375, avg loss = 8.042694091796875
client 4, data condensation 4400, total loss = 21.3167724609375, avg loss = 10.65838623046875
client 4, data condensation 4600, total loss = 25.392303466796875, avg loss = 12.696151733398438
client 4, data condensation 4800, total loss = 9.10302734375, avg loss = 4.551513671875
client 4, data condensation 5000, total loss = 19.18365478515625, avg loss = 9.591827392578125
client 4, data condensation 5200, total loss = 17.73046875, avg loss = 8.865234375
client 4, data condensation 5400, total loss = 14.46844482421875, avg loss = 7.234222412109375
client 4, data condensation 5600, total loss = 21.22955322265625, avg loss = 10.614776611328125
client 4, data condensation 5800, total loss = 10.343841552734375, avg loss = 5.1719207763671875
client 4, data condensation 6000, total loss = 23.029388427734375, avg loss = 11.514694213867188
client 4, data condensation 6200, total loss = 17.846588134765625, avg loss = 8.923294067382812
client 4, data condensation 6400, total loss = 14.517181396484375, avg loss = 7.2585906982421875
client 4, data condensation 6600, total loss = 16.025177001953125, avg loss = 8.012588500976562
client 4, data condensation 6800, total loss = 6.01007080078125, avg loss = 3.005035400390625
client 4, data condensation 7000, total loss = 8.374969482421875, avg loss = 4.1874847412109375
client 4, data condensation 7200, total loss = 12.985198974609375, avg loss = 6.4925994873046875
client 4, data condensation 7400, total loss = 5.7760009765625, avg loss = 2.88800048828125
client 4, data condensation 7600, total loss = 11.971435546875, avg loss = 5.9857177734375
client 4, data condensation 7800, total loss = 14.419921875, avg loss = 7.2099609375
client 4, data condensation 8000, total loss = 32.478759765625, avg loss = 16.2393798828125
client 4, data condensation 8200, total loss = 7.2186279296875, avg loss = 3.60931396484375
client 4, data condensation 8400, total loss = 9.7823486328125, avg loss = 4.89117431640625
client 4, data condensation 8600, total loss = 26.25927734375, avg loss = 13.129638671875
client 4, data condensation 8800, total loss = 7.89202880859375, avg loss = 3.946014404296875
client 4, data condensation 9000, total loss = 13.686065673828125, avg loss = 6.8430328369140625
client 4, data condensation 9200, total loss = 10.739990234375, avg loss = 5.3699951171875
client 4, data condensation 9400, total loss = 18.1917724609375, avg loss = 9.09588623046875
client 4, data condensation 9600, total loss = 9.21002197265625, avg loss = 4.605010986328125
client 4, data condensation 9800, total loss = 30.62445068359375, avg loss = 15.312225341796875
client 4, data condensation 10000, total loss = 12.87774658203125, avg loss = 6.438873291015625
Round 2, client 4 condense time: 826.6557741165161
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [ 763  223  186  131  135  151  161  228  306 2715], bin edged: [0.00012956 0.00013962 0.00014967 0.00015973 0.00016979 0.00017984
 0.0001899  0.00019995 0.00021001 0.00022007 0.00023012]
client 5, data condensation 0, total loss = 170.61700439453125, avg loss = 170.61700439453125
client 5, data condensation 200, total loss = 18.9337158203125, avg loss = 18.9337158203125
client 5, data condensation 400, total loss = 58.849609375, avg loss = 58.849609375
client 5, data condensation 600, total loss = 8.2806396484375, avg loss = 8.2806396484375
client 5, data condensation 800, total loss = 5.396484375, avg loss = 5.396484375
client 5, data condensation 1000, total loss = 4.788299560546875, avg loss = 4.788299560546875
client 5, data condensation 1200, total loss = 2.3211669921875, avg loss = 2.3211669921875
client 5, data condensation 1400, total loss = 1.43963623046875, avg loss = 1.43963623046875
client 5, data condensation 1600, total loss = 16.80975341796875, avg loss = 16.80975341796875
client 5, data condensation 1800, total loss = 5.9779052734375, avg loss = 5.9779052734375
client 5, data condensation 2000, total loss = 12.69122314453125, avg loss = 12.69122314453125
client 5, data condensation 2200, total loss = 9.073974609375, avg loss = 9.073974609375
client 5, data condensation 2400, total loss = 4.656982421875, avg loss = 4.656982421875
client 5, data condensation 2600, total loss = 8.0291748046875, avg loss = 8.0291748046875
client 5, data condensation 2800, total loss = 2.679656982421875, avg loss = 2.679656982421875
client 5, data condensation 3000, total loss = 3.03173828125, avg loss = 3.03173828125
client 5, data condensation 3200, total loss = 34.523590087890625, avg loss = 34.523590087890625
client 5, data condensation 3400, total loss = 3.319793701171875, avg loss = 3.319793701171875
client 5, data condensation 3600, total loss = 6.31121826171875, avg loss = 6.31121826171875
client 5, data condensation 3800, total loss = 14.13995361328125, avg loss = 14.13995361328125
client 5, data condensation 4000, total loss = 11.9837646484375, avg loss = 11.9837646484375
client 5, data condensation 4200, total loss = 9.65789794921875, avg loss = 9.65789794921875
client 5, data condensation 4400, total loss = 2.477874755859375, avg loss = 2.477874755859375
client 5, data condensation 4600, total loss = 6.265625, avg loss = 6.265625
client 5, data condensation 4800, total loss = 17.85626220703125, avg loss = 17.85626220703125
client 5, data condensation 5000, total loss = 4.29193115234375, avg loss = 4.29193115234375
client 5, data condensation 5200, total loss = 12.942138671875, avg loss = 12.942138671875
client 5, data condensation 5400, total loss = 6.85528564453125, avg loss = 6.85528564453125
client 5, data condensation 5600, total loss = 6.1134033203125, avg loss = 6.1134033203125
client 5, data condensation 5800, total loss = 2.755126953125, avg loss = 2.755126953125
client 5, data condensation 6000, total loss = 10.735595703125, avg loss = 10.735595703125
client 5, data condensation 6200, total loss = 21.681365966796875, avg loss = 21.681365966796875
client 5, data condensation 6400, total loss = 5.636322021484375, avg loss = 5.636322021484375
client 5, data condensation 6600, total loss = 28.25836181640625, avg loss = 28.25836181640625
client 5, data condensation 6800, total loss = 4.2333984375, avg loss = 4.2333984375
client 5, data condensation 7000, total loss = 8.99658203125, avg loss = 8.99658203125
client 5, data condensation 7200, total loss = 4.71112060546875, avg loss = 4.71112060546875
client 5, data condensation 7400, total loss = 3.219390869140625, avg loss = 3.219390869140625
client 5, data condensation 7600, total loss = 3.573577880859375, avg loss = 3.573577880859375
client 5, data condensation 7800, total loss = 2.660797119140625, avg loss = 2.660797119140625
client 5, data condensation 8000, total loss = 5.6263427734375, avg loss = 5.6263427734375
client 5, data condensation 8200, total loss = 19.62750244140625, avg loss = 19.62750244140625
client 5, data condensation 8400, total loss = 4.69000244140625, avg loss = 4.69000244140625
client 5, data condensation 8600, total loss = 3.714202880859375, avg loss = 3.714202880859375
client 5, data condensation 8800, total loss = 3.75372314453125, avg loss = 3.75372314453125
client 5, data condensation 9000, total loss = 7.7235107421875, avg loss = 7.7235107421875
client 5, data condensation 9200, total loss = 4.2403564453125, avg loss = 4.2403564453125
client 5, data condensation 9400, total loss = 5.08404541015625, avg loss = 5.08404541015625
client 5, data condensation 9600, total loss = 26.4642333984375, avg loss = 26.4642333984375
client 5, data condensation 9800, total loss = 4.48870849609375, avg loss = 4.48870849609375
client 5, data condensation 10000, total loss = 5.77508544921875, avg loss = 5.77508544921875
Round 2, client 5 condense time: 495.935923576355
client 5, class 8 have 4999 samples
total 24576.0MB, used 2441.06MB, free 22134.94MB
total 24576.0MB, used 2441.06MB, free 22134.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1333  279  173  147  156  155  151  184  244 1543], bin edged: [0.00016306 0.00017571 0.00018837 0.00020102 0.00021368 0.00022633
 0.00023899 0.00025164 0.0002643  0.00027695 0.00028961]
class 6 have 3914 samples, histogram: [1966  206  136  106   95  105   94  113  170  923], bin edged: [0.00019911 0.00021457 0.00023002 0.00024547 0.00026093 0.00027638
 0.00029184 0.00030729 0.00032274 0.0003382  0.00035365]
client 6, data condensation 0, total loss = 105.80230712890625, avg loss = 52.901153564453125
client 6, data condensation 200, total loss = 23.549835205078125, avg loss = 11.774917602539062
client 6, data condensation 400, total loss = 12.5364990234375, avg loss = 6.26824951171875
client 6, data condensation 600, total loss = 17.64410400390625, avg loss = 8.822052001953125
client 6, data condensation 800, total loss = 9.133331298828125, avg loss = 4.5666656494140625
client 6, data condensation 1000, total loss = 14.975555419921875, avg loss = 7.4877777099609375
client 6, data condensation 1200, total loss = 5.31365966796875, avg loss = 2.656829833984375
client 6, data condensation 1400, total loss = 6.04412841796875, avg loss = 3.022064208984375
client 6, data condensation 1600, total loss = 8.732025146484375, avg loss = 4.3660125732421875
client 6, data condensation 1800, total loss = 9.37353515625, avg loss = 4.686767578125
client 6, data condensation 2000, total loss = 27.122894287109375, avg loss = 13.561447143554688
client 6, data condensation 2200, total loss = 13.45318603515625, avg loss = 6.726593017578125
client 6, data condensation 2400, total loss = 8.798187255859375, avg loss = 4.3990936279296875
client 6, data condensation 2600, total loss = 7.75, avg loss = 3.875
client 6, data condensation 2800, total loss = 8.17120361328125, avg loss = 4.085601806640625
client 6, data condensation 3000, total loss = 10.462799072265625, avg loss = 5.2313995361328125
client 6, data condensation 3200, total loss = 5.10992431640625, avg loss = 2.554962158203125
client 6, data condensation 3400, total loss = 4.8138427734375, avg loss = 2.40692138671875
client 6, data condensation 3600, total loss = 10.47119140625, avg loss = 5.235595703125
client 6, data condensation 3800, total loss = 5.627197265625, avg loss = 2.8135986328125
client 6, data condensation 4000, total loss = 6.043060302734375, avg loss = 3.0215301513671875
client 6, data condensation 4200, total loss = 17.292633056640625, avg loss = 8.646316528320312
client 6, data condensation 4400, total loss = 3.982177734375, avg loss = 1.9910888671875
client 6, data condensation 4600, total loss = 6.101776123046875, avg loss = 3.0508880615234375
client 6, data condensation 4800, total loss = 6.75384521484375, avg loss = 3.376922607421875
client 6, data condensation 5000, total loss = 5.274200439453125, avg loss = 2.6371002197265625
client 6, data condensation 5200, total loss = 17.179443359375, avg loss = 8.5897216796875
client 6, data condensation 5400, total loss = 4.2835693359375, avg loss = 2.14178466796875
client 6, data condensation 5600, total loss = 6.110076904296875, avg loss = 3.0550384521484375
client 6, data condensation 5800, total loss = 6.68255615234375, avg loss = 3.341278076171875
client 6, data condensation 6000, total loss = 11.6923828125, avg loss = 5.84619140625
client 6, data condensation 6200, total loss = 29.098785400390625, avg loss = 14.549392700195312
client 6, data condensation 6400, total loss = 6.475189208984375, avg loss = 3.2375946044921875
client 6, data condensation 6600, total loss = 5.65460205078125, avg loss = 2.827301025390625
client 6, data condensation 6800, total loss = 28.50738525390625, avg loss = 14.253692626953125
client 6, data condensation 7000, total loss = 5.79058837890625, avg loss = 2.895294189453125
client 6, data condensation 7200, total loss = 18.830780029296875, avg loss = 9.415390014648438
client 6, data condensation 7400, total loss = 3.7554931640625, avg loss = 1.87774658203125
client 6, data condensation 7600, total loss = 10.488372802734375, avg loss = 5.2441864013671875
client 6, data condensation 7800, total loss = 5.024078369140625, avg loss = 2.5120391845703125
client 6, data condensation 8000, total loss = 12.4298095703125, avg loss = 6.21490478515625
client 6, data condensation 8200, total loss = 10.96038818359375, avg loss = 5.480194091796875
client 6, data condensation 8400, total loss = 5.56854248046875, avg loss = 2.784271240234375
client 6, data condensation 8600, total loss = 7.7313232421875, avg loss = 3.86566162109375
client 6, data condensation 8800, total loss = 6.157958984375, avg loss = 3.0789794921875
client 6, data condensation 9000, total loss = 6.4456787109375, avg loss = 3.22283935546875
client 6, data condensation 9200, total loss = 3.42449951171875, avg loss = 1.712249755859375
client 6, data condensation 9400, total loss = 7.249755859375, avg loss = 3.6248779296875
client 6, data condensation 9600, total loss = 7.8792724609375, avg loss = 3.93963623046875
client 6, data condensation 9800, total loss = 35.843536376953125, avg loss = 17.921768188476562
client 6, data condensation 10000, total loss = 26.604949951171875, avg loss = 13.302474975585938
Round 2, client 6 condense time: 842.8677606582642
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [2947  238  160  117   95   77   90  108  127  646], bin edged: [0.00018277 0.00019696 0.00021114 0.00022533 0.00023951 0.0002537
 0.00026788 0.00028207 0.00029625 0.00031044 0.00032463]
class 3 have 4999 samples, histogram: [ 702  281  225  185  175  192  205  258  369 2407], bin edged: [0.00013128 0.00014147 0.00015166 0.00016184 0.00017203 0.00018222
 0.00019241 0.0002026  0.00021279 0.00022298 0.00023317]
client 7, data condensation 0, total loss = 105.01922607421875, avg loss = 52.509613037109375
client 7, data condensation 200, total loss = 12.87451171875, avg loss = 6.437255859375
client 7, data condensation 400, total loss = 13.80035400390625, avg loss = 6.900177001953125
client 7, data condensation 600, total loss = 9.439727783203125, avg loss = 4.7198638916015625
client 7, data condensation 800, total loss = 3.913818359375, avg loss = 1.9569091796875
client 7, data condensation 1000, total loss = 15.226776123046875, avg loss = 7.6133880615234375
client 7, data condensation 1200, total loss = 7.11114501953125, avg loss = 3.555572509765625
client 7, data condensation 1400, total loss = 14.81195068359375, avg loss = 7.405975341796875
client 7, data condensation 1600, total loss = 6.50457763671875, avg loss = 3.252288818359375
client 7, data condensation 1800, total loss = 5.38909912109375, avg loss = 2.694549560546875
client 7, data condensation 2000, total loss = 7.59466552734375, avg loss = 3.797332763671875
client 7, data condensation 2200, total loss = 22.612030029296875, avg loss = 11.306015014648438
client 7, data condensation 2400, total loss = 12.16192626953125, avg loss = 6.080963134765625
client 7, data condensation 2600, total loss = 4.36419677734375, avg loss = 2.182098388671875
client 7, data condensation 2800, total loss = 15.03363037109375, avg loss = 7.516815185546875
client 7, data condensation 3000, total loss = 11.487701416015625, avg loss = 5.7438507080078125
client 7, data condensation 3200, total loss = 14.217041015625, avg loss = 7.1085205078125
client 7, data condensation 3400, total loss = 9.8590087890625, avg loss = 4.92950439453125
client 7, data condensation 3600, total loss = 5.596893310546875, avg loss = 2.7984466552734375
client 7, data condensation 3800, total loss = 12.2598876953125, avg loss = 6.12994384765625
client 7, data condensation 4000, total loss = 20.7431640625, avg loss = 10.37158203125
client 7, data condensation 4200, total loss = 48.2757568359375, avg loss = 24.13787841796875
client 7, data condensation 4400, total loss = 5.51788330078125, avg loss = 2.758941650390625
client 7, data condensation 4600, total loss = 8.1868896484375, avg loss = 4.09344482421875
client 7, data condensation 4800, total loss = 4.83172607421875, avg loss = 2.415863037109375
client 7, data condensation 5000, total loss = 14.75433349609375, avg loss = 7.377166748046875
client 7, data condensation 5200, total loss = 4.81231689453125, avg loss = 2.406158447265625
client 7, data condensation 5400, total loss = 9.12127685546875, avg loss = 4.560638427734375
client 7, data condensation 5600, total loss = 4.80615234375, avg loss = 2.403076171875
client 7, data condensation 5800, total loss = 17.191162109375, avg loss = 8.5955810546875
client 7, data condensation 6000, total loss = 4.79119873046875, avg loss = 2.395599365234375
client 7, data condensation 6200, total loss = 17.000030517578125, avg loss = 8.500015258789062
client 7, data condensation 6400, total loss = 5.30242919921875, avg loss = 2.651214599609375
client 7, data condensation 6600, total loss = 7.954193115234375, avg loss = 3.9770965576171875
client 7, data condensation 6800, total loss = 8.3466796875, avg loss = 4.17333984375
client 7, data condensation 7000, total loss = 5.94342041015625, avg loss = 2.971710205078125
client 7, data condensation 7200, total loss = 5.6378173828125, avg loss = 2.81890869140625
client 7, data condensation 7400, total loss = 5.61700439453125, avg loss = 2.808502197265625
client 7, data condensation 7600, total loss = 9.713226318359375, avg loss = 4.8566131591796875
client 7, data condensation 7800, total loss = 3.87530517578125, avg loss = 1.937652587890625
client 7, data condensation 8000, total loss = 8.495452880859375, avg loss = 4.2477264404296875
client 7, data condensation 8200, total loss = 14.46136474609375, avg loss = 7.230682373046875
client 7, data condensation 8400, total loss = 14.0068359375, avg loss = 7.00341796875
client 7, data condensation 8600, total loss = 20.428131103515625, avg loss = 10.214065551757812
client 7, data condensation 8800, total loss = 5.36102294921875, avg loss = 2.680511474609375
client 7, data condensation 9000, total loss = 5.382843017578125, avg loss = 2.6914215087890625
client 7, data condensation 9200, total loss = 10.404449462890625, avg loss = 5.2022247314453125
client 7, data condensation 9400, total loss = 11.30755615234375, avg loss = 5.653778076171875
client 7, data condensation 9600, total loss = 3.18316650390625, avg loss = 1.591583251953125
client 7, data condensation 9800, total loss = 16.506072998046875, avg loss = 8.253036499023438
client 7, data condensation 10000, total loss = 9.169464111328125, avg loss = 4.5847320556640625
Round 2, client 7 condense time: 792.5909359455109
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2827.06MB, free 21748.94MB
total 24576.0MB, used 2827.06MB, free 21748.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [231  20  20   9   6   7   4   8  11  48], bin edged: [0.00232351 0.00250385 0.00268418 0.00286452 0.00304486 0.00322519
 0.00340553 0.00358587 0.00376621 0.00394654 0.00412688]
class 5 have 135 samples, histogram: [47  6  6  1  5  5  5  4 13 43], bin edged: [0.0053121  0.00572439 0.00613667 0.00654896 0.00696125 0.00737354
 0.00778583 0.00819812 0.00861041 0.0090227  0.00943499]
class 9 have 4727 samples, histogram: [1993  368  223  168  143  178  159  184  229 1082], bin edged: [0.00016192 0.00017448 0.00018705 0.00019962 0.00021219 0.00022475
 0.00023732 0.00024989 0.00026245 0.00027502 0.00028759]
client 8, data condensation 0, total loss = 152.66241455078125, avg loss = 50.887471516927086
client 8, data condensation 200, total loss = 26.77703857421875, avg loss = 8.925679524739584
client 8, data condensation 400, total loss = 29.872100830078125, avg loss = 9.957366943359375
client 8, data condensation 600, total loss = 19.4462890625, avg loss = 6.482096354166667
client 8, data condensation 800, total loss = 16.5950927734375, avg loss = 5.531697591145833
client 8, data condensation 1000, total loss = 40.9412841796875, avg loss = 13.6470947265625
client 8, data condensation 1200, total loss = 28.1875, avg loss = 9.395833333333334
client 8, data condensation 1400, total loss = 19.581146240234375, avg loss = 6.527048746744792
client 8, data condensation 1600, total loss = 45.48077392578125, avg loss = 15.160257975260416
client 8, data condensation 1800, total loss = 44.8089599609375, avg loss = 14.936319986979166
client 8, data condensation 2000, total loss = 42.764129638671875, avg loss = 14.254709879557291
client 8, data condensation 2200, total loss = 26.23394775390625, avg loss = 8.744649251302084
client 8, data condensation 2400, total loss = 20.264739990234375, avg loss = 6.754913330078125
client 8, data condensation 2600, total loss = 33.659515380859375, avg loss = 11.219838460286459
client 8, data condensation 2800, total loss = 34.027679443359375, avg loss = 11.342559814453125
client 8, data condensation 3000, total loss = 39.09564208984375, avg loss = 13.031880696614584
client 8, data condensation 3200, total loss = 21.287200927734375, avg loss = 7.095733642578125
client 8, data condensation 3400, total loss = 71.28909301757812, avg loss = 23.763031005859375
client 8, data condensation 3600, total loss = 26.9384765625, avg loss = 8.9794921875
client 8, data condensation 3800, total loss = 45.528839111328125, avg loss = 15.176279703776041
client 8, data condensation 4000, total loss = 62.2508544921875, avg loss = 20.750284830729168
client 8, data condensation 4200, total loss = 16.945770263671875, avg loss = 5.648590087890625
client 8, data condensation 4400, total loss = 190.76181030273438, avg loss = 63.58727010091146
client 8, data condensation 4600, total loss = 66.77532958984375, avg loss = 22.258443196614582
client 8, data condensation 4800, total loss = 28.96142578125, avg loss = 9.65380859375
client 8, data condensation 5000, total loss = 13.687744140625, avg loss = 4.562581380208333
client 8, data condensation 5200, total loss = 20.032623291015625, avg loss = 6.677541097005208
client 8, data condensation 5400, total loss = 17.8531494140625, avg loss = 5.9510498046875
client 8, data condensation 5600, total loss = 17.556976318359375, avg loss = 5.852325439453125
client 8, data condensation 5800, total loss = 86.81951904296875, avg loss = 28.939839680989582
client 8, data condensation 6000, total loss = 20.04638671875, avg loss = 6.68212890625
client 8, data condensation 6200, total loss = 28.483856201171875, avg loss = 9.494618733723959
client 8, data condensation 6400, total loss = 16.74908447265625, avg loss = 5.583028157552083
client 8, data condensation 6600, total loss = 17.079742431640625, avg loss = 5.693247477213542
client 8, data condensation 6800, total loss = 24.89312744140625, avg loss = 8.297709147135416
client 8, data condensation 7000, total loss = 50.033782958984375, avg loss = 16.677927652994793
client 8, data condensation 7200, total loss = 22.95745849609375, avg loss = 7.652486165364583
client 8, data condensation 7400, total loss = 17.298248291015625, avg loss = 5.766082763671875
client 8, data condensation 7600, total loss = 20.7841796875, avg loss = 6.928059895833333
client 8, data condensation 7800, total loss = 26.696624755859375, avg loss = 8.898874918619791
client 8, data condensation 8000, total loss = 19.198699951171875, avg loss = 6.399566650390625
client 8, data condensation 8200, total loss = 26.3251953125, avg loss = 8.775065104166666
client 8, data condensation 8400, total loss = 13.25286865234375, avg loss = 4.417622884114583
client 8, data condensation 8600, total loss = 17.17071533203125, avg loss = 5.72357177734375
client 8, data condensation 8800, total loss = 21.374664306640625, avg loss = 7.124888102213542
client 8, data condensation 9000, total loss = 29.0106201171875, avg loss = 9.670206705729166
client 8, data condensation 9200, total loss = 44.86126708984375, avg loss = 14.953755696614584
client 8, data condensation 9400, total loss = 63.987030029296875, avg loss = 21.329010009765625
client 8, data condensation 9600, total loss = 26.288116455078125, avg loss = 8.762705485026041
client 8, data condensation 9800, total loss = 21.9876708984375, avg loss = 7.3292236328125
client 8, data condensation 10000, total loss = 31.1533203125, avg loss = 10.384440104166666
Round 2, client 8 condense time: 990.3343889713287
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3081.06MB, free 21494.94MB
total 24576.0MB, used 3081.06MB, free 21494.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [37  6  5  2  4  3  4  4  9 46], bin edged: [0.00584407 0.00629762 0.00675116 0.00720471 0.00765825 0.0081118
 0.00856534 0.00901888 0.00947243 0.00992597 0.01037952]
class 5 have 192 samples, histogram: [79  5  6  4  7  5  5  6 10 65], bin edged: [0.00380275 0.0040979  0.00439304 0.00468819 0.00498333 0.00527848
 0.00557363 0.00586877 0.00616392 0.00645906 0.00675421]
class 6 have 1075 samples, histogram: [574  56  31  32  30  24  34  32  36 226], bin edged: [0.00073817 0.00079546 0.00085275 0.00091004 0.00096734 0.00102463
 0.00108192 0.00113921 0.0011965  0.0012538  0.00131109]
client 9, data condensation 0, total loss = 189.0416259765625, avg loss = 63.013875325520836
client 9, data condensation 200, total loss = 95.0126953125, avg loss = 31.6708984375
client 9, data condensation 400, total loss = 54.043548583984375, avg loss = 18.014516194661457
client 9, data condensation 600, total loss = 44.471099853515625, avg loss = 14.823699951171875
client 9, data condensation 800, total loss = 38.07501220703125, avg loss = 12.691670735677084
client 9, data condensation 1000, total loss = 31.767333984375, avg loss = 10.589111328125
client 9, data condensation 1200, total loss = 54.719635009765625, avg loss = 18.239878336588543
client 9, data condensation 1400, total loss = 55.146514892578125, avg loss = 18.382171630859375
client 9, data condensation 1600, total loss = 35.32000732421875, avg loss = 11.773335774739584
client 9, data condensation 1800, total loss = 58.15362548828125, avg loss = 19.384541829427082
client 9, data condensation 2000, total loss = 49.60040283203125, avg loss = 16.533467610677082
client 9, data condensation 2200, total loss = 51.991607666015625, avg loss = 17.330535888671875
client 9, data condensation 2400, total loss = 59.2894287109375, avg loss = 19.763142903645832
client 9, data condensation 2600, total loss = 422.65875244140625, avg loss = 140.8862508138021
client 9, data condensation 2800, total loss = 47.8809814453125, avg loss = 15.9603271484375
client 9, data condensation 3000, total loss = 43.3709716796875, avg loss = 14.456990559895834
client 9, data condensation 3200, total loss = 31.085418701171875, avg loss = 10.361806233723959
client 9, data condensation 3400, total loss = 128.40020751953125, avg loss = 42.800069173177086
client 9, data condensation 3600, total loss = 61.839111328125, avg loss = 20.613037109375
client 9, data condensation 3800, total loss = 40.266632080078125, avg loss = 13.422210693359375
client 9, data condensation 4000, total loss = 71.95184326171875, avg loss = 23.98394775390625
client 9, data condensation 4200, total loss = 36.49053955078125, avg loss = 12.16351318359375
client 9, data condensation 4400, total loss = 34.3050537109375, avg loss = 11.435017903645834
client 9, data condensation 4600, total loss = 34.491943359375, avg loss = 11.497314453125
client 9, data condensation 4800, total loss = 780.82177734375, avg loss = 260.27392578125
client 9, data condensation 5000, total loss = 80.82485961914062, avg loss = 26.941619873046875
client 9, data condensation 5200, total loss = 46.57684326171875, avg loss = 15.525614420572916
client 9, data condensation 5400, total loss = 64.88864135742188, avg loss = 21.629547119140625
client 9, data condensation 5600, total loss = 35.72833251953125, avg loss = 11.909444173177084
client 9, data condensation 5800, total loss = 35.24884033203125, avg loss = 11.749613444010416
client 9, data condensation 6000, total loss = 41.898406982421875, avg loss = 13.966135660807291
client 9, data condensation 6200, total loss = 105.14007568359375, avg loss = 35.04669189453125
client 9, data condensation 6400, total loss = 41.905792236328125, avg loss = 13.968597412109375
client 9, data condensation 6600, total loss = 41.6815185546875, avg loss = 13.893839518229166
client 9, data condensation 6800, total loss = 32.900634765625, avg loss = 10.966878255208334
client 9, data condensation 7000, total loss = 38.543548583984375, avg loss = 12.847849527994791
client 9, data condensation 7200, total loss = 31.457611083984375, avg loss = 10.485870361328125
client 9, data condensation 7400, total loss = 39.154296875, avg loss = 13.051432291666666
client 9, data condensation 7600, total loss = 21.33721923828125, avg loss = 7.112406412760417
client 9, data condensation 7800, total loss = 105.9818115234375, avg loss = 35.3272705078125
client 9, data condensation 8000, total loss = 21.7158203125, avg loss = 7.238606770833333
client 9, data condensation 8200, total loss = 21.44970703125, avg loss = 7.14990234375
client 9, data condensation 8400, total loss = 33.19219970703125, avg loss = 11.064066569010416
client 9, data condensation 8600, total loss = 112.885498046875, avg loss = 37.628499348958336
client 9, data condensation 8800, total loss = 39.559539794921875, avg loss = 13.186513264973959
client 9, data condensation 9000, total loss = 37.90863037109375, avg loss = 12.636210123697916
client 9, data condensation 9200, total loss = 44.498016357421875, avg loss = 14.832672119140625
client 9, data condensation 9400, total loss = 67.24444580078125, avg loss = 22.414815266927082
client 9, data condensation 9600, total loss = 32.42388916015625, avg loss = 10.807963053385416
client 9, data condensation 9800, total loss = 37.409912109375, avg loss = 12.469970703125
client 9, data condensation 10000, total loss = 32.6182861328125, avg loss = 10.872762044270834
Round 2, client 9 condense time: 962.3580365180969
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[12.3867,  2.5327,  2.7217, -2.7214, -2.7807, -7.5583, -6.5418, -3.2968,
          3.5728,  1.9730],
        [ 1.8418, 12.7045, -2.7801, -1.8335, -3.5065, -5.9565, -2.6533, -1.0697,
         -2.8957,  7.2760],
        [ 1.1740, -2.4832,  6.8461,  0.5871,  1.9123,  1.0550,  0.2021,  0.8162,
         -5.8535, -2.9856],
        [-2.2061, -1.7238,  1.7760,  6.4883, -0.4119,  4.1117,  2.0586,  0.6699,
         -8.0525, -1.6745],
        [-1.0384, -2.7284,  3.2747, -1.5231,  7.2908,  1.4367,  2.6646,  3.2856,
         -7.9556, -3.4817],
        [-4.1426, -2.8168,  2.8765,  5.6932,  0.0622,  7.9752,  0.9751,  2.7435,
         -9.0778, -3.2183],
        [-1.5791, -0.5362,  1.8852,  3.0029,  1.7315,  1.1973,  9.6106, -1.6601,
         -9.1095, -2.8597],
        [-2.4287, -1.8741,  0.8471,  0.2282,  3.2924,  1.5389, -0.3391,  9.2513,
         -8.8359, -0.2432],
        [ 8.9900,  4.6385, -0.0589, -3.9659, -4.1394, -7.4876, -7.3981, -4.3279,
          8.3789,  5.7433],
        [ 0.7947,  6.3225, -2.2997, -2.6826, -2.7512, -5.1191, -2.8710,  1.0416,
         -2.9374, 11.4496]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 1, 9],
        [1, 9, 0, 7, 3],
        [2, 4, 0, 5, 7],
        [3, 5, 6, 2, 7],
        [4, 7, 2, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 3, 2, 4, 5],
        [7, 4, 5, 2, 3],
        [0, 8, 9, 1, 2],
        [9, 1, 7, 0, 2]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
3
Round 2: # synthetic sample: 3036
total 24576.0MB, used 3079.06MB, free 21496.94MB
{0: {0: 663, 1: 56, 2: 38, 3: 28, 4: 16, 5: 11, 6: 12, 7: 17, 8: 59, 9: 100}, 1: {0: 37, 1: 695, 2: 5, 3: 28, 4: 11, 5: 13, 6: 10, 7: 19, 8: 4, 9: 178}, 2: {0: 138, 1: 32, 2: 319, 3: 117, 4: 140, 5: 72, 6: 74, 7: 70, 8: 10, 9: 28}, 3: {0: 49, 1: 30, 2: 74, 3: 384, 4: 53, 5: 180, 6: 94, 7: 53, 8: 8, 9: 75}, 4: {0: 72, 1: 28, 2: 98, 3: 85, 4: 411, 5: 40, 6: 106, 7: 125, 8: 7, 9: 28}, 5: {0: 28, 1: 15, 2: 83, 3: 257, 4: 60, 5: 392, 6: 48, 7: 85, 8: 7, 9: 25}, 6: {0: 28, 1: 20, 2: 58, 3: 119, 4: 61, 5: 27, 6: 640, 7: 18, 8: 2, 9: 27}, 7: {0: 38, 1: 20, 2: 48, 3: 87, 4: 64, 5: 82, 6: 13, 7: 572, 8: 1, 9: 75}, 8: {0: 292, 1: 136, 2: 20, 3: 32, 4: 12, 5: 3, 6: 6, 7: 6, 8: 295, 9: 198}, 9: {0: 50, 1: 143, 2: 7, 3: 18, 4: 15, 5: 6, 6: 15, 7: 26, 8: 10, 9: 710}}
round 2 evaluation: test acc is 0.5081, test loss = 2.854106
{0: {0: 253, 1: 33, 2: 133, 3: 83, 4: 17, 5: 18, 6: 3, 7: 32, 8: 332, 9: 96}, 1: {0: 14, 1: 520, 2: 5, 3: 73, 4: 4, 5: 24, 6: 5, 7: 16, 8: 117, 9: 222}, 2: {0: 44, 1: 15, 2: 294, 3: 205, 4: 130, 5: 130, 6: 39, 7: 50, 8: 67, 9: 26}, 3: {0: 14, 1: 10, 2: 47, 3: 538, 4: 16, 5: 214, 6: 28, 7: 41, 8: 34, 9: 58}, 4: {0: 20, 1: 23, 2: 108, 3: 155, 4: 287, 5: 104, 6: 91, 7: 114, 8: 67, 9: 31}, 5: {0: 3, 1: 5, 2: 60, 3: 346, 4: 25, 5: 456, 6: 17, 7: 45, 8: 28, 9: 15}, 6: {0: 9, 1: 17, 2: 53, 3: 274, 4: 61, 5: 71, 6: 458, 7: 11, 8: 22, 9: 24}, 7: {0: 7, 1: 13, 2: 36, 3: 170, 4: 44, 5: 129, 6: 4, 7: 514, 8: 26, 9: 57}, 8: {0: 28, 1: 54, 2: 15, 3: 55, 4: 4, 5: 13, 6: 3, 7: 7, 8: 714, 9: 107}, 9: {0: 15, 1: 114, 2: 4, 3: 58, 4: 6, 5: 27, 6: 4, 7: 21, 8: 88, 9: 663}}
epoch 0, train loss avg now = 0.605539, train contrast loss now = 1.223030, test acc now = 0.4697, test loss now = 3.587143
{0: {0: 643, 1: 41, 2: 69, 3: 40, 4: 8, 5: 4, 6: 6, 7: 18, 8: 126, 9: 45}, 1: {0: 48, 1: 753, 2: 15, 3: 20, 4: 4, 5: 4, 6: 2, 7: 26, 8: 27, 9: 101}, 2: {0: 108, 1: 20, 2: 479, 3: 107, 4: 54, 5: 55, 6: 58, 7: 79, 8: 20, 9: 20}, 3: {0: 38, 1: 24, 2: 102, 3: 494, 4: 18, 5: 104, 6: 77, 7: 65, 8: 16, 9: 62}, 4: {0: 59, 1: 16, 2: 196, 3: 79, 4: 260, 5: 56, 6: 97, 7: 193, 8: 25, 9: 19}, 5: {0: 23, 1: 10, 2: 109, 3: 304, 4: 20, 5: 367, 6: 27, 7: 95, 8: 25, 9: 20}, 6: {0: 11, 1: 16, 2: 110, 3: 122, 4: 27, 5: 27, 6: 634, 7: 29, 8: 8, 9: 16}, 7: {0: 23, 1: 18, 2: 67, 3: 80, 4: 20, 5: 54, 6: 14, 7: 676, 8: 9, 9: 39}, 8: {0: 167, 1: 117, 2: 21, 3: 28, 4: 3, 5: 2, 6: 5, 7: 13, 8: 550, 9: 94}, 9: {0: 50, 1: 170, 2: 20, 3: 22, 4: 2, 5: 4, 6: 8, 7: 37, 8: 38, 9: 649}}
epoch 100, train loss avg now = 0.064131, train contrast loss now = 0.167729, test acc now = 0.5505, test loss now = 2.251783
{0: {0: 655, 1: 41, 2: 69, 3: 76, 4: 12, 5: 2, 6: 23, 7: 8, 8: 56, 9: 58}, 1: {0: 42, 1: 713, 2: 9, 3: 78, 4: 13, 5: 3, 6: 19, 7: 8, 8: 11, 9: 104}, 2: {0: 97, 1: 13, 2: 406, 3: 205, 4: 64, 5: 34, 6: 131, 7: 26, 8: 11, 9: 13}, 3: {0: 18, 1: 8, 2: 62, 3: 670, 4: 21, 5: 70, 6: 110, 7: 14, 8: 7, 9: 20}, 4: {0: 55, 1: 10, 2: 151, 3: 145, 4: 330, 5: 21, 6: 207, 7: 53, 8: 15, 9: 13}, 5: {0: 19, 1: 8, 2: 74, 3: 446, 4: 44, 5: 308, 6: 69, 7: 19, 8: 6, 9: 7}, 6: {0: 5, 1: 6, 2: 47, 3: 149, 4: 21, 5: 9, 6: 754, 7: 1, 8: 2, 9: 6}, 7: {0: 25, 1: 13, 2: 51, 3: 229, 4: 68, 5: 53, 6: 42, 7: 468, 8: 2, 9: 49}, 8: {0: 282, 1: 123, 2: 23, 3: 88, 4: 7, 5: 0, 6: 11, 7: 4, 8: 337, 9: 125}, 9: {0: 45, 1: 149, 2: 11, 3: 76, 4: 7, 5: 4, 6: 33, 7: 10, 8: 21, 9: 644}}
epoch 200, train loss avg now = 0.039276, train contrast loss now = 0.155218, test acc now = 0.5285, test loss now = 2.560734
{0: {0: 579, 1: 32, 2: 65, 3: 42, 4: 16, 5: 9, 6: 28, 7: 17, 8: 122, 9: 90}, 1: {0: 29, 1: 576, 2: 13, 3: 32, 4: 9, 5: 13, 6: 33, 7: 29, 8: 39, 9: 227}, 2: {0: 91, 1: 3, 2: 385, 3: 84, 4: 90, 5: 104, 6: 135, 7: 67, 8: 15, 9: 26}, 3: {0: 21, 1: 7, 2: 61, 3: 374, 4: 38, 5: 224, 6: 168, 7: 46, 8: 12, 9: 49}, 4: {0: 47, 1: 3, 2: 97, 3: 50, 4: 374, 5: 63, 6: 185, 7: 144, 8: 20, 9: 17}, 5: {0: 11, 1: 4, 2: 48, 3: 165, 4: 53, 5: 544, 6: 80, 7: 64, 8: 16, 9: 15}, 6: {0: 4, 1: 2, 2: 41, 3: 54, 4: 32, 5: 37, 6: 797, 7: 16, 8: 5, 9: 12}, 7: {0: 17, 1: 4, 2: 33, 3: 66, 4: 36, 5: 121, 6: 41, 7: 635, 8: 2, 9: 45}, 8: {0: 146, 1: 81, 2: 16, 3: 30, 4: 5, 5: 16, 6: 23, 7: 16, 8: 515, 9: 152}, 9: {0: 35, 1: 68, 2: 7, 3: 24, 4: 8, 5: 18, 6: 40, 7: 23, 8: 27, 9: 750}}
epoch 300, train loss avg now = 0.017541, train contrast loss now = 0.152249, test acc now = 0.5529, test loss now = 2.413705
{0: {0: 615, 1: 42, 2: 62, 3: 26, 4: 21, 5: 7, 6: 22, 7: 14, 8: 127, 9: 64}, 1: {0: 44, 1: 694, 2: 4, 3: 13, 4: 12, 5: 8, 6: 14, 7: 17, 8: 30, 9: 164}, 2: {0: 99, 1: 19, 2: 392, 3: 65, 4: 102, 5: 108, 6: 114, 7: 57, 8: 13, 9: 31}, 3: {0: 30, 1: 20, 2: 60, 3: 306, 4: 63, 5: 233, 6: 165, 7: 40, 8: 13, 9: 70}, 4: {0: 48, 1: 12, 2: 108, 3: 42, 4: 437, 5: 59, 6: 143, 7: 114, 8: 13, 9: 24}, 5: {0: 19, 1: 9, 2: 50, 3: 126, 4: 61, 5: 563, 6: 79, 7: 53, 8: 14, 9: 26}, 6: {0: 10, 1: 11, 2: 54, 3: 48, 4: 55, 5: 35, 6: 758, 7: 7, 8: 5, 9: 17}, 7: {0: 19, 1: 12, 2: 31, 3: 49, 4: 70, 5: 112, 6: 37, 7: 601, 8: 7, 9: 62}, 8: {0: 146, 1: 97, 2: 20, 3: 23, 4: 14, 5: 7, 6: 17, 7: 12, 8: 555, 9: 109}, 9: {0: 44, 1: 105, 2: 4, 3: 16, 4: 13, 5: 19, 6: 24, 7: 12, 8: 37, 9: 726}}
epoch 400, train loss avg now = 0.022962, train contrast loss now = 0.149624, test acc now = 0.5647, test loss now = 2.357839
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 664, 1: 38, 2: 60, 3: 31, 4: 9, 5: 9, 6: 13, 7: 16, 8: 98, 9: 62}, 1: {0: 41, 1: 702, 2: 6, 3: 18, 4: 10, 5: 11, 6: 17, 7: 20, 8: 26, 9: 149}, 2: {0: 117, 1: 14, 2: 411, 3: 82, 4: 81, 5: 84, 6: 97, 7: 71, 8: 20, 9: 23}, 3: {0: 31, 1: 17, 2: 68, 3: 403, 4: 46, 5: 191, 6: 108, 7: 51, 8: 12, 9: 73}, 4: {0: 55, 1: 7, 2: 120, 3: 59, 4: 404, 5: 44, 6: 104, 7: 160, 8: 23, 9: 24}, 5: {0: 25, 1: 9, 2: 68, 3: 188, 4: 48, 5: 494, 6: 55, 7: 71, 8: 14, 9: 28}, 6: {0: 12, 1: 7, 2: 59, 3: 60, 4: 52, 5: 26, 6: 729, 7: 25, 8: 6, 9: 24}, 7: {0: 27, 1: 14, 2: 32, 3: 60, 4: 41, 5: 87, 6: 26, 7: 646, 8: 6, 9: 61}, 8: {0: 204, 1: 111, 2: 19, 3: 25, 4: 4, 5: 6, 6: 12, 7: 8, 8: 502, 9: 109}, 9: {0: 53, 1: 105, 2: 5, 3: 13, 4: 8, 5: 15, 6: 17, 7: 18, 8: 32, 9: 734}}
epoch 500, train loss avg now = 0.013234, train contrast loss now = 0.148661, test acc now = 0.5689, test loss now = 2.398293
{0: {0: 633, 1: 38, 2: 53, 3: 33, 4: 15, 5: 7, 6: 15, 7: 12, 8: 130, 9: 64}, 1: {0: 36, 1: 714, 2: 10, 3: 21, 4: 10, 5: 8, 6: 17, 7: 17, 8: 34, 9: 133}, 2: {0: 119, 1: 13, 2: 408, 3: 90, 4: 93, 5: 72, 6: 96, 7: 65, 8: 24, 9: 20}, 3: {0: 28, 1: 17, 2: 72, 3: 428, 4: 45, 5: 150, 6: 139, 7: 47, 8: 15, 9: 59}, 4: {0: 54, 1: 8, 2: 110, 3: 56, 4: 418, 5: 38, 6: 127, 7: 141, 8: 29, 9: 19}, 5: {0: 26, 1: 9, 2: 61, 3: 211, 4: 62, 5: 459, 6: 69, 7: 62, 8: 20, 9: 21}, 6: {0: 9, 1: 9, 2: 61, 3: 58, 4: 50, 5: 18, 6: 757, 7: 17, 8: 6, 9: 15}, 7: {0: 25, 1: 15, 2: 34, 3: 75, 4: 51, 5: 75, 6: 30, 7: 639, 8: 7, 9: 49}, 8: {0: 169, 1: 94, 2: 19, 3: 26, 4: 6, 5: 6, 6: 10, 7: 11, 8: 554, 9: 105}, 9: {0: 45, 1: 114, 2: 4, 3: 20, 4: 10, 5: 11, 6: 19, 7: 17, 8: 39, 9: 721}}
epoch 600, train loss avg now = 0.006041, train contrast loss now = 0.147241, test acc now = 0.5731, test loss now = 2.332832
{0: {0: 630, 1: 42, 2: 55, 3: 34, 4: 13, 5: 9, 6: 12, 7: 13, 8: 121, 9: 71}, 1: {0: 38, 1: 705, 2: 10, 3: 21, 4: 10, 5: 9, 6: 16, 7: 19, 8: 26, 9: 146}, 2: {0: 109, 1: 13, 2: 412, 3: 88, 4: 88, 5: 81, 6: 93, 7: 67, 8: 24, 9: 25}, 3: {0: 25, 1: 17, 2: 73, 3: 414, 4: 45, 5: 168, 6: 123, 7: 53, 8: 14, 9: 68}, 4: {0: 54, 1: 8, 2: 113, 3: 49, 4: 425, 5: 48, 6: 116, 7: 138, 8: 26, 9: 23}, 5: {0: 23, 1: 9, 2: 62, 3: 193, 4: 59, 5: 487, 6: 59, 7: 66, 8: 17, 9: 25}, 6: {0: 8, 1: 7, 2: 58, 3: 62, 4: 49, 5: 22, 6: 752, 7: 19, 8: 6, 9: 17}, 7: {0: 21, 1: 13, 2: 37, 3: 70, 4: 45, 5: 81, 6: 25, 7: 646, 8: 4, 9: 58}, 8: {0: 170, 1: 103, 2: 19, 3: 27, 4: 5, 5: 6, 6: 10, 7: 11, 8: 524, 9: 125}, 9: {0: 40, 1: 112, 2: 7, 3: 18, 4: 9, 5: 15, 6: 13, 7: 17, 8: 32, 9: 737}}
epoch 700, train loss avg now = 0.007632, train contrast loss now = 0.147031, test acc now = 0.5732, test loss now = 2.392783
{0: {0: 633, 1: 38, 2: 52, 3: 35, 4: 17, 5: 8, 6: 12, 7: 13, 8: 122, 9: 70}, 1: {0: 39, 1: 700, 2: 10, 3: 19, 4: 12, 5: 10, 6: 18, 7: 23, 8: 26, 9: 143}, 2: {0: 111, 1: 13, 2: 413, 3: 94, 4: 95, 5: 76, 6: 88, 7: 64, 8: 23, 9: 23}, 3: {0: 27, 1: 17, 2: 68, 3: 430, 4: 50, 5: 171, 6: 117, 7: 49, 8: 14, 9: 57}, 4: {0: 55, 1: 6, 2: 106, 3: 56, 4: 447, 5: 45, 6: 106, 7: 134, 8: 25, 9: 20}, 5: {0: 24, 1: 9, 2: 57, 3: 207, 4: 63, 5: 484, 6: 56, 7: 61, 8: 17, 9: 22}, 6: {0: 7, 1: 6, 2: 60, 3: 65, 4: 50, 5: 24, 6: 749, 7: 19, 8: 5, 9: 15}, 7: {0: 23, 1: 13, 2: 35, 3: 71, 4: 50, 5: 84, 6: 24, 7: 645, 8: 3, 9: 52}, 8: {0: 170, 1: 103, 2: 20, 3: 30, 4: 7, 5: 6, 6: 10, 7: 12, 8: 535, 9: 107}, 9: {0: 45, 1: 117, 2: 5, 3: 20, 4: 12, 5: 14, 6: 16, 7: 17, 8: 33, 9: 721}}
epoch 800, train loss avg now = 0.005585, train contrast loss now = 0.147281, test acc now = 0.5757, test loss now = 2.381230
{0: {0: 631, 1: 41, 2: 55, 3: 32, 4: 15, 5: 8, 6: 14, 7: 13, 8: 121, 9: 70}, 1: {0: 39, 1: 698, 2: 10, 3: 20, 4: 11, 5: 9, 6: 18, 7: 22, 8: 27, 9: 146}, 2: {0: 108, 1: 17, 2: 407, 3: 91, 4: 92, 5: 73, 6: 94, 7: 67, 8: 26, 9: 25}, 3: {0: 28, 1: 19, 2: 67, 3: 418, 4: 46, 5: 166, 6: 129, 7: 51, 8: 13, 9: 63}, 4: {0: 54, 1: 7, 2: 112, 3: 52, 4: 422, 5: 39, 6: 127, 7: 140, 8: 25, 9: 22}, 5: {0: 25, 1: 11, 2: 59, 3: 203, 4: 60, 5: 471, 6: 65, 7: 66, 8: 17, 9: 23}, 6: {0: 9, 1: 7, 2: 53, 3: 58, 4: 51, 5: 20, 6: 764, 7: 19, 8: 5, 9: 14}, 7: {0: 22, 1: 14, 2: 34, 3: 73, 4: 53, 5: 79, 6: 27, 7: 642, 8: 4, 9: 52}, 8: {0: 172, 1: 107, 2: 18, 3: 28, 4: 6, 5: 6, 6: 10, 7: 13, 8: 523, 9: 117}, 9: {0: 43, 1: 109, 2: 4, 3: 18, 4: 14, 5: 12, 6: 19, 7: 15, 8: 32, 9: 734}}
epoch 900, train loss avg now = 0.006130, train contrast loss now = 0.147293, test acc now = 0.5710, test loss now = 2.405807
{0: {0: 643, 1: 39, 2: 55, 3: 35, 4: 12, 5: 7, 6: 14, 7: 13, 8: 118, 9: 64}, 1: {0: 40, 1: 706, 2: 9, 3: 21, 4: 12, 5: 10, 6: 19, 7: 21, 8: 29, 9: 133}, 2: {0: 111, 1: 14, 2: 415, 3: 90, 4: 88, 5: 76, 6: 94, 7: 66, 8: 24, 9: 22}, 3: {0: 29, 1: 17, 2: 69, 3: 441, 4: 46, 5: 154, 6: 129, 7: 47, 8: 12, 9: 56}, 4: {0: 56, 1: 9, 2: 114, 3: 61, 4: 418, 5: 40, 6: 117, 7: 139, 8: 27, 9: 19}, 5: {0: 24, 1: 10, 2: 62, 3: 210, 4: 61, 5: 469, 6: 62, 7: 63, 8: 17, 9: 22}, 6: {0: 11, 1: 7, 2: 57, 3: 67, 4: 57, 5: 19, 6: 744, 7: 19, 8: 5, 9: 14}, 7: {0: 23, 1: 14, 2: 36, 3: 76, 4: 50, 5: 84, 6: 27, 7: 638, 8: 5, 9: 47}, 8: {0: 175, 1: 99, 2: 20, 3: 30, 4: 5, 5: 6, 6: 10, 7: 13, 8: 531, 9: 111}, 9: {0: 46, 1: 118, 2: 5, 3: 20, 4: 12, 5: 14, 6: 19, 7: 16, 8: 39, 9: 711}}
epoch 1000, train loss avg now = 0.005288, train contrast loss now = 0.147539, test acc now = 0.5716, test loss now = 2.413113
epoch avg loss = 5.2875455730252355e-06, total time = 9446.645594358444
total 24576.0MB, used 3079.06MB, free 21496.94MB
Round 2 finish, update the prev_syn_proto
torch.Size([303, 3, 32, 32])
torch.Size([303, 3, 32, 32])
torch.Size([312, 3, 32, 32])
torch.Size([300, 3, 32, 32])
torch.Size([300, 3, 32, 32])
torch.Size([315, 3, 32, 32])
torch.Size([303, 3, 32, 32])
torch.Size([300, 3, 32, 32])
torch.Size([300, 3, 32, 32])
torch.Size([300, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 643, 1: 39, 2: 55, 3: 35, 4: 12, 5: 7, 6: 14, 7: 13, 8: 118, 9: 64}, 1: {0: 40, 1: 706, 2: 9, 3: 21, 4: 12, 5: 10, 6: 19, 7: 21, 8: 29, 9: 133}, 2: {0: 111, 1: 14, 2: 415, 3: 90, 4: 88, 5: 76, 6: 94, 7: 66, 8: 24, 9: 22}, 3: {0: 29, 1: 17, 2: 69, 3: 441, 4: 46, 5: 154, 6: 129, 7: 47, 8: 12, 9: 56}, 4: {0: 56, 1: 9, 2: 114, 3: 61, 4: 418, 5: 40, 6: 117, 7: 139, 8: 27, 9: 19}, 5: {0: 24, 1: 10, 2: 62, 3: 210, 4: 61, 5: 469, 6: 62, 7: 63, 8: 17, 9: 22}, 6: {0: 11, 1: 7, 2: 57, 3: 67, 4: 57, 5: 19, 6: 744, 7: 19, 8: 5, 9: 14}, 7: {0: 23, 1: 14, 2: 36, 3: 76, 4: 50, 5: 84, 6: 27, 7: 638, 8: 5, 9: 47}, 8: {0: 175, 1: 99, 2: 20, 3: 30, 4: 5, 5: 6, 6: 10, 7: 13, 8: 531, 9: 111}, 9: {0: 46, 1: 118, 2: 5, 3: 20, 4: 12, 5: 14, 6: 19, 7: 16, 8: 39, 9: 711}}
round 2 evaluation: test acc is 0.5716, test loss = 2.413113
 ====== round 3 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3079.06MB, free 21496.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1092  205  169  134  116   99  119  145  220 1294], bin edged: [0.00019745 0.00021278 0.0002281  0.00024343 0.00025875 0.00027407
 0.0002894  0.00030472 0.00032005 0.00033537 0.0003507 ]
class 7 have 4999 samples, histogram: [2535  214  165  138  129   97  115  152  208 1246], bin edged: [0.00015534 0.00016739 0.00017945 0.00019151 0.00020356 0.00021562
 0.00022768 0.00023973 0.00025179 0.00026384 0.0002759 ]
client 0, data condensation 0, total loss = 53.95697021484375, avg loss = 26.978485107421875
client 0, data condensation 200, total loss = 16.80023193359375, avg loss = 8.400115966796875
client 0, data condensation 400, total loss = 5.6512451171875, avg loss = 2.82562255859375
client 0, data condensation 600, total loss = 18.3326416015625, avg loss = 9.16632080078125
client 0, data condensation 800, total loss = 5.3621826171875, avg loss = 2.68109130859375
client 0, data condensation 1000, total loss = 12.868194580078125, avg loss = 6.4340972900390625
client 0, data condensation 1200, total loss = 4.60943603515625, avg loss = 2.304718017578125
client 0, data condensation 1400, total loss = 8.6876220703125, avg loss = 4.34381103515625
client 0, data condensation 1600, total loss = 8.49957275390625, avg loss = 4.249786376953125
client 0, data condensation 1800, total loss = 7.65423583984375, avg loss = 3.827117919921875
client 0, data condensation 2000, total loss = 5.80364990234375, avg loss = 2.901824951171875
client 0, data condensation 2200, total loss = 6.06353759765625, avg loss = 3.031768798828125
client 0, data condensation 2400, total loss = 14.4940185546875, avg loss = 7.24700927734375
client 0, data condensation 2600, total loss = 6.38671875, avg loss = 3.193359375
client 0, data condensation 2800, total loss = 6.80609130859375, avg loss = 3.403045654296875
client 0, data condensation 3000, total loss = 9.31463623046875, avg loss = 4.657318115234375
client 0, data condensation 3200, total loss = 36.15838623046875, avg loss = 18.079193115234375
client 0, data condensation 3400, total loss = 14.8179931640625, avg loss = 7.40899658203125
client 0, data condensation 3600, total loss = 4.79437255859375, avg loss = 2.397186279296875
client 0, data condensation 3800, total loss = 7.54742431640625, avg loss = 3.773712158203125
client 0, data condensation 4000, total loss = 5.1478271484375, avg loss = 2.57391357421875
client 0, data condensation 4200, total loss = 14.54510498046875, avg loss = 7.272552490234375
client 0, data condensation 4400, total loss = 12.5472412109375, avg loss = 6.27362060546875
client 0, data condensation 4600, total loss = 26.84979248046875, avg loss = 13.424896240234375
client 0, data condensation 4800, total loss = 15.71685791015625, avg loss = 7.858428955078125
client 0, data condensation 5000, total loss = 5.8812255859375, avg loss = 2.94061279296875
client 0, data condensation 5200, total loss = 10.1019287109375, avg loss = 5.05096435546875
client 0, data condensation 5400, total loss = 36.7777099609375, avg loss = 18.38885498046875
client 0, data condensation 5600, total loss = 5.06304931640625, avg loss = 2.531524658203125
client 0, data condensation 5800, total loss = 7.29388427734375, avg loss = 3.646942138671875
client 0, data condensation 6000, total loss = 7.26971435546875, avg loss = 3.634857177734375
client 0, data condensation 6200, total loss = 7.69378662109375, avg loss = 3.846893310546875
client 0, data condensation 6400, total loss = 5.8958740234375, avg loss = 2.94793701171875
client 0, data condensation 6600, total loss = 7.35540771484375, avg loss = 3.677703857421875
client 0, data condensation 6800, total loss = 3.42462158203125, avg loss = 1.712310791015625
client 0, data condensation 7000, total loss = 7.20697021484375, avg loss = 3.603485107421875
client 0, data condensation 7200, total loss = 8.312408447265625, avg loss = 4.1562042236328125
client 0, data condensation 7400, total loss = 10.70025634765625, avg loss = 5.350128173828125
client 0, data condensation 7600, total loss = 12.0340576171875, avg loss = 6.01702880859375
client 0, data condensation 7800, total loss = 31.09124755859375, avg loss = 15.545623779296875
client 0, data condensation 8000, total loss = 4.76531982421875, avg loss = 2.382659912109375
client 0, data condensation 8200, total loss = 69.9361572265625, avg loss = 34.96807861328125
client 0, data condensation 8400, total loss = 4.36907958984375, avg loss = 2.184539794921875
client 0, data condensation 8600, total loss = 7.13909912109375, avg loss = 3.569549560546875
client 0, data condensation 8800, total loss = 4.42425537109375, avg loss = 2.212127685546875
client 0, data condensation 9000, total loss = 6.4202880859375, avg loss = 3.21014404296875
client 0, data condensation 9200, total loss = 6.7567138671875, avg loss = 3.37835693359375
client 0, data condensation 9400, total loss = 7.82373046875, avg loss = 3.911865234375
client 0, data condensation 9600, total loss = 8.0084228515625, avg loss = 4.00421142578125
client 0, data condensation 9800, total loss = 4.457275390625, avg loss = 2.2286376953125
client 0, data condensation 10000, total loss = 7.9320068359375, avg loss = 3.96600341796875
Round 3, client 0 condense time: 923.2410781383514
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [54  4  9  2  8  6  7  7  9 69], bin edged: [0.00400158 0.00431216 0.00462273 0.00493331 0.00524389 0.00555447
 0.00586505 0.00617562 0.0064862  0.00679678 0.00710736]
class 4 have 4958 samples, histogram: [1265  283  160  142  135  153  177  186  284 2173], bin edged: [0.00013814 0.00014886 0.00015958 0.00017031 0.00018103 0.00019175
 0.00020247 0.00021319 0.00022392 0.00023464 0.00024536]
client 1, data condensation 0, total loss = 140.35943603515625, avg loss = 70.17971801757812
client 1, data condensation 200, total loss = 30.95355224609375, avg loss = 15.476776123046875
client 1, data condensation 400, total loss = 75.6712646484375, avg loss = 37.83563232421875
client 1, data condensation 600, total loss = 33.4661865234375, avg loss = 16.73309326171875
client 1, data condensation 800, total loss = 27.5439453125, avg loss = 13.77197265625
client 1, data condensation 1000, total loss = 24.82159423828125, avg loss = 12.410797119140625
client 1, data condensation 1200, total loss = 11.87066650390625, avg loss = 5.935333251953125
client 1, data condensation 1400, total loss = 61.25433349609375, avg loss = 30.627166748046875
client 1, data condensation 1600, total loss = 33.5604248046875, avg loss = 16.78021240234375
client 1, data condensation 1800, total loss = 29.56707763671875, avg loss = 14.783538818359375
client 1, data condensation 2000, total loss = 507.74395751953125, avg loss = 253.87197875976562
client 1, data condensation 2200, total loss = 407.40899658203125, avg loss = 203.70449829101562
client 1, data condensation 2400, total loss = 31.21978759765625, avg loss = 15.609893798828125
client 1, data condensation 2600, total loss = 31.41326904296875, avg loss = 15.706634521484375
client 1, data condensation 2800, total loss = 108.21826171875, avg loss = 54.109130859375
client 1, data condensation 3000, total loss = 12.65521240234375, avg loss = 6.327606201171875
client 1, data condensation 3200, total loss = 33.40533447265625, avg loss = 16.702667236328125
client 1, data condensation 3400, total loss = 18.90460205078125, avg loss = 9.452301025390625
client 1, data condensation 3600, total loss = 53.54937744140625, avg loss = 26.774688720703125
client 1, data condensation 3800, total loss = 59.27288818359375, avg loss = 29.636444091796875
client 1, data condensation 4000, total loss = 17.49444580078125, avg loss = 8.747222900390625
client 1, data condensation 4200, total loss = 19.37896728515625, avg loss = 9.689483642578125
client 1, data condensation 4400, total loss = 21.08013916015625, avg loss = 10.540069580078125
client 1, data condensation 4600, total loss = 43.75762939453125, avg loss = 21.878814697265625
client 1, data condensation 4800, total loss = 22.640869140625, avg loss = 11.3204345703125
client 1, data condensation 5000, total loss = 26.3328857421875, avg loss = 13.16644287109375
client 1, data condensation 5200, total loss = 38.02410888671875, avg loss = 19.012054443359375
client 1, data condensation 5400, total loss = 40.4205322265625, avg loss = 20.21026611328125
client 1, data condensation 5600, total loss = 30.8375244140625, avg loss = 15.41876220703125
client 1, data condensation 5800, total loss = 14.29876708984375, avg loss = 7.149383544921875
client 1, data condensation 6000, total loss = 30.1944580078125, avg loss = 15.09722900390625
client 1, data condensation 6200, total loss = 24.87725830078125, avg loss = 12.438629150390625
client 1, data condensation 6400, total loss = 31.82861328125, avg loss = 15.914306640625
client 1, data condensation 6600, total loss = 34.11822509765625, avg loss = 17.059112548828125
client 1, data condensation 6800, total loss = 41.35406494140625, avg loss = 20.677032470703125
client 1, data condensation 7000, total loss = 36.82354736328125, avg loss = 18.411773681640625
client 1, data condensation 7200, total loss = 44.8511962890625, avg loss = 22.42559814453125
client 1, data condensation 7400, total loss = 49.95166015625, avg loss = 24.975830078125
client 1, data condensation 7600, total loss = 41.61669921875, avg loss = 20.808349609375
client 1, data condensation 7800, total loss = 18.65008544921875, avg loss = 9.325042724609375
client 1, data condensation 8000, total loss = 49.65826416015625, avg loss = 24.829132080078125
client 1, data condensation 8200, total loss = 12.7640380859375, avg loss = 6.38201904296875
client 1, data condensation 8400, total loss = 17.146240234375, avg loss = 8.5731201171875
client 1, data condensation 8600, total loss = 28.53436279296875, avg loss = 14.267181396484375
client 1, data condensation 8800, total loss = 17.887939453125, avg loss = 8.9439697265625
client 1, data condensation 9000, total loss = 43.3515625, avg loss = 21.67578125
client 1, data condensation 9200, total loss = 39.80194091796875, avg loss = 19.900970458984375
client 1, data condensation 9400, total loss = 48.99713134765625, avg loss = 24.498565673828125
client 1, data condensation 9600, total loss = 58.33795166015625, avg loss = 29.168975830078125
client 1, data condensation 9800, total loss = 18.4161376953125, avg loss = 9.20806884765625
client 1, data condensation 10000, total loss = 34.402099609375, avg loss = 17.2010498046875
Round 3, client 1 condense time: 683.5236859321594
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [140   9   8   5   9   3   2   8   4  54], bin edged: [0.00331934 0.00357696 0.00383459 0.00409222 0.00434984 0.00460747
 0.0048651  0.00512272 0.00538035 0.00563798 0.0058956 ]
client 2, data condensation 0, total loss = 177.83892822265625, avg loss = 177.83892822265625
client 2, data condensation 200, total loss = 31.236328125, avg loss = 31.236328125
client 2, data condensation 400, total loss = 63.80340576171875, avg loss = 63.80340576171875
client 2, data condensation 600, total loss = 22.68719482421875, avg loss = 22.68719482421875
client 2, data condensation 800, total loss = 6.8193359375, avg loss = 6.8193359375
client 2, data condensation 1000, total loss = 23.9134521484375, avg loss = 23.9134521484375
client 2, data condensation 1200, total loss = 17.44024658203125, avg loss = 17.44024658203125
client 2, data condensation 1400, total loss = 40.26385498046875, avg loss = 40.26385498046875
client 2, data condensation 1600, total loss = 58.37506103515625, avg loss = 58.37506103515625
client 2, data condensation 1800, total loss = 23.37115478515625, avg loss = 23.37115478515625
client 2, data condensation 2000, total loss = 39.33660888671875, avg loss = 39.33660888671875
client 2, data condensation 2200, total loss = 36.23187255859375, avg loss = 36.23187255859375
client 2, data condensation 2400, total loss = 40.0211181640625, avg loss = 40.0211181640625
client 2, data condensation 2600, total loss = 20.18841552734375, avg loss = 20.18841552734375
client 2, data condensation 2800, total loss = 23.326171875, avg loss = 23.326171875
client 2, data condensation 3000, total loss = 56.60009765625, avg loss = 56.60009765625
client 2, data condensation 3200, total loss = 20.75177001953125, avg loss = 20.75177001953125
client 2, data condensation 3400, total loss = 45.09356689453125, avg loss = 45.09356689453125
client 2, data condensation 3600, total loss = 21.97833251953125, avg loss = 21.97833251953125
client 2, data condensation 3800, total loss = 23.49932861328125, avg loss = 23.49932861328125
client 2, data condensation 4000, total loss = 25.12481689453125, avg loss = 25.12481689453125
client 2, data condensation 4200, total loss = 23.8094482421875, avg loss = 23.8094482421875
client 2, data condensation 4400, total loss = 21.82098388671875, avg loss = 21.82098388671875
client 2, data condensation 4600, total loss = 54.3787841796875, avg loss = 54.3787841796875
client 2, data condensation 4800, total loss = 24.81781005859375, avg loss = 24.81781005859375
client 2, data condensation 5000, total loss = 23.57672119140625, avg loss = 23.57672119140625
client 2, data condensation 5200, total loss = 17.3323974609375, avg loss = 17.3323974609375
client 2, data condensation 5400, total loss = 24.9713134765625, avg loss = 24.9713134765625
client 2, data condensation 5600, total loss = 31.10638427734375, avg loss = 31.10638427734375
client 2, data condensation 5800, total loss = 38.4776611328125, avg loss = 38.4776611328125
client 2, data condensation 6000, total loss = 4.8697509765625, avg loss = 4.8697509765625
client 2, data condensation 6200, total loss = 9.2591552734375, avg loss = 9.2591552734375
client 2, data condensation 6400, total loss = 11.0308837890625, avg loss = 11.0308837890625
client 2, data condensation 6600, total loss = 21.958251953125, avg loss = 21.958251953125
client 2, data condensation 6800, total loss = 15.40045166015625, avg loss = 15.40045166015625
client 2, data condensation 7000, total loss = 38.50469970703125, avg loss = 38.50469970703125
client 2, data condensation 7200, total loss = 8.8482666015625, avg loss = 8.8482666015625
client 2, data condensation 7400, total loss = 20.1787109375, avg loss = 20.1787109375
client 2, data condensation 7600, total loss = 27.7203369140625, avg loss = 27.7203369140625
client 2, data condensation 7800, total loss = 30.6231689453125, avg loss = 30.6231689453125
client 2, data condensation 8000, total loss = 50.9722900390625, avg loss = 50.9722900390625
client 2, data condensation 8200, total loss = 14.6541748046875, avg loss = 14.6541748046875
client 2, data condensation 8400, total loss = 7.9884033203125, avg loss = 7.9884033203125
client 2, data condensation 8600, total loss = 18.47821044921875, avg loss = 18.47821044921875
client 2, data condensation 8800, total loss = 19.79473876953125, avg loss = 19.79473876953125
client 2, data condensation 9000, total loss = 26.02691650390625, avg loss = 26.02691650390625
client 2, data condensation 9200, total loss = 21.27459716796875, avg loss = 21.27459716796875
client 2, data condensation 9400, total loss = 23.747802734375, avg loss = 23.747802734375
client 2, data condensation 9600, total loss = 20.12188720703125, avg loss = 20.12188720703125
client 2, data condensation 9800, total loss = 26.907958984375, avg loss = 26.907958984375
client 2, data condensation 10000, total loss = 72.96697998046875, avg loss = 72.96697998046875
Round 3, client 2 condense time: 267.5989980697632
client 2, class 9 have 242 samples
total 24576.0MB, used 2437.06MB, free 22138.94MB
total 24576.0MB, used 2437.06MB, free 22138.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [456  47  27  19  14  21  18  17  39 189], bin edged: [0.00093521 0.0010078  0.00108038 0.00115297 0.00122555 0.00129814
 0.00137072 0.00144331 0.0015159  0.00158848 0.00166107]
class 2 have 1094 samples, histogram: [267  61  52  49  37  25  43  36  68 456], bin edged: [0.00063011 0.00067901 0.00072792 0.00077682 0.00082573 0.00087463
 0.00092354 0.00097244 0.00102135 0.00107025 0.00111916]
client 3, data condensation 0, total loss = 74.3084716796875, avg loss = 37.15423583984375
client 3, data condensation 200, total loss = 12.171630859375, avg loss = 6.0858154296875
client 3, data condensation 400, total loss = 8.81414794921875, avg loss = 4.407073974609375
client 3, data condensation 600, total loss = 7.5704345703125, avg loss = 3.78521728515625
client 3, data condensation 800, total loss = 4.30364990234375, avg loss = 2.151824951171875
client 3, data condensation 1000, total loss = 38.668701171875, avg loss = 19.3343505859375
client 3, data condensation 1200, total loss = 10.1363525390625, avg loss = 5.06817626953125
client 3, data condensation 1400, total loss = 15.21502685546875, avg loss = 7.607513427734375
client 3, data condensation 1600, total loss = 3.5938720703125, avg loss = 1.79693603515625
client 3, data condensation 1800, total loss = 5.20196533203125, avg loss = 2.600982666015625
client 3, data condensation 2000, total loss = 7.95733642578125, avg loss = 3.978668212890625
client 3, data condensation 2200, total loss = 7.20068359375, avg loss = 3.600341796875
client 3, data condensation 2400, total loss = 15.3309326171875, avg loss = 7.66546630859375
client 3, data condensation 2600, total loss = 15.4718017578125, avg loss = 7.73590087890625
client 3, data condensation 2800, total loss = 9.8292236328125, avg loss = 4.91461181640625
client 3, data condensation 3000, total loss = 4.23553466796875, avg loss = 2.117767333984375
client 3, data condensation 3200, total loss = 37.298309326171875, avg loss = 18.649154663085938
client 3, data condensation 3400, total loss = 29.40667724609375, avg loss = 14.703338623046875
client 3, data condensation 3600, total loss = 11.52410888671875, avg loss = 5.762054443359375
client 3, data condensation 3800, total loss = 5.11474609375, avg loss = 2.557373046875
client 3, data condensation 4000, total loss = 12.00537109375, avg loss = 6.002685546875
client 3, data condensation 4200, total loss = 8.27069091796875, avg loss = 4.135345458984375
client 3, data condensation 4400, total loss = 34.7198486328125, avg loss = 17.35992431640625
client 3, data condensation 4600, total loss = 13.30615234375, avg loss = 6.653076171875
client 3, data condensation 4800, total loss = 3.7738037109375, avg loss = 1.88690185546875
client 3, data condensation 5000, total loss = 8.9971923828125, avg loss = 4.49859619140625
client 3, data condensation 5200, total loss = 9.99615478515625, avg loss = 4.998077392578125
client 3, data condensation 5400, total loss = 7.71533203125, avg loss = 3.857666015625
client 3, data condensation 5600, total loss = 13.6094970703125, avg loss = 6.80474853515625
client 3, data condensation 5800, total loss = 7.9814453125, avg loss = 3.99072265625
client 3, data condensation 6000, total loss = 12.96148681640625, avg loss = 6.480743408203125
client 3, data condensation 6200, total loss = 10.0606689453125, avg loss = 5.03033447265625
client 3, data condensation 6400, total loss = 9.29107666015625, avg loss = 4.645538330078125
client 3, data condensation 6600, total loss = 9.58367919921875, avg loss = 4.791839599609375
client 3, data condensation 6800, total loss = 3.8037109375, avg loss = 1.90185546875
client 3, data condensation 7000, total loss = 10.48779296875, avg loss = 5.243896484375
client 3, data condensation 7200, total loss = 10.62005615234375, avg loss = 5.310028076171875
client 3, data condensation 7400, total loss = 19.98211669921875, avg loss = 9.991058349609375
client 3, data condensation 7600, total loss = 10.41314697265625, avg loss = 5.206573486328125
client 3, data condensation 7800, total loss = 10.325439453125, avg loss = 5.1627197265625
client 3, data condensation 8000, total loss = 14.8853759765625, avg loss = 7.44268798828125
client 3, data condensation 8200, total loss = 2.71514892578125, avg loss = 1.357574462890625
client 3, data condensation 8400, total loss = 6.44024658203125, avg loss = 3.220123291015625
client 3, data condensation 8600, total loss = 11.17822265625, avg loss = 5.589111328125
client 3, data condensation 8800, total loss = 11.91949462890625, avg loss = 5.959747314453125
client 3, data condensation 9000, total loss = 6.4635009765625, avg loss = 3.23175048828125
client 3, data condensation 9200, total loss = 5.380126953125, avg loss = 2.6900634765625
client 3, data condensation 9400, total loss = 8.19805908203125, avg loss = 4.099029541015625
client 3, data condensation 9600, total loss = 15.127838134765625, avg loss = 7.5639190673828125
client 3, data condensation 9800, total loss = 9.1002197265625, avg loss = 4.55010986328125
client 3, data condensation 10000, total loss = 7.8509521484375, avg loss = 3.92547607421875
Round 3, client 3 condense time: 570.4628994464874
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2819.06MB, free 21756.94MB
total 24576.0MB, used 2819.06MB, free 21756.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2194  182  135  106   93  111  116  118  156  941], bin edged: [0.00018957 0.00020428 0.000219   0.00023371 0.00024842 0.00026314
 0.00027785 0.00029256 0.00030728 0.00032199 0.0003367 ]
class 5 have 307 samples, histogram: [109  21  15   7  13  14   6  12  19  91], bin edged: [0.00239676 0.00258278 0.0027688  0.00295482 0.00314085 0.00332687
 0.00351289 0.00369891 0.00388494 0.00407096 0.00425698]
client 4, data condensation 0, total loss = 64.53271484375, avg loss = 32.266357421875
client 4, data condensation 200, total loss = 29.11761474609375, avg loss = 14.558807373046875
client 4, data condensation 400, total loss = 15.878662109375, avg loss = 7.9393310546875
client 4, data condensation 600, total loss = 17.2618408203125, avg loss = 8.63092041015625
client 4, data condensation 800, total loss = 11.90155029296875, avg loss = 5.950775146484375
client 4, data condensation 1000, total loss = 9.5806884765625, avg loss = 4.79034423828125
client 4, data condensation 1200, total loss = 9.5201416015625, avg loss = 4.76007080078125
client 4, data condensation 1400, total loss = 11.9483642578125, avg loss = 5.97418212890625
client 4, data condensation 1600, total loss = 49.66546630859375, avg loss = 24.832733154296875
client 4, data condensation 1800, total loss = 13.43243408203125, avg loss = 6.716217041015625
client 4, data condensation 2000, total loss = 77.139404296875, avg loss = 38.5697021484375
client 4, data condensation 2200, total loss = 20.39794921875, avg loss = 10.198974609375
client 4, data condensation 2400, total loss = 22.62445068359375, avg loss = 11.312225341796875
client 4, data condensation 2600, total loss = 12.32598876953125, avg loss = 6.162994384765625
client 4, data condensation 2800, total loss = 9.85870361328125, avg loss = 4.929351806640625
client 4, data condensation 3000, total loss = 16.97991943359375, avg loss = 8.489959716796875
client 4, data condensation 3200, total loss = 12.28411865234375, avg loss = 6.142059326171875
client 4, data condensation 3400, total loss = 6.832763671875, avg loss = 3.4163818359375
client 4, data condensation 3600, total loss = 23.05328369140625, avg loss = 11.526641845703125
client 4, data condensation 3800, total loss = 40.98968505859375, avg loss = 20.494842529296875
client 4, data condensation 4000, total loss = 16.24896240234375, avg loss = 8.124481201171875
client 4, data condensation 4200, total loss = 11.1802978515625, avg loss = 5.59014892578125
client 4, data condensation 4400, total loss = 43.440185546875, avg loss = 21.7200927734375
client 4, data condensation 4600, total loss = 27.431396484375, avg loss = 13.7156982421875
client 4, data condensation 4800, total loss = 18.06719970703125, avg loss = 9.033599853515625
client 4, data condensation 5000, total loss = 19.96722412109375, avg loss = 9.983612060546875
client 4, data condensation 5200, total loss = 56.576507568359375, avg loss = 28.288253784179688
client 4, data condensation 5400, total loss = 14.69683837890625, avg loss = 7.348419189453125
client 4, data condensation 5600, total loss = 20.2138671875, avg loss = 10.10693359375
client 4, data condensation 5800, total loss = 10.23687744140625, avg loss = 5.118438720703125
client 4, data condensation 6000, total loss = 8.1572265625, avg loss = 4.07861328125
client 4, data condensation 6200, total loss = 10.40252685546875, avg loss = 5.201263427734375
client 4, data condensation 6400, total loss = 9.01983642578125, avg loss = 4.509918212890625
client 4, data condensation 6600, total loss = 11.9129638671875, avg loss = 5.95648193359375
client 4, data condensation 6800, total loss = 10.74493408203125, avg loss = 5.372467041015625
client 4, data condensation 7000, total loss = 7.97381591796875, avg loss = 3.986907958984375
client 4, data condensation 7200, total loss = 144.8223876953125, avg loss = 72.41119384765625
client 4, data condensation 7400, total loss = 10.81756591796875, avg loss = 5.408782958984375
client 4, data condensation 7600, total loss = 26.682373046875, avg loss = 13.3411865234375
client 4, data condensation 7800, total loss = 7.90350341796875, avg loss = 3.951751708984375
client 4, data condensation 8000, total loss = 18.6658935546875, avg loss = 9.33294677734375
client 4, data condensation 8200, total loss = 18.8980712890625, avg loss = 9.44903564453125
client 4, data condensation 8400, total loss = 20.41510009765625, avg loss = 10.207550048828125
client 4, data condensation 8600, total loss = 19.52398681640625, avg loss = 9.761993408203125
client 4, data condensation 8800, total loss = 24.270263671875, avg loss = 12.1351318359375
client 4, data condensation 9000, total loss = 52.78692626953125, avg loss = 26.393463134765625
client 4, data condensation 9200, total loss = 12.9630126953125, avg loss = 6.48150634765625
client 4, data condensation 9400, total loss = 14.47467041015625, avg loss = 7.237335205078125
client 4, data condensation 9600, total loss = 93.34967041015625, avg loss = 46.674835205078125
client 4, data condensation 9800, total loss = 16.55523681640625, avg loss = 8.277618408203125
client 4, data condensation 10000, total loss = 20.415283203125, avg loss = 10.2076416015625
Round 3, client 4 condense time: 681.8099374771118
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [1557  264  212  158  139  122  140  188  259 1960], bin edged: [0.00014116 0.00015212 0.00016307 0.00017403 0.00018498 0.00019594
 0.0002069  0.00021785 0.00022881 0.00023976 0.00025072]
client 5, data condensation 0, total loss = 85.8944091796875, avg loss = 85.8944091796875
client 5, data condensation 200, total loss = 17.0921630859375, avg loss = 17.0921630859375
client 5, data condensation 400, total loss = 3.84442138671875, avg loss = 3.84442138671875
client 5, data condensation 600, total loss = 14.75909423828125, avg loss = 14.75909423828125
client 5, data condensation 800, total loss = 6.76434326171875, avg loss = 6.76434326171875
client 5, data condensation 1000, total loss = 7.4639892578125, avg loss = 7.4639892578125
client 5, data condensation 1200, total loss = 20.9044189453125, avg loss = 20.9044189453125
client 5, data condensation 1400, total loss = 4.63885498046875, avg loss = 4.63885498046875
client 5, data condensation 1600, total loss = 13.293212890625, avg loss = 13.293212890625
client 5, data condensation 1800, total loss = 4.9124755859375, avg loss = 4.9124755859375
client 5, data condensation 2000, total loss = 7.87530517578125, avg loss = 7.87530517578125
client 5, data condensation 2200, total loss = 7.93084716796875, avg loss = 7.93084716796875
client 5, data condensation 2400, total loss = 4.54437255859375, avg loss = 4.54437255859375
client 5, data condensation 2600, total loss = 5.624267578125, avg loss = 5.624267578125
client 5, data condensation 2800, total loss = 12.8665771484375, avg loss = 12.8665771484375
client 5, data condensation 3000, total loss = 2.849853515625, avg loss = 2.849853515625
client 5, data condensation 3200, total loss = 3.13916015625, avg loss = 3.13916015625
client 5, data condensation 3400, total loss = 2.2666015625, avg loss = 2.2666015625
client 5, data condensation 3600, total loss = 7.945068359375, avg loss = 7.945068359375
client 5, data condensation 3800, total loss = 2.72259521484375, avg loss = 2.72259521484375
client 5, data condensation 4000, total loss = 34.91644287109375, avg loss = 34.91644287109375
client 5, data condensation 4200, total loss = 15.07086181640625, avg loss = 15.07086181640625
client 5, data condensation 4400, total loss = 7.5343017578125, avg loss = 7.5343017578125
client 5, data condensation 4600, total loss = 6.82611083984375, avg loss = 6.82611083984375
client 5, data condensation 4800, total loss = 3.85125732421875, avg loss = 3.85125732421875
client 5, data condensation 5000, total loss = 4.92034912109375, avg loss = 4.92034912109375
client 5, data condensation 5200, total loss = 4.875244140625, avg loss = 4.875244140625
client 5, data condensation 5400, total loss = 15.5762939453125, avg loss = 15.5762939453125
client 5, data condensation 5600, total loss = 3.1705322265625, avg loss = 3.1705322265625
client 5, data condensation 5800, total loss = 3.7982177734375, avg loss = 3.7982177734375
client 5, data condensation 6000, total loss = 6.85302734375, avg loss = 6.85302734375
client 5, data condensation 6200, total loss = 21.098785400390625, avg loss = 21.098785400390625
client 5, data condensation 6400, total loss = 26.839111328125, avg loss = 26.839111328125
client 5, data condensation 6600, total loss = 121.52813720703125, avg loss = 121.52813720703125
client 5, data condensation 6800, total loss = 3.996337890625, avg loss = 3.996337890625
client 5, data condensation 7000, total loss = 27.3662109375, avg loss = 27.3662109375
client 5, data condensation 7200, total loss = 23.1363525390625, avg loss = 23.1363525390625
client 5, data condensation 7400, total loss = 5.328125, avg loss = 5.328125
client 5, data condensation 7600, total loss = 2.46685791015625, avg loss = 2.46685791015625
client 5, data condensation 7800, total loss = 3.44482421875, avg loss = 3.44482421875
client 5, data condensation 8000, total loss = 8.10943603515625, avg loss = 8.10943603515625
client 5, data condensation 8200, total loss = 4.8426513671875, avg loss = 4.8426513671875
client 5, data condensation 8400, total loss = 1.6641845703125, avg loss = 1.6641845703125
client 5, data condensation 8600, total loss = 5.03570556640625, avg loss = 5.03570556640625
client 5, data condensation 8800, total loss = 4.28350830078125, avg loss = 4.28350830078125
client 5, data condensation 9000, total loss = 2.3043212890625, avg loss = 2.3043212890625
client 5, data condensation 9200, total loss = 6.77734375, avg loss = 6.77734375
client 5, data condensation 9400, total loss = 4.1502685546875, avg loss = 4.1502685546875
client 5, data condensation 9600, total loss = 10.84759521484375, avg loss = 10.84759521484375
client 5, data condensation 9800, total loss = 2.65435791015625, avg loss = 2.65435791015625
client 5, data condensation 10000, total loss = 1.736328125, avg loss = 1.736328125
Round 3, client 5 condense time: 444.0612518787384
client 5, class 8 have 4999 samples
total 24576.0MB, used 2439.06MB, free 22136.94MB
total 24576.0MB, used 2439.06MB, free 22136.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1498  221  175  142  134  153  139  171  272 1460], bin edged: [0.00016504 0.00017785 0.00019066 0.00020347 0.00021627 0.00022908
 0.00024189 0.0002547  0.00026751 0.00028032 0.00029313]
class 6 have 3914 samples, histogram: [2247  208  130   86   91   76   93  118  127  738], bin edged: [0.00020696 0.00022302 0.00023908 0.00025514 0.00027121 0.00028727
 0.00030333 0.0003194  0.00033546 0.00035152 0.00036758]
client 6, data condensation 0, total loss = 98.25823974609375, avg loss = 49.129119873046875
client 6, data condensation 200, total loss = 14.364013671875, avg loss = 7.1820068359375
client 6, data condensation 400, total loss = 6.32989501953125, avg loss = 3.164947509765625
client 6, data condensation 600, total loss = 84.93197631835938, avg loss = 42.46598815917969
client 6, data condensation 800, total loss = 20.55889892578125, avg loss = 10.279449462890625
client 6, data condensation 1000, total loss = 10.9810791015625, avg loss = 5.49053955078125
client 6, data condensation 1200, total loss = 7.4619140625, avg loss = 3.73095703125
client 6, data condensation 1400, total loss = 43.401458740234375, avg loss = 21.700729370117188
client 6, data condensation 1600, total loss = 9.1854248046875, avg loss = 4.59271240234375
client 6, data condensation 1800, total loss = 8.0767822265625, avg loss = 4.03839111328125
client 6, data condensation 2000, total loss = 6.7158203125, avg loss = 3.35791015625
client 6, data condensation 2200, total loss = 4.19757080078125, avg loss = 2.098785400390625
client 6, data condensation 2400, total loss = 8.78192138671875, avg loss = 4.390960693359375
client 6, data condensation 2600, total loss = 11.34173583984375, avg loss = 5.670867919921875
client 6, data condensation 2800, total loss = 5.674560546875, avg loss = 2.8372802734375
client 6, data condensation 3000, total loss = 10.876953125, avg loss = 5.4384765625
client 6, data condensation 3200, total loss = 8.81011962890625, avg loss = 4.405059814453125
client 6, data condensation 3400, total loss = 6.6318359375, avg loss = 3.31591796875
client 6, data condensation 3600, total loss = 7.877197265625, avg loss = 3.9385986328125
client 6, data condensation 3800, total loss = 6.27093505859375, avg loss = 3.135467529296875
client 6, data condensation 4000, total loss = 7.582275390625, avg loss = 3.7911376953125
client 6, data condensation 4200, total loss = 12.74066162109375, avg loss = 6.370330810546875
client 6, data condensation 4400, total loss = 5.25311279296875, avg loss = 2.626556396484375
client 6, data condensation 4600, total loss = 5.2081298828125, avg loss = 2.60406494140625
client 6, data condensation 4800, total loss = 5.4217529296875, avg loss = 2.71087646484375
client 6, data condensation 5000, total loss = 7.38702392578125, avg loss = 3.693511962890625
client 6, data condensation 5200, total loss = 5.21002197265625, avg loss = 2.605010986328125
client 6, data condensation 5400, total loss = 6.5850830078125, avg loss = 3.29254150390625
client 6, data condensation 5600, total loss = 15.20806884765625, avg loss = 7.604034423828125
client 6, data condensation 5800, total loss = 9.537841796875, avg loss = 4.7689208984375
client 6, data condensation 6000, total loss = 5.0166015625, avg loss = 2.50830078125
client 6, data condensation 6200, total loss = 9.32958984375, avg loss = 4.664794921875
client 6, data condensation 6400, total loss = 16.21697998046875, avg loss = 8.108489990234375
client 6, data condensation 6600, total loss = 6.5501708984375, avg loss = 3.27508544921875
client 6, data condensation 6800, total loss = 5.46673583984375, avg loss = 2.733367919921875
client 6, data condensation 7000, total loss = 7.0384521484375, avg loss = 3.51922607421875
client 6, data condensation 7200, total loss = 5.1900634765625, avg loss = 2.59503173828125
client 6, data condensation 7400, total loss = 8.62158203125, avg loss = 4.310791015625
client 6, data condensation 7600, total loss = 17.7608642578125, avg loss = 8.88043212890625
client 6, data condensation 7800, total loss = 7.515869140625, avg loss = 3.7579345703125
client 6, data condensation 8000, total loss = 8.4005126953125, avg loss = 4.20025634765625
client 6, data condensation 8200, total loss = 5.03125, avg loss = 2.515625
client 6, data condensation 8400, total loss = 27.74896240234375, avg loss = 13.874481201171875
client 6, data condensation 8600, total loss = 6.49273681640625, avg loss = 3.246368408203125
client 6, data condensation 8800, total loss = 9.4962158203125, avg loss = 4.74810791015625
client 6, data condensation 9000, total loss = 4.79730224609375, avg loss = 2.398651123046875
client 6, data condensation 9200, total loss = 5.62701416015625, avg loss = 2.813507080078125
client 6, data condensation 9400, total loss = 7.2332763671875, avg loss = 3.61663818359375
client 6, data condensation 9600, total loss = 7.9434814453125, avg loss = 3.97174072265625
client 6, data condensation 9800, total loss = 9.1966552734375, avg loss = 4.59832763671875
client 6, data condensation 10000, total loss = 20.6845703125, avg loss = 10.34228515625
Round 3, client 6 condense time: 929.4325244426727
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [2855  222  164  129   95  100   97   95  137  711], bin edged: [0.00018067 0.00019469 0.00020872 0.00022274 0.00023676 0.00025078
 0.00026481 0.00027883 0.00029285 0.00030687 0.0003209 ]
class 3 have 4999 samples, histogram: [1197  329  237  167  192  199  183  242  341 1912], bin edged: [0.00013848 0.00014923 0.00015997 0.00017072 0.00018147 0.00019222
 0.00020297 0.00021371 0.00022446 0.00023521 0.00024596]
client 7, data condensation 0, total loss = 63.0634765625, avg loss = 31.53173828125
client 7, data condensation 200, total loss = 20.37554931640625, avg loss = 10.187774658203125
client 7, data condensation 400, total loss = 7.385986328125, avg loss = 3.6929931640625
client 7, data condensation 600, total loss = 5.37786865234375, avg loss = 2.688934326171875
client 7, data condensation 800, total loss = 6.28436279296875, avg loss = 3.142181396484375
client 7, data condensation 1000, total loss = 36.384765625, avg loss = 18.1923828125
client 7, data condensation 1200, total loss = 6.4461669921875, avg loss = 3.22308349609375
client 7, data condensation 1400, total loss = 6.9879150390625, avg loss = 3.49395751953125
client 7, data condensation 1600, total loss = 6.8968505859375, avg loss = 3.44842529296875
client 7, data condensation 1800, total loss = 12.318115234375, avg loss = 6.1590576171875
client 7, data condensation 2000, total loss = 11.76763916015625, avg loss = 5.883819580078125
client 7, data condensation 2200, total loss = 6.09686279296875, avg loss = 3.048431396484375
client 7, data condensation 2400, total loss = 6.92010498046875, avg loss = 3.460052490234375
client 7, data condensation 2600, total loss = 49.24676513671875, avg loss = 24.623382568359375
client 7, data condensation 2800, total loss = 8.08294677734375, avg loss = 4.041473388671875
client 7, data condensation 3000, total loss = 17.87506103515625, avg loss = 8.937530517578125
client 7, data condensation 3200, total loss = 11.80755615234375, avg loss = 5.903778076171875
client 7, data condensation 3400, total loss = 5.91009521484375, avg loss = 2.955047607421875
client 7, data condensation 3600, total loss = 5.13153076171875, avg loss = 2.565765380859375
client 7, data condensation 3800, total loss = 12.096923828125, avg loss = 6.0484619140625
client 7, data condensation 4000, total loss = 10.73272705078125, avg loss = 5.366363525390625
client 7, data condensation 4200, total loss = 13.3619384765625, avg loss = 6.68096923828125
client 7, data condensation 4400, total loss = 7.8741455078125, avg loss = 3.93707275390625
client 7, data condensation 4600, total loss = 9.2686767578125, avg loss = 4.63433837890625
client 7, data condensation 4800, total loss = 26.9837646484375, avg loss = 13.49188232421875
client 7, data condensation 5000, total loss = 8.4822998046875, avg loss = 4.24114990234375
client 7, data condensation 5200, total loss = 5.94769287109375, avg loss = 2.973846435546875
client 7, data condensation 5400, total loss = 6.08294677734375, avg loss = 3.041473388671875
client 7, data condensation 5600, total loss = 6.26129150390625, avg loss = 3.130645751953125
client 7, data condensation 5800, total loss = 21.0181884765625, avg loss = 10.50909423828125
client 7, data condensation 6000, total loss = 7.076904296875, avg loss = 3.5384521484375
client 7, data condensation 6200, total loss = 4.621826171875, avg loss = 2.3109130859375
client 7, data condensation 6400, total loss = 10.76959228515625, avg loss = 5.384796142578125
client 7, data condensation 6600, total loss = 28.077850341796875, avg loss = 14.038925170898438
client 7, data condensation 6800, total loss = 6.84625244140625, avg loss = 3.423126220703125
client 7, data condensation 7000, total loss = 14.10186767578125, avg loss = 7.050933837890625
client 7, data condensation 7200, total loss = 18.4884033203125, avg loss = 9.24420166015625
client 7, data condensation 7400, total loss = 17.469482421875, avg loss = 8.7347412109375
client 7, data condensation 7600, total loss = 11.89190673828125, avg loss = 5.945953369140625
client 7, data condensation 7800, total loss = 12.16424560546875, avg loss = 6.082122802734375
client 7, data condensation 8000, total loss = 6.58697509765625, avg loss = 3.293487548828125
client 7, data condensation 8200, total loss = 18.3546142578125, avg loss = 9.17730712890625
client 7, data condensation 8400, total loss = 22.43231201171875, avg loss = 11.216156005859375
client 7, data condensation 8600, total loss = 9.930908203125, avg loss = 4.9654541015625
client 7, data condensation 8800, total loss = 10.1690673828125, avg loss = 5.08453369140625
client 7, data condensation 9000, total loss = 8.2724609375, avg loss = 4.13623046875
client 7, data condensation 9200, total loss = 5.77459716796875, avg loss = 2.887298583984375
client 7, data condensation 9400, total loss = 17.1920166015625, avg loss = 8.59600830078125
client 7, data condensation 9600, total loss = 6.300048828125, avg loss = 3.1500244140625
client 7, data condensation 9800, total loss = 16.73162841796875, avg loss = 8.365814208984375
client 7, data condensation 10000, total loss = 7.2860107421875, avg loss = 3.64300537109375
Round 3, client 7 condense time: 986.8523914813995
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [227  20   9   6   6   6  10  11  10  59], bin edged: [0.00227615 0.00245282 0.00262948 0.00280614 0.0029828  0.00315946
 0.00333612 0.00351278 0.00368944 0.00386611 0.00404277]
class 5 have 135 samples, histogram: [49  5  5  4  0  4  8  6  8 46], bin edged: [0.00532758 0.00574108 0.00615457 0.00656807 0.00698156 0.00739506
 0.00780855 0.00822205 0.00863554 0.00904904 0.00946253]
class 9 have 4727 samples, histogram: [2821  249  144  126   88   97   99   96  156  851], bin edged: [0.00017324 0.00018669 0.00020013 0.00021358 0.00022703 0.00024047
 0.00025392 0.00026736 0.00028081 0.00029426 0.0003077 ]
client 8, data condensation 0, total loss = 113.58709716796875, avg loss = 37.86236572265625
client 8, data condensation 200, total loss = 43.49102783203125, avg loss = 14.49700927734375
client 8, data condensation 400, total loss = 28.69879150390625, avg loss = 9.566263834635416
client 8, data condensation 600, total loss = 36.69683837890625, avg loss = 12.232279459635416
client 8, data condensation 800, total loss = 26.57965087890625, avg loss = 8.859883626302084
client 8, data condensation 1000, total loss = 29.89178466796875, avg loss = 9.96392822265625
client 8, data condensation 1200, total loss = 34.35760498046875, avg loss = 11.452534993489584
client 8, data condensation 1400, total loss = 43.30450439453125, avg loss = 14.434834798177084
client 8, data condensation 1600, total loss = 36.8253173828125, avg loss = 12.275105794270834
client 8, data condensation 1800, total loss = 39.67193603515625, avg loss = 13.223978678385416
client 8, data condensation 2000, total loss = 48.203125, avg loss = 16.067708333333332
client 8, data condensation 2200, total loss = 36.5677490234375, avg loss = 12.189249674479166
client 8, data condensation 2400, total loss = 60.483673095703125, avg loss = 20.161224365234375
client 8, data condensation 2600, total loss = 51.94677734375, avg loss = 17.315592447916668
client 8, data condensation 2800, total loss = 44.9022216796875, avg loss = 14.9674072265625
client 8, data condensation 3000, total loss = 157.7445068359375, avg loss = 52.581502278645836
client 8, data condensation 3200, total loss = 39.85833740234375, avg loss = 13.286112467447916
client 8, data condensation 3400, total loss = 29.90191650390625, avg loss = 9.967305501302084
client 8, data condensation 3600, total loss = 121.9554443359375, avg loss = 40.651814778645836
client 8, data condensation 3800, total loss = 46.829681396484375, avg loss = 15.609893798828125
client 8, data condensation 4000, total loss = 88.01959228515625, avg loss = 29.339864095052082
client 8, data condensation 4200, total loss = 34.8038330078125, avg loss = 11.601277669270834
client 8, data condensation 4400, total loss = 35.0797119140625, avg loss = 11.6932373046875
client 8, data condensation 4600, total loss = 37.41790771484375, avg loss = 12.472635904947916
client 8, data condensation 4800, total loss = 28.00579833984375, avg loss = 9.33526611328125
client 8, data condensation 5000, total loss = 52.09197998046875, avg loss = 17.363993326822918
client 8, data condensation 5200, total loss = 67.80389404296875, avg loss = 22.601298014322918
client 8, data condensation 5400, total loss = 48.5247802734375, avg loss = 16.1749267578125
client 8, data condensation 5600, total loss = 31.16943359375, avg loss = 10.389811197916666
client 8, data condensation 5800, total loss = 37.8994140625, avg loss = 12.633138020833334
client 8, data condensation 6000, total loss = 25.399658203125, avg loss = 8.466552734375
client 8, data condensation 6200, total loss = 31.33087158203125, avg loss = 10.443623860677084
client 8, data condensation 6400, total loss = 53.952239990234375, avg loss = 17.984079996744793
client 8, data condensation 6600, total loss = 34.1014404296875, avg loss = 11.367146809895834
client 8, data condensation 6800, total loss = 46.6380615234375, avg loss = 15.5460205078125
client 8, data condensation 7000, total loss = 22.8131103515625, avg loss = 7.6043701171875
client 8, data condensation 7200, total loss = 60.779052734375, avg loss = 20.259684244791668
client 8, data condensation 7400, total loss = 107.43914794921875, avg loss = 35.81304931640625
client 8, data condensation 7600, total loss = 16.7369384765625, avg loss = 5.5789794921875
client 8, data condensation 7800, total loss = 60.1962890625, avg loss = 20.0654296875
client 8, data condensation 8000, total loss = 30.47076416015625, avg loss = 10.15692138671875
client 8, data condensation 8200, total loss = 42.1845703125, avg loss = 14.0615234375
client 8, data condensation 8400, total loss = 202.98175048828125, avg loss = 67.66058349609375
client 8, data condensation 8600, total loss = 34.83154296875, avg loss = 11.610514322916666
client 8, data condensation 8800, total loss = 203.6685791015625, avg loss = 67.8895263671875
client 8, data condensation 9000, total loss = 102.43853759765625, avg loss = 34.14617919921875
client 8, data condensation 9200, total loss = 34.1446533203125, avg loss = 11.381551106770834
client 8, data condensation 9400, total loss = 20.73236083984375, avg loss = 6.910786946614583
client 8, data condensation 9600, total loss = 49.114288330078125, avg loss = 16.371429443359375
client 8, data condensation 9800, total loss = 79.14825439453125, avg loss = 26.38275146484375
client 8, data condensation 10000, total loss = 41.070068359375, avg loss = 13.690022786458334
Round 3, client 8 condense time: 1045.1624715328217
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3081.06MB, free 21494.94MB
total 24576.0MB, used 3081.06MB, free 21494.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [42  9  3  5  2  4  2  8  3 42], bin edged: [0.0060262  0.00649391 0.00696162 0.00742933 0.00789704 0.00836475
 0.00883245 0.00930016 0.00976787 0.01023558 0.01070329]
class 5 have 192 samples, histogram: [74 11  4  5  7  7 10  8 11 55], bin edged: [0.00383703 0.00413484 0.00443265 0.00473046 0.00502826 0.00532607
 0.00562388 0.00592169 0.0062195  0.0065173  0.00681511]
class 6 have 1075 samples, histogram: [651  53  44  31  21  26  18  20  31 180], bin edged: [0.00076914 0.00082884 0.00088854 0.00094823 0.00100793 0.00106762
 0.00112732 0.00118702 0.00124671 0.00130641 0.00136611]
client 9, data condensation 0, total loss = 197.27145385742188, avg loss = 65.7571512858073
client 9, data condensation 200, total loss = 61.7066650390625, avg loss = 20.568888346354168
client 9, data condensation 400, total loss = 39.64794921875, avg loss = 13.215983072916666
client 9, data condensation 600, total loss = 39.87274169921875, avg loss = 13.290913899739584
client 9, data condensation 800, total loss = 22.3306884765625, avg loss = 7.443562825520833
client 9, data condensation 1000, total loss = 42.163330078125, avg loss = 14.054443359375
client 9, data condensation 1200, total loss = 49.93524169921875, avg loss = 16.64508056640625
client 9, data condensation 1400, total loss = 44.1025390625, avg loss = 14.700846354166666
client 9, data condensation 1600, total loss = 39.2718505859375, avg loss = 13.090616861979166
client 9, data condensation 1800, total loss = 53.45709228515625, avg loss = 17.81903076171875
client 9, data condensation 2000, total loss = 92.38262939453125, avg loss = 30.794209798177082
client 9, data condensation 2200, total loss = 38.32489013671875, avg loss = 12.77496337890625
client 9, data condensation 2400, total loss = 36.59063720703125, avg loss = 12.196879069010416
client 9, data condensation 2600, total loss = 67.7010498046875, avg loss = 22.5670166015625
client 9, data condensation 2800, total loss = 104.51531982421875, avg loss = 34.83843994140625
client 9, data condensation 3000, total loss = 64.39892578125, avg loss = 21.46630859375
client 9, data condensation 3200, total loss = 64.716552734375, avg loss = 21.572184244791668
client 9, data condensation 3400, total loss = 49.066162109375, avg loss = 16.355387369791668
client 9, data condensation 3600, total loss = 53.41259765625, avg loss = 17.80419921875
client 9, data condensation 3800, total loss = 27.79876708984375, avg loss = 9.266255696614584
client 9, data condensation 4000, total loss = 233.93560791015625, avg loss = 77.97853597005208
client 9, data condensation 4200, total loss = 53.407318115234375, avg loss = 17.802439371744793
client 9, data condensation 4400, total loss = 59.730926513671875, avg loss = 19.910308837890625
client 9, data condensation 4600, total loss = 56.463287353515625, avg loss = 18.821095784505207
client 9, data condensation 4800, total loss = 46.9202880859375, avg loss = 15.640096028645834
client 9, data condensation 5000, total loss = 41.806396484375, avg loss = 13.935465494791666
client 9, data condensation 5200, total loss = 29.65374755859375, avg loss = 9.88458251953125
client 9, data condensation 5400, total loss = 50.467529296875, avg loss = 16.822509765625
client 9, data condensation 5600, total loss = 39.67181396484375, avg loss = 13.22393798828125
client 9, data condensation 5800, total loss = 70.75604248046875, avg loss = 23.585347493489582
client 9, data condensation 6000, total loss = 50.56195068359375, avg loss = 16.853983561197918
client 9, data condensation 6200, total loss = 37.6009521484375, avg loss = 12.533650716145834
client 9, data condensation 6400, total loss = 114.5089111328125, avg loss = 38.169637044270836
client 9, data condensation 6600, total loss = 28.3929443359375, avg loss = 9.464314778645834
client 9, data condensation 6800, total loss = 52.29052734375, avg loss = 17.43017578125
client 9, data condensation 7000, total loss = 119.85296630859375, avg loss = 39.95098876953125
client 9, data condensation 7200, total loss = 24.76605224609375, avg loss = 8.255350748697916
client 9, data condensation 7400, total loss = 42.1605224609375, avg loss = 14.053507486979166
client 9, data condensation 7600, total loss = 73.43707275390625, avg loss = 24.479024251302082
client 9, data condensation 7800, total loss = 49.53887939453125, avg loss = 16.512959798177082
client 9, data condensation 8000, total loss = 48.879364013671875, avg loss = 16.293121337890625
client 9, data condensation 8200, total loss = 88.09735107421875, avg loss = 29.36578369140625
client 9, data condensation 8400, total loss = 51.1129150390625, avg loss = 17.037638346354168
client 9, data condensation 8600, total loss = 31.81103515625, avg loss = 10.603678385416666
client 9, data condensation 8800, total loss = 60.57696533203125, avg loss = 20.19232177734375
client 9, data condensation 9000, total loss = 856.7009887695312, avg loss = 285.56699625651044
client 9, data condensation 9200, total loss = 98.63812255859375, avg loss = 32.879374186197914
client 9, data condensation 9400, total loss = 106.87353515625, avg loss = 35.62451171875
client 9, data condensation 9600, total loss = 136.17230224609375, avg loss = 45.390767415364586
client 9, data condensation 9800, total loss = 62.2667236328125, avg loss = 20.755574544270832
client 9, data condensation 10000, total loss = 35.70208740234375, avg loss = 11.90069580078125
Round 3, client 9 condense time: 935.2845079898834
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 13.2382,   0.4465,   3.1073,  -3.7473,  -0.9458,  -7.8949,  -7.3573,
          -2.9150,   5.5290,   0.9853],
        [  1.4692,  13.0722,  -4.2169,  -1.9756,  -3.0649,  -5.5969,  -3.4636,
          -1.3026,  -0.3898,   6.9210],
        [  0.7656,  -4.0106,   8.4579,   0.6614,   2.9953,   1.5018,   0.7654,
           0.6918,  -6.5654,  -4.0997],
        [ -2.9590,  -2.6519,   1.5073,   7.6755,  -0.1955,   4.9001,   2.8252,
           0.0229,  -7.4354,  -2.4936],
        [ -1.5608,  -3.4608,   4.3085,  -1.3422,   7.7890,   1.0454,   2.8599,
           3.3796,  -8.1460,  -3.8979],
        [ -4.9163,  -3.9223,   2.4886,   6.4735,   0.0881,   9.5541,   1.4137,
           2.0596,  -8.2737,  -3.7457],
        [ -3.5887,  -1.6177,   2.6843,   2.8729,   3.0820,   1.5681,  11.5655,
          -1.4147, -10.0025,  -3.6589],
        [ -2.7181,  -2.8482,   0.9247,  -0.5344,   3.4488,   1.0903,  -0.5544,
          10.8588,  -7.8173,  -0.2929],
        [  8.5598,   3.4326,  -0.8584,  -3.2070,  -3.1855,  -7.4489,  -8.2852,
          -4.4533,  11.9668,   4.2090],
        [  0.3494,   5.9740,  -3.6391,  -2.3536,  -2.5496,  -5.3039,  -3.8574,
           0.9958,  -0.2826,  12.0003]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 5, 0, 6],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 5, 2, 9],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
4
Round 3: # synthetic sample: 4048
total 24576.0MB, used 3079.06MB, free 21496.94MB
{0: {0: 643, 1: 39, 2: 55, 3: 35, 4: 12, 5: 7, 6: 14, 7: 13, 8: 118, 9: 64}, 1: {0: 40, 1: 706, 2: 9, 3: 21, 4: 12, 5: 10, 6: 19, 7: 21, 8: 29, 9: 133}, 2: {0: 111, 1: 14, 2: 415, 3: 90, 4: 88, 5: 76, 6: 94, 7: 66, 8: 24, 9: 22}, 3: {0: 29, 1: 17, 2: 69, 3: 441, 4: 46, 5: 154, 6: 129, 7: 47, 8: 12, 9: 56}, 4: {0: 56, 1: 9, 2: 114, 3: 61, 4: 418, 5: 40, 6: 117, 7: 139, 8: 27, 9: 19}, 5: {0: 24, 1: 10, 2: 62, 3: 210, 4: 61, 5: 469, 6: 62, 7: 63, 8: 17, 9: 22}, 6: {0: 11, 1: 7, 2: 57, 3: 67, 4: 57, 5: 19, 6: 744, 7: 19, 8: 5, 9: 14}, 7: {0: 23, 1: 14, 2: 36, 3: 76, 4: 50, 5: 84, 6: 27, 7: 638, 8: 5, 9: 47}, 8: {0: 175, 1: 99, 2: 20, 3: 30, 4: 5, 5: 6, 6: 10, 7: 13, 8: 531, 9: 111}, 9: {0: 46, 1: 118, 2: 5, 3: 20, 4: 12, 5: 14, 6: 19, 7: 16, 8: 39, 9: 711}}
round 3 evaluation: test acc is 0.5716, test loss = 2.413113
{0: {0: 559, 1: 45, 2: 33, 3: 22, 4: 14, 5: 13, 6: 17, 7: 26, 8: 190, 9: 81}, 1: {0: 34, 1: 583, 2: 6, 3: 3, 4: 13, 5: 25, 6: 12, 7: 17, 8: 62, 9: 245}, 2: {0: 127, 1: 12, 2: 207, 3: 57, 4: 148, 5: 132, 6: 141, 7: 85, 8: 46, 9: 45}, 3: {0: 33, 1: 31, 2: 30, 3: 167, 4: 35, 5: 334, 6: 149, 7: 65, 8: 38, 9: 118}, 4: {0: 53, 1: 10, 2: 38, 3: 18, 4: 390, 5: 73, 6: 185, 7: 146, 8: 46, 9: 41}, 5: {0: 20, 1: 10, 2: 24, 3: 88, 4: 47, 5: 589, 6: 80, 7: 73, 8: 33, 9: 36}, 6: {0: 13, 1: 7, 2: 19, 3: 31, 4: 49, 5: 40, 6: 754, 7: 23, 8: 18, 9: 46}, 7: {0: 18, 1: 14, 2: 21, 3: 19, 4: 30, 5: 124, 6: 36, 7: 626, 8: 22, 9: 90}, 8: {0: 101, 1: 108, 2: 4, 3: 11, 4: 5, 5: 14, 6: 12, 7: 10, 8: 622, 9: 113}, 9: {0: 32, 1: 78, 2: 4, 3: 7, 4: 6, 5: 20, 6: 10, 7: 10, 8: 59, 9: 774}}
epoch 0, train loss avg now = 0.183760, train contrast loss now = 1.001901, test acc now = 0.5271, test loss now = 3.072792
{0: {0: 621, 1: 22, 2: 53, 3: 33, 4: 27, 5: 7, 6: 15, 7: 14, 8: 133, 9: 75}, 1: {0: 51, 1: 582, 2: 9, 3: 14, 4: 16, 5: 17, 6: 20, 7: 28, 8: 26, 9: 237}, 2: {0: 101, 1: 6, 2: 418, 3: 79, 4: 116, 5: 70, 6: 92, 7: 72, 8: 19, 9: 27}, 3: {0: 21, 1: 9, 2: 66, 3: 408, 4: 67, 5: 188, 6: 113, 7: 48, 8: 14, 9: 66}, 4: {0: 42, 1: 4, 2: 77, 3: 37, 4: 519, 5: 46, 6: 100, 7: 128, 8: 17, 9: 30}, 5: {0: 14, 1: 7, 2: 67, 3: 180, 4: 67, 5: 511, 6: 47, 7: 69, 8: 14, 9: 24}, 6: {0: 6, 1: 3, 2: 60, 3: 75, 4: 60, 5: 24, 6: 734, 7: 12, 8: 3, 9: 23}, 7: {0: 17, 1: 7, 2: 40, 3: 75, 4: 54, 5: 85, 6: 23, 7: 630, 8: 9, 9: 60}, 8: {0: 134, 1: 56, 2: 13, 3: 25, 4: 9, 5: 9, 6: 13, 7: 14, 8: 606, 9: 121}, 9: {0: 41, 1: 60, 2: 5, 3: 19, 4: 11, 5: 11, 6: 16, 7: 15, 8: 34, 9: 788}}
epoch 100, train loss avg now = 0.030775, train contrast loss now = 0.248352, test acc now = 0.5817, test loss now = 2.142913
{0: {0: 641, 1: 41, 2: 55, 3: 22, 4: 21, 5: 12, 6: 12, 7: 16, 8: 117, 9: 63}, 1: {0: 35, 1: 742, 2: 8, 3: 12, 4: 8, 5: 13, 6: 16, 7: 16, 8: 30, 9: 120}, 2: {0: 113, 1: 15, 2: 369, 3: 59, 4: 143, 5: 97, 6: 95, 7: 68, 8: 20, 9: 21}, 3: {0: 27, 1: 19, 2: 64, 3: 313, 4: 58, 5: 271, 6: 126, 7: 55, 8: 13, 9: 54}, 4: {0: 50, 1: 11, 2: 61, 3: 29, 4: 528, 5: 57, 6: 106, 7: 123, 8: 20, 9: 15}, 5: {0: 21, 1: 10, 2: 39, 3: 106, 4: 75, 5: 584, 6: 58, 7: 75, 8: 18, 9: 14}, 6: {0: 8, 1: 7, 2: 43, 3: 56, 4: 66, 5: 33, 6: 752, 7: 12, 8: 6, 9: 17}, 7: {0: 20, 1: 13, 2: 28, 3: 40, 4: 65, 5: 100, 6: 29, 7: 662, 8: 6, 9: 37}, 8: {0: 141, 1: 92, 2: 15, 3: 19, 4: 8, 5: 13, 6: 11, 7: 14, 8: 584, 9: 103}, 9: {0: 38, 1: 118, 2: 7, 3: 11, 4: 10, 5: 15, 6: 18, 7: 16, 8: 40, 9: 727}}
epoch 200, train loss avg now = 0.021561, train contrast loss now = 0.242327, test acc now = 0.5902, test loss now = 2.196432
{0: {0: 645, 1: 35, 2: 64, 3: 18, 4: 10, 5: 10, 6: 16, 7: 14, 8: 140, 9: 48}, 1: {0: 48, 1: 751, 2: 7, 3: 6, 4: 10, 5: 12, 6: 14, 7: 19, 8: 35, 9: 98}, 2: {0: 127, 1: 13, 2: 460, 3: 43, 4: 88, 5: 48, 6: 95, 7: 81, 8: 27, 9: 18}, 3: {0: 48, 1: 28, 2: 82, 3: 280, 4: 50, 5: 192, 6: 167, 7: 85, 8: 25, 9: 43}, 4: {0: 66, 1: 11, 2: 110, 3: 22, 4: 415, 5: 30, 6: 124, 7: 178, 8: 30, 9: 14}, 5: {0: 25, 1: 10, 2: 81, 3: 119, 4: 48, 5: 487, 6: 71, 7: 113, 8: 30, 9: 16}, 6: {0: 12, 1: 15, 2: 70, 3: 33, 4: 47, 5: 16, 6: 761, 7: 23, 8: 10, 9: 13}, 7: {0: 28, 1: 12, 2: 40, 3: 41, 4: 32, 5: 55, 6: 30, 7: 716, 8: 14, 9: 32}, 8: {0: 151, 1: 82, 2: 16, 3: 14, 4: 4, 5: 7, 6: 10, 7: 16, 8: 639, 9: 61}, 9: {0: 47, 1: 141, 2: 5, 3: 10, 4: 9, 5: 9, 6: 16, 7: 23, 8: 62, 9: 678}}
epoch 300, train loss avg now = 0.019007, train contrast loss now = 0.240692, test acc now = 0.5832, test loss now = 2.328513
{0: {0: 708, 1: 38, 2: 61, 3: 17, 4: 9, 5: 14, 6: 5, 7: 13, 8: 79, 9: 56}, 1: {0: 54, 1: 762, 2: 8, 3: 12, 4: 9, 5: 11, 6: 7, 7: 24, 8: 18, 9: 95}, 2: {0: 137, 1: 12, 2: 469, 3: 76, 4: 72, 5: 56, 6: 79, 7: 66, 8: 17, 9: 16}, 3: {0: 45, 1: 27, 2: 91, 3: 417, 4: 40, 5: 183, 6: 87, 7: 51, 8: 11, 9: 48}, 4: {0: 71, 1: 12, 2: 136, 3: 56, 4: 413, 5: 40, 6: 100, 7: 142, 8: 15, 9: 15}, 5: {0: 30, 1: 12, 2: 73, 3: 183, 4: 42, 5: 520, 6: 42, 7: 70, 8: 13, 9: 15}, 6: {0: 10, 1: 9, 2: 74, 3: 88, 4: 51, 5: 29, 6: 703, 7: 14, 8: 6, 9: 16}, 7: {0: 33, 1: 18, 2: 50, 3: 58, 4: 31, 5: 78, 6: 17, 7: 672, 8: 8, 9: 35}, 8: {0: 248, 1: 95, 2: 17, 3: 20, 4: 5, 5: 8, 6: 8, 7: 11, 8: 501, 9: 87}, 9: {0: 56, 1: 141, 2: 10, 3: 14, 4: 6, 5: 17, 6: 11, 7: 19, 8: 36, 9: 690}}
epoch 400, train loss avg now = 0.012475, train contrast loss now = 0.239278, test acc now = 0.5855, test loss now = 2.373078
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 639, 1: 38, 2: 62, 3: 34, 4: 17, 5: 10, 6: 9, 7: 13, 8: 119, 9: 59}, 1: {0: 40, 1: 730, 2: 9, 3: 13, 4: 9, 5: 14, 6: 11, 7: 13, 8: 29, 9: 132}, 2: {0: 112, 1: 13, 2: 436, 3: 77, 4: 124, 5: 64, 6: 73, 7: 60, 8: 20, 9: 21}, 3: {0: 25, 1: 16, 2: 91, 3: 422, 4: 61, 5: 179, 6: 102, 7: 40, 8: 15, 9: 49}, 4: {0: 51, 1: 9, 2: 103, 3: 53, 4: 513, 5: 35, 6: 89, 7: 109, 8: 19, 9: 19}, 5: {0: 26, 1: 8, 2: 69, 3: 184, 4: 64, 5: 509, 6: 45, 7: 58, 8: 20, 9: 17}, 6: {0: 8, 1: 6, 2: 63, 3: 71, 4: 84, 5: 23, 6: 710, 7: 12, 8: 5, 9: 18}, 7: {0: 20, 1: 13, 2: 46, 3: 76, 4: 54, 5: 76, 6: 21, 7: 637, 8: 9, 9: 48}, 8: {0: 154, 1: 83, 2: 17, 3: 27, 4: 6, 5: 7, 6: 10, 7: 10, 8: 583, 9: 103}, 9: {0: 43, 1: 108, 2: 7, 3: 17, 4: 9, 5: 18, 6: 12, 7: 16, 8: 40, 9: 730}}
epoch 500, train loss avg now = 0.010439, train contrast loss now = 0.238760, test acc now = 0.5909, test loss now = 2.340213
{0: {0: 684, 1: 33, 2: 48, 3: 29, 4: 15, 5: 12, 6: 11, 7: 15, 8: 91, 9: 62}, 1: {0: 48, 1: 714, 2: 7, 3: 11, 4: 9, 5: 13, 6: 12, 7: 16, 8: 19, 9: 151}, 2: {0: 135, 1: 11, 2: 403, 3: 92, 4: 98, 5: 70, 6: 84, 7: 68, 8: 17, 9: 22}, 3: {0: 36, 1: 19, 2: 76, 3: 415, 4: 43, 5: 193, 6: 116, 7: 44, 8: 9, 9: 49}, 4: {0: 61, 1: 10, 2: 94, 3: 56, 4: 447, 5: 42, 6: 124, 7: 127, 8: 17, 9: 22}, 5: {0: 28, 1: 10, 2: 52, 3: 187, 4: 57, 5: 512, 6: 57, 7: 68, 8: 13, 9: 16}, 6: {0: 9, 1: 7, 2: 56, 3: 70, 4: 54, 5: 23, 6: 751, 7: 11, 8: 5, 9: 14}, 7: {0: 27, 1: 14, 2: 36, 3: 69, 4: 46, 5: 73, 6: 30, 7: 651, 8: 7, 9: 47}, 8: {0: 188, 1: 88, 2: 16, 3: 30, 4: 5, 5: 8, 6: 10, 7: 9, 8: 530, 9: 116}, 9: {0: 41, 1: 105, 2: 5, 3: 18, 4: 7, 5: 14, 6: 14, 7: 16, 8: 33, 9: 747}}
epoch 600, train loss avg now = 0.005859, train contrast loss now = 0.237517, test acc now = 0.5854, test loss now = 2.322507
{0: {0: 681, 1: 33, 2: 47, 3: 30, 4: 17, 5: 14, 6: 11, 7: 14, 8: 89, 9: 64}, 1: {0: 49, 1: 725, 2: 7, 3: 11, 4: 9, 5: 13, 6: 12, 7: 17, 8: 16, 9: 141}, 2: {0: 128, 1: 11, 2: 404, 3: 94, 4: 101, 5: 71, 6: 84, 7: 68, 8: 18, 9: 21}, 3: {0: 29, 1: 20, 2: 74, 3: 413, 4: 45, 5: 201, 6: 118, 7: 44, 8: 10, 9: 46}, 4: {0: 61, 1: 11, 2: 90, 3: 58, 4: 451, 5: 42, 6: 120, 7: 131, 8: 14, 9: 22}, 5: {0: 26, 1: 10, 2: 52, 3: 178, 4: 57, 5: 524, 6: 55, 7: 70, 8: 13, 9: 15}, 6: {0: 9, 1: 7, 2: 54, 3: 71, 4: 61, 5: 28, 6: 741, 7: 9, 8: 5, 9: 15}, 7: {0: 25, 1: 16, 2: 34, 3: 62, 4: 44, 5: 82, 6: 26, 7: 661, 8: 8, 9: 42}, 8: {0: 183, 1: 96, 2: 16, 3: 28, 4: 4, 5: 9, 6: 12, 7: 17, 8: 528, 9: 107}, 9: {0: 46, 1: 108, 2: 7, 3: 17, 4: 7, 5: 14, 6: 12, 7: 20, 8: 30, 9: 739}}
epoch 700, train loss avg now = 0.007158, train contrast loss now = 0.237310, test acc now = 0.5867, test loss now = 2.335067
{0: {0: 680, 1: 35, 2: 46, 3: 28, 4: 17, 5: 11, 6: 11, 7: 15, 8: 93, 9: 64}, 1: {0: 48, 1: 718, 2: 7, 3: 12, 4: 9, 5: 13, 6: 12, 7: 16, 8: 17, 9: 148}, 2: {0: 138, 1: 10, 2: 410, 3: 89, 4: 91, 5: 72, 6: 81, 7: 69, 8: 18, 9: 22}, 3: {0: 37, 1: 20, 2: 73, 3: 388, 4: 46, 5: 206, 6: 116, 7: 49, 8: 10, 9: 55}, 4: {0: 63, 1: 10, 2: 95, 3: 52, 4: 446, 5: 40, 6: 114, 7: 143, 8: 15, 9: 22}, 5: {0: 28, 1: 10, 2: 57, 3: 167, 4: 52, 5: 527, 6: 51, 7: 75, 8: 13, 9: 20}, 6: {0: 7, 1: 7, 2: 60, 3: 70, 4: 60, 5: 26, 6: 737, 7: 11, 8: 5, 9: 17}, 7: {0: 26, 1: 15, 2: 37, 3: 60, 4: 41, 5: 80, 6: 27, 7: 662, 8: 7, 9: 45}, 8: {0: 190, 1: 92, 2: 16, 3: 23, 4: 4, 5: 8, 6: 11, 7: 10, 8: 533, 9: 113}, 9: {0: 45, 1: 106, 2: 7, 3: 17, 4: 7, 5: 12, 6: 11, 7: 19, 8: 29, 9: 747}}
epoch 800, train loss avg now = 0.004539, train contrast loss now = 0.237618, test acc now = 0.5848, test loss now = 2.326516
{0: {0: 670, 1: 35, 2: 50, 3: 27, 4: 17, 5: 12, 6: 11, 7: 17, 8: 95, 9: 66}, 1: {0: 47, 1: 714, 2: 7, 3: 11, 4: 8, 5: 13, 6: 13, 7: 14, 8: 16, 9: 157}, 2: {0: 128, 1: 13, 2: 412, 3: 92, 4: 96, 5: 62, 6: 85, 7: 68, 8: 19, 9: 25}, 3: {0: 32, 1: 19, 2: 73, 3: 422, 4: 47, 5: 181, 6: 116, 7: 47, 8: 10, 9: 53}, 4: {0: 61, 1: 11, 2: 94, 3: 54, 4: 446, 5: 35, 6: 128, 7: 133, 8: 16, 9: 22}, 5: {0: 26, 1: 11, 2: 56, 3: 187, 4: 54, 5: 502, 6: 59, 7: 75, 8: 13, 9: 17}, 6: {0: 9, 1: 9, 2: 60, 3: 73, 4: 56, 5: 23, 6: 741, 7: 9, 8: 4, 9: 16}, 7: {0: 25, 1: 15, 2: 39, 3: 60, 4: 41, 5: 74, 6: 32, 7: 660, 8: 7, 9: 47}, 8: {0: 190, 1: 93, 2: 18, 3: 25, 4: 4, 5: 8, 6: 11, 7: 14, 8: 523, 9: 114}, 9: {0: 40, 1: 98, 2: 6, 3: 17, 4: 7, 5: 10, 6: 15, 7: 19, 8: 32, 9: 756}}
epoch 900, train loss avg now = 0.003127, train contrast loss now = 0.237616, test acc now = 0.5846, test loss now = 2.351988
{0: {0: 682, 1: 34, 2: 45, 3: 31, 4: 18, 5: 12, 6: 12, 7: 16, 8: 89, 9: 61}, 1: {0: 50, 1: 721, 2: 6, 3: 12, 4: 10, 5: 13, 6: 12, 7: 17, 8: 17, 9: 142}, 2: {0: 132, 1: 10, 2: 402, 3: 94, 4: 95, 5: 66, 6: 89, 7: 70, 8: 19, 9: 23}, 3: {0: 33, 1: 19, 2: 71, 3: 413, 4: 44, 5: 199, 6: 118, 7: 45, 8: 10, 9: 48}, 4: {0: 62, 1: 12, 2: 95, 3: 56, 4: 449, 5: 36, 6: 121, 7: 133, 8: 16, 9: 20}, 5: {0: 27, 1: 11, 2: 57, 3: 179, 4: 53, 5: 518, 6: 59, 7: 68, 8: 13, 9: 15}, 6: {0: 8, 1: 8, 2: 56, 3: 66, 4: 61, 5: 23, 6: 746, 7: 11, 8: 5, 9: 16}, 7: {0: 27, 1: 13, 2: 36, 3: 62, 4: 41, 5: 76, 6: 29, 7: 665, 8: 7, 9: 44}, 8: {0: 193, 1: 97, 2: 18, 3: 25, 4: 5, 5: 8, 6: 12, 7: 14, 8: 520, 9: 108}, 9: {0: 44, 1: 108, 2: 5, 3: 21, 4: 6, 5: 12, 6: 14, 7: 20, 8: 30, 9: 740}}
epoch 1000, train loss avg now = 0.004586, train contrast loss now = 0.237428, test acc now = 0.5856, test loss now = 2.383208
epoch avg loss = 4.5859283287254836e-06, total time = 9497.724417448044
total 24576.0MB, used 3079.06MB, free 21496.94MB
Round 3 finish, update the prev_syn_proto
torch.Size([404, 3, 32, 32])
torch.Size([404, 3, 32, 32])
torch.Size([416, 3, 32, 32])
torch.Size([400, 3, 32, 32])
torch.Size([400, 3, 32, 32])
torch.Size([420, 3, 32, 32])
torch.Size([404, 3, 32, 32])
torch.Size([400, 3, 32, 32])
torch.Size([400, 3, 32, 32])
torch.Size([400, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 682, 1: 34, 2: 45, 3: 31, 4: 18, 5: 12, 6: 12, 7: 16, 8: 89, 9: 61}, 1: {0: 50, 1: 721, 2: 6, 3: 12, 4: 10, 5: 13, 6: 12, 7: 17, 8: 17, 9: 142}, 2: {0: 132, 1: 10, 2: 402, 3: 94, 4: 95, 5: 66, 6: 89, 7: 70, 8: 19, 9: 23}, 3: {0: 33, 1: 19, 2: 71, 3: 413, 4: 44, 5: 199, 6: 118, 7: 45, 8: 10, 9: 48}, 4: {0: 62, 1: 12, 2: 95, 3: 56, 4: 449, 5: 36, 6: 121, 7: 133, 8: 16, 9: 20}, 5: {0: 27, 1: 11, 2: 57, 3: 179, 4: 53, 5: 518, 6: 59, 7: 68, 8: 13, 9: 15}, 6: {0: 8, 1: 8, 2: 56, 3: 66, 4: 61, 5: 23, 6: 746, 7: 11, 8: 5, 9: 16}, 7: {0: 27, 1: 13, 2: 36, 3: 62, 4: 41, 5: 76, 6: 29, 7: 665, 8: 7, 9: 44}, 8: {0: 193, 1: 97, 2: 18, 3: 25, 4: 5, 5: 8, 6: 12, 7: 14, 8: 520, 9: 108}, 9: {0: 44, 1: 108, 2: 5, 3: 21, 4: 6, 5: 12, 6: 14, 7: 20, 8: 30, 9: 740}}
round 3 evaluation: test acc is 0.5856, test loss = 2.383208
 ====== round 4 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3079.06MB, free 21496.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1186  193  123   97  104   92   96  143  192 1367], bin edged: [0.0001976  0.00021293 0.00022827 0.00024361 0.00025894 0.00027428
 0.00028962 0.00030495 0.00032029 0.00033562 0.00035096]
class 7 have 4999 samples, histogram: [2861  213  124   91   90   87  107  106  173 1147], bin edged: [0.00015956 0.00017194 0.00018433 0.00019671 0.00020909 0.00022148
 0.00023386 0.00024625 0.00025863 0.00027101 0.0002834 ]
client 0, data condensation 0, total loss = 53.05810546875, avg loss = 26.529052734375
client 0, data condensation 200, total loss = 6.2161865234375, avg loss = 3.10809326171875
client 0, data condensation 400, total loss = 6.08184814453125, avg loss = 3.040924072265625
client 0, data condensation 600, total loss = 6.0911865234375, avg loss = 3.04559326171875
client 0, data condensation 800, total loss = 9.28173828125, avg loss = 4.640869140625
client 0, data condensation 1000, total loss = 11.378662109375, avg loss = 5.6893310546875
client 0, data condensation 1200, total loss = 11.92791748046875, avg loss = 5.963958740234375
client 0, data condensation 1400, total loss = 5.11822509765625, avg loss = 2.559112548828125
client 0, data condensation 1600, total loss = 5.89990234375, avg loss = 2.949951171875
client 0, data condensation 1800, total loss = 10.778076171875, avg loss = 5.3890380859375
client 0, data condensation 2000, total loss = 7.353515625, avg loss = 3.6767578125
client 0, data condensation 2200, total loss = 5.3173828125, avg loss = 2.65869140625
client 0, data condensation 2400, total loss = 7.464111328125, avg loss = 3.7320556640625
client 0, data condensation 2600, total loss = 4.78350830078125, avg loss = 2.391754150390625
client 0, data condensation 2800, total loss = 21.7218017578125, avg loss = 10.86090087890625
client 0, data condensation 3000, total loss = 32.30267333984375, avg loss = 16.151336669921875
client 0, data condensation 3200, total loss = 45.040283203125, avg loss = 22.5201416015625
client 0, data condensation 3400, total loss = 7.71820068359375, avg loss = 3.859100341796875
client 0, data condensation 3600, total loss = 7.84161376953125, avg loss = 3.920806884765625
client 0, data condensation 3800, total loss = 6.71002197265625, avg loss = 3.355010986328125
client 0, data condensation 4000, total loss = 11.381103515625, avg loss = 5.6905517578125
client 0, data condensation 4200, total loss = 18.931396484375, avg loss = 9.4656982421875
client 0, data condensation 4400, total loss = 9.34967041015625, avg loss = 4.674835205078125
client 0, data condensation 4600, total loss = 6.023681640625, avg loss = 3.0118408203125
client 0, data condensation 4800, total loss = 16.8470458984375, avg loss = 8.42352294921875
client 0, data condensation 5000, total loss = 8.3160400390625, avg loss = 4.15802001953125
client 0, data condensation 5200, total loss = 4.20660400390625, avg loss = 2.103302001953125
client 0, data condensation 5400, total loss = 21.34539794921875, avg loss = 10.672698974609375
client 0, data condensation 5600, total loss = 8.087158203125, avg loss = 4.0435791015625
client 0, data condensation 5800, total loss = 11.5457763671875, avg loss = 5.77288818359375
client 0, data condensation 6000, total loss = 5.28558349609375, avg loss = 2.642791748046875
client 0, data condensation 6200, total loss = 15.4862060546875, avg loss = 7.74310302734375
client 0, data condensation 6400, total loss = 15.6641845703125, avg loss = 7.83209228515625
client 0, data condensation 6600, total loss = 6.675048828125, avg loss = 3.3375244140625
client 0, data condensation 6800, total loss = 6.79766845703125, avg loss = 3.398834228515625
client 0, data condensation 7000, total loss = 5.05657958984375, avg loss = 2.528289794921875
client 0, data condensation 7200, total loss = 8.1524658203125, avg loss = 4.07623291015625
client 0, data condensation 7400, total loss = 20.5242919921875, avg loss = 10.26214599609375
client 0, data condensation 7600, total loss = 7.85009765625, avg loss = 3.925048828125
client 0, data condensation 7800, total loss = 37.458740234375, avg loss = 18.7293701171875
client 0, data condensation 8000, total loss = 21.23724365234375, avg loss = 10.618621826171875
client 0, data condensation 8200, total loss = 15.30657958984375, avg loss = 7.653289794921875
client 0, data condensation 8400, total loss = 6.43182373046875, avg loss = 3.215911865234375
client 0, data condensation 8600, total loss = 6.08734130859375, avg loss = 3.043670654296875
client 0, data condensation 8800, total loss = 10.10235595703125, avg loss = 5.051177978515625
client 0, data condensation 9000, total loss = 4.83538818359375, avg loss = 2.417694091796875
client 0, data condensation 9200, total loss = 7.6514892578125, avg loss = 3.82574462890625
client 0, data condensation 9400, total loss = 6.45013427734375, avg loss = 3.225067138671875
client 0, data condensation 9600, total loss = 9.82476806640625, avg loss = 4.912384033203125
client 0, data condensation 9800, total loss = 7.82659912109375, avg loss = 3.913299560546875
client 0, data condensation 10000, total loss = 5.3505859375, avg loss = 2.67529296875
Round 4, client 0 condense time: 838.9136536121368
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
total 24576.0MB, used 3079.06MB, free 21496.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [49 10  7  3  7  7  4  8  7 73], bin edged: [0.00397929 0.00428814 0.00459699 0.00490584 0.00521469 0.00552354
 0.00583239 0.00614123 0.00645008 0.00675893 0.00706778]
class 4 have 4958 samples, histogram: [1437  282  165  154  137  127  134  184  259 2079], bin edged: [0.00014034 0.00015123 0.00016213 0.00017302 0.00018391 0.0001948
 0.0002057  0.00021659 0.00022748 0.00023837 0.00024927]
client 1, data condensation 0, total loss = 122.68060302734375, avg loss = 61.340301513671875
client 1, data condensation 200, total loss = 35.10943603515625, avg loss = 17.554718017578125
client 1, data condensation 400, total loss = 68.1551513671875, avg loss = 34.07757568359375
client 1, data condensation 600, total loss = 377.25958251953125, avg loss = 188.62979125976562
client 1, data condensation 800, total loss = 121.3197021484375, avg loss = 60.65985107421875
client 1, data condensation 1000, total loss = 33.111083984375, avg loss = 16.5555419921875
client 1, data condensation 1200, total loss = 39.6822509765625, avg loss = 19.84112548828125
client 1, data condensation 1400, total loss = 196.07568359375, avg loss = 98.037841796875
client 1, data condensation 1600, total loss = 94.9251708984375, avg loss = 47.46258544921875
client 1, data condensation 1800, total loss = 121.03961181640625, avg loss = 60.519805908203125
client 1, data condensation 2000, total loss = 185.4573974609375, avg loss = 92.72869873046875
client 1, data condensation 2200, total loss = 110.55792236328125, avg loss = 55.278961181640625
client 1, data condensation 2400, total loss = 79.4334716796875, avg loss = 39.71673583984375
client 1, data condensation 2600, total loss = 39.704345703125, avg loss = 19.8521728515625
client 1, data condensation 2800, total loss = 129.83233642578125, avg loss = 64.91616821289062
client 1, data condensation 3000, total loss = 127.9608154296875, avg loss = 63.98040771484375
client 1, data condensation 3200, total loss = 74.03143310546875, avg loss = 37.015716552734375
client 1, data condensation 3400, total loss = 104.6651611328125, avg loss = 52.33258056640625
client 1, data condensation 3600, total loss = 45.88433837890625, avg loss = 22.942169189453125
client 1, data condensation 3800, total loss = 55.729736328125, avg loss = 27.8648681640625
client 1, data condensation 4000, total loss = 59.57763671875, avg loss = 29.788818359375
client 1, data condensation 4200, total loss = 46.2330322265625, avg loss = 23.11651611328125
client 1, data condensation 4400, total loss = 41.102294921875, avg loss = 20.5511474609375
client 1, data condensation 4600, total loss = 40.20721435546875, avg loss = 20.103607177734375
client 1, data condensation 4800, total loss = 89.5443115234375, avg loss = 44.77215576171875
client 1, data condensation 5000, total loss = 95.40802001953125, avg loss = 47.704010009765625
client 1, data condensation 5200, total loss = 206.37548828125, avg loss = 103.187744140625
client 1, data condensation 5400, total loss = 88.53033447265625, avg loss = 44.265167236328125
client 1, data condensation 5600, total loss = 1585.7314453125, avg loss = 792.86572265625
client 1, data condensation 5800, total loss = 43.44732666015625, avg loss = 21.723663330078125
client 1, data condensation 6000, total loss = 251.1005859375, avg loss = 125.55029296875
client 1, data condensation 6200, total loss = 29.17816162109375, avg loss = 14.589080810546875
client 1, data condensation 6400, total loss = 56.1497802734375, avg loss = 28.07489013671875
client 1, data condensation 6600, total loss = 93.9150390625, avg loss = 46.95751953125
client 1, data condensation 6800, total loss = 51.396484375, avg loss = 25.6982421875
client 1, data condensation 7000, total loss = 2080.009521484375, avg loss = 1040.0047607421875
client 1, data condensation 7200, total loss = 30.6783447265625, avg loss = 15.33917236328125
client 1, data condensation 7400, total loss = 40.70904541015625, avg loss = 20.354522705078125
client 1, data condensation 7600, total loss = 110.52273559570312, avg loss = 55.26136779785156
client 1, data condensation 7800, total loss = 24.38299560546875, avg loss = 12.191497802734375
client 1, data condensation 8000, total loss = 53.340087890625, avg loss = 26.6700439453125
client 1, data condensation 8200, total loss = 45.09332275390625, avg loss = 22.546661376953125
client 1, data condensation 8400, total loss = 73.13470458984375, avg loss = 36.567352294921875
client 1, data condensation 8600, total loss = 18.51513671875, avg loss = 9.257568359375
client 1, data condensation 8800, total loss = 35.237548828125, avg loss = 17.6187744140625
client 1, data condensation 9000, total loss = 110.40625, avg loss = 55.203125
client 1, data condensation 9200, total loss = 37.66583251953125, avg loss = 18.832916259765625
client 1, data condensation 9400, total loss = 112.82635498046875, avg loss = 56.413177490234375
client 1, data condensation 9600, total loss = 24.530517578125, avg loss = 12.2652587890625
client 1, data condensation 9800, total loss = 111.79022216796875, avg loss = 55.895111083984375
client 1, data condensation 10000, total loss = 104.1197509765625, avg loss = 52.05987548828125
Round 4, client 1 condense time: 803.2144024372101
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2823.06MB, free 21752.94MB
total 24576.0MB, used 2823.06MB, free 21752.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [138  15   7   3   5   4   7   4   9  50], bin edged: [0.00333117 0.00358971 0.00384826 0.0041068  0.00436535 0.00462389
 0.00488244 0.00514098 0.00539953 0.00565807 0.00591662]
client 2, data condensation 0, total loss = 105.66107177734375, avg loss = 105.66107177734375
client 2, data condensation 200, total loss = 115.544677734375, avg loss = 115.544677734375
client 2, data condensation 400, total loss = 53.05108642578125, avg loss = 53.05108642578125
client 2, data condensation 600, total loss = 92.1607666015625, avg loss = 92.1607666015625
client 2, data condensation 800, total loss = 111.095458984375, avg loss = 111.095458984375
client 2, data condensation 1000, total loss = 29.115966796875, avg loss = 29.115966796875
client 2, data condensation 1200, total loss = 1757.030029296875, avg loss = 1757.030029296875
client 2, data condensation 1400, total loss = 41.21502685546875, avg loss = 41.21502685546875
client 2, data condensation 1600, total loss = 70.76953125, avg loss = 70.76953125
client 2, data condensation 1800, total loss = 25.3330078125, avg loss = 25.3330078125
client 2, data condensation 2000, total loss = 32.819580078125, avg loss = 32.819580078125
client 2, data condensation 2200, total loss = 92.81732177734375, avg loss = 92.81732177734375
client 2, data condensation 2400, total loss = 54.39129638671875, avg loss = 54.39129638671875
client 2, data condensation 2600, total loss = 48.132080078125, avg loss = 48.132080078125
client 2, data condensation 2800, total loss = 33.204833984375, avg loss = 33.204833984375
client 2, data condensation 3000, total loss = 55.3504638671875, avg loss = 55.3504638671875
client 2, data condensation 3200, total loss = 19.45330810546875, avg loss = 19.45330810546875
client 2, data condensation 3400, total loss = 42.50946044921875, avg loss = 42.50946044921875
client 2, data condensation 3600, total loss = 49.983642578125, avg loss = 49.983642578125
client 2, data condensation 3800, total loss = 26.22900390625, avg loss = 26.22900390625
client 2, data condensation 4000, total loss = 101.6588134765625, avg loss = 101.6588134765625
client 2, data condensation 4200, total loss = 27.51446533203125, avg loss = 27.51446533203125
client 2, data condensation 4400, total loss = 93.67181396484375, avg loss = 93.67181396484375
client 2, data condensation 4600, total loss = 25.4532470703125, avg loss = 25.4532470703125
client 2, data condensation 4800, total loss = 59.42144775390625, avg loss = 59.42144775390625
client 2, data condensation 5000, total loss = 44.79620361328125, avg loss = 44.79620361328125
client 2, data condensation 5200, total loss = 116.521240234375, avg loss = 116.521240234375
client 2, data condensation 5400, total loss = 138.2481689453125, avg loss = 138.2481689453125
client 2, data condensation 5600, total loss = 22.704833984375, avg loss = 22.704833984375
client 2, data condensation 5800, total loss = 35.91827392578125, avg loss = 35.91827392578125
client 2, data condensation 6000, total loss = 226.06951904296875, avg loss = 226.06951904296875
client 2, data condensation 6200, total loss = 314.9775390625, avg loss = 314.9775390625
client 2, data condensation 6400, total loss = 19.90911865234375, avg loss = 19.90911865234375
client 2, data condensation 6600, total loss = 37.29107666015625, avg loss = 37.29107666015625
client 2, data condensation 6800, total loss = 28.92236328125, avg loss = 28.92236328125
client 2, data condensation 7000, total loss = 53.75482177734375, avg loss = 53.75482177734375
client 2, data condensation 7200, total loss = 26.3836669921875, avg loss = 26.3836669921875
client 2, data condensation 7400, total loss = 15.37255859375, avg loss = 15.37255859375
client 2, data condensation 7600, total loss = 72.0693359375, avg loss = 72.0693359375
client 2, data condensation 7800, total loss = 41.5477294921875, avg loss = 41.5477294921875
client 2, data condensation 8000, total loss = 76.07891845703125, avg loss = 76.07891845703125
client 2, data condensation 8200, total loss = 10.49066162109375, avg loss = 10.49066162109375
client 2, data condensation 8400, total loss = 196.25933837890625, avg loss = 196.25933837890625
client 2, data condensation 8600, total loss = 47.643798828125, avg loss = 47.643798828125
client 2, data condensation 8800, total loss = 164.96514892578125, avg loss = 164.96514892578125
client 2, data condensation 9000, total loss = 78.9522705078125, avg loss = 78.9522705078125
client 2, data condensation 9200, total loss = 44.4053955078125, avg loss = 44.4053955078125
client 2, data condensation 9400, total loss = 71.1275634765625, avg loss = 71.1275634765625
client 2, data condensation 9600, total loss = 24.38653564453125, avg loss = 24.38653564453125
client 2, data condensation 9800, total loss = 222.383544921875, avg loss = 222.383544921875
client 2, data condensation 10000, total loss = 31.2174072265625, avg loss = 31.2174072265625
Round 4, client 2 condense time: 355.5740990638733
client 2, class 9 have 242 samples
total 24576.0MB, used 2437.06MB, free 22138.94MB
total 24576.0MB, used 2437.06MB, free 22138.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [486  41  20  18  17  18  19  18  25 185], bin edged: [0.00094695 0.00102045 0.00109395 0.00116744 0.00124094 0.00131444
 0.00138794 0.00146143 0.00153493 0.00160843 0.00168192]
class 2 have 1094 samples, histogram: [310  63  36  30  33  35  35  44  59 449], bin edged: [0.00063537 0.00068469 0.000734   0.00078332 0.00083263 0.00088194
 0.00093126 0.00098057 0.00102988 0.0010792  0.00112851]
client 3, data condensation 0, total loss = 95.30474853515625, avg loss = 47.652374267578125
client 3, data condensation 200, total loss = 8.007080078125, avg loss = 4.0035400390625
client 3, data condensation 400, total loss = 35.52569580078125, avg loss = 17.762847900390625
client 3, data condensation 600, total loss = 13.75390625, avg loss = 6.876953125
client 3, data condensation 800, total loss = 16.77490234375, avg loss = 8.387451171875
client 3, data condensation 1000, total loss = 9.23565673828125, avg loss = 4.617828369140625
client 3, data condensation 1200, total loss = 10.16485595703125, avg loss = 5.082427978515625
client 3, data condensation 1400, total loss = 34.70599365234375, avg loss = 17.352996826171875
client 3, data condensation 1600, total loss = 74.6607666015625, avg loss = 37.33038330078125
client 3, data condensation 1800, total loss = 17.60699462890625, avg loss = 8.803497314453125
client 3, data condensation 2000, total loss = 10.06390380859375, avg loss = 5.031951904296875
client 3, data condensation 2200, total loss = 22.77508544921875, avg loss = 11.387542724609375
client 3, data condensation 2400, total loss = 9.88043212890625, avg loss = 4.940216064453125
client 3, data condensation 2600, total loss = 12.951171875, avg loss = 6.4755859375
client 3, data condensation 2800, total loss = 11.6058349609375, avg loss = 5.80291748046875
client 3, data condensation 3000, total loss = 6.9345703125, avg loss = 3.46728515625
client 3, data condensation 3200, total loss = 11.85858154296875, avg loss = 5.929290771484375
client 3, data condensation 3400, total loss = 10.6221923828125, avg loss = 5.31109619140625
client 3, data condensation 3600, total loss = 19.70550537109375, avg loss = 9.852752685546875
client 3, data condensation 3800, total loss = 6.6072998046875, avg loss = 3.30364990234375
client 3, data condensation 4000, total loss = 17.05706787109375, avg loss = 8.528533935546875
client 3, data condensation 4200, total loss = 10.73309326171875, avg loss = 5.366546630859375
client 3, data condensation 4400, total loss = 9.70611572265625, avg loss = 4.853057861328125
client 3, data condensation 4600, total loss = 13.08233642578125, avg loss = 6.541168212890625
client 3, data condensation 4800, total loss = 7.17388916015625, avg loss = 3.586944580078125
client 3, data condensation 5000, total loss = 18.6490478515625, avg loss = 9.32452392578125
client 3, data condensation 5200, total loss = 3.00872802734375, avg loss = 1.504364013671875
client 3, data condensation 5400, total loss = 13.6943359375, avg loss = 6.84716796875
client 3, data condensation 5600, total loss = 12.00164794921875, avg loss = 6.000823974609375
client 3, data condensation 5800, total loss = 11.188232421875, avg loss = 5.5941162109375
client 3, data condensation 6000, total loss = 22.3560791015625, avg loss = 11.17803955078125
client 3, data condensation 6200, total loss = 9.372802734375, avg loss = 4.6864013671875
client 3, data condensation 6400, total loss = 7.5025634765625, avg loss = 3.75128173828125
client 3, data condensation 6600, total loss = 8.3834228515625, avg loss = 4.19171142578125
client 3, data condensation 6800, total loss = 8.088623046875, avg loss = 4.0443115234375
client 3, data condensation 7000, total loss = 6.83123779296875, avg loss = 3.415618896484375
client 3, data condensation 7200, total loss = 13.51861572265625, avg loss = 6.759307861328125
client 3, data condensation 7400, total loss = 10.3441162109375, avg loss = 5.17205810546875
client 3, data condensation 7600, total loss = 8.2728271484375, avg loss = 4.13641357421875
client 3, data condensation 7800, total loss = 46.70550537109375, avg loss = 23.352752685546875
client 3, data condensation 8000, total loss = 23.34619140625, avg loss = 11.673095703125
client 3, data condensation 8200, total loss = 16.92071533203125, avg loss = 8.460357666015625
client 3, data condensation 8400, total loss = 9.2230224609375, avg loss = 4.61151123046875
client 3, data condensation 8600, total loss = 6.73095703125, avg loss = 3.365478515625
client 3, data condensation 8800, total loss = 16.55718994140625, avg loss = 8.278594970703125
client 3, data condensation 9000, total loss = 4.48809814453125, avg loss = 2.244049072265625
client 3, data condensation 9200, total loss = 18.4151611328125, avg loss = 9.20758056640625
client 3, data condensation 9400, total loss = 4.70086669921875, avg loss = 2.350433349609375
client 3, data condensation 9600, total loss = 18.34259033203125, avg loss = 9.171295166015625
client 3, data condensation 9800, total loss = 18.43316650390625, avg loss = 9.216583251953125
client 3, data condensation 10000, total loss = 9.60382080078125, avg loss = 4.801910400390625
Round 4, client 3 condense time: 769.1052763462067
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2819.06MB, free 21756.94MB
total 24576.0MB, used 2819.06MB, free 21756.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2391  191  148  120   86   76   79   87  113  861], bin edged: [0.00019472 0.00020983 0.00022495 0.00024006 0.00025517 0.00027029
 0.0002854  0.00030051 0.00031563 0.00033074 0.00034585]
class 5 have 307 samples, histogram: [130  18  13   7   4  11  12  13  14  85], bin edged: [0.00244608 0.00263593 0.00282578 0.00301563 0.00320548 0.00339533
 0.00358518 0.00377503 0.00396488 0.00415473 0.00434458]
client 4, data condensation 0, total loss = 74.08056640625, avg loss = 37.040283203125
client 4, data condensation 200, total loss = 94.03607177734375, avg loss = 47.018035888671875
client 4, data condensation 400, total loss = 25.68841552734375, avg loss = 12.844207763671875
client 4, data condensation 600, total loss = 16.1090087890625, avg loss = 8.05450439453125
client 4, data condensation 800, total loss = 35.71026611328125, avg loss = 17.855133056640625
client 4, data condensation 1000, total loss = 59.28515625, avg loss = 29.642578125
client 4, data condensation 1200, total loss = 14.1544189453125, avg loss = 7.07720947265625
client 4, data condensation 1400, total loss = 45.77398681640625, avg loss = 22.886993408203125
client 4, data condensation 1600, total loss = 29.86700439453125, avg loss = 14.933502197265625
client 4, data condensation 1800, total loss = 16.0726318359375, avg loss = 8.03631591796875
client 4, data condensation 2000, total loss = 23.5264892578125, avg loss = 11.76324462890625
client 4, data condensation 2200, total loss = 36.45623779296875, avg loss = 18.228118896484375
client 4, data condensation 2400, total loss = 16.7623291015625, avg loss = 8.38116455078125
client 4, data condensation 2600, total loss = 17.1614990234375, avg loss = 8.58074951171875
client 4, data condensation 2800, total loss = 50.78375244140625, avg loss = 25.391876220703125
client 4, data condensation 3000, total loss = 114.544921875, avg loss = 57.2724609375
client 4, data condensation 3200, total loss = 19.9652099609375, avg loss = 9.98260498046875
client 4, data condensation 3400, total loss = 54.73394775390625, avg loss = 27.366973876953125
client 4, data condensation 3600, total loss = 8.003662109375, avg loss = 4.0018310546875
client 4, data condensation 3800, total loss = 20.669189453125, avg loss = 10.3345947265625
client 4, data condensation 4000, total loss = 53.429443359375, avg loss = 26.7147216796875
client 4, data condensation 4200, total loss = 30.959716796875, avg loss = 15.4798583984375
client 4, data condensation 4400, total loss = 17.91204833984375, avg loss = 8.956024169921875
client 4, data condensation 4600, total loss = 14.703125, avg loss = 7.3515625
client 4, data condensation 4800, total loss = 39.287353515625, avg loss = 19.6436767578125
client 4, data condensation 5000, total loss = 16.6322021484375, avg loss = 8.31610107421875
client 4, data condensation 5200, total loss = 16.11651611328125, avg loss = 8.058258056640625
client 4, data condensation 5400, total loss = 48.08428955078125, avg loss = 24.042144775390625
client 4, data condensation 5600, total loss = 51.14080810546875, avg loss = 25.570404052734375
client 4, data condensation 5800, total loss = 39.487060546875, avg loss = 19.7435302734375
client 4, data condensation 6000, total loss = 18.7685546875, avg loss = 9.38427734375
client 4, data condensation 6200, total loss = 24.62640380859375, avg loss = 12.313201904296875
client 4, data condensation 6400, total loss = 26.82354736328125, avg loss = 13.411773681640625
client 4, data condensation 6600, total loss = 14.4908447265625, avg loss = 7.24542236328125
client 4, data condensation 6800, total loss = 33.868896484375, avg loss = 16.9344482421875
client 4, data condensation 7000, total loss = 19.6739501953125, avg loss = 9.83697509765625
client 4, data condensation 7200, total loss = 22.75408935546875, avg loss = 11.377044677734375
client 4, data condensation 7400, total loss = 26.25592041015625, avg loss = 13.127960205078125
client 4, data condensation 7600, total loss = 16.93408203125, avg loss = 8.467041015625
client 4, data condensation 7800, total loss = 25.64166259765625, avg loss = 12.820831298828125
client 4, data condensation 8000, total loss = 28.66851806640625, avg loss = 14.334259033203125
client 4, data condensation 8200, total loss = 21.9189453125, avg loss = 10.95947265625
client 4, data condensation 8400, total loss = 8.478759765625, avg loss = 4.2393798828125
client 4, data condensation 8600, total loss = 348.86627197265625, avg loss = 174.43313598632812
client 4, data condensation 8800, total loss = 19.2100830078125, avg loss = 9.60504150390625
client 4, data condensation 9000, total loss = 32.4490966796875, avg loss = 16.22454833984375
client 4, data condensation 9200, total loss = 21.05047607421875, avg loss = 10.525238037109375
client 4, data condensation 9400, total loss = 29.7606201171875, avg loss = 14.88031005859375
client 4, data condensation 9600, total loss = 1413.4130859375, avg loss = 706.70654296875
client 4, data condensation 9800, total loss = 31.230712890625, avg loss = 15.6153564453125
client 4, data condensation 10000, total loss = 16.027099609375, avg loss = 8.0135498046875
Round 4, client 4 condense time: 884.6007137298584
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [1723  245  197  153  140  134  144  163  238 1862], bin edged: [0.00014311 0.00015422 0.00016533 0.00017644 0.00018755 0.00019865
 0.00020976 0.00022087 0.00023198 0.00024308 0.00025419]
client 5, data condensation 0, total loss = 105.56390380859375, avg loss = 105.56390380859375
client 5, data condensation 200, total loss = 47.32666015625, avg loss = 47.32666015625
client 5, data condensation 400, total loss = 12.60040283203125, avg loss = 12.60040283203125
client 5, data condensation 600, total loss = 5.9600830078125, avg loss = 5.9600830078125
client 5, data condensation 800, total loss = 4.80401611328125, avg loss = 4.80401611328125
client 5, data condensation 1000, total loss = 5.38134765625, avg loss = 5.38134765625
client 5, data condensation 1200, total loss = 10.61419677734375, avg loss = 10.61419677734375
client 5, data condensation 1400, total loss = 4.02960205078125, avg loss = 4.02960205078125
client 5, data condensation 1600, total loss = 12.3773193359375, avg loss = 12.3773193359375
client 5, data condensation 1800, total loss = 23.11505126953125, avg loss = 23.11505126953125
client 5, data condensation 2000, total loss = 4.34991455078125, avg loss = 4.34991455078125
client 5, data condensation 2200, total loss = 3.80322265625, avg loss = 3.80322265625
client 5, data condensation 2400, total loss = 2.37359619140625, avg loss = 2.37359619140625
client 5, data condensation 2600, total loss = 6.80181884765625, avg loss = 6.80181884765625
client 5, data condensation 2800, total loss = 3.21612548828125, avg loss = 3.21612548828125
client 5, data condensation 3000, total loss = 48.83428955078125, avg loss = 48.83428955078125
client 5, data condensation 3200, total loss = 14.97113037109375, avg loss = 14.97113037109375
client 5, data condensation 3400, total loss = 2.8590087890625, avg loss = 2.8590087890625
client 5, data condensation 3600, total loss = 25.54949951171875, avg loss = 25.54949951171875
client 5, data condensation 3800, total loss = 5.503662109375, avg loss = 5.503662109375
client 5, data condensation 4000, total loss = 11.8497314453125, avg loss = 11.8497314453125
client 5, data condensation 4200, total loss = 4.084228515625, avg loss = 4.084228515625
client 5, data condensation 4400, total loss = 5.87054443359375, avg loss = 5.87054443359375
client 5, data condensation 4600, total loss = 19.0830078125, avg loss = 19.0830078125
client 5, data condensation 4800, total loss = 26.20172119140625, avg loss = 26.20172119140625
client 5, data condensation 5000, total loss = 8.106201171875, avg loss = 8.106201171875
client 5, data condensation 5200, total loss = 9.65618896484375, avg loss = 9.65618896484375
client 5, data condensation 5400, total loss = 41.896728515625, avg loss = 41.896728515625
client 5, data condensation 5600, total loss = 3.9033203125, avg loss = 3.9033203125
client 5, data condensation 5800, total loss = 2.4560546875, avg loss = 2.4560546875
client 5, data condensation 6000, total loss = 5.7852783203125, avg loss = 5.7852783203125
client 5, data condensation 6200, total loss = 6.1221923828125, avg loss = 6.1221923828125
client 5, data condensation 6400, total loss = 2.544921875, avg loss = 2.544921875
client 5, data condensation 6600, total loss = 32.292724609375, avg loss = 32.292724609375
client 5, data condensation 6800, total loss = 4.62933349609375, avg loss = 4.62933349609375
client 5, data condensation 7000, total loss = 3.537353515625, avg loss = 3.537353515625
client 5, data condensation 7200, total loss = 4.5877685546875, avg loss = 4.5877685546875
client 5, data condensation 7400, total loss = 12.59271240234375, avg loss = 12.59271240234375
client 5, data condensation 7600, total loss = 4.341552734375, avg loss = 4.341552734375
client 5, data condensation 7800, total loss = 23.1612548828125, avg loss = 23.1612548828125
client 5, data condensation 8000, total loss = 4.82061767578125, avg loss = 4.82061767578125
client 5, data condensation 8200, total loss = 20.33892822265625, avg loss = 20.33892822265625
client 5, data condensation 8400, total loss = 7.83392333984375, avg loss = 7.83392333984375
client 5, data condensation 8600, total loss = 3.9495849609375, avg loss = 3.9495849609375
client 5, data condensation 8800, total loss = 9.56842041015625, avg loss = 9.56842041015625
client 5, data condensation 9000, total loss = 2.8482666015625, avg loss = 2.8482666015625
client 5, data condensation 9200, total loss = 4.96429443359375, avg loss = 4.96429443359375
client 5, data condensation 9400, total loss = 9.61968994140625, avg loss = 9.61968994140625
client 5, data condensation 9600, total loss = 2.9764404296875, avg loss = 2.9764404296875
client 5, data condensation 9800, total loss = 4.60833740234375, avg loss = 4.60833740234375
client 5, data condensation 10000, total loss = 34.567138671875, avg loss = 34.567138671875
Round 4, client 5 condense time: 468.0295743942261
client 5, class 8 have 4999 samples
total 24576.0MB, used 2441.06MB, free 22134.94MB
total 24576.0MB, used 2441.06MB, free 22134.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1629  210  163  150  100  127  148  136  218 1484], bin edged: [0.00016668 0.00017962 0.00019256 0.00020549 0.00021843 0.00023137
 0.0002443  0.00025724 0.00027018 0.00028311 0.00029605]
class 6 have 3914 samples, histogram: [2420  187  109   91   64   69   82   76   95  721], bin edged: [0.00021056 0.0002269  0.00024325 0.00025959 0.00027593 0.00029227
 0.00030861 0.00032496 0.0003413  0.00035764 0.00037398]
client 6, data condensation 0, total loss = 82.0556640625, avg loss = 41.02783203125
client 6, data condensation 200, total loss = 15.28173828125, avg loss = 7.640869140625
client 6, data condensation 400, total loss = 20.45166015625, avg loss = 10.225830078125
client 6, data condensation 600, total loss = 4.61566162109375, avg loss = 2.307830810546875
client 6, data condensation 800, total loss = 14.26336669921875, avg loss = 7.131683349609375
client 6, data condensation 1000, total loss = 8.85626220703125, avg loss = 4.428131103515625
client 6, data condensation 1200, total loss = 5.05047607421875, avg loss = 2.525238037109375
client 6, data condensation 1400, total loss = 10.58544921875, avg loss = 5.292724609375
client 6, data condensation 1600, total loss = 10.31781005859375, avg loss = 5.158905029296875
client 6, data condensation 1800, total loss = 6.4642333984375, avg loss = 3.23211669921875
client 6, data condensation 2000, total loss = 19.83612060546875, avg loss = 9.918060302734375
client 6, data condensation 2200, total loss = 10.5125732421875, avg loss = 5.25628662109375
client 6, data condensation 2400, total loss = 11.98944091796875, avg loss = 5.994720458984375
client 6, data condensation 2600, total loss = 9.8502197265625, avg loss = 4.92510986328125
client 6, data condensation 2800, total loss = 16.2196044921875, avg loss = 8.10980224609375
client 6, data condensation 3000, total loss = 7.46734619140625, avg loss = 3.733673095703125
client 6, data condensation 3200, total loss = 6.7188720703125, avg loss = 3.35943603515625
client 6, data condensation 3400, total loss = 3.80401611328125, avg loss = 1.902008056640625
client 6, data condensation 3600, total loss = 6.0072021484375, avg loss = 3.00360107421875
client 6, data condensation 3800, total loss = 5.6171875, avg loss = 2.80859375
client 6, data condensation 4000, total loss = 51.004913330078125, avg loss = 25.502456665039062
client 6, data condensation 4200, total loss = 7.29107666015625, avg loss = 3.645538330078125
client 6, data condensation 4400, total loss = 7.6903076171875, avg loss = 3.84515380859375
client 6, data condensation 4600, total loss = 8.5244140625, avg loss = 4.26220703125
client 6, data condensation 4800, total loss = 36.165771484375, avg loss = 18.0828857421875
client 6, data condensation 5000, total loss = 6.38507080078125, avg loss = 3.192535400390625
client 6, data condensation 5200, total loss = 16.85809326171875, avg loss = 8.429046630859375
client 6, data condensation 5400, total loss = 6.53057861328125, avg loss = 3.265289306640625
client 6, data condensation 5600, total loss = 11.24029541015625, avg loss = 5.620147705078125
client 6, data condensation 5800, total loss = 12.8936767578125, avg loss = 6.44683837890625
client 6, data condensation 6000, total loss = 12.5894775390625, avg loss = 6.29473876953125
client 6, data condensation 6200, total loss = 17.23358154296875, avg loss = 8.616790771484375
client 6, data condensation 6400, total loss = 11.00390625, avg loss = 5.501953125
client 6, data condensation 6600, total loss = 14.46124267578125, avg loss = 7.230621337890625
client 6, data condensation 6800, total loss = 14.535400390625, avg loss = 7.2677001953125
client 6, data condensation 7000, total loss = 7.0389404296875, avg loss = 3.51947021484375
client 6, data condensation 7200, total loss = 7.93963623046875, avg loss = 3.969818115234375
client 6, data condensation 7400, total loss = 44.604522705078125, avg loss = 22.302261352539062
client 6, data condensation 7600, total loss = 6.41290283203125, avg loss = 3.206451416015625
client 6, data condensation 7800, total loss = 4.7608642578125, avg loss = 2.38043212890625
client 6, data condensation 8000, total loss = 23.52655029296875, avg loss = 11.763275146484375
client 6, data condensation 8200, total loss = 7.2618408203125, avg loss = 3.63092041015625
client 6, data condensation 8400, total loss = 12.62353515625, avg loss = 6.311767578125
client 6, data condensation 8600, total loss = 5.51690673828125, avg loss = 2.758453369140625
client 6, data condensation 8800, total loss = 8.37335205078125, avg loss = 4.186676025390625
client 6, data condensation 9000, total loss = 5.96356201171875, avg loss = 2.981781005859375
client 6, data condensation 9200, total loss = 9.55938720703125, avg loss = 4.779693603515625
client 6, data condensation 9400, total loss = 9.295654296875, avg loss = 4.6478271484375
client 6, data condensation 9600, total loss = 17.38037109375, avg loss = 8.690185546875
client 6, data condensation 9800, total loss = 18.20806884765625, avg loss = 9.104034423828125
client 6, data condensation 10000, total loss = 13.1400146484375, avg loss = 6.57000732421875
Round 4, client 6 condense time: 831.2964630126953
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3037  203  135   76   96   77   86   92  123  680], bin edged: [0.00018316 0.00019738 0.00021159 0.00022581 0.00024002 0.00025424
 0.00026846 0.00028267 0.00029689 0.0003111  0.00032532]
class 3 have 4999 samples, histogram: [1341  310  215  169  163  162  169  215  317 1938], bin edged: [0.00013942 0.00015024 0.00016107 0.00017189 0.00018271 0.00019353
 0.00020435 0.00021517 0.00022599 0.00023681 0.00024763]
client 7, data condensation 0, total loss = 81.74395751953125, avg loss = 40.871978759765625
client 7, data condensation 200, total loss = 8.48382568359375, avg loss = 4.241912841796875
client 7, data condensation 400, total loss = 5.94708251953125, avg loss = 2.973541259765625
client 7, data condensation 600, total loss = 3.3594970703125, avg loss = 1.67974853515625
client 7, data condensation 800, total loss = 10.1322021484375, avg loss = 5.06610107421875
client 7, data condensation 1000, total loss = 7.2789306640625, avg loss = 3.63946533203125
client 7, data condensation 1200, total loss = 6.63580322265625, avg loss = 3.317901611328125
client 7, data condensation 1400, total loss = 7.1383056640625, avg loss = 3.56915283203125
client 7, data condensation 1600, total loss = 9.34161376953125, avg loss = 4.670806884765625
client 7, data condensation 1800, total loss = 10.2916259765625, avg loss = 5.14581298828125
client 7, data condensation 2000, total loss = 6.47064208984375, avg loss = 3.235321044921875
client 7, data condensation 2200, total loss = 21.53045654296875, avg loss = 10.765228271484375
client 7, data condensation 2400, total loss = 6.963134765625, avg loss = 3.4815673828125
client 7, data condensation 2600, total loss = 6.606201171875, avg loss = 3.3031005859375
client 7, data condensation 2800, total loss = 11.53631591796875, avg loss = 5.768157958984375
client 7, data condensation 3000, total loss = 38.0333251953125, avg loss = 19.01666259765625
client 7, data condensation 3200, total loss = 6.54949951171875, avg loss = 3.274749755859375
client 7, data condensation 3400, total loss = 9.58709716796875, avg loss = 4.793548583984375
client 7, data condensation 3600, total loss = 36.25421142578125, avg loss = 18.127105712890625
client 7, data condensation 3800, total loss = 22.49285888671875, avg loss = 11.246429443359375
client 7, data condensation 4000, total loss = 18.2183837890625, avg loss = 9.10919189453125
client 7, data condensation 4200, total loss = 6.91943359375, avg loss = 3.459716796875
client 7, data condensation 4400, total loss = 26.2900390625, avg loss = 13.14501953125
client 7, data condensation 4600, total loss = 6.89263916015625, avg loss = 3.446319580078125
client 7, data condensation 4800, total loss = 9.03076171875, avg loss = 4.515380859375
client 7, data condensation 5000, total loss = 5.4217529296875, avg loss = 2.71087646484375
client 7, data condensation 5200, total loss = 6.13525390625, avg loss = 3.067626953125
client 7, data condensation 5400, total loss = 9.44610595703125, avg loss = 4.723052978515625
client 7, data condensation 5600, total loss = 8.95404052734375, avg loss = 4.477020263671875
client 7, data condensation 5800, total loss = 11.79534912109375, avg loss = 5.897674560546875
client 7, data condensation 6000, total loss = 8.39019775390625, avg loss = 4.195098876953125
client 7, data condensation 6200, total loss = 8.89251708984375, avg loss = 4.446258544921875
client 7, data condensation 6400, total loss = 7.1160888671875, avg loss = 3.55804443359375
client 7, data condensation 6600, total loss = 5.10845947265625, avg loss = 2.554229736328125
client 7, data condensation 6800, total loss = 8.1436767578125, avg loss = 4.07183837890625
client 7, data condensation 7000, total loss = 8.135009765625, avg loss = 4.0675048828125
client 7, data condensation 7200, total loss = 3.9459228515625, avg loss = 1.97296142578125
client 7, data condensation 7400, total loss = 11.84820556640625, avg loss = 5.924102783203125
client 7, data condensation 7600, total loss = 9.41943359375, avg loss = 4.709716796875
client 7, data condensation 7800, total loss = 5.4610595703125, avg loss = 2.73052978515625
client 7, data condensation 8000, total loss = 11.1260986328125, avg loss = 5.56304931640625
client 7, data condensation 8200, total loss = 13.31787109375, avg loss = 6.658935546875
client 7, data condensation 8400, total loss = 11.1922607421875, avg loss = 5.59613037109375
client 7, data condensation 8600, total loss = 12.74212646484375, avg loss = 6.371063232421875
client 7, data condensation 8800, total loss = 34.1258544921875, avg loss = 17.06292724609375
client 7, data condensation 9000, total loss = 7.3956298828125, avg loss = 3.69781494140625
client 7, data condensation 9200, total loss = 9.66943359375, avg loss = 4.834716796875
client 7, data condensation 9400, total loss = 6.17303466796875, avg loss = 3.086517333984375
client 7, data condensation 9600, total loss = 6.36175537109375, avg loss = 3.180877685546875
client 7, data condensation 9800, total loss = 23.7650146484375, avg loss = 11.88250732421875
client 7, data condensation 10000, total loss = 11.06695556640625, avg loss = 5.533477783203125
Round 4, client 7 condense time: 808.5080327987671
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 3081.06MB, free 21494.94MB
total 24576.0MB, used 3081.06MB, free 21494.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [241  21   8   8   4   4   8  11   6  53], bin edged: [0.00232583 0.00250635 0.00268686 0.00286738 0.0030479  0.00322841
 0.00340893 0.00358945 0.00376996 0.00395048 0.004131  ]
class 5 have 135 samples, histogram: [55  4  4  6  3  7  5  1  8 42], bin edged: [0.00546535 0.00588954 0.00631373 0.00673791 0.0071621  0.00758629
 0.00801048 0.00843467 0.00885885 0.00928304 0.00970723]
class 9 have 4727 samples, histogram: [2910  215  158  112   94   90   85  111  155  797], bin edged: [0.00017469 0.00018824 0.0002018  0.00021536 0.00022892 0.00024248
 0.00025604 0.00026959 0.00028315 0.00029671 0.00031027]
client 8, data condensation 0, total loss = 174.401123046875, avg loss = 58.133707682291664
client 8, data condensation 200, total loss = 447.28070068359375, avg loss = 149.09356689453125
client 8, data condensation 400, total loss = 174.76373291015625, avg loss = 58.25457763671875
client 8, data condensation 600, total loss = 21.37469482421875, avg loss = 7.124898274739583
client 8, data condensation 800, total loss = 110.28143310546875, avg loss = 36.760477701822914
client 8, data condensation 1000, total loss = 64.285400390625, avg loss = 21.428466796875
client 8, data condensation 1200, total loss = 49.117431640625, avg loss = 16.372477213541668
client 8, data condensation 1400, total loss = 143.87750244140625, avg loss = 47.95916748046875
client 8, data condensation 1600, total loss = 83.5538330078125, avg loss = 27.851277669270832
client 8, data condensation 1800, total loss = 270.00140380859375, avg loss = 90.00046793619792
client 8, data condensation 2000, total loss = 140.83953857421875, avg loss = 46.946512858072914
client 8, data condensation 2200, total loss = 45.321044921875, avg loss = 15.107014973958334
client 8, data condensation 2400, total loss = 262.19891357421875, avg loss = 87.39963785807292
client 8, data condensation 2600, total loss = 78.4588623046875, avg loss = 26.1529541015625
client 8, data condensation 2800, total loss = 88.02813720703125, avg loss = 29.34271240234375
client 8, data condensation 3000, total loss = 71.25164794921875, avg loss = 23.75054931640625
client 8, data condensation 3200, total loss = 68.0672607421875, avg loss = 22.6890869140625
client 8, data condensation 3400, total loss = 85.19049072265625, avg loss = 28.396830240885418
client 8, data condensation 3600, total loss = 53.87554931640625, avg loss = 17.958516438802082
client 8, data condensation 3800, total loss = 67.2425537109375, avg loss = 22.4141845703125
client 8, data condensation 4000, total loss = 36.05059814453125, avg loss = 12.016866048177084
client 8, data condensation 4200, total loss = 42.41943359375, avg loss = 14.139811197916666
client 8, data condensation 4400, total loss = 92.684326171875, avg loss = 30.894775390625
client 8, data condensation 4600, total loss = 56.176513671875, avg loss = 18.725504557291668
client 8, data condensation 4800, total loss = 49.789306640625, avg loss = 16.596435546875
client 8, data condensation 5000, total loss = 52.44598388671875, avg loss = 17.48199462890625
client 8, data condensation 5200, total loss = 67.387939453125, avg loss = 22.462646484375
client 8, data condensation 5400, total loss = 18.5849609375, avg loss = 6.194986979166667
client 8, data condensation 5600, total loss = 36.7225341796875, avg loss = 12.2408447265625
client 8, data condensation 5800, total loss = 105.3443603515625, avg loss = 35.114786783854164
client 8, data condensation 6000, total loss = 80.3160400390625, avg loss = 26.772013346354168
client 8, data condensation 6200, total loss = 54.71734619140625, avg loss = 18.239115397135418
client 8, data condensation 6400, total loss = 739.1914672851562, avg loss = 246.39715576171875
client 8, data condensation 6600, total loss = 83.948974609375, avg loss = 27.982991536458332
client 8, data condensation 6800, total loss = 89.652099609375, avg loss = 29.884033203125
client 8, data condensation 7000, total loss = 123.2694091796875, avg loss = 41.089803059895836
client 8, data condensation 7200, total loss = 243.149169921875, avg loss = 81.04972330729167
client 8, data condensation 7400, total loss = 72.29388427734375, avg loss = 24.09796142578125
client 8, data condensation 7600, total loss = 53.98681640625, avg loss = 17.99560546875
client 8, data condensation 7800, total loss = 29.4410400390625, avg loss = 9.813680013020834
client 8, data condensation 8000, total loss = 256.9376220703125, avg loss = 85.6458740234375
client 8, data condensation 8200, total loss = 31.923095703125, avg loss = 10.641031901041666
client 8, data condensation 8400, total loss = 79.36004638671875, avg loss = 26.453348795572918
client 8, data condensation 8600, total loss = 69.00274658203125, avg loss = 23.00091552734375
client 8, data condensation 8800, total loss = 56.79022216796875, avg loss = 18.930074055989582
client 8, data condensation 9000, total loss = 46.5802001953125, avg loss = 15.5267333984375
client 8, data condensation 9200, total loss = 560.5231323242188, avg loss = 186.8410441080729
client 8, data condensation 9400, total loss = 82.13861083984375, avg loss = 27.379536946614582
client 8, data condensation 9600, total loss = 96.376953125, avg loss = 32.125651041666664
client 8, data condensation 9800, total loss = 140.6475830078125, avg loss = 46.882527669270836
client 8, data condensation 10000, total loss = 77.76031494140625, avg loss = 25.92010498046875
Round 4, client 8 condense time: 921.8012173175812
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
total 24576.0MB, used 3079.06MB, free 21496.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [45  8  3  5  4  2  2  6  4 41], bin edged: [0.00608711 0.00655955 0.00703198 0.00750441 0.00797685 0.00844928
 0.00892171 0.00939414 0.00986658 0.01033901 0.01081144]
class 5 have 192 samples, histogram: [80 10  4  6  6  3  8  4  8 63], bin edged: [0.00385996 0.00415955 0.00445914 0.00475872 0.00505831 0.0053579
 0.00565749 0.00595707 0.00625666 0.00655625 0.00685583]
class 6 have 1075 samples, histogram: [658  53  29  26  21  20  25  32  36 175], bin edged: [0.00076792 0.00082752 0.00088712 0.00094672 0.00100632 0.00106592
 0.00112552 0.00118512 0.00124472 0.00130432 0.00136393]
client 9, data condensation 0, total loss = 175.969482421875, avg loss = 58.656494140625
client 9, data condensation 200, total loss = 126.33038330078125, avg loss = 42.110127766927086
client 9, data condensation 400, total loss = 181.7694091796875, avg loss = 60.589803059895836
client 9, data condensation 600, total loss = 69.01141357421875, avg loss = 23.003804524739582
client 9, data condensation 800, total loss = 305.88275146484375, avg loss = 101.96091715494792
client 9, data condensation 1000, total loss = 72.50390625, avg loss = 24.16796875
client 9, data condensation 1200, total loss = 143.91265869140625, avg loss = 47.97088623046875
client 9, data condensation 1400, total loss = 186.6201171875, avg loss = 62.206705729166664
client 9, data condensation 1600, total loss = 92.8822021484375, avg loss = 30.960734049479168
client 9, data condensation 1800, total loss = 65.868408203125, avg loss = 21.956136067708332
client 9, data condensation 2000, total loss = 81.67803955078125, avg loss = 27.22601318359375
client 9, data condensation 2200, total loss = 194.62567138671875, avg loss = 64.87522379557292
client 9, data condensation 2400, total loss = 1828.10009765625, avg loss = 609.36669921875
client 9, data condensation 2600, total loss = 229.89373779296875, avg loss = 76.63124593098958
client 9, data condensation 2800, total loss = 127.762451171875, avg loss = 42.587483723958336
client 9, data condensation 3000, total loss = 78.34967041015625, avg loss = 26.116556803385418
client 9, data condensation 3200, total loss = 90.810302734375, avg loss = 30.270100911458332
client 9, data condensation 3400, total loss = 78.59466552734375, avg loss = 26.198221842447918
client 9, data condensation 3600, total loss = 85.600830078125, avg loss = 28.533610026041668
client 9, data condensation 3800, total loss = 41.5308837890625, avg loss = 13.8436279296875
client 9, data condensation 4000, total loss = 92.62576293945312, avg loss = 30.875254313151043
client 9, data condensation 4200, total loss = 275.36810302734375, avg loss = 91.78936767578125
client 9, data condensation 4400, total loss = 705.1326293945312, avg loss = 235.0442097981771
client 9, data condensation 4600, total loss = 94.6834716796875, avg loss = 31.5611572265625
client 9, data condensation 4800, total loss = 184.331787109375, avg loss = 61.443929036458336
client 9, data condensation 5000, total loss = 76.52288818359375, avg loss = 25.50762939453125
client 9, data condensation 5200, total loss = 52.62774658203125, avg loss = 17.542582194010418
client 9, data condensation 5400, total loss = 99.21966552734375, avg loss = 33.073221842447914
client 9, data condensation 5600, total loss = 95.99609375, avg loss = 31.998697916666668
client 9, data condensation 5800, total loss = 109.04473876953125, avg loss = 36.348246256510414
client 9, data condensation 6000, total loss = 127.8980712890625, avg loss = 42.6326904296875
client 9, data condensation 6200, total loss = 166.68408203125, avg loss = 55.561360677083336
client 9, data condensation 6400, total loss = 110.58355712890625, avg loss = 36.861185709635414
client 9, data condensation 6600, total loss = 49.56915283203125, avg loss = 16.523050944010418
client 9, data condensation 6800, total loss = 122.53729248046875, avg loss = 40.84576416015625
client 9, data condensation 7000, total loss = 132.235595703125, avg loss = 44.078531901041664
client 9, data condensation 7200, total loss = 115.51690673828125, avg loss = 38.505635579427086
client 9, data condensation 7400, total loss = 96.90960693359375, avg loss = 32.303202311197914
client 9, data condensation 7600, total loss = 44.4927978515625, avg loss = 14.8309326171875
client 9, data condensation 7800, total loss = 682.7498168945312, avg loss = 227.5832722981771
client 9, data condensation 8000, total loss = 422.00653076171875, avg loss = 140.6688435872396
client 9, data condensation 8200, total loss = 251.28326416015625, avg loss = 83.76108805338542
client 9, data condensation 8400, total loss = 129.3724365234375, avg loss = 43.1241455078125
client 9, data condensation 8600, total loss = 92.42724609375, avg loss = 30.80908203125
client 9, data condensation 8800, total loss = 35.98687744140625, avg loss = 11.995625813802084
client 9, data condensation 9000, total loss = 217.73760986328125, avg loss = 72.57920328776042
client 9, data condensation 9200, total loss = 91.09512329101562, avg loss = 30.365041097005207
client 9, data condensation 9400, total loss = 116.43206787109375, avg loss = 38.810689290364586
client 9, data condensation 9600, total loss = 127.5946044921875, avg loss = 42.531534830729164
client 9, data condensation 9800, total loss = 72.17291259765625, avg loss = 24.057637532552082
client 9, data condensation 10000, total loss = 65.9378662109375, avg loss = 21.979288736979168
Round 4, client 9 condense time: 819.1649589538574
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 14.5425,   1.2310,   3.3733,  -3.6978,  -0.7823,  -9.3595,  -8.2743,
          -2.6263,   5.0139,   1.1617],
        [  2.0139,  14.7586,  -4.3242,  -3.0317,  -3.5237,  -5.9913,  -4.6301,
          -1.0250,  -0.5896,   7.8874],
        [  0.6121,  -4.1922,   9.1418,   1.5599,   3.5339,   0.8295,   0.4563,
           1.2086,  -7.4334,  -4.5655],
        [ -3.7064,  -2.7623,   1.4383,   8.3827,  -0.3367,   5.4614,   2.5987,
           0.8413,  -7.9722,  -2.7914],
        [ -2.0872,  -3.6472,   4.6804,  -0.4788,   8.8336,   0.3773,   2.8295,
           3.9768,  -9.1429,  -4.4110],
        [ -5.7256,  -4.0115,   2.5441,   6.8941,   0.0276,  10.7593,   0.9550,
           3.0491,  -9.1389,  -4.1378],
        [ -5.0284,  -2.1758,   2.6981,   3.9387,   3.7733,   1.3016,  12.9738,
          -1.5290, -10.8658,  -3.7194],
        [ -3.3102,  -2.7245,   1.6974,  -0.6585,   3.8653,   0.6373,  -1.2796,
          12.7531,  -8.9858,  -0.5265],
        [  9.5966,   4.2646,  -0.7455,  -3.5000,  -3.4505,  -8.4776,  -9.5695,
          -4.1401,  12.2093,   4.6858],
        [  0.8410,   6.9794,  -3.7263,  -3.1115,  -3.5095,  -6.0110,  -4.8074,
           1.4862,  -0.5641,  13.7390]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 1, 9],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 7, 5],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 7, 2, 6],
        [6, 3, 4, 2, 5],
        [7, 4, 2, 5, 9],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
5
Round 4: # synthetic sample: 5060
total 24576.0MB, used 3079.06MB, free 21496.94MB
{0: {0: 682, 1: 34, 2: 45, 3: 31, 4: 18, 5: 12, 6: 12, 7: 16, 8: 89, 9: 61}, 1: {0: 50, 1: 721, 2: 6, 3: 12, 4: 10, 5: 13, 6: 12, 7: 17, 8: 17, 9: 142}, 2: {0: 132, 1: 10, 2: 402, 3: 94, 4: 95, 5: 66, 6: 89, 7: 70, 8: 19, 9: 23}, 3: {0: 33, 1: 19, 2: 71, 3: 413, 4: 44, 5: 199, 6: 118, 7: 45, 8: 10, 9: 48}, 4: {0: 62, 1: 12, 2: 95, 3: 56, 4: 449, 5: 36, 6: 121, 7: 133, 8: 16, 9: 20}, 5: {0: 27, 1: 11, 2: 57, 3: 179, 4: 53, 5: 518, 6: 59, 7: 68, 8: 13, 9: 15}, 6: {0: 8, 1: 8, 2: 56, 3: 66, 4: 61, 5: 23, 6: 746, 7: 11, 8: 5, 9: 16}, 7: {0: 27, 1: 13, 2: 36, 3: 62, 4: 41, 5: 76, 6: 29, 7: 665, 8: 7, 9: 44}, 8: {0: 193, 1: 97, 2: 18, 3: 25, 4: 5, 5: 8, 6: 12, 7: 14, 8: 520, 9: 108}, 9: {0: 44, 1: 108, 2: 5, 3: 21, 4: 6, 5: 12, 6: 14, 7: 20, 8: 30, 9: 740}}
round 4 evaluation: test acc is 0.5856, test loss = 2.383208
{0: {0: 685, 1: 76, 2: 43, 3: 33, 4: 21, 5: 10, 6: 11, 7: 18, 8: 20, 9: 83}, 1: {0: 26, 1: 775, 2: 6, 3: 10, 4: 9, 5: 7, 6: 5, 7: 10, 8: 0, 9: 152}, 2: {0: 127, 1: 38, 2: 310, 3: 116, 4: 136, 5: 95, 6: 52, 7: 82, 8: 2, 9: 42}, 3: {0: 32, 1: 56, 2: 32, 3: 407, 4: 65, 5: 208, 6: 62, 7: 54, 8: 3, 9: 81}, 4: {0: 61, 1: 23, 2: 52, 3: 67, 4: 495, 5: 58, 6: 67, 7: 134, 8: 2, 9: 41}, 5: {0: 22, 1: 24, 2: 31, 3: 214, 4: 79, 5: 484, 6: 26, 7: 81, 8: 2, 9: 37}, 6: {0: 10, 1: 31, 2: 30, 3: 104, 4: 95, 5: 40, 6: 630, 7: 22, 8: 1, 9: 37}, 7: {0: 20, 1: 22, 2: 16, 3: 55, 4: 57, 5: 70, 6: 14, 7: 659, 8: 1, 9: 86}, 8: {0: 342, 1: 218, 2: 13, 3: 28, 4: 9, 5: 10, 6: 5, 7: 16, 8: 157, 9: 202}, 9: {0: 34, 1: 145, 2: 4, 3: 19, 4: 4, 5: 8, 6: 3, 7: 11, 8: 4, 9: 768}}
epoch 0, train loss avg now = 0.109576, train contrast loss now = 0.972982, test acc now = 0.5370, test loss now = 3.124847
{0: {0: 678, 1: 52, 2: 54, 3: 26, 4: 11, 5: 9, 6: 12, 7: 9, 8: 103, 9: 46}, 1: {0: 33, 1: 808, 2: 8, 3: 16, 4: 4, 5: 9, 6: 13, 7: 5, 8: 24, 9: 80}, 2: {0: 127, 1: 21, 2: 423, 3: 100, 4: 115, 5: 67, 6: 80, 7: 29, 8: 17, 9: 21}, 3: {0: 33, 1: 41, 2: 79, 3: 451, 4: 42, 5: 184, 6: 93, 7: 19, 8: 15, 9: 43}, 4: {0: 72, 1: 18, 2: 94, 3: 73, 4: 462, 5: 45, 6: 124, 7: 79, 8: 15, 9: 18}, 5: {0: 25, 1: 18, 2: 61, 3: 201, 4: 51, 5: 526, 6: 49, 7: 39, 8: 13, 9: 17}, 6: {0: 11, 1: 19, 2: 61, 3: 103, 4: 49, 5: 27, 6: 706, 7: 5, 8: 4, 9: 15}, 7: {0: 40, 1: 21, 2: 50, 3: 79, 4: 45, 5: 100, 6: 29, 7: 591, 8: 7, 9: 38}, 8: {0: 169, 1: 103, 2: 11, 3: 33, 4: 3, 5: 5, 6: 9, 7: 4, 8: 598, 9: 65}, 9: {0: 49, 1: 197, 2: 8, 3: 21, 4: 4, 5: 14, 6: 9, 7: 7, 8: 38, 9: 653}}
epoch 100, train loss avg now = 0.021410, train contrast loss now = 0.302913, test acc now = 0.5896, test loss now = 2.292504
{0: {0: 559, 1: 54, 2: 65, 3: 29, 4: 14, 5: 10, 6: 19, 7: 18, 8: 167, 9: 65}, 1: {0: 14, 1: 772, 2: 9, 3: 14, 4: 7, 5: 14, 6: 18, 7: 16, 8: 30, 9: 106}, 2: {0: 88, 1: 11, 2: 422, 3: 106, 4: 103, 5: 69, 6: 86, 7: 57, 8: 39, 9: 19}, 3: {0: 16, 1: 19, 2: 66, 3: 450, 4: 67, 5: 183, 6: 106, 7: 38, 8: 16, 9: 39}, 4: {0: 40, 1: 10, 2: 84, 3: 59, 4: 518, 5: 33, 6: 116, 7: 95, 8: 29, 9: 16}, 5: {0: 11, 1: 9, 2: 51, 3: 184, 4: 69, 5: 531, 6: 47, 7: 60, 8: 22, 9: 16}, 6: {0: 6, 1: 5, 2: 62, 3: 78, 4: 63, 5: 18, 6: 731, 7: 9, 8: 10, 9: 18}, 7: {0: 18, 1: 14, 2: 38, 3: 64, 4: 69, 5: 86, 6: 22, 7: 638, 8: 14, 9: 37}, 8: {0: 85, 1: 97, 2: 12, 3: 29, 4: 3, 5: 10, 6: 11, 7: 12, 8: 661, 9: 80}, 9: {0: 26, 1: 141, 2: 3, 3: 19, 4: 10, 5: 16, 6: 13, 7: 17, 8: 41, 9: 714}}
epoch 200, train loss avg now = 0.013933, train contrast loss now = 0.299760, test acc now = 0.5996, test loss now = 2.226605
{0: {0: 683, 1: 55, 2: 50, 3: 35, 4: 21, 5: 4, 6: 9, 7: 4, 8: 77, 9: 62}, 1: {0: 34, 1: 817, 2: 8, 3: 16, 4: 11, 5: 3, 6: 13, 7: 3, 8: 14, 9: 81}, 2: {0: 124, 1: 20, 2: 423, 3: 147, 4: 118, 5: 43, 6: 66, 7: 21, 8: 21, 9: 17}, 3: {0: 27, 1: 28, 2: 66, 3: 562, 4: 70, 5: 107, 6: 74, 7: 15, 8: 10, 9: 41}, 4: {0: 59, 1: 15, 2: 95, 3: 92, 4: 523, 5: 19, 6: 104, 7: 62, 8: 16, 9: 15}, 5: {0: 15, 1: 21, 2: 63, 3: 327, 4: 74, 5: 411, 6: 29, 7: 30, 8: 12, 9: 18}, 6: {0: 10, 1: 12, 2: 53, 3: 108, 4: 86, 5: 15, 6: 694, 7: 2, 8: 5, 9: 15}, 7: {0: 33, 1: 29, 2: 41, 3: 114, 4: 100, 5: 64, 6: 22, 7: 533, 8: 6, 9: 58}, 8: {0: 191, 1: 134, 2: 14, 3: 32, 4: 7, 5: 3, 6: 6, 7: 5, 8: 510, 9: 98}, 9: {0: 42, 1: 177, 2: 2, 3: 26, 4: 15, 5: 6, 6: 10, 7: 5, 8: 21, 9: 696}}
epoch 300, train loss avg now = 0.012504, train contrast loss now = 0.298769, test acc now = 0.5852, test loss now = 2.444361
{0: {0: 672, 1: 23, 2: 39, 3: 50, 4: 14, 5: 7, 6: 12, 7: 8, 8: 107, 9: 68}, 1: {0: 45, 1: 675, 2: 9, 3: 31, 4: 9, 5: 10, 6: 24, 7: 17, 8: 28, 9: 152}, 2: {0: 122, 1: 8, 2: 370, 3: 160, 4: 140, 5: 44, 6: 86, 7: 33, 8: 18, 9: 19}, 3: {0: 23, 1: 8, 2: 56, 3: 628, 4: 59, 5: 88, 6: 80, 7: 20, 8: 11, 9: 27}, 4: {0: 61, 1: 5, 2: 59, 3: 112, 4: 531, 5: 19, 6: 114, 7: 70, 8: 15, 9: 14}, 5: {0: 21, 1: 7, 2: 47, 3: 354, 4: 66, 5: 384, 6: 43, 7: 48, 8: 17, 9: 13}, 6: {0: 7, 1: 3, 2: 40, 3: 112, 4: 56, 5: 14, 6: 751, 7: 3, 8: 3, 9: 11}, 7: {0: 29, 1: 9, 2: 32, 3: 127, 4: 67, 5: 52, 6: 34, 7: 606, 8: 10, 9: 34}, 8: {0: 187, 1: 61, 2: 16, 3: 49, 4: 7, 5: 6, 6: 6, 7: 7, 8: 562, 9: 99}, 9: {0: 43, 1: 96, 2: 2, 3: 40, 4: 11, 5: 8, 6: 13, 7: 13, 8: 30, 9: 744}}
epoch 400, train loss avg now = 0.009945, train contrast loss now = 0.297504, test acc now = 0.5923, test loss now = 2.478506
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 664, 1: 44, 2: 52, 3: 26, 4: 15, 5: 8, 6: 9, 7: 14, 8: 107, 9: 61}, 1: {0: 27, 1: 747, 2: 7, 3: 14, 4: 6, 5: 15, 6: 18, 7: 16, 8: 25, 9: 125}, 2: {0: 125, 1: 14, 2: 424, 3: 93, 4: 121, 5: 62, 6: 75, 7: 46, 8: 21, 9: 19}, 3: {0: 30, 1: 27, 2: 94, 3: 434, 4: 55, 5: 178, 6: 96, 7: 32, 8: 12, 9: 42}, 4: {0: 58, 1: 7, 2: 86, 3: 57, 4: 529, 5: 33, 6: 96, 7: 92, 8: 22, 9: 20}, 5: {0: 23, 1: 14, 2: 59, 3: 199, 4: 60, 5: 504, 6: 43, 7: 62, 8: 17, 9: 19}, 6: {0: 10, 1: 5, 2: 57, 3: 61, 4: 81, 5: 25, 6: 730, 7: 10, 8: 6, 9: 15}, 7: {0: 26, 1: 16, 2: 48, 3: 64, 4: 56, 5: 82, 6: 21, 7: 640, 8: 7, 9: 40}, 8: {0: 164, 1: 86, 2: 15, 3: 27, 4: 4, 5: 6, 6: 10, 7: 10, 8: 578, 9: 100}, 9: {0: 39, 1: 132, 2: 6, 3: 20, 4: 8, 5: 12, 6: 10, 7: 16, 8: 33, 9: 724}}
epoch 500, train loss avg now = 0.008853, train contrast loss now = 0.297049, test acc now = 0.5974, test loss now = 2.328576
{0: {0: 694, 1: 42, 2: 44, 3: 35, 4: 11, 5: 10, 6: 10, 7: 12, 8: 75, 9: 67}, 1: {0: 30, 1: 743, 2: 7, 3: 17, 4: 4, 5: 16, 6: 22, 7: 17, 8: 13, 9: 131}, 2: {0: 129, 1: 16, 2: 405, 3: 115, 4: 102, 5: 59, 6: 92, 7: 49, 8: 13, 9: 20}, 3: {0: 27, 1: 18, 2: 72, 3: 474, 4: 56, 5: 168, 6: 105, 7: 31, 8: 6, 9: 43}, 4: {0: 58, 1: 10, 2: 87, 3: 67, 4: 493, 5: 32, 6: 117, 7: 102, 8: 15, 9: 19}, 5: {0: 21, 1: 14, 2: 54, 3: 208, 4: 56, 5: 513, 6: 48, 7: 59, 8: 12, 9: 15}, 6: {0: 9, 1: 7, 2: 51, 3: 75, 4: 52, 5: 20, 6: 762, 7: 7, 8: 3, 9: 14}, 7: {0: 25, 1: 14, 2: 41, 3: 71, 4: 50, 5: 83, 6: 24, 7: 645, 8: 6, 9: 41}, 8: {0: 191, 1: 101, 2: 14, 3: 31, 4: 4, 5: 9, 6: 9, 7: 12, 8: 522, 9: 107}, 9: {0: 40, 1: 116, 2: 4, 3: 25, 4: 7, 5: 15, 6: 14, 7: 12, 8: 23, 9: 744}}
epoch 600, train loss avg now = 0.003828, train contrast loss now = 0.296120, test acc now = 0.5995, test loss now = 2.397265
{0: {0: 684, 1: 37, 2: 47, 3: 32, 4: 11, 5: 6, 6: 12, 7: 9, 8: 93, 9: 69}, 1: {0: 30, 1: 747, 2: 7, 3: 12, 4: 5, 5: 13, 6: 20, 7: 16, 8: 20, 9: 130}, 2: {0: 131, 1: 14, 2: 411, 3: 106, 4: 104, 5: 54, 6: 87, 7: 51, 8: 20, 9: 22}, 3: {0: 32, 1: 27, 2: 71, 3: 468, 4: 60, 5: 155, 6: 96, 7: 32, 8: 13, 9: 46}, 4: {0: 59, 1: 10, 2: 83, 3: 62, 4: 494, 5: 29, 6: 119, 7: 105, 8: 18, 9: 21}, 5: {0: 21, 1: 15, 2: 54, 3: 216, 4: 60, 5: 492, 6: 46, 7: 60, 8: 16, 9: 20}, 6: {0: 10, 1: 6, 2: 52, 3: 76, 4: 60, 5: 19, 6: 745, 7: 6, 8: 6, 9: 20}, 7: {0: 25, 1: 13, 2: 41, 3: 68, 4: 44, 5: 78, 6: 25, 7: 653, 8: 7, 9: 46}, 8: {0: 190, 1: 89, 2: 14, 3: 28, 4: 3, 5: 8, 6: 8, 7: 11, 8: 547, 9: 102}, 9: {0: 40, 1: 117, 2: 3, 3: 20, 4: 9, 5: 12, 6: 13, 7: 14, 8: 27, 9: 745}}
epoch 700, train loss avg now = 0.004312, train contrast loss now = 0.295719, test acc now = 0.5986, test loss now = 2.353116
{0: {0: 695, 1: 42, 2: 43, 3: 29, 4: 10, 5: 7, 6: 14, 7: 9, 8: 85, 9: 66}, 1: {0: 29, 1: 751, 2: 9, 3: 14, 4: 4, 5: 14, 6: 20, 7: 14, 8: 21, 9: 124}, 2: {0: 137, 1: 12, 2: 408, 3: 103, 4: 96, 5: 58, 6: 93, 7: 53, 8: 18, 9: 22}, 3: {0: 35, 1: 23, 2: 78, 3: 450, 4: 58, 5: 159, 6: 105, 7: 31, 8: 15, 9: 46}, 4: {0: 65, 1: 9, 2: 82, 3: 57, 4: 485, 5: 29, 6: 131, 7: 101, 8: 18, 9: 23}, 5: {0: 25, 1: 14, 2: 49, 3: 206, 4: 55, 5: 504, 6: 52, 7: 62, 8: 14, 9: 19}, 6: {0: 11, 1: 6, 2: 54, 3: 70, 4: 50, 5: 21, 6: 759, 7: 6, 8: 5, 9: 18}, 7: {0: 29, 1: 16, 2: 44, 3: 66, 4: 39, 5: 79, 6: 28, 7: 654, 8: 7, 9: 38}, 8: {0: 187, 1: 95, 2: 10, 3: 28, 4: 3, 5: 7, 6: 10, 7: 10, 8: 553, 9: 97}, 9: {0: 44, 1: 115, 2: 4, 3: 20, 4: 8, 5: 13, 6: 12, 7: 13, 8: 28, 9: 743}}
epoch 800, train loss avg now = 0.003651, train contrast loss now = 0.295840, test acc now = 0.6002, test loss now = 2.389877
{0: {0: 685, 1: 39, 2: 45, 3: 31, 4: 11, 5: 7, 6: 11, 7: 11, 8: 90, 9: 70}, 1: {0: 29, 1: 739, 2: 7, 3: 14, 4: 4, 5: 16, 6: 20, 7: 15, 8: 19, 9: 137}, 2: {0: 130, 1: 14, 2: 403, 3: 114, 4: 101, 5: 59, 6: 87, 7: 51, 8: 18, 9: 23}, 3: {0: 29, 1: 24, 2: 69, 3: 469, 4: 61, 5: 159, 6: 97, 7: 30, 8: 14, 9: 48}, 4: {0: 58, 1: 11, 2: 82, 3: 64, 4: 493, 5: 35, 6: 118, 7: 99, 8: 17, 9: 23}, 5: {0: 22, 1: 14, 2: 48, 3: 213, 4: 55, 5: 509, 6: 44, 7: 60, 8: 15, 9: 20}, 6: {0: 11, 1: 7, 2: 51, 3: 79, 4: 58, 5: 20, 6: 744, 7: 6, 8: 4, 9: 20}, 7: {0: 25, 1: 14, 2: 37, 3: 69, 4: 45, 5: 87, 6: 25, 7: 646, 8: 6, 9: 46}, 8: {0: 179, 1: 92, 2: 10, 3: 31, 4: 3, 5: 7, 6: 10, 7: 10, 8: 556, 9: 102}, 9: {0: 39, 1: 109, 2: 3, 3: 21, 4: 9, 5: 11, 6: 12, 7: 11, 8: 29, 9: 756}}
epoch 900, train loss avg now = 0.003867, train contrast loss now = 0.296234, test acc now = 0.6000, test loss now = 2.359448
{0: {0: 692, 1: 40, 2: 43, 3: 31, 4: 11, 5: 10, 6: 9, 7: 12, 8: 81, 9: 71}, 1: {0: 31, 1: 745, 2: 7, 3: 14, 4: 4, 5: 15, 6: 21, 7: 17, 8: 19, 9: 127}, 2: {0: 132, 1: 15, 2: 394, 3: 109, 4: 109, 5: 60, 6: 87, 7: 57, 8: 15, 9: 22}, 3: {0: 32, 1: 21, 2: 69, 3: 462, 4: 66, 5: 160, 6: 102, 7: 31, 8: 11, 9: 46}, 4: {0: 59, 1: 9, 2: 79, 3: 63, 4: 504, 5: 33, 6: 114, 7: 105, 8: 15, 9: 19}, 5: {0: 21, 1: 14, 2: 51, 3: 213, 4: 58, 5: 508, 6: 42, 7: 60, 8: 14, 9: 19}, 6: {0: 9, 1: 6, 2: 56, 3: 80, 4: 67, 5: 22, 6: 730, 7: 8, 8: 4, 9: 18}, 7: {0: 27, 1: 13, 2: 37, 3: 68, 4: 49, 5: 84, 6: 24, 7: 652, 8: 6, 9: 40}, 8: {0: 189, 1: 96, 2: 12, 3: 30, 4: 5, 5: 8, 6: 12, 7: 11, 8: 537, 9: 100}, 9: {0: 42, 1: 114, 2: 2, 3: 23, 4: 10, 5: 11, 6: 12, 7: 14, 8: 26, 9: 746}}
epoch 1000, train loss avg now = 0.003634, train contrast loss now = 0.296199, test acc now = 0.5970, test loss now = 2.403461
epoch avg loss = 3.6336243003533176e-06, total time = 10609.42526435852
total 24576.0MB, used 3207.06MB, free 21368.94MB
Round 4 finish, update the prev_syn_proto
torch.Size([505, 3, 32, 32])
torch.Size([505, 3, 32, 32])
torch.Size([520, 3, 32, 32])
torch.Size([500, 3, 32, 32])
torch.Size([500, 3, 32, 32])
torch.Size([525, 3, 32, 32])
torch.Size([505, 3, 32, 32])
torch.Size([500, 3, 32, 32])
torch.Size([500, 3, 32, 32])
torch.Size([500, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 692, 1: 40, 2: 43, 3: 31, 4: 11, 5: 10, 6: 9, 7: 12, 8: 81, 9: 71}, 1: {0: 31, 1: 745, 2: 7, 3: 14, 4: 4, 5: 15, 6: 21, 7: 17, 8: 19, 9: 127}, 2: {0: 132, 1: 15, 2: 394, 3: 109, 4: 109, 5: 60, 6: 87, 7: 57, 8: 15, 9: 22}, 3: {0: 32, 1: 21, 2: 69, 3: 462, 4: 66, 5: 160, 6: 102, 7: 31, 8: 11, 9: 46}, 4: {0: 59, 1: 9, 2: 79, 3: 63, 4: 504, 5: 33, 6: 114, 7: 105, 8: 15, 9: 19}, 5: {0: 21, 1: 14, 2: 51, 3: 213, 4: 58, 5: 508, 6: 42, 7: 60, 8: 14, 9: 19}, 6: {0: 9, 1: 6, 2: 56, 3: 80, 4: 67, 5: 22, 6: 730, 7: 8, 8: 4, 9: 18}, 7: {0: 27, 1: 13, 2: 37, 3: 68, 4: 49, 5: 84, 6: 24, 7: 652, 8: 6, 9: 40}, 8: {0: 189, 1: 96, 2: 12, 3: 30, 4: 5, 5: 8, 6: 12, 7: 11, 8: 537, 9: 100}, 9: {0: 42, 1: 114, 2: 2, 3: 23, 4: 10, 5: 11, 6: 12, 7: 14, 8: 26, 9: 746}}
round 4 evaluation: test acc is 0.5970, test loss = 2.403461
 ====== round 5 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3207.06MB, free 21368.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1135  180  111   99   80  100   98  123  184 1483], bin edged: [0.0001952  0.00021035 0.0002255  0.00024065 0.0002558  0.00027095
 0.0002861  0.00030125 0.0003164  0.00033155 0.0003467 ]
class 7 have 4999 samples, histogram: [2862  194  111   97   96   84   87  114  156 1198], bin edged: [0.00015908 0.00017142 0.00018377 0.00019612 0.00020846 0.00022081
 0.00023316 0.0002455  0.00025785 0.0002702  0.00028254]
client 0, data condensation 0, total loss = 56.59210205078125, avg loss = 28.296051025390625
client 0, data condensation 200, total loss = 5.43011474609375, avg loss = 2.715057373046875
client 0, data condensation 400, total loss = 7.43682861328125, avg loss = 3.718414306640625
client 0, data condensation 600, total loss = 9.501953125, avg loss = 4.7509765625
client 0, data condensation 800, total loss = 13.580810546875, avg loss = 6.7904052734375
client 0, data condensation 1000, total loss = 10.88702392578125, avg loss = 5.443511962890625
client 0, data condensation 1200, total loss = 3.964111328125, avg loss = 1.9820556640625
client 0, data condensation 1400, total loss = 10.91314697265625, avg loss = 5.456573486328125
client 0, data condensation 1600, total loss = 17.7252197265625, avg loss = 8.86260986328125
client 0, data condensation 1800, total loss = 31.19512939453125, avg loss = 15.597564697265625
client 0, data condensation 2000, total loss = 27.9039306640625, avg loss = 13.95196533203125
client 0, data condensation 2200, total loss = 6.25750732421875, avg loss = 3.128753662109375
client 0, data condensation 2400, total loss = 40.8311767578125, avg loss = 20.41558837890625
client 0, data condensation 2600, total loss = 5.97564697265625, avg loss = 2.987823486328125
client 0, data condensation 2800, total loss = 4.2640380859375, avg loss = 2.13201904296875
client 0, data condensation 3000, total loss = 8.07672119140625, avg loss = 4.038360595703125
client 0, data condensation 3200, total loss = 16.9200439453125, avg loss = 8.46002197265625
client 0, data condensation 3400, total loss = 30.2724609375, avg loss = 15.13623046875
client 0, data condensation 3600, total loss = 5.0029296875, avg loss = 2.50146484375
client 0, data condensation 3800, total loss = 25.81890869140625, avg loss = 12.909454345703125
client 0, data condensation 4000, total loss = 5.12322998046875, avg loss = 2.561614990234375
client 0, data condensation 4200, total loss = 16.78289794921875, avg loss = 8.391448974609375
client 0, data condensation 4400, total loss = 5.09783935546875, avg loss = 2.548919677734375
client 0, data condensation 4600, total loss = 9.29974365234375, avg loss = 4.649871826171875
client 0, data condensation 4800, total loss = 4.5614013671875, avg loss = 2.28070068359375
client 0, data condensation 5000, total loss = 10.11566162109375, avg loss = 5.057830810546875
client 0, data condensation 5200, total loss = 11.4473876953125, avg loss = 5.72369384765625
client 0, data condensation 5400, total loss = 7.3739013671875, avg loss = 3.68695068359375
client 0, data condensation 5600, total loss = 33.731536865234375, avg loss = 16.865768432617188
client 0, data condensation 5800, total loss = 8.8939208984375, avg loss = 4.44696044921875
client 0, data condensation 6000, total loss = 4.78826904296875, avg loss = 2.394134521484375
client 0, data condensation 6200, total loss = 35.997955322265625, avg loss = 17.998977661132812
client 0, data condensation 6400, total loss = 9.7421875, avg loss = 4.87109375
client 0, data condensation 6600, total loss = 9.79510498046875, avg loss = 4.897552490234375
client 0, data condensation 6800, total loss = 7.7547607421875, avg loss = 3.87738037109375
client 0, data condensation 7000, total loss = 53.3558349609375, avg loss = 26.67791748046875
client 0, data condensation 7200, total loss = 9.56512451171875, avg loss = 4.782562255859375
client 0, data condensation 7400, total loss = 14.37353515625, avg loss = 7.186767578125
client 0, data condensation 7600, total loss = 5.00494384765625, avg loss = 2.502471923828125
client 0, data condensation 7800, total loss = 28.45025634765625, avg loss = 14.225128173828125
client 0, data condensation 8000, total loss = 20.08575439453125, avg loss = 10.042877197265625
client 0, data condensation 8200, total loss = 24.3912353515625, avg loss = 12.19561767578125
client 0, data condensation 8400, total loss = 10.30224609375, avg loss = 5.151123046875
client 0, data condensation 8600, total loss = 8.10113525390625, avg loss = 4.050567626953125
client 0, data condensation 8800, total loss = 4.6160888671875, avg loss = 2.30804443359375
client 0, data condensation 9000, total loss = 31.570556640625, avg loss = 15.7852783203125
client 0, data condensation 9200, total loss = 3.830810546875, avg loss = 1.9154052734375
client 0, data condensation 9400, total loss = 12.21051025390625, avg loss = 6.105255126953125
client 0, data condensation 9600, total loss = 5.80645751953125, avg loss = 2.903228759765625
client 0, data condensation 9800, total loss = 5.82379150390625, avg loss = 2.911895751953125
client 0, data condensation 10000, total loss = 8.2655029296875, avg loss = 4.13275146484375
Round 5, client 0 condense time: 762.0864672660828
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [48 10  7  8  5  3  6  3 10 75], bin edged: [0.00396789 0.00427586 0.00458382 0.00489179 0.00519975 0.00550771
 0.00581568 0.00612364 0.00643161 0.00673957 0.00704753]
class 4 have 4958 samples, histogram: [1832  276  177  130  119  140  120  146  225 1793], bin edged: [0.00014599 0.00015732 0.00016866 0.00017999 0.00019132 0.00020265
 0.00021398 0.00022531 0.00023664 0.00024797 0.00025931]
client 1, data condensation 0, total loss = 162.9124755859375, avg loss = 81.45623779296875
client 1, data condensation 200, total loss = 115.89019775390625, avg loss = 57.945098876953125
client 1, data condensation 400, total loss = 96.62091064453125, avg loss = 48.310455322265625
client 1, data condensation 600, total loss = 216.35479736328125, avg loss = 108.17739868164062
client 1, data condensation 800, total loss = 46.23028564453125, avg loss = 23.115142822265625
client 1, data condensation 1000, total loss = 63.77435302734375, avg loss = 31.887176513671875
client 1, data condensation 1200, total loss = 158.12762451171875, avg loss = 79.06381225585938
client 1, data condensation 1400, total loss = 96.39483642578125, avg loss = 48.197418212890625
client 1, data condensation 1600, total loss = 165.547607421875, avg loss = 82.7738037109375
client 1, data condensation 1800, total loss = 85.15081787109375, avg loss = 42.575408935546875
client 1, data condensation 2000, total loss = 60.4698486328125, avg loss = 30.23492431640625
client 1, data condensation 2200, total loss = 89.26385498046875, avg loss = 44.631927490234375
client 1, data condensation 2400, total loss = 68.13177490234375, avg loss = 34.065887451171875
client 1, data condensation 2600, total loss = 150.0281982421875, avg loss = 75.01409912109375
client 1, data condensation 2800, total loss = 57.69390869140625, avg loss = 28.846954345703125
client 1, data condensation 3000, total loss = 146.826904296875, avg loss = 73.4134521484375
client 1, data condensation 3200, total loss = 73.80548095703125, avg loss = 36.902740478515625
client 1, data condensation 3400, total loss = 37.0498046875, avg loss = 18.52490234375
client 1, data condensation 3600, total loss = 106.34185791015625, avg loss = 53.170928955078125
client 1, data condensation 3800, total loss = 43.63214111328125, avg loss = 21.816070556640625
client 1, data condensation 4000, total loss = 115.563720703125, avg loss = 57.7818603515625
client 1, data condensation 4200, total loss = 106.32244873046875, avg loss = 53.161224365234375
client 1, data condensation 4400, total loss = 269.4124755859375, avg loss = 134.70623779296875
client 1, data condensation 4600, total loss = 38.53375244140625, avg loss = 19.266876220703125
client 1, data condensation 4800, total loss = 37.42584228515625, avg loss = 18.712921142578125
client 1, data condensation 5000, total loss = 32.6280517578125, avg loss = 16.31402587890625
client 1, data condensation 5200, total loss = 124.223388671875, avg loss = 62.1116943359375
client 1, data condensation 5400, total loss = 52.168212890625, avg loss = 26.0841064453125
client 1, data condensation 5600, total loss = 175.27703857421875, avg loss = 87.63851928710938
client 1, data condensation 5800, total loss = 14.426513671875, avg loss = 7.2132568359375
client 1, data condensation 6000, total loss = 77.76287841796875, avg loss = 38.881439208984375
client 1, data condensation 6200, total loss = 77.88507080078125, avg loss = 38.942535400390625
client 1, data condensation 6400, total loss = 76.2972412109375, avg loss = 38.14862060546875
client 1, data condensation 6600, total loss = 33.84173583984375, avg loss = 16.920867919921875
client 1, data condensation 6800, total loss = 10.536376953125, avg loss = 5.2681884765625
client 1, data condensation 7000, total loss = 276.3564453125, avg loss = 138.17822265625
client 1, data condensation 7200, total loss = 527.8881225585938, avg loss = 263.9440612792969
client 1, data condensation 7400, total loss = 46.91180419921875, avg loss = 23.455902099609375
client 1, data condensation 7600, total loss = 78.17236328125, avg loss = 39.086181640625
client 1, data condensation 7800, total loss = 67.1236572265625, avg loss = 33.56182861328125
client 1, data condensation 8000, total loss = 27.43365478515625, avg loss = 13.716827392578125
client 1, data condensation 8200, total loss = 29.21124267578125, avg loss = 14.605621337890625
client 1, data condensation 8400, total loss = 41.92291259765625, avg loss = 20.961456298828125
client 1, data condensation 8600, total loss = 74.48455810546875, avg loss = 37.242279052734375
client 1, data condensation 8800, total loss = 59.66778564453125, avg loss = 29.833892822265625
client 1, data condensation 9000, total loss = 58.16650390625, avg loss = 29.083251953125
client 1, data condensation 9200, total loss = 49.751220703125, avg loss = 24.8756103515625
client 1, data condensation 9400, total loss = 44.625732421875, avg loss = 22.3128662109375
client 1, data condensation 9600, total loss = 81.8223876953125, avg loss = 40.91119384765625
client 1, data condensation 9800, total loss = 50.4822998046875, avg loss = 25.24114990234375
client 1, data condensation 10000, total loss = 30.30987548828125, avg loss = 15.154937744140625
Round 5, client 1 condense time: 675.1018421649933
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2823.06MB, free 21752.94MB
total 24576.0MB, used 2823.06MB, free 21752.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [142   8   9   6   5   6   4   5   6  51], bin edged: [0.00334203 0.00360142 0.00386081 0.00412019 0.00437958 0.00463897
 0.00489836 0.00515775 0.00541714 0.00567652 0.00593591]
client 2, data condensation 0, total loss = 82.40301513671875, avg loss = 82.40301513671875
client 2, data condensation 200, total loss = 46.44696044921875, avg loss = 46.44696044921875
client 2, data condensation 400, total loss = 40.56884765625, avg loss = 40.56884765625
client 2, data condensation 600, total loss = 45.3446044921875, avg loss = 45.3446044921875
client 2, data condensation 800, total loss = 43.66845703125, avg loss = 43.66845703125
client 2, data condensation 1000, total loss = 41.44635009765625, avg loss = 41.44635009765625
client 2, data condensation 1200, total loss = 59.08538818359375, avg loss = 59.08538818359375
client 2, data condensation 1400, total loss = 313.8006591796875, avg loss = 313.8006591796875
client 2, data condensation 1600, total loss = 110.9461669921875, avg loss = 110.9461669921875
client 2, data condensation 1800, total loss = 594.7823486328125, avg loss = 594.7823486328125
client 2, data condensation 2000, total loss = 905.8287353515625, avg loss = 905.8287353515625
client 2, data condensation 2200, total loss = 76.0506591796875, avg loss = 76.0506591796875
client 2, data condensation 2400, total loss = 51.9482421875, avg loss = 51.9482421875
client 2, data condensation 2600, total loss = 20.914306640625, avg loss = 20.914306640625
client 2, data condensation 2800, total loss = 163.54791259765625, avg loss = 163.54791259765625
client 2, data condensation 3000, total loss = 41.70745849609375, avg loss = 41.70745849609375
client 2, data condensation 3200, total loss = 39.838623046875, avg loss = 39.838623046875
client 2, data condensation 3400, total loss = 60.230712890625, avg loss = 60.230712890625
client 2, data condensation 3600, total loss = 54.7806396484375, avg loss = 54.7806396484375
client 2, data condensation 3800, total loss = 22.73712158203125, avg loss = 22.73712158203125
client 2, data condensation 4000, total loss = 93.20623779296875, avg loss = 93.20623779296875
client 2, data condensation 4200, total loss = 26.76580810546875, avg loss = 26.76580810546875
client 2, data condensation 4400, total loss = 48.7529296875, avg loss = 48.7529296875
client 2, data condensation 4600, total loss = 83.7003173828125, avg loss = 83.7003173828125
client 2, data condensation 4800, total loss = 12.96038818359375, avg loss = 12.96038818359375
client 2, data condensation 5000, total loss = 100.1861572265625, avg loss = 100.1861572265625
client 2, data condensation 5200, total loss = 25.287109375, avg loss = 25.287109375
client 2, data condensation 5400, total loss = 67.8436279296875, avg loss = 67.8436279296875
client 2, data condensation 5600, total loss = 96.62225341796875, avg loss = 96.62225341796875
client 2, data condensation 5800, total loss = 51.55328369140625, avg loss = 51.55328369140625
client 2, data condensation 6000, total loss = 37.00445556640625, avg loss = 37.00445556640625
client 2, data condensation 6200, total loss = 70.02911376953125, avg loss = 70.02911376953125
client 2, data condensation 6400, total loss = 83.04705810546875, avg loss = 83.04705810546875
client 2, data condensation 6600, total loss = 13.719970703125, avg loss = 13.719970703125
client 2, data condensation 6800, total loss = 37.96575927734375, avg loss = 37.96575927734375
client 2, data condensation 7000, total loss = 75.537353515625, avg loss = 75.537353515625
client 2, data condensation 7200, total loss = 38.979736328125, avg loss = 38.979736328125
client 2, data condensation 7400, total loss = 39.23883056640625, avg loss = 39.23883056640625
client 2, data condensation 7600, total loss = 29.39642333984375, avg loss = 29.39642333984375
client 2, data condensation 7800, total loss = 21.61383056640625, avg loss = 21.61383056640625
client 2, data condensation 8000, total loss = 31.226806640625, avg loss = 31.226806640625
client 2, data condensation 8200, total loss = 29.320556640625, avg loss = 29.320556640625
client 2, data condensation 8400, total loss = 47.0198974609375, avg loss = 47.0198974609375
client 2, data condensation 8600, total loss = 89.13836669921875, avg loss = 89.13836669921875
client 2, data condensation 8800, total loss = 24.25408935546875, avg loss = 24.25408935546875
client 2, data condensation 9000, total loss = 45.73193359375, avg loss = 45.73193359375
client 2, data condensation 9200, total loss = 75.28509521484375, avg loss = 75.28509521484375
client 2, data condensation 9400, total loss = 15.39630126953125, avg loss = 15.39630126953125
client 2, data condensation 9600, total loss = 81.10626220703125, avg loss = 81.10626220703125
client 2, data condensation 9800, total loss = 407.01593017578125, avg loss = 407.01593017578125
client 2, data condensation 10000, total loss = 49.12701416015625, avg loss = 49.12701416015625
Round 5, client 2 condense time: 269.070588350296
client 2, class 9 have 242 samples
total 24576.0MB, used 2437.06MB, free 22138.94MB
total 24576.0MB, used 2437.06MB, free 22138.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [504  32  18  14  22  18  12  19  23 185], bin edged: [0.0009526  0.00102653 0.00110047 0.0011744  0.00124833 0.00132227
 0.0013962  0.00147014 0.00154407 0.00161801 0.00169194]
class 2 have 1094 samples, histogram: [294  45  34  27  30  29  31  39  57 508], bin edged: [0.00062324 0.00067161 0.00071998 0.00076835 0.00081673 0.0008651
 0.00091347 0.00096184 0.00101022 0.00105859 0.00110696]
client 3, data condensation 0, total loss = 59.21221923828125, avg loss = 29.606109619140625
client 3, data condensation 200, total loss = 11.4315185546875, avg loss = 5.71575927734375
client 3, data condensation 400, total loss = 2.9921875, avg loss = 1.49609375
client 3, data condensation 600, total loss = 14.25579833984375, avg loss = 7.127899169921875
client 3, data condensation 800, total loss = 8.68341064453125, avg loss = 4.341705322265625
client 3, data condensation 1000, total loss = 9.79534912109375, avg loss = 4.897674560546875
client 3, data condensation 1200, total loss = 10.5064697265625, avg loss = 5.25323486328125
client 3, data condensation 1400, total loss = 28.05853271484375, avg loss = 14.029266357421875
client 3, data condensation 1600, total loss = 15.22113037109375, avg loss = 7.610565185546875
client 3, data condensation 1800, total loss = 30.1739501953125, avg loss = 15.08697509765625
client 3, data condensation 2000, total loss = 10.81976318359375, avg loss = 5.409881591796875
client 3, data condensation 2200, total loss = 12.3890380859375, avg loss = 6.19451904296875
client 3, data condensation 2400, total loss = 25.85919189453125, avg loss = 12.929595947265625
client 3, data condensation 2600, total loss = 20.72723388671875, avg loss = 10.363616943359375
client 3, data condensation 2800, total loss = 11.0650634765625, avg loss = 5.53253173828125
client 3, data condensation 3000, total loss = 15.1568603515625, avg loss = 7.57843017578125
client 3, data condensation 3200, total loss = 35.1829833984375, avg loss = 17.59149169921875
client 3, data condensation 3400, total loss = 9.2169189453125, avg loss = 4.60845947265625
client 3, data condensation 3600, total loss = 8.45489501953125, avg loss = 4.227447509765625
client 3, data condensation 3800, total loss = 32.2178955078125, avg loss = 16.10894775390625
client 3, data condensation 4000, total loss = 13.19305419921875, avg loss = 6.596527099609375
client 3, data condensation 4200, total loss = 18.072509765625, avg loss = 9.0362548828125
client 3, data condensation 4400, total loss = 11.3404541015625, avg loss = 5.67022705078125
client 3, data condensation 4600, total loss = 7.30767822265625, avg loss = 3.653839111328125
client 3, data condensation 4800, total loss = 13.1947021484375, avg loss = 6.59735107421875
client 3, data condensation 5000, total loss = 9.79022216796875, avg loss = 4.895111083984375
client 3, data condensation 5200, total loss = 18.697509765625, avg loss = 9.3487548828125
client 3, data condensation 5400, total loss = 12.0003662109375, avg loss = 6.00018310546875
client 3, data condensation 5600, total loss = 8.57379150390625, avg loss = 4.286895751953125
client 3, data condensation 5800, total loss = 20.925537109375, avg loss = 10.4627685546875
client 3, data condensation 6000, total loss = 12.40032958984375, avg loss = 6.200164794921875
client 3, data condensation 6200, total loss = 11.89422607421875, avg loss = 5.947113037109375
client 3, data condensation 6400, total loss = 23.52679443359375, avg loss = 11.763397216796875
client 3, data condensation 6600, total loss = 12.499267578125, avg loss = 6.2496337890625
client 3, data condensation 6800, total loss = 8.57037353515625, avg loss = 4.285186767578125
client 3, data condensation 7000, total loss = 11.10955810546875, avg loss = 5.554779052734375
client 3, data condensation 7200, total loss = 10.94476318359375, avg loss = 5.472381591796875
client 3, data condensation 7400, total loss = 12.94146728515625, avg loss = 6.470733642578125
client 3, data condensation 7600, total loss = 11.14727783203125, avg loss = 5.573638916015625
client 3, data condensation 7800, total loss = 10.2374267578125, avg loss = 5.11871337890625
client 3, data condensation 8000, total loss = 24.5899658203125, avg loss = 12.29498291015625
client 3, data condensation 8200, total loss = 25.113037109375, avg loss = 12.5565185546875
client 3, data condensation 8400, total loss = 14.088134765625, avg loss = 7.0440673828125
client 3, data condensation 8600, total loss = 5.28125, avg loss = 2.640625
client 3, data condensation 8800, total loss = 5.34423828125, avg loss = 2.672119140625
client 3, data condensation 9000, total loss = 19.87945556640625, avg loss = 9.939727783203125
client 3, data condensation 9200, total loss = 19.0797119140625, avg loss = 9.53985595703125
client 3, data condensation 9400, total loss = 5.02130126953125, avg loss = 2.510650634765625
client 3, data condensation 9600, total loss = 13.05267333984375, avg loss = 6.526336669921875
client 3, data condensation 9800, total loss = 7.538330078125, avg loss = 3.7691650390625
client 3, data condensation 10000, total loss = 10.14111328125, avg loss = 5.070556640625
Round 5, client 3 condense time: 559.8921847343445
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2819.06MB, free 21756.94MB
total 24576.0MB, used 2819.06MB, free 21756.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2537  166  107   85   74   80   75   84  123  821], bin edged: [0.0001966  0.00021186 0.00022711 0.00024237 0.00025763 0.00027289
 0.00028815 0.00030341 0.00031867 0.00033393 0.00034918]
class 5 have 307 samples, histogram: [129  14   9  10   7   7   9  11  13  98], bin edged: [0.00241432 0.00260171 0.00278909 0.00297648 0.00316386 0.00335125
 0.00353863 0.00372602 0.0039134  0.00410079 0.00428817]
client 4, data condensation 0, total loss = 65.3878173828125, avg loss = 32.69390869140625
client 4, data condensation 200, total loss = 30.99652099609375, avg loss = 15.498260498046875
client 4, data condensation 400, total loss = 40.2030029296875, avg loss = 20.10150146484375
client 4, data condensation 600, total loss = 24.8543701171875, avg loss = 12.42718505859375
client 4, data condensation 800, total loss = 37.52410888671875, avg loss = 18.762054443359375
client 4, data condensation 1000, total loss = 42.85369873046875, avg loss = 21.426849365234375
client 4, data condensation 1200, total loss = 20.94427490234375, avg loss = 10.472137451171875
client 4, data condensation 1400, total loss = 25.70660400390625, avg loss = 12.853302001953125
client 4, data condensation 1600, total loss = 15.75634765625, avg loss = 7.878173828125
client 4, data condensation 1800, total loss = 27.71392822265625, avg loss = 13.856964111328125
client 4, data condensation 2000, total loss = 47.925537109375, avg loss = 23.9627685546875
client 4, data condensation 2200, total loss = 19.4473876953125, avg loss = 9.72369384765625
client 4, data condensation 2400, total loss = 36.7843017578125, avg loss = 18.39215087890625
client 4, data condensation 2600, total loss = 78.112548828125, avg loss = 39.0562744140625
client 4, data condensation 2800, total loss = 22.86090087890625, avg loss = 11.430450439453125
client 4, data condensation 3000, total loss = 37.66204833984375, avg loss = 18.831024169921875
client 4, data condensation 3200, total loss = 221.792724609375, avg loss = 110.8963623046875
client 4, data condensation 3400, total loss = 12.1217041015625, avg loss = 6.06085205078125
client 4, data condensation 3600, total loss = 24.51409912109375, avg loss = 12.257049560546875
client 4, data condensation 3800, total loss = 49.087646484375, avg loss = 24.5438232421875
client 4, data condensation 4000, total loss = 12.320556640625, avg loss = 6.1602783203125
client 4, data condensation 4200, total loss = 1323.637939453125, avg loss = 661.8189697265625
client 4, data condensation 4400, total loss = 26.4930419921875, avg loss = 13.24652099609375
client 4, data condensation 4600, total loss = 58.9188232421875, avg loss = 29.45941162109375
client 4, data condensation 4800, total loss = 32.1092529296875, avg loss = 16.05462646484375
client 4, data condensation 5000, total loss = 17.10498046875, avg loss = 8.552490234375
client 4, data condensation 5200, total loss = 70.37451171875, avg loss = 35.187255859375
client 4, data condensation 5400, total loss = 21.5284423828125, avg loss = 10.76422119140625
client 4, data condensation 5600, total loss = 50.9840087890625, avg loss = 25.49200439453125
client 4, data condensation 5800, total loss = 42.2196044921875, avg loss = 21.10980224609375
client 4, data condensation 6000, total loss = 26.496337890625, avg loss = 13.2481689453125
client 4, data condensation 6200, total loss = 31.41961669921875, avg loss = 15.709808349609375
client 4, data condensation 6400, total loss = 97.98101806640625, avg loss = 48.990509033203125
client 4, data condensation 6600, total loss = 38.05145263671875, avg loss = 19.025726318359375
client 4, data condensation 6800, total loss = 28.04278564453125, avg loss = 14.021392822265625
client 4, data condensation 7000, total loss = 18.6361083984375, avg loss = 9.31805419921875
client 4, data condensation 7200, total loss = 12.4906005859375, avg loss = 6.24530029296875
client 4, data condensation 7400, total loss = 18.886474609375, avg loss = 9.4432373046875
client 4, data condensation 7600, total loss = 25.657470703125, avg loss = 12.8287353515625
client 4, data condensation 7800, total loss = 36.5565185546875, avg loss = 18.27825927734375
client 4, data condensation 8000, total loss = 20.11004638671875, avg loss = 10.055023193359375
client 4, data condensation 8200, total loss = 20.8887939453125, avg loss = 10.44439697265625
client 4, data condensation 8400, total loss = 50.100830078125, avg loss = 25.0504150390625
client 4, data condensation 8600, total loss = 29.403076171875, avg loss = 14.7015380859375
client 4, data condensation 8800, total loss = 55.05328369140625, avg loss = 27.526641845703125
client 4, data condensation 9000, total loss = 50.0047607421875, avg loss = 25.00238037109375
client 4, data condensation 9200, total loss = 61.39984130859375, avg loss = 30.699920654296875
client 4, data condensation 9400, total loss = 34.88140869140625, avg loss = 17.440704345703125
client 4, data condensation 9600, total loss = 14.4091796875, avg loss = 7.20458984375
client 4, data condensation 9800, total loss = 8.8765869140625, avg loss = 4.43829345703125
client 4, data condensation 10000, total loss = 22.15478515625, avg loss = 11.077392578125
Round 5, client 4 condense time: 670.9175786972046
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [1890  289  158  117  135  127  127  158  220 1778], bin edged: [0.0001453  0.00015658 0.00016786 0.00017914 0.00019041 0.00020169
 0.00021297 0.00022425 0.00023552 0.0002468  0.00025808]
client 5, data condensation 0, total loss = 59.52618408203125, avg loss = 59.52618408203125
client 5, data condensation 200, total loss = 5.83502197265625, avg loss = 5.83502197265625
client 5, data condensation 400, total loss = 10.19482421875, avg loss = 10.19482421875
client 5, data condensation 600, total loss = 52.76812744140625, avg loss = 52.76812744140625
client 5, data condensation 800, total loss = 1.9599609375, avg loss = 1.9599609375
client 5, data condensation 1000, total loss = 5.9588623046875, avg loss = 5.9588623046875
client 5, data condensation 1200, total loss = 4.6436767578125, avg loss = 4.6436767578125
client 5, data condensation 1400, total loss = 3.258056640625, avg loss = 3.258056640625
client 5, data condensation 1600, total loss = 2.45684814453125, avg loss = 2.45684814453125
client 5, data condensation 1800, total loss = 22.34368896484375, avg loss = 22.34368896484375
client 5, data condensation 2000, total loss = 2.38134765625, avg loss = 2.38134765625
client 5, data condensation 2200, total loss = 9.60101318359375, avg loss = 9.60101318359375
client 5, data condensation 2400, total loss = 3.26422119140625, avg loss = 3.26422119140625
client 5, data condensation 2600, total loss = 4.0921630859375, avg loss = 4.0921630859375
client 5, data condensation 2800, total loss = 3.375, avg loss = 3.375
client 5, data condensation 3000, total loss = 100.61419677734375, avg loss = 100.61419677734375
client 5, data condensation 3200, total loss = 3.07000732421875, avg loss = 3.07000732421875
client 5, data condensation 3400, total loss = 26.41815185546875, avg loss = 26.41815185546875
client 5, data condensation 3600, total loss = 6.14501953125, avg loss = 6.14501953125
client 5, data condensation 3800, total loss = 19.03326416015625, avg loss = 19.03326416015625
client 5, data condensation 4000, total loss = 3.727294921875, avg loss = 3.727294921875
client 5, data condensation 4200, total loss = 5.703125, avg loss = 5.703125
client 5, data condensation 4400, total loss = 4.5516357421875, avg loss = 4.5516357421875
client 5, data condensation 4600, total loss = 21.318603515625, avg loss = 21.318603515625
client 5, data condensation 4800, total loss = 7.12664794921875, avg loss = 7.12664794921875
client 5, data condensation 5000, total loss = 5.19085693359375, avg loss = 5.19085693359375
client 5, data condensation 5200, total loss = 2.55810546875, avg loss = 2.55810546875
client 5, data condensation 5400, total loss = 6.96881103515625, avg loss = 6.96881103515625
client 5, data condensation 5600, total loss = 20.8388671875, avg loss = 20.8388671875
client 5, data condensation 5800, total loss = 3.603759765625, avg loss = 3.603759765625
client 5, data condensation 6000, total loss = 9.75152587890625, avg loss = 9.75152587890625
client 5, data condensation 6200, total loss = 5.0809326171875, avg loss = 5.0809326171875
client 5, data condensation 6400, total loss = 5.5201416015625, avg loss = 5.5201416015625
client 5, data condensation 6600, total loss = 3.31365966796875, avg loss = 3.31365966796875
client 5, data condensation 6800, total loss = 14.3189697265625, avg loss = 14.3189697265625
client 5, data condensation 7000, total loss = 7.30352783203125, avg loss = 7.30352783203125
client 5, data condensation 7200, total loss = 8.92059326171875, avg loss = 8.92059326171875
client 5, data condensation 7400, total loss = 2.89324951171875, avg loss = 2.89324951171875
client 5, data condensation 7600, total loss = 4.4967041015625, avg loss = 4.4967041015625
client 5, data condensation 7800, total loss = 9.50701904296875, avg loss = 9.50701904296875
client 5, data condensation 8000, total loss = 12.09381103515625, avg loss = 12.09381103515625
client 5, data condensation 8200, total loss = 7.2041015625, avg loss = 7.2041015625
client 5, data condensation 8400, total loss = 5.23736572265625, avg loss = 5.23736572265625
client 5, data condensation 8600, total loss = 2.96881103515625, avg loss = 2.96881103515625
client 5, data condensation 8800, total loss = 8.1715087890625, avg loss = 8.1715087890625
client 5, data condensation 9000, total loss = 1.954345703125, avg loss = 1.954345703125
client 5, data condensation 9200, total loss = 5.14556884765625, avg loss = 5.14556884765625
client 5, data condensation 9400, total loss = 12.85736083984375, avg loss = 12.85736083984375
client 5, data condensation 9600, total loss = 5.01605224609375, avg loss = 5.01605224609375
client 5, data condensation 9800, total loss = 2.17523193359375, avg loss = 2.17523193359375
client 5, data condensation 10000, total loss = 2.3897705078125, avg loss = 2.3897705078125
Round 5, client 5 condense time: 441.9899516105652
client 5, class 8 have 4999 samples
total 24576.0MB, used 2441.06MB, free 22134.94MB
total 24576.0MB, used 2441.06MB, free 22134.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1769  178  161  131  113   90  123  146  182 1472], bin edged: [0.00016827 0.00018133 0.00019439 0.00020745 0.00022051 0.00023357
 0.00024663 0.00025969 0.00027275 0.00028581 0.00029887]
class 6 have 3914 samples, histogram: [2376  187  102   68   75   74   67   81  100  784], bin edged: [0.0002086  0.00022479 0.00024098 0.00025717 0.00027336 0.00028955
 0.00030574 0.00032193 0.00033812 0.00035431 0.0003705 ]
client 6, data condensation 0, total loss = 73.3660888671875, avg loss = 36.68304443359375
client 6, data condensation 200, total loss = 8.275634765625, avg loss = 4.1378173828125
client 6, data condensation 400, total loss = 11.10003662109375, avg loss = 5.550018310546875
client 6, data condensation 600, total loss = 6.03692626953125, avg loss = 3.018463134765625
client 6, data condensation 800, total loss = 8.791259765625, avg loss = 4.3956298828125
client 6, data condensation 1000, total loss = 12.4306640625, avg loss = 6.21533203125
client 6, data condensation 1200, total loss = 9.19427490234375, avg loss = 4.597137451171875
client 6, data condensation 1400, total loss = 7.8963623046875, avg loss = 3.94818115234375
client 6, data condensation 1600, total loss = 28.37908935546875, avg loss = 14.189544677734375
client 6, data condensation 1800, total loss = 5.44476318359375, avg loss = 2.722381591796875
client 6, data condensation 2000, total loss = 4.58636474609375, avg loss = 2.293182373046875
client 6, data condensation 2200, total loss = 6.80267333984375, avg loss = 3.401336669921875
client 6, data condensation 2400, total loss = 7.75531005859375, avg loss = 3.877655029296875
client 6, data condensation 2600, total loss = 10.17425537109375, avg loss = 5.087127685546875
client 6, data condensation 2800, total loss = 31.6351318359375, avg loss = 15.81756591796875
client 6, data condensation 3000, total loss = 44.715087890625, avg loss = 22.3575439453125
client 6, data condensation 3200, total loss = 8.2183837890625, avg loss = 4.10919189453125
client 6, data condensation 3400, total loss = 6.57464599609375, avg loss = 3.287322998046875
client 6, data condensation 3600, total loss = 7.5589599609375, avg loss = 3.77947998046875
client 6, data condensation 3800, total loss = 8.40850830078125, avg loss = 4.204254150390625
client 6, data condensation 4000, total loss = 5.908935546875, avg loss = 2.9544677734375
client 6, data condensation 4200, total loss = 5.2578125, avg loss = 2.62890625
client 6, data condensation 4400, total loss = 14.52252197265625, avg loss = 7.261260986328125
client 6, data condensation 4600, total loss = 26.24188232421875, avg loss = 13.120941162109375
client 6, data condensation 4800, total loss = 34.181640625, avg loss = 17.0908203125
client 6, data condensation 5000, total loss = 9.19189453125, avg loss = 4.595947265625
client 6, data condensation 5200, total loss = 9.26226806640625, avg loss = 4.631134033203125
client 6, data condensation 5400, total loss = 15.7794189453125, avg loss = 7.88970947265625
client 6, data condensation 5600, total loss = 14.53619384765625, avg loss = 7.268096923828125
client 6, data condensation 5800, total loss = 4.43170166015625, avg loss = 2.215850830078125
client 6, data condensation 6000, total loss = 26.30548095703125, avg loss = 13.152740478515625
client 6, data condensation 6200, total loss = 7.8255615234375, avg loss = 3.91278076171875
client 6, data condensation 6400, total loss = 5.11639404296875, avg loss = 2.558197021484375
client 6, data condensation 6600, total loss = 9.10687255859375, avg loss = 4.553436279296875
client 6, data condensation 6800, total loss = 7.68719482421875, avg loss = 3.843597412109375
client 6, data condensation 7000, total loss = 4.81964111328125, avg loss = 2.409820556640625
client 6, data condensation 7200, total loss = 17.87371826171875, avg loss = 8.936859130859375
client 6, data condensation 7400, total loss = 12.30706787109375, avg loss = 6.153533935546875
client 6, data condensation 7600, total loss = 25.967041015625, avg loss = 12.9835205078125
client 6, data condensation 7800, total loss = 15.4102783203125, avg loss = 7.70513916015625
client 6, data condensation 8000, total loss = 20.21600341796875, avg loss = 10.108001708984375
client 6, data condensation 8200, total loss = 16.08245849609375, avg loss = 8.041229248046875
client 6, data condensation 8400, total loss = 16.7755126953125, avg loss = 8.38775634765625
client 6, data condensation 8600, total loss = 8.8333740234375, avg loss = 4.41668701171875
client 6, data condensation 8800, total loss = 15.14215087890625, avg loss = 7.571075439453125
client 6, data condensation 9000, total loss = 14.48077392578125, avg loss = 7.240386962890625
client 6, data condensation 9200, total loss = 10.88385009765625, avg loss = 5.441925048828125
client 6, data condensation 9400, total loss = 7.40386962890625, avg loss = 3.701934814453125
client 6, data condensation 9600, total loss = 19.27960205078125, avg loss = 9.639801025390625
client 6, data condensation 9800, total loss = 4.70147705078125, avg loss = 2.350738525390625
client 6, data condensation 10000, total loss = 16.95220947265625, avg loss = 8.476104736328125
Round 5, client 6 condense time: 820.3542075157166
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3092  179  113   81   81   94   74  107  112  672], bin edged: [0.00018367 0.00019793 0.00021218 0.00022644 0.00024069 0.00025495
 0.0002692  0.00028346 0.00029771 0.00031197 0.00032622]
class 3 have 4999 samples, histogram: [1565  315  213  160  167  149  170  181  286 1793], bin edged: [0.00014255 0.00015362 0.00016468 0.00017575 0.00018681 0.00019787
 0.00020894 0.00022    0.00023107 0.00024213 0.00025319]
client 7, data condensation 0, total loss = 46.87615966796875, avg loss = 23.438079833984375
client 7, data condensation 200, total loss = 17.65618896484375, avg loss = 8.828094482421875
client 7, data condensation 400, total loss = 6.54144287109375, avg loss = 3.270721435546875
client 7, data condensation 600, total loss = 10.0877685546875, avg loss = 5.04388427734375
client 7, data condensation 800, total loss = 14.76617431640625, avg loss = 7.383087158203125
client 7, data condensation 1000, total loss = 13.3270263671875, avg loss = 6.66351318359375
client 7, data condensation 1200, total loss = 5.00177001953125, avg loss = 2.500885009765625
client 7, data condensation 1400, total loss = 5.60125732421875, avg loss = 2.800628662109375
client 7, data condensation 1600, total loss = 4.71282958984375, avg loss = 2.356414794921875
client 7, data condensation 1800, total loss = 5.44091796875, avg loss = 2.720458984375
client 7, data condensation 2000, total loss = 4.77459716796875, avg loss = 2.387298583984375
client 7, data condensation 2200, total loss = 22.6529541015625, avg loss = 11.32647705078125
client 7, data condensation 2400, total loss = 23.177978515625, avg loss = 11.5889892578125
client 7, data condensation 2600, total loss = 6.02191162109375, avg loss = 3.010955810546875
client 7, data condensation 2800, total loss = 10.91949462890625, avg loss = 5.459747314453125
client 7, data condensation 3000, total loss = 30.890045166015625, avg loss = 15.445022583007812
client 7, data condensation 3200, total loss = 7.1697998046875, avg loss = 3.58489990234375
client 7, data condensation 3400, total loss = 7.752197265625, avg loss = 3.8760986328125
client 7, data condensation 3600, total loss = 17.60400390625, avg loss = 8.802001953125
client 7, data condensation 3800, total loss = 9.4873046875, avg loss = 4.74365234375
client 7, data condensation 4000, total loss = 6.93359375, avg loss = 3.466796875
client 7, data condensation 4200, total loss = 8.38104248046875, avg loss = 4.190521240234375
client 7, data condensation 4400, total loss = 6.11871337890625, avg loss = 3.059356689453125
client 7, data condensation 4600, total loss = 23.26397705078125, avg loss = 11.631988525390625
client 7, data condensation 4800, total loss = 8.089111328125, avg loss = 4.0445556640625
client 7, data condensation 5000, total loss = 12.55059814453125, avg loss = 6.275299072265625
client 7, data condensation 5200, total loss = 9.60589599609375, avg loss = 4.802947998046875
client 7, data condensation 5400, total loss = 11.69708251953125, avg loss = 5.848541259765625
client 7, data condensation 5600, total loss = 4.4892578125, avg loss = 2.24462890625
client 7, data condensation 5800, total loss = 24.3433837890625, avg loss = 12.17169189453125
client 7, data condensation 6000, total loss = 9.6591796875, avg loss = 4.82958984375
client 7, data condensation 6200, total loss = 12.432373046875, avg loss = 6.2161865234375
client 7, data condensation 6400, total loss = 8.13525390625, avg loss = 4.067626953125
client 7, data condensation 6600, total loss = 18.7252197265625, avg loss = 9.36260986328125
client 7, data condensation 6800, total loss = 20.33917236328125, avg loss = 10.169586181640625
client 7, data condensation 7000, total loss = 5.57537841796875, avg loss = 2.787689208984375
client 7, data condensation 7200, total loss = 19.06689453125, avg loss = 9.533447265625
client 7, data condensation 7400, total loss = 8.11968994140625, avg loss = 4.059844970703125
client 7, data condensation 7600, total loss = 6.41619873046875, avg loss = 3.208099365234375
client 7, data condensation 7800, total loss = 6.47528076171875, avg loss = 3.237640380859375
client 7, data condensation 8000, total loss = 20.43463134765625, avg loss = 10.217315673828125
client 7, data condensation 8200, total loss = 7.92205810546875, avg loss = 3.961029052734375
client 7, data condensation 8400, total loss = 4.69232177734375, avg loss = 2.346160888671875
client 7, data condensation 8600, total loss = 7.96441650390625, avg loss = 3.982208251953125
client 7, data condensation 8800, total loss = 8.45684814453125, avg loss = 4.228424072265625
client 7, data condensation 9000, total loss = 6.68975830078125, avg loss = 3.344879150390625
client 7, data condensation 9200, total loss = 6.6607666015625, avg loss = 3.33038330078125
client 7, data condensation 9400, total loss = 17.266357421875, avg loss = 8.6331787109375
client 7, data condensation 9600, total loss = 11.6109619140625, avg loss = 5.80548095703125
client 7, data condensation 9800, total loss = 5.6715087890625, avg loss = 2.83575439453125
client 7, data condensation 10000, total loss = 7.11767578125, avg loss = 3.558837890625
Round 5, client 7 condense time: 944.0140438079834
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2827.06MB, free 21748.94MB
total 24576.0MB, used 2827.06MB, free 21748.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [249  12  11  10   4   7   3   5   7  56], bin edged: [0.00233503 0.00251626 0.00269749 0.00287873 0.00305996 0.00324119
 0.00342242 0.00360365 0.00378488 0.00396611 0.00414734]
class 5 have 135 samples, histogram: [53  5  5  5  3  6  3  2 10 43], bin edged: [0.00541476 0.00583502 0.00625528 0.00667554 0.0070958  0.00751606
 0.00793632 0.00835659 0.00877685 0.00919711 0.00961737]
class 9 have 4727 samples, histogram: [3013  188  129  100   86   89   89  103  134  796], bin edged: [0.00017573 0.00018937 0.00020301 0.00021665 0.00023029 0.00024393
 0.00025757 0.0002712  0.00028484 0.00029848 0.00031212]
client 8, data condensation 0, total loss = 296.8990478515625, avg loss = 98.96634928385417
client 8, data condensation 200, total loss = 76.29833984375, avg loss = 25.432779947916668
client 8, data condensation 400, total loss = 219.51495361328125, avg loss = 73.17165120442708
client 8, data condensation 600, total loss = 41.57745361328125, avg loss = 13.859151204427084
client 8, data condensation 800, total loss = 44.6507568359375, avg loss = 14.883585611979166
client 8, data condensation 1000, total loss = 463.735107421875, avg loss = 154.578369140625
client 8, data condensation 1200, total loss = 60.31396484375, avg loss = 20.104654947916668
client 8, data condensation 1400, total loss = 556.1989135742188, avg loss = 185.3996378580729
client 8, data condensation 1600, total loss = 49.28094482421875, avg loss = 16.426981608072918
client 8, data condensation 1800, total loss = 115.16845703125, avg loss = 38.389485677083336
client 8, data condensation 2000, total loss = 64.41961669921875, avg loss = 21.47320556640625
client 8, data condensation 2200, total loss = 79.4072265625, avg loss = 26.469075520833332
client 8, data condensation 2400, total loss = 22.1851806640625, avg loss = 7.395060221354167
client 8, data condensation 2600, total loss = 100.4169921875, avg loss = 33.472330729166664
client 8, data condensation 2800, total loss = 45.94384765625, avg loss = 15.314615885416666
client 8, data condensation 3000, total loss = 49.94244384765625, avg loss = 16.647481282552082
client 8, data condensation 3200, total loss = 57.4326171875, avg loss = 19.144205729166668
client 8, data condensation 3400, total loss = 310.830810546875, avg loss = 103.61027018229167
client 8, data condensation 3600, total loss = 85.39837646484375, avg loss = 28.46612548828125
client 8, data condensation 3800, total loss = 209.4619140625, avg loss = 69.82063802083333
client 8, data condensation 4000, total loss = 65.31011962890625, avg loss = 21.770039876302082
client 8, data condensation 4200, total loss = 238.744873046875, avg loss = 79.58162434895833
client 8, data condensation 4400, total loss = 82.59228515625, avg loss = 27.53076171875
client 8, data condensation 4600, total loss = 61.36700439453125, avg loss = 20.455668131510418
client 8, data condensation 4800, total loss = 61.8775634765625, avg loss = 20.6258544921875
client 8, data condensation 5000, total loss = 74.664306640625, avg loss = 24.888102213541668
client 8, data condensation 5200, total loss = 1084.4091796875, avg loss = 361.4697265625
client 8, data condensation 5400, total loss = 62.61785888671875, avg loss = 20.87261962890625
client 8, data condensation 5600, total loss = 49.6700439453125, avg loss = 16.556681315104168
client 8, data condensation 5800, total loss = 126.63739013671875, avg loss = 42.21246337890625
client 8, data condensation 6000, total loss = 49.78558349609375, avg loss = 16.595194498697918
client 8, data condensation 6200, total loss = 73.48223876953125, avg loss = 24.49407958984375
client 8, data condensation 6400, total loss = 58.03436279296875, avg loss = 19.34478759765625
client 8, data condensation 6600, total loss = 87.49981689453125, avg loss = 29.166605631510418
client 8, data condensation 6800, total loss = 57.29931640625, avg loss = 19.099772135416668
client 8, data condensation 7000, total loss = 124.287109375, avg loss = 41.429036458333336
client 8, data condensation 7200, total loss = 132.589599609375, avg loss = 44.196533203125
client 8, data condensation 7400, total loss = 70.634033203125, avg loss = 23.544677734375
client 8, data condensation 7600, total loss = 908.32373046875, avg loss = 302.7745768229167
client 8, data condensation 7800, total loss = 89.80682373046875, avg loss = 29.93560791015625
client 8, data condensation 8000, total loss = 48.90216064453125, avg loss = 16.30072021484375
client 8, data condensation 8200, total loss = 78.94757080078125, avg loss = 26.31585693359375
client 8, data condensation 8400, total loss = 90.37530517578125, avg loss = 30.125101725260418
client 8, data condensation 8600, total loss = 68.422607421875, avg loss = 22.807535807291668
client 8, data condensation 8800, total loss = 42.9365234375, avg loss = 14.312174479166666
client 8, data condensation 9000, total loss = 59.56866455078125, avg loss = 19.856221516927082
client 8, data condensation 9200, total loss = 63.19415283203125, avg loss = 21.064717610677082
client 8, data condensation 9400, total loss = 24.40899658203125, avg loss = 8.136332194010416
client 8, data condensation 9600, total loss = 26.79962158203125, avg loss = 8.933207194010416
client 8, data condensation 9800, total loss = 77.04034423828125, avg loss = 25.68011474609375
client 8, data condensation 10000, total loss = 410.4722900390625, avg loss = 136.8240966796875
Round 5, client 8 condense time: 1153.9167897701263
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 23428.0MB, free 1148.0MB
total 24576.0MB, used 23428.0MB, free 1148.0MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [49  4  4  2  1  4  2  4  8 42], bin edged: [0.00603343 0.0065017  0.00696997 0.00743825 0.00790652 0.00837479
 0.00884306 0.00931134 0.00977961 0.01024788 0.01071616]
class 5 have 192 samples, histogram: [80 11  3  6  4  3  6  5 10 64], bin edged: [0.00383389 0.00413145 0.00442902 0.00472658 0.00502414 0.00532171
 0.00561927 0.00591684 0.0062144  0.00651196 0.00680953]
class 6 have 1075 samples, histogram: [670  39  23  22  22  34  21  28  32 184], bin edged: [0.00076648 0.00082597 0.00088546 0.00094495 0.00100444 0.00106393
 0.00112342 0.00118291 0.0012424  0.00130189 0.00136138]
client 9, data condensation 0, total loss = 426.31353759765625, avg loss = 142.1045125325521
client 9, data condensation 200, total loss = 93.2044677734375, avg loss = 31.068155924479168
client 9, data condensation 400, total loss = 104.55548095703125, avg loss = 34.851826985677086
client 9, data condensation 600, total loss = 87.72625732421875, avg loss = 29.242085774739582
client 9, data condensation 800, total loss = 804.2260131835938, avg loss = 268.07533772786456
client 9, data condensation 1000, total loss = 416.28375244140625, avg loss = 138.7612508138021
client 9, data condensation 1200, total loss = 115.2542724609375, avg loss = 38.4180908203125
client 9, data condensation 1400, total loss = 147.5396728515625, avg loss = 49.179890950520836
client 9, data condensation 1600, total loss = 99.62640380859375, avg loss = 33.20880126953125
client 9, data condensation 1800, total loss = 147.19940185546875, avg loss = 49.06646728515625
client 9, data condensation 2000, total loss = 56.24835205078125, avg loss = 18.74945068359375
client 9, data condensation 2200, total loss = 94.6949462890625, avg loss = 31.564982096354168
client 9, data condensation 2400, total loss = 69.6036376953125, avg loss = 23.201212565104168
client 9, data condensation 2600, total loss = 163.97723388671875, avg loss = 54.659077962239586
client 9, data condensation 2800, total loss = 626.8367919921875, avg loss = 208.94559733072916
client 9, data condensation 3000, total loss = 95.6929931640625, avg loss = 31.897664388020832
client 9, data condensation 3200, total loss = 166.71551513671875, avg loss = 55.57183837890625
client 9, data condensation 3400, total loss = 77.8443603515625, avg loss = 25.9481201171875
client 9, data condensation 3600, total loss = 74.6048583984375, avg loss = 24.8682861328125
client 9, data condensation 3800, total loss = 349.56024169921875, avg loss = 116.52008056640625
client 9, data condensation 4000, total loss = 185.7188720703125, avg loss = 61.906290690104164
client 9, data condensation 4200, total loss = 65.8983154296875, avg loss = 21.966105143229168
client 9, data condensation 4400, total loss = 464.69525146484375, avg loss = 154.8984171549479
client 9, data condensation 4600, total loss = 308.09234619140625, avg loss = 102.69744873046875
client 9, data condensation 4800, total loss = 86.67486572265625, avg loss = 28.891621907552082
client 9, data condensation 5000, total loss = 102.98892211914062, avg loss = 34.32964070638021
client 9, data condensation 5200, total loss = 116.88885498046875, avg loss = 38.96295166015625
client 9, data condensation 5400, total loss = 64.66009521484375, avg loss = 21.553365071614582
client 9, data condensation 5600, total loss = 373.70977783203125, avg loss = 124.56992594401042
client 9, data condensation 5800, total loss = 128.4033203125, avg loss = 42.801106770833336
client 9, data condensation 6000, total loss = 61.94158935546875, avg loss = 20.647196451822918
client 9, data condensation 6200, total loss = 61.01708984375, avg loss = 20.339029947916668
client 9, data condensation 6400, total loss = 42.4083251953125, avg loss = 14.1361083984375
client 9, data condensation 6600, total loss = 87.333740234375, avg loss = 29.111246744791668
client 9, data condensation 6800, total loss = 539.0286254882812, avg loss = 179.67620849609375
client 9, data condensation 7000, total loss = 116.1103515625, avg loss = 38.703450520833336
client 9, data condensation 7200, total loss = 52.92559814453125, avg loss = 17.641866048177082
client 9, data condensation 7400, total loss = 178.403076171875, avg loss = 59.467692057291664
client 9, data condensation 7600, total loss = 489.3297119140625, avg loss = 163.10990397135416
client 9, data condensation 7800, total loss = 469.8875732421875, avg loss = 156.62919108072916
client 9, data condensation 8000, total loss = 117.43524169921875, avg loss = 39.14508056640625
client 9, data condensation 8200, total loss = 496.330078125, avg loss = 165.443359375
client 9, data condensation 8400, total loss = 60.63775634765625, avg loss = 20.21258544921875
client 9, data condensation 8600, total loss = 141.73236083984375, avg loss = 47.244120279947914
client 9, data condensation 8800, total loss = 602.2157592773438, avg loss = 200.73858642578125
client 9, data condensation 9000, total loss = 67.157958984375, avg loss = 22.385986328125
client 9, data condensation 9200, total loss = 84.5296630859375, avg loss = 28.176554361979168
client 9, data condensation 9400, total loss = 96.7677001953125, avg loss = 32.255900065104164
client 9, data condensation 9600, total loss = 104.72601318359375, avg loss = 34.908671061197914
client 9, data condensation 9800, total loss = 123.57513427734375, avg loss = 41.19171142578125
client 9, data condensation 10000, total loss = 103.53875732421875, avg loss = 34.512919108072914
Round 5, client 9 condense time: 973.8632566928864
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 15.7552,   1.2959,   3.3020,  -3.5606,  -0.9850,  -9.7462,  -9.5598,
          -3.3493,   5.3309,   2.0591],
        [  1.5732,  16.4333,  -4.7853,  -2.2625,  -4.8479,  -6.0996,  -5.6294,
          -1.5621,  -0.1875,   8.8746],
        [  0.4778,  -4.4988,   9.5513,   2.3466,   4.0514,   0.8381,   0.1184,
           0.8689,  -8.0092,  -4.6681],
        [ -4.0926,  -2.8962,   1.4232,   9.7963,   0.0396,   5.9225,   2.2515,
           0.6038,  -8.8405,  -3.0829],
        [ -2.7736,  -4.0033,   4.6279,   0.3024,  10.0994,   0.5167,   3.1170,
           3.8151,  -9.9409,  -4.8957],
        [ -6.2031,  -4.1379,   2.4004,   8.5500,   0.3247,  11.3776,   0.5881,
           2.8059, -10.1574,  -4.3433],
        [ -6.2675,  -2.6040,   2.9117,   4.8904,   4.9714,   1.4216,  14.1929,
          -1.9056, -12.1499,  -4.1553],
        [ -4.1176,  -2.8133,   1.8066,  -0.0849,   4.4606,   1.1022,  -1.7866,
          13.6006, -10.0129,  -0.7151],
        [ 10.1968,   4.6188,  -1.1260,  -3.0874,  -3.8730,  -8.7088, -10.8091,
          -5.1295,  13.5152,   5.2697],
        [  0.6861,   7.8291,  -4.1528,  -2.5904,  -4.4101,  -6.0115,  -5.8460,
           1.1360,  -0.4005,  15.0515]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 7, 5],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 7, 2, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 2, 5, 3],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
6
Round 5: # synthetic sample: 6072
total 24576.0MB, used 3079.06MB, free 21496.94MB
{0: {0: 692, 1: 40, 2: 43, 3: 31, 4: 11, 5: 10, 6: 9, 7: 12, 8: 81, 9: 71}, 1: {0: 31, 1: 745, 2: 7, 3: 14, 4: 4, 5: 15, 6: 21, 7: 17, 8: 19, 9: 127}, 2: {0: 132, 1: 15, 2: 394, 3: 109, 4: 109, 5: 60, 6: 87, 7: 57, 8: 15, 9: 22}, 3: {0: 32, 1: 21, 2: 69, 3: 462, 4: 66, 5: 160, 6: 102, 7: 31, 8: 11, 9: 46}, 4: {0: 59, 1: 9, 2: 79, 3: 63, 4: 504, 5: 33, 6: 114, 7: 105, 8: 15, 9: 19}, 5: {0: 21, 1: 14, 2: 51, 3: 213, 4: 58, 5: 508, 6: 42, 7: 60, 8: 14, 9: 19}, 6: {0: 9, 1: 6, 2: 56, 3: 80, 4: 67, 5: 22, 6: 730, 7: 8, 8: 4, 9: 18}, 7: {0: 27, 1: 13, 2: 37, 3: 68, 4: 49, 5: 84, 6: 24, 7: 652, 8: 6, 9: 40}, 8: {0: 189, 1: 96, 2: 12, 3: 30, 4: 5, 5: 8, 6: 12, 7: 11, 8: 537, 9: 100}, 9: {0: 42, 1: 114, 2: 2, 3: 23, 4: 10, 5: 11, 6: 12, 7: 14, 8: 26, 9: 746}}
round 5 evaluation: test acc is 0.5970, test loss = 2.403461
{0: {0: 676, 1: 24, 2: 39, 3: 32, 4: 22, 5: 7, 6: 12, 7: 11, 8: 118, 9: 59}, 1: {0: 47, 1: 662, 2: 4, 3: 14, 4: 8, 5: 13, 6: 16, 7: 9, 8: 38, 9: 189}, 2: {0: 130, 1: 14, 2: 359, 3: 120, 4: 158, 5: 53, 6: 73, 7: 43, 8: 23, 9: 27}, 3: {0: 32, 1: 20, 2: 76, 3: 498, 4: 64, 5: 130, 6: 71, 7: 27, 8: 16, 9: 66}, 4: {0: 67, 1: 8, 2: 58, 3: 82, 4: 553, 5: 26, 6: 92, 7: 69, 8: 17, 9: 28}, 5: {0: 18, 1: 10, 2: 50, 3: 284, 4: 81, 5: 435, 6: 31, 7: 49, 8: 18, 9: 24}, 6: {0: 15, 1: 6, 2: 54, 3: 85, 4: 92, 5: 20, 6: 682, 7: 8, 8: 8, 9: 30}, 7: {0: 34, 1: 9, 2: 36, 3: 85, 4: 64, 5: 74, 6: 20, 7: 612, 8: 13, 9: 53}, 8: {0: 155, 1: 58, 2: 9, 3: 29, 4: 6, 5: 7, 6: 10, 7: 9, 8: 634, 9: 83}, 9: {0: 46, 1: 85, 2: 7, 3: 21, 4: 9, 5: 11, 6: 9, 7: 10, 8: 37, 9: 765}}
epoch 0, train loss avg now = 0.049234, train contrast loss now = 1.176737, test acc now = 0.5876, test loss now = 2.562127
{0: {0: 658, 1: 28, 2: 46, 3: 29, 4: 13, 5: 5, 6: 14, 7: 11, 8: 150, 9: 46}, 1: {0: 39, 1: 720, 2: 14, 3: 14, 4: 7, 5: 6, 6: 21, 7: 15, 8: 51, 9: 113}, 2: {0: 124, 1: 9, 2: 376, 3: 121, 4: 139, 5: 60, 6: 80, 7: 40, 8: 33, 9: 18}, 3: {0: 34, 1: 23, 2: 77, 3: 494, 4: 66, 5: 141, 6: 94, 7: 23, 8: 18, 9: 30}, 4: {0: 59, 1: 7, 2: 69, 3: 62, 4: 566, 5: 21, 6: 115, 7: 66, 8: 20, 9: 15}, 5: {0: 19, 1: 11, 2: 63, 3: 259, 4: 77, 5: 440, 6: 53, 7: 43, 8: 20, 9: 15}, 6: {0: 9, 1: 4, 2: 47, 3: 80, 4: 73, 5: 16, 6: 744, 7: 7, 8: 8, 9: 12}, 7: {0: 30, 1: 9, 2: 49, 3: 79, 4: 71, 5: 71, 6: 26, 7: 612, 8: 13, 9: 40}, 8: {0: 108, 1: 60, 2: 14, 3: 20, 4: 3, 5: 3, 6: 9, 7: 5, 8: 719, 9: 59}, 9: {0: 47, 1: 104, 2: 4, 3: 21, 4: 10, 5: 5, 6: 15, 7: 10, 8: 59, 9: 725}}
epoch 100, train loss avg now = 0.015378, train contrast loss now = 0.343698, test acc now = 0.6054, test loss now = 2.274471
{0: {0: 675, 1: 46, 2: 44, 3: 35, 4: 10, 5: 5, 6: 11, 7: 10, 8: 96, 9: 68}, 1: {0: 30, 1: 782, 2: 5, 3: 18, 4: 4, 5: 10, 6: 15, 7: 11, 8: 15, 9: 110}, 2: {0: 136, 1: 13, 2: 416, 3: 105, 4: 98, 5: 78, 6: 76, 7: 37, 8: 18, 9: 23}, 3: {0: 24, 1: 22, 2: 83, 3: 502, 4: 51, 5: 147, 6: 87, 7: 27, 8: 13, 9: 44}, 4: {0: 61, 1: 11, 2: 92, 3: 76, 4: 502, 5: 33, 6: 108, 7: 83, 8: 14, 9: 20}, 5: {0: 21, 1: 11, 2: 55, 3: 269, 4: 57, 5: 461, 6: 41, 7: 47, 8: 17, 9: 21}, 6: {0: 12, 1: 5, 2: 66, 3: 87, 4: 55, 5: 24, 6: 728, 7: 5, 8: 6, 9: 12}, 7: {0: 32, 1: 17, 2: 40, 3: 89, 4: 54, 5: 83, 6: 23, 7: 601, 8: 9, 9: 52}, 8: {0: 133, 1: 92, 2: 7, 3: 24, 4: 2, 5: 11, 6: 12, 7: 6, 8: 627, 9: 86}, 9: {0: 41, 1: 116, 2: 4, 3: 26, 4: 8, 5: 10, 6: 11, 7: 9, 8: 25, 9: 750}}
epoch 200, train loss avg now = 0.010265, train contrast loss now = 0.340655, test acc now = 0.6044, test loss now = 2.351272
{0: {0: 657, 1: 44, 2: 47, 3: 33, 4: 8, 5: 6, 6: 16, 7: 14, 8: 103, 9: 72}, 1: {0: 24, 1: 703, 2: 7, 3: 14, 4: 5, 5: 12, 6: 21, 7: 13, 8: 26, 9: 175}, 2: {0: 120, 1: 15, 2: 406, 3: 107, 4: 92, 5: 65, 6: 99, 7: 53, 8: 19, 9: 24}, 3: {0: 27, 1: 22, 2: 64, 3: 456, 4: 54, 5: 167, 6: 114, 7: 41, 8: 18, 9: 37}, 4: {0: 54, 1: 9, 2: 99, 3: 62, 4: 451, 5: 34, 6: 141, 7: 117, 8: 14, 9: 19}, 5: {0: 19, 1: 9, 2: 50, 3: 221, 4: 54, 5: 487, 6: 51, 7: 71, 8: 19, 9: 19}, 6: {0: 9, 1: 3, 2: 57, 3: 69, 4: 31, 5: 20, 6: 778, 7: 10, 8: 5, 9: 18}, 7: {0: 25, 1: 10, 2: 35, 3: 63, 4: 37, 5: 82, 6: 19, 7: 677, 8: 11, 9: 41}, 8: {0: 134, 1: 88, 2: 11, 3: 16, 4: 1, 5: 4, 6: 13, 7: 8, 8: 628, 9: 97}, 9: {0: 31, 1: 91, 2: 3, 3: 14, 4: 5, 5: 11, 6: 16, 7: 17, 8: 25, 9: 787}}
epoch 300, train loss avg now = 0.008121, train contrast loss now = 0.340856, test acc now = 0.6030, test loss now = 2.427295
{0: {0: 660, 1: 41, 2: 51, 3: 30, 4: 10, 5: 6, 6: 14, 7: 17, 8: 118, 9: 53}, 1: {0: 26, 1: 762, 2: 8, 3: 17, 4: 6, 5: 10, 6: 15, 7: 13, 8: 28, 9: 115}, 2: {0: 132, 1: 12, 2: 416, 3: 99, 4: 107, 5: 58, 6: 88, 7: 50, 8: 19, 9: 19}, 3: {0: 38, 1: 26, 2: 80, 3: 463, 4: 56, 5: 149, 6: 92, 7: 38, 8: 16, 9: 42}, 4: {0: 64, 1: 8, 2: 100, 3: 56, 4: 505, 5: 21, 6: 114, 7: 99, 8: 17, 9: 16}, 5: {0: 19, 1: 10, 2: 54, 3: 229, 4: 72, 5: 470, 6: 46, 7: 59, 8: 19, 9: 22}, 6: {0: 10, 1: 5, 2: 63, 3: 70, 4: 51, 5: 19, 6: 755, 7: 7, 8: 6, 9: 14}, 7: {0: 31, 1: 14, 2: 41, 3: 69, 4: 47, 5: 75, 6: 25, 7: 649, 8: 8, 9: 41}, 8: {0: 117, 1: 82, 2: 11, 3: 18, 4: 1, 5: 8, 6: 10, 7: 11, 8: 665, 9: 77}, 9: {0: 43, 1: 119, 2: 5, 3: 25, 4: 9, 5: 6, 6: 13, 7: 13, 8: 32, 9: 735}}
epoch 400, train loss avg now = 0.006288, train contrast loss now = 0.339922, test acc now = 0.6080, test loss now = 2.360403
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 683, 1: 43, 2: 47, 3: 22, 4: 11, 5: 8, 6: 19, 7: 16, 8: 92, 9: 59}, 1: {0: 24, 1: 757, 2: 8, 3: 15, 4: 6, 5: 10, 6: 20, 7: 14, 8: 20, 9: 126}, 2: {0: 124, 1: 11, 2: 399, 3: 81, 4: 123, 5: 64, 6: 110, 7: 48, 8: 22, 9: 18}, 3: {0: 31, 1: 28, 2: 87, 3: 394, 4: 60, 5: 164, 6: 140, 7: 40, 8: 16, 9: 40}, 4: {0: 57, 1: 9, 2: 74, 3: 45, 4: 521, 5: 30, 6: 146, 7: 87, 8: 14, 9: 17}, 5: {0: 23, 1: 13, 2: 60, 3: 202, 4: 75, 5: 478, 6: 63, 7: 53, 8: 14, 9: 19}, 6: {0: 7, 1: 5, 2: 48, 3: 50, 4: 45, 5: 20, 6: 799, 7: 7, 8: 4, 9: 15}, 7: {0: 30, 1: 11, 2: 39, 3: 58, 4: 62, 5: 75, 6: 31, 7: 642, 8: 8, 9: 44}, 8: {0: 135, 1: 89, 2: 15, 3: 14, 4: 1, 5: 7, 6: 12, 7: 14, 8: 632, 9: 81}, 9: {0: 41, 1: 112, 2: 5, 3: 13, 4: 11, 5: 7, 6: 15, 7: 11, 8: 27, 9: 758}}
epoch 500, train loss avg now = 0.005561, train contrast loss now = 0.338892, test acc now = 0.6063, test loss now = 2.426681
{0: {0: 674, 1: 42, 2: 49, 3: 33, 4: 9, 5: 7, 6: 15, 7: 13, 8: 97, 9: 61}, 1: {0: 25, 1: 765, 2: 8, 3: 15, 4: 6, 5: 11, 6: 18, 7: 16, 8: 16, 9: 120}, 2: {0: 125, 1: 13, 2: 404, 3: 107, 4: 103, 5: 72, 6: 94, 7: 48, 8: 15, 9: 19}, 3: {0: 28, 1: 25, 2: 72, 3: 475, 4: 54, 5: 160, 6: 103, 7: 30, 8: 16, 9: 37}, 4: {0: 51, 1: 10, 2: 79, 3: 59, 4: 503, 5: 35, 6: 136, 7: 94, 8: 15, 9: 18}, 5: {0: 20, 1: 12, 2: 50, 3: 238, 4: 57, 5: 485, 6: 50, 7: 55, 8: 13, 9: 20}, 6: {0: 8, 1: 3, 2: 48, 3: 72, 4: 43, 5: 24, 6: 776, 7: 8, 8: 5, 9: 13}, 7: {0: 28, 1: 13, 2: 42, 3: 73, 4: 52, 5: 84, 6: 24, 7: 636, 8: 7, 9: 41}, 8: {0: 131, 1: 94, 2: 12, 3: 21, 4: 2, 5: 6, 6: 11, 7: 12, 8: 623, 9: 88}, 9: {0: 39, 1: 112, 2: 2, 3: 22, 4: 10, 5: 9, 6: 13, 7: 12, 8: 22, 9: 759}}
epoch 600, train loss avg now = 0.003790, train contrast loss now = 0.338895, test acc now = 0.6100, test loss now = 2.422477
{0: {0: 668, 1: 41, 2: 43, 3: 31, 4: 10, 5: 7, 6: 15, 7: 12, 8: 108, 9: 65}, 1: {0: 26, 1: 747, 2: 7, 3: 13, 4: 6, 5: 11, 6: 20, 7: 14, 8: 20, 9: 136}, 2: {0: 128, 1: 13, 2: 395, 3: 97, 4: 120, 5: 65, 6: 92, 7: 48, 8: 21, 9: 21}, 3: {0: 28, 1: 26, 2: 72, 3: 450, 4: 63, 5: 155, 6: 109, 7: 32, 8: 18, 9: 47}, 4: {0: 53, 1: 10, 2: 66, 3: 53, 4: 528, 5: 31, 6: 129, 7: 96, 8: 15, 9: 19}, 5: {0: 19, 1: 14, 2: 43, 3: 231, 4: 68, 5: 477, 6: 52, 7: 54, 8: 18, 9: 24}, 6: {0: 8, 1: 3, 2: 52, 3: 61, 4: 49, 5: 21, 6: 777, 7: 7, 8: 6, 9: 16}, 7: {0: 29, 1: 12, 2: 38, 3: 64, 4: 54, 5: 78, 6: 27, 7: 640, 8: 9, 9: 49}, 8: {0: 127, 1: 89, 2: 9, 3: 19, 4: 1, 5: 5, 6: 10, 7: 12, 8: 638, 9: 90}, 9: {0: 35, 1: 104, 2: 3, 3: 17, 4: 11, 5: 8, 6: 14, 7: 13, 8: 24, 9: 771}}
epoch 700, train loss avg now = 0.002736, train contrast loss now = 0.339047, test acc now = 0.6091, test loss now = 2.416444
{0: {0: 671, 1: 43, 2: 48, 3: 30, 4: 9, 5: 7, 6: 15, 7: 13, 8: 102, 9: 62}, 1: {0: 27, 1: 753, 2: 7, 3: 15, 4: 6, 5: 11, 6: 19, 7: 15, 8: 24, 9: 123}, 2: {0: 130, 1: 13, 2: 408, 3: 99, 4: 104, 5: 62, 6: 94, 7: 50, 8: 20, 9: 20}, 3: {0: 31, 1: 25, 2: 81, 3: 447, 4: 54, 5: 160, 6: 111, 7: 34, 8: 17, 9: 40}, 4: {0: 53, 1: 9, 2: 78, 3: 55, 4: 511, 5: 32, 6: 136, 7: 92, 8: 15, 9: 19}, 5: {0: 21, 1: 12, 2: 50, 3: 226, 4: 62, 5: 481, 6: 53, 7: 53, 8: 19, 9: 23}, 6: {0: 9, 1: 4, 2: 55, 3: 61, 4: 42, 5: 22, 6: 778, 7: 8, 8: 6, 9: 15}, 7: {0: 29, 1: 12, 2: 38, 3: 66, 4: 50, 5: 78, 6: 27, 7: 648, 8: 9, 9: 43}, 8: {0: 131, 1: 89, 2: 12, 3: 18, 4: 1, 5: 6, 6: 10, 7: 12, 8: 635, 9: 86}, 9: {0: 39, 1: 108, 2: 4, 3: 17, 4: 10, 5: 9, 6: 14, 7: 12, 8: 26, 9: 761}}
epoch 800, train loss avg now = 0.002035, train contrast loss now = 0.339005, test acc now = 0.6093, test loss now = 2.432378
{0: {0: 670, 1: 46, 2: 46, 3: 32, 4: 8, 5: 7, 6: 14, 7: 12, 8: 95, 9: 70}, 1: {0: 23, 1: 765, 2: 7, 3: 15, 4: 6, 5: 11, 6: 18, 7: 13, 8: 17, 9: 125}, 2: {0: 126, 1: 16, 2: 391, 3: 112, 4: 103, 5: 68, 6: 93, 7: 53, 8: 19, 9: 19}, 3: {0: 28, 1: 26, 2: 70, 3: 471, 4: 52, 5: 165, 6: 102, 7: 30, 8: 15, 9: 41}, 4: {0: 55, 1: 10, 2: 71, 3: 64, 4: 500, 5: 36, 6: 133, 7: 98, 8: 13, 9: 20}, 5: {0: 20, 1: 12, 2: 45, 3: 239, 4: 59, 5: 485, 6: 50, 7: 53, 8: 14, 9: 23}, 6: {0: 9, 1: 4, 2: 49, 3: 75, 4: 40, 5: 21, 6: 774, 7: 8, 8: 6, 9: 14}, 7: {0: 28, 1: 12, 2: 38, 3: 77, 4: 46, 5: 79, 6: 26, 7: 641, 8: 8, 9: 45}, 8: {0: 131, 1: 94, 2: 11, 3: 23, 4: 2, 5: 5, 6: 10, 7: 11, 8: 619, 9: 94}, 9: {0: 36, 1: 108, 2: 3, 3: 19, 4: 9, 5: 9, 6: 14, 7: 11, 8: 22, 9: 769}}
epoch 900, train loss avg now = 0.003025, train contrast loss now = 0.338816, test acc now = 0.6085, test loss now = 2.471787
{0: {0: 666, 1: 42, 2: 49, 3: 33, 4: 9, 5: 7, 6: 15, 7: 12, 8: 101, 9: 66}, 1: {0: 28, 1: 749, 2: 7, 3: 15, 4: 6, 5: 13, 6: 19, 7: 15, 8: 20, 9: 128}, 2: {0: 130, 1: 13, 2: 402, 3: 97, 4: 107, 5: 69, 6: 94, 7: 50, 8: 18, 9: 20}, 3: {0: 30, 1: 25, 2: 76, 3: 444, 4: 55, 5: 173, 6: 108, 7: 33, 8: 16, 9: 40}, 4: {0: 54, 1: 10, 2: 75, 3: 56, 4: 502, 5: 36, 6: 137, 7: 96, 8: 16, 9: 18}, 5: {0: 19, 1: 12, 2: 46, 3: 216, 4: 64, 5: 499, 6: 52, 7: 52, 8: 18, 9: 22}, 6: {0: 9, 1: 3, 2: 51, 3: 68, 4: 44, 5: 25, 6: 774, 7: 7, 8: 5, 9: 14}, 7: {0: 28, 1: 12, 2: 41, 3: 67, 4: 51, 5: 87, 6: 27, 7: 636, 8: 8, 9: 43}, 8: {0: 129, 1: 87, 2: 13, 3: 22, 4: 2, 5: 5, 6: 10, 7: 13, 8: 631, 9: 88}, 9: {0: 38, 1: 107, 2: 3, 3: 19, 4: 9, 5: 10, 6: 14, 7: 12, 8: 23, 9: 765}}
epoch 1000, train loss avg now = 0.002582, train contrast loss now = 0.338800, test acc now = 0.6068, test loss now = 2.452573
epoch avg loss = 2.5823779983325753e-06, total time = 10343.17430639267
total 24576.0MB, used 3079.06MB, free 21496.94MB
Round 5 finish, update the prev_syn_proto
torch.Size([606, 3, 32, 32])
torch.Size([606, 3, 32, 32])
torch.Size([624, 3, 32, 32])
torch.Size([600, 3, 32, 32])
torch.Size([600, 3, 32, 32])
torch.Size([630, 3, 32, 32])
torch.Size([606, 3, 32, 32])
torch.Size([600, 3, 32, 32])
torch.Size([600, 3, 32, 32])
torch.Size([600, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 666, 1: 42, 2: 49, 3: 33, 4: 9, 5: 7, 6: 15, 7: 12, 8: 101, 9: 66}, 1: {0: 28, 1: 749, 2: 7, 3: 15, 4: 6, 5: 13, 6: 19, 7: 15, 8: 20, 9: 128}, 2: {0: 130, 1: 13, 2: 402, 3: 97, 4: 107, 5: 69, 6: 94, 7: 50, 8: 18, 9: 20}, 3: {0: 30, 1: 25, 2: 76, 3: 444, 4: 55, 5: 173, 6: 108, 7: 33, 8: 16, 9: 40}, 4: {0: 54, 1: 10, 2: 75, 3: 56, 4: 502, 5: 36, 6: 137, 7: 96, 8: 16, 9: 18}, 5: {0: 19, 1: 12, 2: 46, 3: 216, 4: 64, 5: 499, 6: 52, 7: 52, 8: 18, 9: 22}, 6: {0: 9, 1: 3, 2: 51, 3: 68, 4: 44, 5: 25, 6: 774, 7: 7, 8: 5, 9: 14}, 7: {0: 28, 1: 12, 2: 41, 3: 67, 4: 51, 5: 87, 6: 27, 7: 636, 8: 8, 9: 43}, 8: {0: 129, 1: 87, 2: 13, 3: 22, 4: 2, 5: 5, 6: 10, 7: 13, 8: 631, 9: 88}, 9: {0: 38, 1: 107, 2: 3, 3: 19, 4: 9, 5: 10, 6: 14, 7: 12, 8: 23, 9: 765}}
round 5 evaluation: test acc is 0.6068, test loss = 2.452573
 ====== round 6 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3079.06MB, free 21496.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1174  149  113   96   89   88   98  134  185 1467], bin edged: [0.00019553 0.00021071 0.00022589 0.00024106 0.00025624 0.00027141
 0.00028659 0.00030177 0.00031694 0.00033212 0.00034729]
class 7 have 4999 samples, histogram: [2872  187  105   98   79   91   80   95  153 1239], bin edged: [0.00015884 0.00017117 0.0001835  0.00019582 0.00020815 0.00022048
 0.00023281 0.00024514 0.00025746 0.00026979 0.00028212]
client 0, data condensation 0, total loss = 45.38201904296875, avg loss = 22.691009521484375
client 0, data condensation 200, total loss = 9.2164306640625, avg loss = 4.60821533203125
client 0, data condensation 400, total loss = 14.896728515625, avg loss = 7.4483642578125
client 0, data condensation 600, total loss = 14.8802490234375, avg loss = 7.44012451171875
client 0, data condensation 800, total loss = 10.43603515625, avg loss = 5.218017578125
client 0, data condensation 1000, total loss = 6.8798828125, avg loss = 3.43994140625
client 0, data condensation 1200, total loss = 5.22149658203125, avg loss = 2.610748291015625
client 0, data condensation 1400, total loss = 8.137451171875, avg loss = 4.0687255859375
client 0, data condensation 1600, total loss = 5.8143310546875, avg loss = 2.90716552734375
client 0, data condensation 1800, total loss = 6.45159912109375, avg loss = 3.225799560546875
client 0, data condensation 2000, total loss = 4.68255615234375, avg loss = 2.341278076171875
client 0, data condensation 2200, total loss = 24.6312255859375, avg loss = 12.31561279296875
client 0, data condensation 2400, total loss = 8.14947509765625, avg loss = 4.074737548828125
client 0, data condensation 2600, total loss = 9.297607421875, avg loss = 4.6488037109375
client 0, data condensation 2800, total loss = 10.11669921875, avg loss = 5.058349609375
client 0, data condensation 3000, total loss = 12.11944580078125, avg loss = 6.059722900390625
client 0, data condensation 3200, total loss = 7.71923828125, avg loss = 3.859619140625
client 0, data condensation 3400, total loss = 9.86273193359375, avg loss = 4.931365966796875
client 0, data condensation 3600, total loss = 10.23406982421875, avg loss = 5.117034912109375
client 0, data condensation 3800, total loss = 11.3836669921875, avg loss = 5.69183349609375
client 0, data condensation 4000, total loss = 6.56298828125, avg loss = 3.281494140625
client 0, data condensation 4200, total loss = 13.3836669921875, avg loss = 6.69183349609375
client 0, data condensation 4400, total loss = 12.7659912109375, avg loss = 6.38299560546875
client 0, data condensation 4600, total loss = 6.2325439453125, avg loss = 3.11627197265625
client 0, data condensation 4800, total loss = 8.64422607421875, avg loss = 4.322113037109375
client 0, data condensation 5000, total loss = 5.32025146484375, avg loss = 2.660125732421875
client 0, data condensation 5200, total loss = 8.17950439453125, avg loss = 4.089752197265625
client 0, data condensation 5400, total loss = 7.6923828125, avg loss = 3.84619140625
client 0, data condensation 5600, total loss = 14.49334716796875, avg loss = 7.246673583984375
client 0, data condensation 5800, total loss = 16.77716064453125, avg loss = 8.388580322265625
client 0, data condensation 6000, total loss = 11.67047119140625, avg loss = 5.835235595703125
client 0, data condensation 6200, total loss = 16.953369140625, avg loss = 8.4766845703125
client 0, data condensation 6400, total loss = 8.8160400390625, avg loss = 4.40802001953125
client 0, data condensation 6600, total loss = 6.09619140625, avg loss = 3.048095703125
client 0, data condensation 6800, total loss = 6.838623046875, avg loss = 3.4193115234375
client 0, data condensation 7000, total loss = 8.48944091796875, avg loss = 4.244720458984375
client 0, data condensation 7200, total loss = 12.28118896484375, avg loss = 6.140594482421875
client 0, data condensation 7400, total loss = 14.2254638671875, avg loss = 7.11273193359375
client 0, data condensation 7600, total loss = 46.74322509765625, avg loss = 23.371612548828125
client 0, data condensation 7800, total loss = 6.05615234375, avg loss = 3.028076171875
client 0, data condensation 8000, total loss = 8.240478515625, avg loss = 4.1202392578125
client 0, data condensation 8200, total loss = 31.76678466796875, avg loss = 15.883392333984375
client 0, data condensation 8400, total loss = 10.697021484375, avg loss = 5.3485107421875
client 0, data condensation 8600, total loss = 6.18060302734375, avg loss = 3.090301513671875
client 0, data condensation 8800, total loss = 31.78778076171875, avg loss = 15.893890380859375
client 0, data condensation 9000, total loss = 37.66497802734375, avg loss = 18.832489013671875
client 0, data condensation 9200, total loss = 38.39508056640625, avg loss = 19.197540283203125
client 0, data condensation 9400, total loss = 12.57794189453125, avg loss = 6.288970947265625
client 0, data condensation 9600, total loss = 6.71014404296875, avg loss = 3.355072021484375
client 0, data condensation 9800, total loss = 14.20501708984375, avg loss = 7.102508544921875
client 0, data condensation 10000, total loss = 16.976806640625, avg loss = 8.4884033203125
Round 6, client 0 condense time: 812.7140166759491
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [56  6  5  5  4  7  2  8 10 72], bin edged: [0.00399039 0.0043001  0.0046098  0.00491951 0.00522922 0.00553893
 0.00584864 0.00615835 0.00646806 0.00677777 0.00708748]
class 4 have 4958 samples, histogram: [1793  266  162  142  137  118  115  159  239 1827], bin edged: [0.00014518 0.00015645 0.00016772 0.00017899 0.00019026 0.00020153
 0.00021279 0.00022406 0.00023533 0.0002466  0.00025787]
client 1, data condensation 0, total loss = 120.19921875, avg loss = 60.099609375
client 1, data condensation 200, total loss = 63.3599853515625, avg loss = 31.67999267578125
client 1, data condensation 400, total loss = 359.97235107421875, avg loss = 179.98617553710938
client 1, data condensation 600, total loss = 527.8153076171875, avg loss = 263.90765380859375
client 1, data condensation 800, total loss = 104.08477783203125, avg loss = 52.042388916015625
client 1, data condensation 1000, total loss = 68.31585693359375, avg loss = 34.157928466796875
client 1, data condensation 1200, total loss = 1340.222412109375, avg loss = 670.1112060546875
client 1, data condensation 1400, total loss = 118.035888671875, avg loss = 59.0179443359375
client 1, data condensation 1600, total loss = 62.8026123046875, avg loss = 31.40130615234375
client 1, data condensation 1800, total loss = 77.86322021484375, avg loss = 38.931610107421875
client 1, data condensation 2000, total loss = 57.59698486328125, avg loss = 28.798492431640625
client 1, data condensation 2200, total loss = 665.886474609375, avg loss = 332.9432373046875
client 1, data condensation 2400, total loss = 920.1337280273438, avg loss = 460.0668640136719
client 1, data condensation 2600, total loss = 67.65234375, avg loss = 33.826171875
client 1, data condensation 2800, total loss = 129.95751953125, avg loss = 64.978759765625
client 1, data condensation 3000, total loss = 47.9698486328125, avg loss = 23.98492431640625
client 1, data condensation 3200, total loss = 167.42041015625, avg loss = 83.710205078125
client 1, data condensation 3400, total loss = 113.51336669921875, avg loss = 56.756683349609375
client 1, data condensation 3600, total loss = 163.92608642578125, avg loss = 81.96304321289062
client 1, data condensation 3800, total loss = 313.51654052734375, avg loss = 156.75827026367188
client 1, data condensation 4000, total loss = 68.4361572265625, avg loss = 34.21807861328125
client 1, data condensation 4200, total loss = 85.6634521484375, avg loss = 42.83172607421875
client 1, data condensation 4400, total loss = 113.61895751953125, avg loss = 56.809478759765625
client 1, data condensation 4600, total loss = 84.23590087890625, avg loss = 42.117950439453125
client 1, data condensation 4800, total loss = 121.1341552734375, avg loss = 60.56707763671875
client 1, data condensation 5000, total loss = 31.2431640625, avg loss = 15.62158203125
client 1, data condensation 5200, total loss = 77.9635009765625, avg loss = 38.98175048828125
client 1, data condensation 5400, total loss = 66.26397705078125, avg loss = 33.131988525390625
client 1, data condensation 5600, total loss = 64.7489013671875, avg loss = 32.37445068359375
client 1, data condensation 5800, total loss = 108.785888671875, avg loss = 54.3929443359375
client 1, data condensation 6000, total loss = 52.184326171875, avg loss = 26.0921630859375
client 1, data condensation 6200, total loss = 120.0057373046875, avg loss = 60.00286865234375
client 1, data condensation 6400, total loss = 62.5452880859375, avg loss = 31.27264404296875
client 1, data condensation 6600, total loss = 68.54315185546875, avg loss = 34.271575927734375
client 1, data condensation 6800, total loss = 45.4393310546875, avg loss = 22.71966552734375
client 1, data condensation 7000, total loss = 60.134521484375, avg loss = 30.0672607421875
client 1, data condensation 7200, total loss = 153.033935546875, avg loss = 76.5169677734375
client 1, data condensation 7400, total loss = 51.19921875, avg loss = 25.599609375
client 1, data condensation 7600, total loss = 76.835693359375, avg loss = 38.4178466796875
client 1, data condensation 7800, total loss = 72.08575439453125, avg loss = 36.042877197265625
client 1, data condensation 8000, total loss = 685.973876953125, avg loss = 342.9869384765625
client 1, data condensation 8200, total loss = 542.238037109375, avg loss = 271.1190185546875
client 1, data condensation 8400, total loss = 212.2401123046875, avg loss = 106.12005615234375
client 1, data condensation 8600, total loss = 152.760009765625, avg loss = 76.3800048828125
client 1, data condensation 8800, total loss = 22.60150146484375, avg loss = 11.300750732421875
client 1, data condensation 9000, total loss = 67.15228271484375, avg loss = 33.576141357421875
client 1, data condensation 9200, total loss = 118.631103515625, avg loss = 59.3155517578125
client 1, data condensation 9400, total loss = 301.261962890625, avg loss = 150.6309814453125
client 1, data condensation 9600, total loss = 108.4344482421875, avg loss = 54.21722412109375
client 1, data condensation 9800, total loss = 67.97418212890625, avg loss = 33.987091064453125
client 1, data condensation 10000, total loss = 310.31591796875, avg loss = 155.157958984375
Round 6, client 1 condense time: 727.5724899768829
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [146   8   7   4   5   7   3  10   8  44], bin edged: [0.00336317 0.0036242  0.00388523 0.00414626 0.00440729 0.00466832
 0.00492935 0.00519038 0.0054514  0.00571243 0.00597346]
client 2, data condensation 0, total loss = 79.2257080078125, avg loss = 79.2257080078125
client 2, data condensation 200, total loss = 189.1099853515625, avg loss = 189.1099853515625
client 2, data condensation 400, total loss = 52.7281494140625, avg loss = 52.7281494140625
client 2, data condensation 600, total loss = 44.10626220703125, avg loss = 44.10626220703125
client 2, data condensation 800, total loss = 106.5699462890625, avg loss = 106.5699462890625
client 2, data condensation 1000, total loss = 61.74700927734375, avg loss = 61.74700927734375
client 2, data condensation 1200, total loss = 53.1591796875, avg loss = 53.1591796875
client 2, data condensation 1400, total loss = 132.893310546875, avg loss = 132.893310546875
client 2, data condensation 1600, total loss = 52.87933349609375, avg loss = 52.87933349609375
client 2, data condensation 1800, total loss = 35.39593505859375, avg loss = 35.39593505859375
client 2, data condensation 2000, total loss = 299.15185546875, avg loss = 299.15185546875
client 2, data condensation 2200, total loss = 230.6513671875, avg loss = 230.6513671875
client 2, data condensation 2400, total loss = 97.1673583984375, avg loss = 97.1673583984375
client 2, data condensation 2600, total loss = 110.40252685546875, avg loss = 110.40252685546875
client 2, data condensation 2800, total loss = 32.322509765625, avg loss = 32.322509765625
client 2, data condensation 3000, total loss = 78.33837890625, avg loss = 78.33837890625
client 2, data condensation 3200, total loss = 55.37872314453125, avg loss = 55.37872314453125
client 2, data condensation 3400, total loss = 55.6375732421875, avg loss = 55.6375732421875
client 2, data condensation 3600, total loss = 64.03790283203125, avg loss = 64.03790283203125
client 2, data condensation 3800, total loss = 62.7490234375, avg loss = 62.7490234375
client 2, data condensation 4000, total loss = 49.83770751953125, avg loss = 49.83770751953125
client 2, data condensation 4200, total loss = 33.636962890625, avg loss = 33.636962890625
client 2, data condensation 4400, total loss = 62.1912841796875, avg loss = 62.1912841796875
client 2, data condensation 4600, total loss = 62.8314208984375, avg loss = 62.8314208984375
client 2, data condensation 4800, total loss = 167.74981689453125, avg loss = 167.74981689453125
client 2, data condensation 5000, total loss = 59.697998046875, avg loss = 59.697998046875
client 2, data condensation 5200, total loss = 33.5291748046875, avg loss = 33.5291748046875
client 2, data condensation 5400, total loss = 7.9832763671875, avg loss = 7.9832763671875
client 2, data condensation 5600, total loss = 36.59637451171875, avg loss = 36.59637451171875
client 2, data condensation 5800, total loss = 53.55023193359375, avg loss = 53.55023193359375
client 2, data condensation 6000, total loss = 75.46185302734375, avg loss = 75.46185302734375
client 2, data condensation 6200, total loss = 26.83203125, avg loss = 26.83203125
client 2, data condensation 6400, total loss = 43.757568359375, avg loss = 43.757568359375
client 2, data condensation 6600, total loss = 14.46502685546875, avg loss = 14.46502685546875
client 2, data condensation 6800, total loss = 131.91790771484375, avg loss = 131.91790771484375
client 2, data condensation 7000, total loss = 48.06976318359375, avg loss = 48.06976318359375
client 2, data condensation 7200, total loss = 50.9207763671875, avg loss = 50.9207763671875
client 2, data condensation 7400, total loss = 144.65606689453125, avg loss = 144.65606689453125
client 2, data condensation 7600, total loss = 73.0926513671875, avg loss = 73.0926513671875
client 2, data condensation 7800, total loss = 27.80047607421875, avg loss = 27.80047607421875
client 2, data condensation 8000, total loss = 125.2764892578125, avg loss = 125.2764892578125
client 2, data condensation 8200, total loss = 36.7281494140625, avg loss = 36.7281494140625
client 2, data condensation 8400, total loss = 40.83935546875, avg loss = 40.83935546875
client 2, data condensation 8600, total loss = 78.65582275390625, avg loss = 78.65582275390625
client 2, data condensation 8800, total loss = 115.56085205078125, avg loss = 115.56085205078125
client 2, data condensation 9000, total loss = 34.4000244140625, avg loss = 34.4000244140625
client 2, data condensation 9200, total loss = 128.64312744140625, avg loss = 128.64312744140625
client 2, data condensation 9400, total loss = 38.97198486328125, avg loss = 38.97198486328125
client 2, data condensation 9600, total loss = 33.36517333984375, avg loss = 33.36517333984375
client 2, data condensation 9800, total loss = 76.58349609375, avg loss = 76.58349609375
client 2, data condensation 10000, total loss = 94.87835693359375, avg loss = 94.87835693359375
Round 6, client 2 condense time: 257.2165787220001
client 2, class 9 have 242 samples
total 24576.0MB, used 2437.06MB, free 22138.94MB
total 24576.0MB, used 2437.06MB, free 22138.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [481  30  18  19  18  17  17  18  22 207], bin edged: [0.00093728 0.00101003 0.00108277 0.00115552 0.00122827 0.00130101
 0.00137376 0.00144651 0.00151925 0.001592   0.00166474]
class 2 have 1094 samples, histogram: [305  51  30  24  32  25  31  34  62 500], bin edged: [0.000626   0.00067459 0.00072318 0.00077176 0.00082035 0.00086894
 0.00091752 0.00096611 0.00101469 0.00106328 0.00111187]
client 3, data condensation 0, total loss = 57.2498779296875, avg loss = 28.62493896484375
client 3, data condensation 200, total loss = 9.70477294921875, avg loss = 4.852386474609375
client 3, data condensation 400, total loss = 16.52056884765625, avg loss = 8.260284423828125
client 3, data condensation 600, total loss = 11.76513671875, avg loss = 5.882568359375
client 3, data condensation 800, total loss = 54.47076416015625, avg loss = 27.235382080078125
client 3, data condensation 1000, total loss = 11.7412109375, avg loss = 5.87060546875
client 3, data condensation 1200, total loss = 19.093994140625, avg loss = 9.5469970703125
client 3, data condensation 1400, total loss = 53.522705078125, avg loss = 26.7613525390625
client 3, data condensation 1600, total loss = 17.24261474609375, avg loss = 8.621307373046875
client 3, data condensation 1800, total loss = 13.013916015625, avg loss = 6.5069580078125
client 3, data condensation 2000, total loss = 26.7728271484375, avg loss = 13.38641357421875
client 3, data condensation 2200, total loss = 22.06610107421875, avg loss = 11.033050537109375
client 3, data condensation 2400, total loss = 30.2960205078125, avg loss = 15.14801025390625
client 3, data condensation 2600, total loss = 12.8187255859375, avg loss = 6.40936279296875
client 3, data condensation 2800, total loss = 37.2034912109375, avg loss = 18.60174560546875
client 3, data condensation 3000, total loss = 17.9691162109375, avg loss = 8.98455810546875
client 3, data condensation 3200, total loss = 37.416259765625, avg loss = 18.7081298828125
client 3, data condensation 3400, total loss = 17.7301025390625, avg loss = 8.86505126953125
client 3, data condensation 3600, total loss = 60.698760986328125, avg loss = 30.349380493164062
client 3, data condensation 3800, total loss = 8.71087646484375, avg loss = 4.355438232421875
client 3, data condensation 4000, total loss = 9.5093994140625, avg loss = 4.75469970703125
client 3, data condensation 4200, total loss = 30.51171875, avg loss = 15.255859375
client 3, data condensation 4400, total loss = 9.16998291015625, avg loss = 4.584991455078125
client 3, data condensation 4600, total loss = 34.24322509765625, avg loss = 17.121612548828125
client 3, data condensation 4800, total loss = 7.2667236328125, avg loss = 3.63336181640625
client 3, data condensation 5000, total loss = 11.10687255859375, avg loss = 5.553436279296875
client 3, data condensation 5200, total loss = 14.29296875, avg loss = 7.146484375
client 3, data condensation 5400, total loss = 9.93927001953125, avg loss = 4.969635009765625
client 3, data condensation 5600, total loss = 23.23138427734375, avg loss = 11.615692138671875
client 3, data condensation 5800, total loss = 11.08111572265625, avg loss = 5.540557861328125
client 3, data condensation 6000, total loss = 2.91363525390625, avg loss = 1.456817626953125
client 3, data condensation 6200, total loss = 8.1068115234375, avg loss = 4.05340576171875
client 3, data condensation 6400, total loss = 8.690673828125, avg loss = 4.3453369140625
client 3, data condensation 6600, total loss = 17.6044921875, avg loss = 8.80224609375
client 3, data condensation 6800, total loss = 15.15826416015625, avg loss = 7.579132080078125
client 3, data condensation 7000, total loss = 5.0098876953125, avg loss = 2.50494384765625
client 3, data condensation 7200, total loss = 12.0736083984375, avg loss = 6.03680419921875
client 3, data condensation 7400, total loss = 7.52093505859375, avg loss = 3.760467529296875
client 3, data condensation 7600, total loss = 5.8106689453125, avg loss = 2.90533447265625
client 3, data condensation 7800, total loss = 6.38507080078125, avg loss = 3.192535400390625
client 3, data condensation 8000, total loss = 6.48504638671875, avg loss = 3.242523193359375
client 3, data condensation 8200, total loss = 14.49517822265625, avg loss = 7.247589111328125
client 3, data condensation 8400, total loss = 28.12786865234375, avg loss = 14.063934326171875
client 3, data condensation 8600, total loss = 24.1181640625, avg loss = 12.05908203125
client 3, data condensation 8800, total loss = 14.3907470703125, avg loss = 7.19537353515625
client 3, data condensation 9000, total loss = 8.35491943359375, avg loss = 4.177459716796875
client 3, data condensation 9200, total loss = 3.678955078125, avg loss = 1.8394775390625
client 3, data condensation 9400, total loss = 17.38848876953125, avg loss = 8.694244384765625
client 3, data condensation 9600, total loss = 35.32080078125, avg loss = 17.660400390625
client 3, data condensation 9800, total loss = 27.96063232421875, avg loss = 13.980316162109375
client 3, data condensation 10000, total loss = 12.21356201171875, avg loss = 6.106781005859375
Round 6, client 3 condense time: 511.38116693496704
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2819.06MB, free 21756.94MB
total 24576.0MB, used 2819.06MB, free 21756.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2422  162  121   71   75   77   79   90  120  935], bin edged: [0.00019321 0.0002082  0.0002232  0.00023819 0.00025319 0.00026818
 0.00028318 0.00029817 0.00031317 0.00032816 0.00034316]
class 5 have 307 samples, histogram: [131  12   7   9  13   7   7   4  11 106], bin edged: [0.00240672 0.00259351 0.00278031 0.0029671  0.0031539  0.00334069
 0.00352749 0.00371428 0.00390108 0.00408787 0.00427467]
client 4, data condensation 0, total loss = 54.4068603515625, avg loss = 27.20343017578125
client 4, data condensation 200, total loss = 36.41217041015625, avg loss = 18.206085205078125
client 4, data condensation 400, total loss = 18.11541748046875, avg loss = 9.057708740234375
client 4, data condensation 600, total loss = 264.131103515625, avg loss = 132.0655517578125
client 4, data condensation 800, total loss = 29.83270263671875, avg loss = 14.916351318359375
client 4, data condensation 1000, total loss = 37.84576416015625, avg loss = 18.922882080078125
client 4, data condensation 1200, total loss = 46.42303466796875, avg loss = 23.211517333984375
client 4, data condensation 1400, total loss = 45.96856689453125, avg loss = 22.984283447265625
client 4, data condensation 1600, total loss = 28.83721923828125, avg loss = 14.418609619140625
client 4, data condensation 1800, total loss = 43.60601806640625, avg loss = 21.803009033203125
client 4, data condensation 2000, total loss = 80.887451171875, avg loss = 40.4437255859375
client 4, data condensation 2200, total loss = 94.7421875, avg loss = 47.37109375
client 4, data condensation 2400, total loss = 20.4310302734375, avg loss = 10.21551513671875
client 4, data condensation 2600, total loss = 18.97796630859375, avg loss = 9.488983154296875
client 4, data condensation 2800, total loss = 23.96875, avg loss = 11.984375
client 4, data condensation 3000, total loss = 25.47515869140625, avg loss = 12.737579345703125
client 4, data condensation 3200, total loss = 22.92340087890625, avg loss = 11.461700439453125
client 4, data condensation 3400, total loss = 28.0787353515625, avg loss = 14.03936767578125
client 4, data condensation 3600, total loss = 78.927734375, avg loss = 39.4638671875
client 4, data condensation 3800, total loss = 21.7333984375, avg loss = 10.86669921875
client 4, data condensation 4000, total loss = 41.48040771484375, avg loss = 20.740203857421875
client 4, data condensation 4200, total loss = 42.32940673828125, avg loss = 21.164703369140625
client 4, data condensation 4400, total loss = 28.23455810546875, avg loss = 14.117279052734375
client 4, data condensation 4600, total loss = 13.55499267578125, avg loss = 6.777496337890625
client 4, data condensation 4800, total loss = 33.18646240234375, avg loss = 16.593231201171875
client 4, data condensation 5000, total loss = 23.8062744140625, avg loss = 11.90313720703125
client 4, data condensation 5200, total loss = 41.16839599609375, avg loss = 20.584197998046875
client 4, data condensation 5400, total loss = 20.9141845703125, avg loss = 10.45709228515625
client 4, data condensation 5600, total loss = 45.294921875, avg loss = 22.6474609375
client 4, data condensation 5800, total loss = 68.7281494140625, avg loss = 34.36407470703125
client 4, data condensation 6000, total loss = 8.9664306640625, avg loss = 4.48321533203125
client 4, data condensation 6200, total loss = 40.61907958984375, avg loss = 20.309539794921875
client 4, data condensation 6400, total loss = 34.260986328125, avg loss = 17.1304931640625
client 4, data condensation 6600, total loss = 76.30072021484375, avg loss = 38.150360107421875
client 4, data condensation 6800, total loss = 33.73858642578125, avg loss = 16.869293212890625
client 4, data condensation 7000, total loss = 27.70947265625, avg loss = 13.854736328125
client 4, data condensation 7200, total loss = 60.01177978515625, avg loss = 30.005889892578125
client 4, data condensation 7400, total loss = 27.1695556640625, avg loss = 13.58477783203125
client 4, data condensation 7600, total loss = 27.657470703125, avg loss = 13.8287353515625
client 4, data condensation 7800, total loss = 47.31597900390625, avg loss = 23.657989501953125
client 4, data condensation 8000, total loss = 21.95709228515625, avg loss = 10.978546142578125
client 4, data condensation 8200, total loss = 34.6632080078125, avg loss = 17.33160400390625
client 4, data condensation 8400, total loss = 9.2398681640625, avg loss = 4.61993408203125
client 4, data condensation 8600, total loss = 54.42083740234375, avg loss = 27.210418701171875
client 4, data condensation 8800, total loss = 29.25750732421875, avg loss = 14.628753662109375
client 4, data condensation 9000, total loss = 87.6533203125, avg loss = 43.82666015625
client 4, data condensation 9200, total loss = 49.0640869140625, avg loss = 24.53204345703125
client 4, data condensation 9400, total loss = 20.2662353515625, avg loss = 10.13311767578125
client 4, data condensation 9600, total loss = 19.46356201171875, avg loss = 9.731781005859375
client 4, data condensation 9800, total loss = 38.758544921875, avg loss = 19.3792724609375
client 4, data condensation 10000, total loss = 19.3861083984375, avg loss = 9.69305419921875
Round 6, client 4 condense time: 571.3475096225739
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2823.06MB, free 21752.94MB
total 24576.0MB, used 2823.06MB, free 21752.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [2365  230  144  113  110  103  120  148  183 1483], bin edged: [0.00015163 0.0001634  0.00017517 0.00018694 0.0001987  0.00021047
 0.00022224 0.00023401 0.00024578 0.00025755 0.00026932]
client 5, data condensation 0, total loss = 112.1851806640625, avg loss = 112.1851806640625
client 5, data condensation 200, total loss = 7.337646484375, avg loss = 7.337646484375
client 5, data condensation 400, total loss = 13.843505859375, avg loss = 13.843505859375
client 5, data condensation 600, total loss = 6.46893310546875, avg loss = 6.46893310546875
client 5, data condensation 800, total loss = 3.682861328125, avg loss = 3.682861328125
client 5, data condensation 1000, total loss = 4.8455810546875, avg loss = 4.8455810546875
client 5, data condensation 1200, total loss = 7.26239013671875, avg loss = 7.26239013671875
client 5, data condensation 1400, total loss = 8.38714599609375, avg loss = 8.38714599609375
client 5, data condensation 1600, total loss = 13.2000732421875, avg loss = 13.2000732421875
client 5, data condensation 1800, total loss = 28.27294921875, avg loss = 28.27294921875
client 5, data condensation 2000, total loss = 4.4476318359375, avg loss = 4.4476318359375
client 5, data condensation 2200, total loss = 8.028076171875, avg loss = 8.028076171875
client 5, data condensation 2400, total loss = 2.899169921875, avg loss = 2.899169921875
client 5, data condensation 2600, total loss = 8.8619384765625, avg loss = 8.8619384765625
client 5, data condensation 2800, total loss = 6.116943359375, avg loss = 6.116943359375
client 5, data condensation 3000, total loss = 8.0504150390625, avg loss = 8.0504150390625
client 5, data condensation 3200, total loss = 11.8179931640625, avg loss = 11.8179931640625
client 5, data condensation 3400, total loss = 4.02276611328125, avg loss = 4.02276611328125
client 5, data condensation 3600, total loss = 3.702392578125, avg loss = 3.702392578125
client 5, data condensation 3800, total loss = 8.32513427734375, avg loss = 8.32513427734375
client 5, data condensation 4000, total loss = 10.44036865234375, avg loss = 10.44036865234375
client 5, data condensation 4200, total loss = 8.49102783203125, avg loss = 8.49102783203125
client 5, data condensation 4400, total loss = 4.57305908203125, avg loss = 4.57305908203125
client 5, data condensation 4600, total loss = 3.404052734375, avg loss = 3.404052734375
client 5, data condensation 4800, total loss = 9.5380859375, avg loss = 9.5380859375
client 5, data condensation 5000, total loss = 3.2481689453125, avg loss = 3.2481689453125
client 5, data condensation 5200, total loss = 11.25286865234375, avg loss = 11.25286865234375
client 5, data condensation 5400, total loss = 3.10699462890625, avg loss = 3.10699462890625
client 5, data condensation 5600, total loss = 6.84002685546875, avg loss = 6.84002685546875
client 5, data condensation 5800, total loss = 8.464111328125, avg loss = 8.464111328125
client 5, data condensation 6000, total loss = 4.8460693359375, avg loss = 4.8460693359375
client 5, data condensation 6200, total loss = 4.1075439453125, avg loss = 4.1075439453125
client 5, data condensation 6400, total loss = 3.31524658203125, avg loss = 3.31524658203125
client 5, data condensation 6600, total loss = 14.322509765625, avg loss = 14.322509765625
client 5, data condensation 6800, total loss = 3.66009521484375, avg loss = 3.66009521484375
client 5, data condensation 7000, total loss = 7.09063720703125, avg loss = 7.09063720703125
client 5, data condensation 7200, total loss = 35.5614013671875, avg loss = 35.5614013671875
client 5, data condensation 7400, total loss = 23.28326416015625, avg loss = 23.28326416015625
client 5, data condensation 7600, total loss = 8.46142578125, avg loss = 8.46142578125
client 5, data condensation 7800, total loss = 4.2049560546875, avg loss = 4.2049560546875
client 5, data condensation 8000, total loss = 2.76458740234375, avg loss = 2.76458740234375
client 5, data condensation 8200, total loss = 4.5159912109375, avg loss = 4.5159912109375
client 5, data condensation 8400, total loss = 4.77423095703125, avg loss = 4.77423095703125
client 5, data condensation 8600, total loss = 5.57275390625, avg loss = 5.57275390625
client 5, data condensation 8800, total loss = 7.402587890625, avg loss = 7.402587890625
client 5, data condensation 9000, total loss = 8.17083740234375, avg loss = 8.17083740234375
client 5, data condensation 9200, total loss = 5.184326171875, avg loss = 5.184326171875
client 5, data condensation 9400, total loss = 11.00372314453125, avg loss = 11.00372314453125
client 5, data condensation 9600, total loss = 23.33172607421875, avg loss = 23.33172607421875
client 5, data condensation 9800, total loss = 3.7220458984375, avg loss = 3.7220458984375
client 5, data condensation 10000, total loss = 3.7052001953125, avg loss = 3.7052001953125
Round 6, client 5 condense time: 348.5277807712555
client 5, class 8 have 4999 samples
total 24576.0MB, used 2441.06MB, free 22134.94MB
total 24576.0MB, used 2441.06MB, free 22134.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1793  197  132  106  109  102  114  123  179 1510], bin edged: [0.00016815 0.0001812  0.00019425 0.0002073  0.00022035 0.0002334
 0.00024645 0.0002595  0.00027255 0.0002856  0.00029866]
class 6 have 3914 samples, histogram: [2549  145   91   66   74   68   50   53  104  714], bin edged: [0.00021252 0.00022902 0.00024551 0.00026201 0.0002785  0.000295
 0.00031149 0.00032799 0.00034448 0.00036097 0.00037747]
client 6, data condensation 0, total loss = 65.95550537109375, avg loss = 32.977752685546875
client 6, data condensation 200, total loss = 12.43096923828125, avg loss = 6.215484619140625
client 6, data condensation 400, total loss = 6.1114501953125, avg loss = 3.05572509765625
client 6, data condensation 600, total loss = 8.699462890625, avg loss = 4.3497314453125
client 6, data condensation 800, total loss = 6.10614013671875, avg loss = 3.053070068359375
client 6, data condensation 1000, total loss = 6.63018798828125, avg loss = 3.315093994140625
client 6, data condensation 1200, total loss = 10.161376953125, avg loss = 5.0806884765625
client 6, data condensation 1400, total loss = 9.4268798828125, avg loss = 4.71343994140625
client 6, data condensation 1600, total loss = 8.14093017578125, avg loss = 4.070465087890625
client 6, data condensation 1800, total loss = 9.73101806640625, avg loss = 4.865509033203125
client 6, data condensation 2000, total loss = 38.0140380859375, avg loss = 19.00701904296875
client 6, data condensation 2200, total loss = 15.9964599609375, avg loss = 7.99822998046875
client 6, data condensation 2400, total loss = 19.11962890625, avg loss = 9.559814453125
client 6, data condensation 2600, total loss = 21.73193359375, avg loss = 10.865966796875
client 6, data condensation 2800, total loss = 7.89959716796875, avg loss = 3.949798583984375
client 6, data condensation 3000, total loss = 14.09783935546875, avg loss = 7.048919677734375
client 6, data condensation 3200, total loss = 31.4395751953125, avg loss = 15.71978759765625
client 6, data condensation 3400, total loss = 9.56060791015625, avg loss = 4.780303955078125
client 6, data condensation 3600, total loss = 4.3692626953125, avg loss = 2.18463134765625
client 6, data condensation 3800, total loss = 19.82733154296875, avg loss = 9.913665771484375
client 6, data condensation 4000, total loss = 43.176513671875, avg loss = 21.5882568359375
client 6, data condensation 4200, total loss = 13.12933349609375, avg loss = 6.564666748046875
client 6, data condensation 4400, total loss = 7.38043212890625, avg loss = 3.690216064453125
client 6, data condensation 4600, total loss = 20.078125, avg loss = 10.0390625
client 6, data condensation 4800, total loss = 8.15960693359375, avg loss = 4.079803466796875
client 6, data condensation 5000, total loss = 6.5037841796875, avg loss = 3.25189208984375
client 6, data condensation 5200, total loss = 11.3394775390625, avg loss = 5.66973876953125
client 6, data condensation 5400, total loss = 3.876220703125, avg loss = 1.9381103515625
client 6, data condensation 5600, total loss = 18.68756103515625, avg loss = 9.343780517578125
client 6, data condensation 5800, total loss = 5.1884765625, avg loss = 2.59423828125
client 6, data condensation 6000, total loss = 3.5164794921875, avg loss = 1.75823974609375
client 6, data condensation 6200, total loss = 17.05517578125, avg loss = 8.527587890625
client 6, data condensation 6400, total loss = 12.64776611328125, avg loss = 6.323883056640625
client 6, data condensation 6600, total loss = 9.15399169921875, avg loss = 4.576995849609375
client 6, data condensation 6800, total loss = 7.47772216796875, avg loss = 3.738861083984375
client 6, data condensation 7000, total loss = 27.6663818359375, avg loss = 13.83319091796875
client 6, data condensation 7200, total loss = 11.77435302734375, avg loss = 5.887176513671875
client 6, data condensation 7400, total loss = 8.0980224609375, avg loss = 4.04901123046875
client 6, data condensation 7600, total loss = 9.1905517578125, avg loss = 4.59527587890625
client 6, data condensation 7800, total loss = 7.4681396484375, avg loss = 3.73406982421875
client 6, data condensation 8000, total loss = 6.20208740234375, avg loss = 3.101043701171875
client 6, data condensation 8200, total loss = 9.613525390625, avg loss = 4.8067626953125
client 6, data condensation 8400, total loss = 25.38067626953125, avg loss = 12.690338134765625
client 6, data condensation 8600, total loss = 10.7393798828125, avg loss = 5.36968994140625
client 6, data condensation 8800, total loss = 21.83660888671875, avg loss = 10.918304443359375
client 6, data condensation 9000, total loss = 10.888671875, avg loss = 5.4443359375
client 6, data condensation 9200, total loss = 18.30426025390625, avg loss = 9.152130126953125
client 6, data condensation 9400, total loss = 5.9615478515625, avg loss = 2.98077392578125
client 6, data condensation 9600, total loss = 5.60137939453125, avg loss = 2.800689697265625
client 6, data condensation 9800, total loss = 37.44805908203125, avg loss = 18.724029541015625
client 6, data condensation 10000, total loss = 37.697265625, avg loss = 18.8486328125
Round 6, client 6 condense time: 790.8118515014648
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2825.06MB, free 21750.94MB
total 24576.0MB, used 2825.06MB, free 21750.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3161  167  111   85   77   75   67   76  102  684], bin edged: [0.00018476 0.0001991  0.00021344 0.00022778 0.00024212 0.00025646
 0.0002708  0.00028514 0.00029948 0.00031382 0.00032816]
class 3 have 4999 samples, histogram: [1657  276  189  158  138  159  139  187  257 1839], bin edged: [0.00014274 0.00015382 0.0001649  0.00017597 0.00018705 0.00019813
 0.00020921 0.00022029 0.00023137 0.00024244 0.00025352]
client 7, data condensation 0, total loss = 35.7215576171875, avg loss = 17.86077880859375
client 7, data condensation 200, total loss = 8.2132568359375, avg loss = 4.10662841796875
client 7, data condensation 400, total loss = 8.13677978515625, avg loss = 4.068389892578125
client 7, data condensation 600, total loss = 6.21124267578125, avg loss = 3.105621337890625
client 7, data condensation 800, total loss = 23.933837890625, avg loss = 11.9669189453125
client 7, data condensation 1000, total loss = 24.1605224609375, avg loss = 12.08026123046875
client 7, data condensation 1200, total loss = 15.2890625, avg loss = 7.64453125
client 7, data condensation 1400, total loss = 6.44000244140625, avg loss = 3.220001220703125
client 7, data condensation 1600, total loss = 26.52020263671875, avg loss = 13.260101318359375
client 7, data condensation 1800, total loss = 15.43646240234375, avg loss = 7.718231201171875
client 7, data condensation 2000, total loss = 10.81732177734375, avg loss = 5.408660888671875
client 7, data condensation 2200, total loss = 17.94793701171875, avg loss = 8.973968505859375
client 7, data condensation 2400, total loss = 6.70361328125, avg loss = 3.351806640625
client 7, data condensation 2600, total loss = 12.474365234375, avg loss = 6.2371826171875
client 7, data condensation 2800, total loss = 32.39117431640625, avg loss = 16.195587158203125
client 7, data condensation 3000, total loss = 11.120849609375, avg loss = 5.5604248046875
client 7, data condensation 3200, total loss = 8.8492431640625, avg loss = 4.42462158203125
client 7, data condensation 3400, total loss = 15.88336181640625, avg loss = 7.941680908203125
client 7, data condensation 3600, total loss = 3.855712890625, avg loss = 1.9278564453125
client 7, data condensation 3800, total loss = 38.72332763671875, avg loss = 19.361663818359375
client 7, data condensation 4000, total loss = 24.215087890625, avg loss = 12.1075439453125
client 7, data condensation 4200, total loss = 6.34649658203125, avg loss = 3.173248291015625
client 7, data condensation 4400, total loss = 6.0579833984375, avg loss = 3.02899169921875
client 7, data condensation 4600, total loss = 10.77044677734375, avg loss = 5.385223388671875
client 7, data condensation 4800, total loss = 4.5322265625, avg loss = 2.26611328125
client 7, data condensation 5000, total loss = 15.31683349609375, avg loss = 7.658416748046875
client 7, data condensation 5200, total loss = 12.515625, avg loss = 6.2578125
client 7, data condensation 5400, total loss = 5.63763427734375, avg loss = 2.818817138671875
client 7, data condensation 5600, total loss = 5.627197265625, avg loss = 2.8135986328125
client 7, data condensation 5800, total loss = 13.46826171875, avg loss = 6.734130859375
client 7, data condensation 6000, total loss = 5.51947021484375, avg loss = 2.759735107421875
client 7, data condensation 6200, total loss = 8.78875732421875, avg loss = 4.394378662109375
client 7, data condensation 6400, total loss = 9.05340576171875, avg loss = 4.526702880859375
client 7, data condensation 6600, total loss = 9.41876220703125, avg loss = 4.709381103515625
client 7, data condensation 6800, total loss = 14.4954833984375, avg loss = 7.24774169921875
client 7, data condensation 7000, total loss = 7.0137939453125, avg loss = 3.50689697265625
client 7, data condensation 7200, total loss = 14.8603515625, avg loss = 7.43017578125
client 7, data condensation 7400, total loss = 5.22100830078125, avg loss = 2.610504150390625
client 7, data condensation 7600, total loss = 28.9727783203125, avg loss = 14.48638916015625
client 7, data condensation 7800, total loss = 7.2205810546875, avg loss = 3.61029052734375
client 7, data condensation 8000, total loss = 25.35723876953125, avg loss = 12.678619384765625
client 7, data condensation 8200, total loss = 11.95916748046875, avg loss = 5.979583740234375
client 7, data condensation 8400, total loss = 6.2508544921875, avg loss = 3.12542724609375
client 7, data condensation 8600, total loss = 8.8729248046875, avg loss = 4.43646240234375
client 7, data condensation 8800, total loss = 8.4434814453125, avg loss = 4.22174072265625
client 7, data condensation 9000, total loss = 4.257080078125, avg loss = 2.1285400390625
client 7, data condensation 9200, total loss = 3.9466552734375, avg loss = 1.97332763671875
client 7, data condensation 9400, total loss = 3.06768798828125, avg loss = 1.533843994140625
client 7, data condensation 9600, total loss = 5.4351806640625, avg loss = 2.71759033203125
client 7, data condensation 9800, total loss = 5.51043701171875, avg loss = 2.755218505859375
client 7, data condensation 10000, total loss = 8.68096923828125, avg loss = 4.340484619140625
Round 6, client 7 condense time: 722.1706852912903
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2827.06MB, free 21748.94MB
total 24576.0MB, used 2827.06MB, free 21748.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [246  11  10  15   4   4   4   3  10  57], bin edged: [0.00232335 0.00250367 0.00268399 0.00286432 0.00304464 0.00322497
 0.00340529 0.00358562 0.00376594 0.00394626 0.00412659]
class 5 have 135 samples, histogram: [51  7  7  1  6  4  1  3  7 48], bin edged: [0.0054049  0.00582439 0.00624389 0.00666338 0.00708288 0.00750238
 0.00792187 0.00834137 0.00876086 0.00918036 0.00959985]
class 9 have 4727 samples, histogram: [3105  190  118   89   67   75   86   84  112  801], bin edged: [0.00017696 0.0001907  0.00020443 0.00021817 0.0002319  0.00024564
 0.00025937 0.00027311 0.00028684 0.00030058 0.00031431]
client 8, data condensation 0, total loss = 275.337158203125, avg loss = 91.779052734375
client 8, data condensation 200, total loss = 101.61444091796875, avg loss = 33.871480305989586
client 8, data condensation 400, total loss = 75.19775390625, avg loss = 25.06591796875
client 8, data condensation 600, total loss = 23.73175048828125, avg loss = 7.91058349609375
client 8, data condensation 800, total loss = 68.30877685546875, avg loss = 22.76959228515625
client 8, data condensation 1000, total loss = 242.62750244140625, avg loss = 80.87583414713542
client 8, data condensation 1200, total loss = 131.30609130859375, avg loss = 43.768697102864586
client 8, data condensation 1400, total loss = 106.8248291015625, avg loss = 35.6082763671875
client 8, data condensation 1600, total loss = 30.73956298828125, avg loss = 10.24652099609375
client 8, data condensation 1800, total loss = 64.496826171875, avg loss = 21.498942057291668
client 8, data condensation 2000, total loss = 62.085205078125, avg loss = 20.695068359375
client 8, data condensation 2200, total loss = 91.74822998046875, avg loss = 30.582743326822918
client 8, data condensation 2400, total loss = 89.3653564453125, avg loss = 29.7884521484375
client 8, data condensation 2600, total loss = 97.07159423828125, avg loss = 32.357198079427086
client 8, data condensation 2800, total loss = 160.882568359375, avg loss = 53.627522786458336
client 8, data condensation 3000, total loss = 32.0438232421875, avg loss = 10.6812744140625
client 8, data condensation 3200, total loss = 118.77288818359375, avg loss = 39.590962727864586
client 8, data condensation 3400, total loss = 91.73236083984375, avg loss = 30.57745361328125
client 8, data condensation 3600, total loss = 31.1287841796875, avg loss = 10.376261393229166
client 8, data condensation 3800, total loss = 301.54241943359375, avg loss = 100.51413981119792
client 8, data condensation 4000, total loss = 40.430908203125, avg loss = 13.476969401041666
client 8, data condensation 4200, total loss = 157.12554931640625, avg loss = 52.37518310546875
client 8, data condensation 4400, total loss = 160.26239013671875, avg loss = 53.420796712239586
client 8, data condensation 4600, total loss = 58.76458740234375, avg loss = 19.58819580078125
client 8, data condensation 4800, total loss = 568.5675659179688, avg loss = 189.52252197265625
client 8, data condensation 5000, total loss = 60.3466796875, avg loss = 20.115559895833332
client 8, data condensation 5200, total loss = 63.39910888671875, avg loss = 21.133036295572918
client 8, data condensation 5400, total loss = 50.169921875, avg loss = 16.723307291666668
client 8, data condensation 5600, total loss = 40.71893310546875, avg loss = 13.572977701822916
client 8, data condensation 5800, total loss = 40.24053955078125, avg loss = 13.41351318359375
client 8, data condensation 6000, total loss = 96.51776123046875, avg loss = 32.172587076822914
client 8, data condensation 6200, total loss = 89.14263916015625, avg loss = 29.714213053385418
client 8, data condensation 6400, total loss = 61.46759033203125, avg loss = 20.48919677734375
client 8, data condensation 6600, total loss = 57.37615966796875, avg loss = 19.125386555989582
client 8, data condensation 6800, total loss = 80.0155029296875, avg loss = 26.671834309895832
client 8, data condensation 7000, total loss = 59.6285400390625, avg loss = 19.876180013020832
client 8, data condensation 7200, total loss = 70.69573974609375, avg loss = 23.56524658203125
client 8, data condensation 7400, total loss = 57.95233154296875, avg loss = 19.31744384765625
client 8, data condensation 7600, total loss = 192.451904296875, avg loss = 64.150634765625
client 8, data condensation 7800, total loss = 108.17755126953125, avg loss = 36.059183756510414
client 8, data condensation 8000, total loss = 91.20391845703125, avg loss = 30.40130615234375
client 8, data condensation 8200, total loss = 115.3951416015625, avg loss = 38.465047200520836
client 8, data condensation 8400, total loss = 64.97613525390625, avg loss = 21.658711751302082
client 8, data condensation 8600, total loss = 81.08074951171875, avg loss = 27.02691650390625
client 8, data condensation 8800, total loss = 55.56463623046875, avg loss = 18.52154541015625
client 8, data condensation 9000, total loss = 41.3570556640625, avg loss = 13.785685221354166
client 8, data condensation 9200, total loss = 54.097900390625, avg loss = 18.032633463541668
client 8, data condensation 9400, total loss = 40.84930419921875, avg loss = 13.616434733072916
client 8, data condensation 9600, total loss = 89.7406005859375, avg loss = 29.913533528645832
client 8, data condensation 9800, total loss = 105.9949951171875, avg loss = 35.3316650390625
client 8, data condensation 10000, total loss = 48.545166015625, avg loss = 16.181722005208332
Round 6, client 8 condense time: 693.2179048061371
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3081.06MB, free 21494.94MB
total 24576.0MB, used 3081.06MB, free 21494.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [50  6  3  3  1  1  5  3  2 46], bin edged: [0.00608866 0.00656122 0.00703379 0.00750635 0.00797892 0.00845148
 0.00892405 0.00939661 0.00986917 0.01034174 0.0108143 ]
class 5 have 192 samples, histogram: [84  6  4  3  4  3  5  8  8 67], bin edged: [0.00382171 0.00411833 0.00441495 0.00471157 0.00500818 0.0053048
 0.00560142 0.00589804 0.00619466 0.00649128 0.00678789]
class 6 have 1075 samples, histogram: [696  33  25  28  11  19  23  26  32 182], bin edged: [0.00077291 0.0008329  0.00089289 0.00095288 0.00101287 0.00107286
 0.00113285 0.00119283 0.00125282 0.00131281 0.0013728 ]
client 9, data condensation 0, total loss = 309.21710205078125, avg loss = 103.07236735026042
client 9, data condensation 200, total loss = 394.83453369140625, avg loss = 131.61151123046875
client 9, data condensation 400, total loss = 304.75457763671875, avg loss = 101.58485921223958
client 9, data condensation 600, total loss = 234.53826904296875, avg loss = 78.17942301432292
client 9, data condensation 800, total loss = 213.35791015625, avg loss = 71.11930338541667
client 9, data condensation 1000, total loss = 281.93341064453125, avg loss = 93.97780354817708
client 9, data condensation 1200, total loss = 232.522216796875, avg loss = 77.50740559895833
client 9, data condensation 1400, total loss = 339.25103759765625, avg loss = 113.08367919921875
client 9, data condensation 1600, total loss = 93.80096435546875, avg loss = 31.266988118489582
client 9, data condensation 1800, total loss = 92.56207275390625, avg loss = 30.854024251302082
client 9, data condensation 2000, total loss = 215.3177490234375, avg loss = 71.7725830078125
client 9, data condensation 2200, total loss = 144.82159423828125, avg loss = 48.27386474609375
client 9, data condensation 2400, total loss = 169.47186279296875, avg loss = 56.490620930989586
client 9, data condensation 2600, total loss = 102.10577392578125, avg loss = 34.035257975260414
client 9, data condensation 2800, total loss = 122.4781494140625, avg loss = 40.8260498046875
client 9, data condensation 3000, total loss = 457.503173828125, avg loss = 152.50105794270834
client 9, data condensation 3200, total loss = 258.84234619140625, avg loss = 86.28078206380208
client 9, data condensation 3400, total loss = 207.90087890625, avg loss = 69.30029296875
client 9, data condensation 3600, total loss = 132.27288818359375, avg loss = 44.090962727864586
client 9, data condensation 3800, total loss = 133.80401611328125, avg loss = 44.601338704427086
client 9, data condensation 4000, total loss = 584.1090698242188, avg loss = 194.7030232747396
client 9, data condensation 4200, total loss = 123.93658447265625, avg loss = 41.31219482421875
client 9, data condensation 4400, total loss = 143.62939453125, avg loss = 47.87646484375
client 9, data condensation 4600, total loss = 162.25042724609375, avg loss = 54.083475748697914
client 9, data condensation 4800, total loss = 172.35845947265625, avg loss = 57.45281982421875
client 9, data condensation 5000, total loss = 263.5701904296875, avg loss = 87.85673014322917
client 9, data condensation 5200, total loss = 110.99041748046875, avg loss = 36.996805826822914
client 9, data condensation 5400, total loss = 176.83343505859375, avg loss = 58.944478352864586
client 9, data condensation 5600, total loss = 99.42822265625, avg loss = 33.142740885416664
client 9, data condensation 5800, total loss = 159.67376708984375, avg loss = 53.224589029947914
client 9, data condensation 6000, total loss = 167.088134765625, avg loss = 55.696044921875
client 9, data condensation 6200, total loss = 157.8646240234375, avg loss = 52.621541341145836
client 9, data condensation 6400, total loss = 261.6077880859375, avg loss = 87.20259602864583
client 9, data condensation 6600, total loss = 122.69427490234375, avg loss = 40.898091634114586
client 9, data condensation 6800, total loss = 119.36944580078125, avg loss = 39.789815266927086
client 9, data condensation 7000, total loss = 212.01763916015625, avg loss = 70.67254638671875
client 9, data condensation 7200, total loss = 154.78033447265625, avg loss = 51.59344482421875
client 9, data condensation 7400, total loss = 141.97247314453125, avg loss = 47.32415771484375
client 9, data condensation 7600, total loss = 132.74945068359375, avg loss = 44.24981689453125
client 9, data condensation 7800, total loss = 885.7344970703125, avg loss = 295.2448323567708
client 9, data condensation 8000, total loss = 777.2206420898438, avg loss = 259.07354736328125
client 9, data condensation 8200, total loss = 150.083251953125, avg loss = 50.027750651041664
client 9, data condensation 8400, total loss = 253.5201416015625, avg loss = 84.5067138671875
client 9, data condensation 8600, total loss = 166.02294921875, avg loss = 55.340983072916664
client 9, data condensation 8800, total loss = 153.3109130859375, avg loss = 51.1036376953125
client 9, data condensation 9000, total loss = 300.72747802734375, avg loss = 100.24249267578125
client 9, data condensation 9200, total loss = 122.8760986328125, avg loss = 40.958699544270836
client 9, data condensation 9400, total loss = 59.26318359375, avg loss = 19.75439453125
client 9, data condensation 9600, total loss = 336.678466796875, avg loss = 112.22615559895833
client 9, data condensation 9800, total loss = 102.808837890625, avg loss = 34.269612630208336
client 9, data condensation 10000, total loss = 114.871826171875, avg loss = 38.290608723958336
Round 6, client 9 condense time: 592.6520862579346
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3079.06MB, free 21496.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 16.4455,   1.4151,   3.2745,  -4.4278,  -0.8417,  -9.5372, -10.5773,
          -3.2598,   6.3283,   1.8225],
        [  1.1726,  17.2122,  -5.7569,  -2.5298,  -4.8820,  -6.0231,  -5.7447,
          -1.7339,   0.5520,   9.2181],
        [  0.3240,  -4.6708,  10.0834,   1.8515,   4.0974,   1.3407,   0.2793,
           0.9044,  -7.7767,  -5.3939],
        [ -4.4782,  -3.2270,   1.6194,  10.1179,  -0.1789,   6.6334,   2.5967,
           0.4259,  -8.9538,  -3.4719],
        [ -3.2826,  -4.1940,   4.9515,  -0.0782,  10.8034,   0.6068,   3.6139,
           4.1345,  -9.9996,  -5.7003],
        [ -6.6544,  -4.5638,   2.8227,   8.8350,   0.0481,  12.1850,   0.8060,
           2.8110, -10.3384,  -4.7042],
        [ -6.9468,  -2.7573,   3.2357,   4.5949,   5.3027,   1.7134,  15.6376,
          -2.2372, -12.5245,  -4.9433],
        [ -4.5683,  -3.0011,   1.8709,   0.3061,   4.7735,   1.1799,  -1.7481,
          14.3344, -10.6516,  -1.0838],
        [ 10.4200,   4.9000,  -1.7800,  -4.2152,  -3.5386,  -8.7957, -11.7978,
          -5.0402,  15.6591,   5.2069],
        [  0.0397,   7.8658,  -5.2182,  -2.3447,  -4.4656,  -5.7592,  -6.0707,
           1.1429,  -0.0173,  16.0866]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 5, 7],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 2, 5, 3],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
7
Round 6: # synthetic sample: 7084
total 24576.0MB, used 3079.06MB, free 21496.94MB
{0: {0: 666, 1: 42, 2: 49, 3: 33, 4: 9, 5: 7, 6: 15, 7: 12, 8: 101, 9: 66}, 1: {0: 28, 1: 749, 2: 7, 3: 15, 4: 6, 5: 13, 6: 19, 7: 15, 8: 20, 9: 128}, 2: {0: 130, 1: 13, 2: 402, 3: 97, 4: 107, 5: 69, 6: 94, 7: 50, 8: 18, 9: 20}, 3: {0: 30, 1: 25, 2: 76, 3: 444, 4: 55, 5: 173, 6: 108, 7: 33, 8: 16, 9: 40}, 4: {0: 54, 1: 10, 2: 75, 3: 56, 4: 502, 5: 36, 6: 137, 7: 96, 8: 16, 9: 18}, 5: {0: 19, 1: 12, 2: 46, 3: 216, 4: 64, 5: 499, 6: 52, 7: 52, 8: 18, 9: 22}, 6: {0: 9, 1: 3, 2: 51, 3: 68, 4: 44, 5: 25, 6: 774, 7: 7, 8: 5, 9: 14}, 7: {0: 28, 1: 12, 2: 41, 3: 67, 4: 51, 5: 87, 6: 27, 7: 636, 8: 8, 9: 43}, 8: {0: 129, 1: 87, 2: 13, 3: 22, 4: 2, 5: 5, 6: 10, 7: 13, 8: 631, 9: 88}, 9: {0: 38, 1: 107, 2: 3, 3: 19, 4: 9, 5: 10, 6: 14, 7: 12, 8: 23, 9: 765}}
round 6 evaluation: test acc is 0.6068, test loss = 2.452573
{0: {0: 784, 1: 22, 2: 40, 3: 13, 4: 7, 5: 10, 6: 7, 7: 11, 8: 42, 9: 64}, 1: {0: 70, 1: 686, 2: 10, 3: 3, 4: 5, 5: 15, 6: 15, 7: 13, 8: 18, 9: 165}, 2: {0: 176, 1: 10, 2: 415, 3: 61, 4: 96, 5: 97, 6: 66, 7: 41, 8: 11, 9: 27}, 3: {0: 75, 1: 20, 2: 91, 3: 299, 4: 52, 5: 272, 6: 77, 7: 27, 8: 13, 9: 74}, 4: {0: 104, 1: 10, 2: 119, 3: 36, 4: 456, 5: 52, 6: 100, 7: 97, 8: 7, 9: 19}, 5: {0: 39, 1: 10, 2: 64, 3: 115, 4: 51, 5: 591, 6: 34, 7: 45, 8: 17, 9: 34}, 6: {0: 24, 1: 5, 2: 91, 3: 52, 4: 60, 5: 46, 6: 670, 7: 10, 8: 6, 9: 36}, 7: {0: 52, 1: 8, 2: 45, 3: 36, 4: 40, 5: 121, 6: 17, 7: 619, 8: 4, 9: 58}, 8: {0: 292, 1: 72, 2: 16, 3: 8, 4: 3, 5: 9, 6: 3, 7: 10, 8: 501, 9: 86}, 9: {0: 61, 1: 89, 2: 3, 3: 8, 4: 6, 5: 12, 6: 8, 7: 10, 8: 17, 9: 786}}
epoch 0, train loss avg now = 0.047882, train contrast loss now = 1.449850, test acc now = 0.5807, test loss now = 2.727114
{0: {0: 667, 1: 54, 2: 31, 3: 27, 4: 17, 5: 8, 6: 16, 7: 21, 8: 79, 9: 80}, 1: {0: 25, 1: 750, 2: 4, 3: 12, 4: 5, 5: 13, 6: 17, 7: 17, 8: 17, 9: 140}, 2: {0: 127, 1: 15, 2: 339, 3: 98, 4: 114, 5: 103, 6: 115, 7: 46, 8: 13, 9: 30}, 3: {0: 35, 1: 29, 2: 50, 3: 394, 4: 48, 5: 227, 6: 127, 7: 34, 8: 11, 9: 45}, 4: {0: 51, 1: 13, 2: 75, 3: 50, 4: 463, 5: 50, 6: 164, 7: 97, 8: 13, 9: 24}, 5: {0: 15, 1: 12, 2: 33, 3: 169, 4: 56, 5: 566, 6: 57, 7: 52, 8: 15, 9: 25}, 6: {0: 9, 1: 3, 2: 33, 3: 63, 4: 23, 5: 39, 6: 803, 7: 5, 8: 4, 9: 18}, 7: {0: 19, 1: 12, 2: 18, 3: 61, 4: 52, 5: 101, 6: 26, 7: 656, 8: 8, 9: 47}, 8: {0: 123, 1: 97, 2: 7, 3: 17, 4: 4, 5: 12, 6: 12, 7: 15, 8: 600, 9: 113}, 9: {0: 32, 1: 117, 2: 2, 3: 19, 4: 4, 5: 8, 6: 16, 7: 14, 8: 14, 9: 774}}
epoch 100, train loss avg now = 0.017518, train contrast loss now = 0.376992, test acc now = 0.6012, test loss now = 2.413293
{0: {0: 660, 1: 52, 2: 36, 3: 53, 4: 17, 5: 5, 6: 12, 7: 19, 8: 77, 9: 69}, 1: {0: 20, 1: 763, 2: 8, 3: 21, 4: 8, 5: 7, 6: 21, 7: 14, 8: 18, 9: 120}, 2: {0: 111, 1: 11, 2: 342, 3: 143, 4: 146, 5: 62, 6: 87, 7: 54, 8: 16, 9: 28}, 3: {0: 28, 1: 16, 2: 44, 3: 546, 4: 60, 5: 119, 6: 95, 7: 41, 8: 12, 9: 39}, 4: {0: 54, 1: 8, 2: 52, 3: 77, 4: 541, 5: 26, 6: 104, 7: 109, 8: 7, 9: 22}, 5: {0: 15, 1: 13, 2: 38, 3: 304, 4: 80, 5: 396, 6: 51, 7: 71, 8: 11, 9: 21}, 6: {0: 5, 1: 4, 2: 51, 3: 98, 4: 58, 5: 18, 6: 743, 7: 9, 8: 4, 9: 10}, 7: {0: 23, 1: 10, 2: 27, 3: 99, 4: 51, 5: 57, 6: 18, 7: 652, 8: 7, 9: 56}, 8: {0: 152, 1: 107, 2: 4, 3: 42, 4: 6, 5: 3, 6: 10, 7: 12, 8: 572, 9: 92}, 9: {0: 31, 1: 125, 2: 3, 3: 33, 4: 9, 5: 2, 6: 11, 7: 9, 8: 18, 9: 759}}
epoch 200, train loss avg now = 0.010549, train contrast loss now = 0.373559, test acc now = 0.5974, test loss now = 2.541546
{0: {0: 632, 1: 40, 2: 40, 3: 43, 4: 22, 5: 9, 6: 19, 7: 13, 8: 105, 9: 77}, 1: {0: 17, 1: 736, 2: 7, 3: 23, 4: 9, 5: 10, 6: 19, 7: 11, 8: 27, 9: 141}, 2: {0: 116, 1: 9, 2: 342, 3: 129, 4: 150, 5: 76, 6: 100, 7: 35, 8: 18, 9: 25}, 3: {0: 26, 1: 15, 2: 46, 3: 528, 4: 68, 5: 138, 6: 108, 7: 22, 8: 11, 9: 38}, 4: {0: 49, 1: 11, 2: 64, 3: 66, 4: 547, 5: 28, 6: 122, 7: 78, 8: 14, 9: 21}, 5: {0: 13, 1: 7, 2: 40, 3: 272, 4: 85, 5: 440, 6: 62, 7: 40, 8: 16, 9: 25}, 6: {0: 6, 1: 3, 2: 34, 3: 86, 4: 64, 5: 21, 6: 761, 7: 5, 8: 5, 9: 15}, 7: {0: 26, 1: 9, 2: 23, 3: 95, 4: 78, 5: 79, 6: 31, 7: 597, 8: 8, 9: 54}, 8: {0: 117, 1: 85, 2: 5, 3: 32, 4: 4, 5: 5, 6: 15, 7: 8, 8: 640, 9: 89}, 9: {0: 29, 1: 102, 2: 1, 3: 26, 4: 11, 5: 8, 6: 12, 7: 12, 8: 26, 9: 773}}
epoch 300, train loss avg now = 0.009806, train contrast loss now = 0.372392, test acc now = 0.5996, test loss now = 2.556466
{0: {0: 661, 1: 37, 2: 50, 3: 49, 4: 8, 5: 5, 6: 16, 7: 20, 8: 99, 9: 55}, 1: {0: 26, 1: 734, 2: 10, 3: 23, 4: 5, 5: 10, 6: 21, 7: 11, 8: 23, 9: 137}, 2: {0: 109, 1: 9, 2: 454, 3: 126, 4: 66, 5: 63, 6: 98, 7: 32, 8: 21, 9: 22}, 3: {0: 25, 1: 18, 2: 84, 3: 534, 4: 38, 5: 141, 6: 104, 7: 26, 8: 9, 9: 21}, 4: {0: 54, 1: 10, 2: 124, 3: 74, 4: 439, 5: 33, 6: 148, 7: 83, 8: 14, 9: 21}, 5: {0: 14, 1: 12, 2: 73, 3: 270, 4: 46, 5: 459, 6: 54, 7: 35, 8: 18, 9: 19}, 6: {0: 6, 1: 3, 2: 64, 3: 75, 4: 18, 5: 25, 6: 791, 7: 4, 8: 4, 9: 10}, 7: {0: 23, 1: 12, 2: 52, 3: 93, 4: 45, 5: 85, 6: 31, 7: 609, 8: 6, 9: 44}, 8: {0: 127, 1: 81, 2: 12, 3: 30, 4: 3, 5: 7, 6: 9, 7: 12, 8: 626, 9: 93}, 9: {0: 37, 1: 111, 2: 6, 3: 28, 4: 5, 5: 9, 6: 16, 7: 8, 8: 22, 9: 758}}
epoch 400, train loss avg now = 0.009398, train contrast loss now = 0.371943, test acc now = 0.6065, test loss now = 2.468916
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 673, 1: 43, 2: 46, 3: 31, 4: 9, 5: 4, 6: 13, 7: 14, 8: 99, 9: 68}, 1: {0: 27, 1: 743, 2: 7, 3: 17, 4: 7, 5: 7, 6: 12, 7: 12, 8: 23, 9: 145}, 2: {0: 134, 1: 12, 2: 404, 3: 110, 4: 106, 5: 61, 6: 76, 7: 51, 8: 21, 9: 25}, 3: {0: 29, 1: 20, 2: 81, 3: 475, 4: 52, 5: 151, 6: 101, 7: 39, 8: 13, 9: 39}, 4: {0: 58, 1: 12, 2: 110, 3: 54, 4: 497, 5: 27, 6: 108, 7: 102, 8: 13, 9: 19}, 5: {0: 17, 1: 14, 2: 67, 3: 219, 4: 68, 5: 453, 6: 55, 7: 62, 8: 24, 9: 21}, 6: {0: 10, 1: 3, 2: 54, 3: 75, 4: 48, 5: 25, 6: 751, 7: 6, 8: 8, 9: 20}, 7: {0: 34, 1: 10, 2: 38, 3: 72, 4: 49, 5: 69, 6: 24, 7: 655, 8: 8, 9: 41}, 8: {0: 114, 1: 84, 2: 9, 3: 25, 4: 4, 5: 4, 6: 8, 7: 10, 8: 654, 9: 88}, 9: {0: 36, 1: 117, 2: 2, 3: 22, 4: 7, 5: 4, 6: 14, 7: 12, 8: 20, 9: 766}}
epoch 500, train loss avg now = 0.004757, train contrast loss now = 0.372189, test acc now = 0.6071, test loss now = 2.539290
{0: {0: 684, 1: 46, 2: 42, 3: 34, 4: 10, 5: 6, 6: 12, 7: 15, 8: 91, 9: 60}, 1: {0: 25, 1: 748, 2: 7, 3: 19, 4: 7, 5: 7, 6: 14, 7: 14, 8: 22, 9: 137}, 2: {0: 131, 1: 9, 2: 375, 3: 117, 4: 113, 5: 68, 6: 93, 7: 48, 8: 23, 9: 23}, 3: {0: 33, 1: 25, 2: 68, 3: 489, 4: 54, 5: 149, 6: 95, 7: 36, 8: 13, 9: 38}, 4: {0: 58, 1: 10, 2: 80, 3: 58, 4: 505, 5: 31, 6: 119, 7: 105, 8: 13, 9: 21}, 5: {0: 22, 1: 15, 2: 39, 3: 227, 4: 68, 5: 469, 6: 54, 7: 63, 8: 19, 9: 24}, 6: {0: 9, 1: 4, 2: 46, 3: 74, 4: 40, 5: 25, 6: 777, 7: 6, 8: 6, 9: 13}, 7: {0: 31, 1: 9, 2: 35, 3: 70, 4: 49, 5: 73, 6: 24, 7: 658, 8: 6, 9: 45}, 8: {0: 132, 1: 93, 2: 6, 3: 24, 4: 2, 5: 5, 6: 11, 7: 11, 8: 634, 9: 82}, 9: {0: 39, 1: 123, 2: 2, 3: 23, 4: 10, 5: 4, 6: 11, 7: 13, 8: 22, 9: 753}}
epoch 600, train loss avg now = 0.002693, train contrast loss now = 0.371540, test acc now = 0.6092, test loss now = 2.536129
{0: {0: 677, 1: 43, 2: 43, 3: 36, 4: 12, 5: 7, 6: 15, 7: 17, 8: 90, 9: 60}, 1: {0: 23, 1: 750, 2: 7, 3: 19, 4: 7, 5: 7, 6: 20, 7: 14, 8: 20, 9: 133}, 2: {0: 121, 1: 10, 2: 391, 3: 113, 4: 112, 5: 65, 6: 103, 7: 45, 8: 17, 9: 23}, 3: {0: 28, 1: 24, 2: 73, 3: 495, 4: 52, 5: 148, 6: 103, 7: 34, 8: 11, 9: 32}, 4: {0: 57, 1: 10, 2: 79, 3: 59, 4: 512, 5: 30, 6: 126, 7: 95, 8: 12, 9: 20}, 5: {0: 19, 1: 14, 2: 43, 3: 227, 4: 72, 5: 476, 6: 55, 7: 54, 8: 19, 9: 21}, 6: {0: 7, 1: 3, 2: 46, 3: 70, 4: 38, 5: 26, 6: 790, 7: 4, 8: 5, 9: 11}, 7: {0: 29, 1: 11, 2: 37, 3: 71, 4: 54, 5: 78, 6: 26, 7: 645, 8: 6, 9: 43}, 8: {0: 127, 1: 90, 2: 7, 3: 27, 4: 4, 5: 5, 6: 13, 7: 12, 8: 632, 9: 83}, 9: {0: 35, 1: 122, 2: 2, 3: 26, 4: 10, 5: 5, 6: 12, 7: 15, 8: 20, 9: 753}}
epoch 700, train loss avg now = 0.002635, train contrast loss now = 0.371499, test acc now = 0.6121, test loss now = 2.524354
{0: {0: 688, 1: 42, 2: 41, 3: 33, 4: 7, 5: 7, 6: 15, 7: 16, 8: 90, 9: 61}, 1: {0: 23, 1: 745, 2: 7, 3: 19, 4: 7, 5: 8, 6: 17, 7: 13, 8: 21, 9: 140}, 2: {0: 130, 1: 8, 2: 391, 3: 112, 4: 107, 5: 64, 6: 94, 7: 49, 8: 21, 9: 24}, 3: {0: 34, 1: 24, 2: 73, 3: 490, 4: 55, 5: 136, 6: 102, 7: 35, 8: 13, 9: 38}, 4: {0: 59, 1: 9, 2: 88, 3: 54, 4: 507, 5: 28, 6: 124, 7: 98, 8: 14, 9: 19}, 5: {0: 21, 1: 14, 2: 47, 3: 229, 4: 70, 5: 465, 6: 56, 7: 56, 8: 18, 9: 24}, 6: {0: 7, 1: 3, 2: 44, 3: 71, 4: 38, 5: 24, 6: 788, 7: 5, 8: 6, 9: 14}, 7: {0: 33, 1: 10, 2: 36, 3: 71, 4: 53, 5: 75, 6: 26, 7: 647, 8: 6, 9: 43}, 8: {0: 135, 1: 87, 2: 8, 3: 23, 4: 1, 5: 4, 6: 12, 7: 9, 8: 633, 9: 88}, 9: {0: 36, 1: 107, 2: 2, 3: 23, 4: 10, 5: 5, 6: 11, 7: 13, 8: 23, 9: 770}}
epoch 800, train loss avg now = 0.002359, train contrast loss now = 0.371594, test acc now = 0.6124, test loss now = 2.514050
{0: {0: 680, 1: 41, 2: 42, 3: 35, 4: 10, 5: 7, 6: 15, 7: 15, 8: 89, 9: 66}, 1: {0: 23, 1: 747, 2: 8, 3: 18, 4: 8, 5: 7, 6: 20, 7: 12, 8: 20, 9: 137}, 2: {0: 118, 1: 10, 2: 387, 3: 120, 4: 113, 5: 65, 6: 102, 7: 44, 8: 17, 9: 24}, 3: {0: 26, 1: 24, 2: 69, 3: 502, 4: 54, 5: 145, 6: 102, 7: 32, 8: 12, 9: 34}, 4: {0: 52, 1: 8, 2: 81, 3: 61, 4: 518, 5: 28, 6: 129, 7: 91, 8: 13, 9: 19}, 5: {0: 18, 1: 13, 2: 45, 3: 237, 4: 70, 5: 468, 6: 56, 7: 52, 8: 18, 9: 23}, 6: {0: 6, 1: 3, 2: 45, 3: 76, 4: 37, 5: 24, 6: 791, 7: 4, 8: 4, 9: 10}, 7: {0: 28, 1: 9, 2: 34, 3: 76, 4: 58, 5: 78, 6: 29, 7: 640, 8: 6, 9: 42}, 8: {0: 132, 1: 91, 2: 8, 3: 25, 4: 4, 5: 4, 6: 13, 7: 11, 8: 625, 9: 87}, 9: {0: 36, 1: 116, 2: 2, 3: 26, 4: 10, 5: 6, 6: 12, 7: 14, 8: 21, 9: 757}}
epoch 900, train loss avg now = 0.002549, train contrast loss now = 0.371512, test acc now = 0.6115, test loss now = 2.530953
{0: {0: 680, 1: 44, 2: 41, 3: 35, 4: 9, 5: 7, 6: 13, 7: 16, 8: 94, 9: 61}, 1: {0: 23, 1: 751, 2: 8, 3: 18, 4: 8, 5: 6, 6: 17, 7: 13, 8: 22, 9: 134}, 2: {0: 126, 1: 8, 2: 399, 3: 124, 4: 104, 5: 57, 6: 90, 7: 49, 8: 21, 9: 22}, 3: {0: 26, 1: 23, 2: 76, 3: 507, 4: 53, 5: 140, 6: 99, 7: 31, 8: 12, 9: 33}, 4: {0: 58, 1: 10, 2: 87, 3: 62, 4: 510, 5: 28, 6: 111, 7: 101, 8: 14, 9: 19}, 5: {0: 18, 1: 13, 2: 48, 3: 251, 4: 71, 5: 459, 6: 50, 7: 51, 8: 18, 9: 21}, 6: {0: 6, 1: 3, 2: 50, 3: 75, 4: 41, 5: 25, 6: 780, 7: 5, 8: 4, 9: 11}, 7: {0: 31, 1: 10, 2: 38, 3: 75, 4: 52, 5: 76, 6: 23, 7: 645, 8: 5, 9: 45}, 8: {0: 133, 1: 86, 2: 10, 3: 24, 4: 4, 5: 4, 6: 11, 7: 10, 8: 635, 9: 83}, 9: {0: 37, 1: 116, 2: 2, 3: 28, 4: 10, 5: 7, 6: 11, 7: 13, 8: 23, 9: 753}}
epoch 1000, train loss avg now = 0.002083, train contrast loss now = 0.371319, test acc now = 0.6119, test loss now = 2.512489
epoch avg loss = 2.0832870021014924e-06, total time = 8715.170429468155
total 24576.0MB, used 23554.0MB, free 1022.0MB
Round 6 finish, update the prev_syn_proto
torch.Size([707, 3, 32, 32])
torch.Size([707, 3, 32, 32])
torch.Size([728, 3, 32, 32])
torch.Size([700, 3, 32, 32])
torch.Size([700, 3, 32, 32])
torch.Size([735, 3, 32, 32])
torch.Size([707, 3, 32, 32])
torch.Size([700, 3, 32, 32])
torch.Size([700, 3, 32, 32])
torch.Size([700, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 680, 1: 44, 2: 41, 3: 35, 4: 9, 5: 7, 6: 13, 7: 16, 8: 94, 9: 61}, 1: {0: 23, 1: 751, 2: 8, 3: 18, 4: 8, 5: 6, 6: 17, 7: 13, 8: 22, 9: 134}, 2: {0: 126, 1: 8, 2: 399, 3: 124, 4: 104, 5: 57, 6: 90, 7: 49, 8: 21, 9: 22}, 3: {0: 26, 1: 23, 2: 76, 3: 507, 4: 53, 5: 140, 6: 99, 7: 31, 8: 12, 9: 33}, 4: {0: 58, 1: 10, 2: 87, 3: 62, 4: 510, 5: 28, 6: 111, 7: 101, 8: 14, 9: 19}, 5: {0: 18, 1: 13, 2: 48, 3: 251, 4: 71, 5: 459, 6: 50, 7: 51, 8: 18, 9: 21}, 6: {0: 6, 1: 3, 2: 50, 3: 75, 4: 41, 5: 25, 6: 780, 7: 5, 8: 4, 9: 11}, 7: {0: 31, 1: 10, 2: 38, 3: 75, 4: 52, 5: 76, 6: 23, 7: 645, 8: 5, 9: 45}, 8: {0: 133, 1: 86, 2: 10, 3: 24, 4: 4, 5: 4, 6: 11, 7: 10, 8: 635, 9: 83}, 9: {0: 37, 1: 116, 2: 2, 3: 28, 4: 10, 5: 7, 6: 11, 7: 13, 8: 23, 9: 753}}
round 6 evaluation: test acc is 0.6119, test loss = 2.512489
 ====== round 7 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 23554.0MB, free 1022.0MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1195  142  103   81   90   73   87  103  184 1535], bin edged: [0.00019499 0.00021012 0.00022526 0.00024039 0.00025552 0.00027066
 0.00028579 0.00030092 0.00031606 0.00033119 0.00034633]
class 7 have 4999 samples, histogram: [2899  169  122   87   75   68   81  107  162 1229], bin edged: [0.00015903 0.00017137 0.00018371 0.00019606 0.0002084  0.00022074
 0.00023308 0.00024543 0.00025777 0.00027011 0.00028245]
client 0, data condensation 0, total loss = 57.47015380859375, avg loss = 28.735076904296875
client 0, data condensation 200, total loss = 10.78204345703125, avg loss = 5.391021728515625
client 0, data condensation 400, total loss = 9.34375, avg loss = 4.671875
client 0, data condensation 600, total loss = 6.65008544921875, avg loss = 3.325042724609375
client 0, data condensation 800, total loss = 6.940673828125, avg loss = 3.4703369140625
client 0, data condensation 1000, total loss = 12.03314208984375, avg loss = 6.016571044921875
client 0, data condensation 1200, total loss = 6.4859619140625, avg loss = 3.24298095703125
client 0, data condensation 1400, total loss = 8.329345703125, avg loss = 4.1646728515625
client 0, data condensation 1600, total loss = 11.62872314453125, avg loss = 5.814361572265625
client 0, data condensation 1800, total loss = 10.19439697265625, avg loss = 5.097198486328125
client 0, data condensation 2000, total loss = 16.9515380859375, avg loss = 8.47576904296875
client 0, data condensation 2200, total loss = 7.1387939453125, avg loss = 3.56939697265625
client 0, data condensation 2400, total loss = 6.5430908203125, avg loss = 3.27154541015625
client 0, data condensation 2600, total loss = 5.0115966796875, avg loss = 2.50579833984375
client 0, data condensation 2800, total loss = 11.375732421875, avg loss = 5.6878662109375
client 0, data condensation 3000, total loss = 17.7740478515625, avg loss = 8.88702392578125
client 0, data condensation 3200, total loss = 7.22027587890625, avg loss = 3.610137939453125
client 0, data condensation 3400, total loss = 5.67529296875, avg loss = 2.837646484375
client 0, data condensation 3600, total loss = 5.57452392578125, avg loss = 2.787261962890625
client 0, data condensation 3800, total loss = 20.60809326171875, avg loss = 10.304046630859375
client 0, data condensation 4000, total loss = 21.441162109375, avg loss = 10.7205810546875
client 0, data condensation 4200, total loss = 6.57666015625, avg loss = 3.288330078125
client 0, data condensation 4400, total loss = 8.36126708984375, avg loss = 4.180633544921875
client 0, data condensation 4600, total loss = 8.33880615234375, avg loss = 4.169403076171875
client 0, data condensation 4800, total loss = 41.6876220703125, avg loss = 20.84381103515625
client 0, data condensation 5000, total loss = 25.0875244140625, avg loss = 12.54376220703125
client 0, data condensation 5200, total loss = 10.1251220703125, avg loss = 5.06256103515625
client 0, data condensation 5400, total loss = 9.98687744140625, avg loss = 4.993438720703125
client 0, data condensation 5600, total loss = 11.61590576171875, avg loss = 5.807952880859375
client 0, data condensation 5800, total loss = 7.07769775390625, avg loss = 3.538848876953125
client 0, data condensation 6000, total loss = 5.8760986328125, avg loss = 2.93804931640625
client 0, data condensation 6200, total loss = 32.6138916015625, avg loss = 16.30694580078125
client 0, data condensation 6400, total loss = 7.5762939453125, avg loss = 3.78814697265625
client 0, data condensation 6600, total loss = 20.37103271484375, avg loss = 10.185516357421875
client 0, data condensation 6800, total loss = 14.58990478515625, avg loss = 7.294952392578125
client 0, data condensation 7000, total loss = 24.95245361328125, avg loss = 12.476226806640625
client 0, data condensation 7200, total loss = 20.2413330078125, avg loss = 10.12066650390625
client 0, data condensation 7400, total loss = 6.718994140625, avg loss = 3.3594970703125
client 0, data condensation 7600, total loss = 5.774658203125, avg loss = 2.8873291015625
client 0, data condensation 7800, total loss = 3.4832763671875, avg loss = 1.74163818359375
client 0, data condensation 8000, total loss = 7.1146240234375, avg loss = 3.55731201171875
client 0, data condensation 8200, total loss = 12.0045166015625, avg loss = 6.00225830078125
client 0, data condensation 8400, total loss = 7.60345458984375, avg loss = 3.801727294921875
client 0, data condensation 8600, total loss = 7.23260498046875, avg loss = 3.616302490234375
client 0, data condensation 8800, total loss = 64.88421630859375, avg loss = 32.442108154296875
client 0, data condensation 9000, total loss = 39.87762451171875, avg loss = 19.938812255859375
client 0, data condensation 9200, total loss = 19.00189208984375, avg loss = 9.500946044921875
client 0, data condensation 9400, total loss = 6.739013671875, avg loss = 3.3695068359375
client 0, data condensation 9600, total loss = 7.86376953125, avg loss = 3.931884765625
client 0, data condensation 9800, total loss = 26.8314208984375, avg loss = 13.41571044921875
client 0, data condensation 10000, total loss = 7.1854248046875, avg loss = 3.59271240234375
Round 7, client 0 condense time: 739.2805263996124
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2951.06MB, free 21624.94MB
total 24576.0MB, used 2951.06MB, free 21624.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [53  9  8  5  5  5  4  7 10 69], bin edged: [0.00401227 0.00432367 0.00463508 0.00494649 0.0052579  0.00556931
 0.00588071 0.00619212 0.00650353 0.00681494 0.00712635]
class 4 have 4958 samples, histogram: [1866  246  144  136  123  127  128  154  217 1817], bin edged: [0.00014576 0.00015708 0.00016839 0.0001797  0.00019102 0.00020233
 0.00021364 0.00022496 0.00023627 0.00024758 0.00025889]
client 1, data condensation 0, total loss = 244.502685546875, avg loss = 122.2513427734375
client 1, data condensation 200, total loss = 401.92291259765625, avg loss = 200.96145629882812
client 1, data condensation 400, total loss = 162.9241943359375, avg loss = 81.46209716796875
client 1, data condensation 600, total loss = 144.94482421875, avg loss = 72.472412109375
client 1, data condensation 800, total loss = 114.2789306640625, avg loss = 57.13946533203125
client 1, data condensation 1000, total loss = 129.3704833984375, avg loss = 64.68524169921875
client 1, data condensation 1200, total loss = 243.927734375, avg loss = 121.9638671875
client 1, data condensation 1400, total loss = 75.2537841796875, avg loss = 37.62689208984375
client 1, data condensation 1600, total loss = 90.16644287109375, avg loss = 45.083221435546875
client 1, data condensation 1800, total loss = 127.220703125, avg loss = 63.6103515625
client 1, data condensation 2000, total loss = 140.65155029296875, avg loss = 70.32577514648438
client 1, data condensation 2200, total loss = 78.715576171875, avg loss = 39.3577880859375
client 1, data condensation 2400, total loss = 60.044677734375, avg loss = 30.0223388671875
client 1, data condensation 2600, total loss = 556.7322387695312, avg loss = 278.3661193847656
client 1, data condensation 2800, total loss = 155.518798828125, avg loss = 77.7593994140625
client 1, data condensation 3000, total loss = 103.41387939453125, avg loss = 51.706939697265625
client 1, data condensation 3200, total loss = 188.27276611328125, avg loss = 94.13638305664062
client 1, data condensation 3400, total loss = 102.19757080078125, avg loss = 51.098785400390625
client 1, data condensation 3600, total loss = 171.42120361328125, avg loss = 85.71060180664062
client 1, data condensation 3800, total loss = 307.1556396484375, avg loss = 153.57781982421875
client 1, data condensation 4000, total loss = 1087.183837890625, avg loss = 543.5919189453125
client 1, data condensation 4200, total loss = 59.1494140625, avg loss = 29.57470703125
client 1, data condensation 4400, total loss = 578.3760986328125, avg loss = 289.18804931640625
client 1, data condensation 4600, total loss = 73.6715087890625, avg loss = 36.83575439453125
client 1, data condensation 4800, total loss = 216.42974853515625, avg loss = 108.21487426757812
client 1, data condensation 5000, total loss = 90.06573486328125, avg loss = 45.032867431640625
client 1, data condensation 5200, total loss = 345.5238037109375, avg loss = 172.76190185546875
client 1, data condensation 5400, total loss = 327.9482421875, avg loss = 163.97412109375
client 1, data condensation 5600, total loss = 77.72723388671875, avg loss = 38.863616943359375
client 1, data condensation 5800, total loss = 159.11541748046875, avg loss = 79.55770874023438
client 1, data condensation 6000, total loss = 62.0364990234375, avg loss = 31.01824951171875
client 1, data condensation 6200, total loss = 125.79937744140625, avg loss = 62.899688720703125
client 1, data condensation 6400, total loss = 197.21636962890625, avg loss = 98.60818481445312
client 1, data condensation 6600, total loss = 90.95806884765625, avg loss = 45.479034423828125
client 1, data condensation 6800, total loss = 133.6748046875, avg loss = 66.83740234375
client 1, data condensation 7000, total loss = 366.330078125, avg loss = 183.1650390625
client 1, data condensation 7200, total loss = 46.2076416015625, avg loss = 23.10382080078125
client 1, data condensation 7400, total loss = 132.8475341796875, avg loss = 66.42376708984375
client 1, data condensation 7600, total loss = 413.40509033203125, avg loss = 206.70254516601562
client 1, data condensation 7800, total loss = 93.58966064453125, avg loss = 46.794830322265625
client 1, data condensation 8000, total loss = 82.70465087890625, avg loss = 41.352325439453125
client 1, data condensation 8200, total loss = 458.2816162109375, avg loss = 229.14080810546875
client 1, data condensation 8400, total loss = 139.01763916015625, avg loss = 69.50881958007812
client 1, data condensation 8600, total loss = 576.599853515625, avg loss = 288.2999267578125
client 1, data condensation 8800, total loss = 124.63946533203125, avg loss = 62.319732666015625
client 1, data condensation 9000, total loss = 90.69287109375, avg loss = 45.346435546875
client 1, data condensation 9200, total loss = 131.0087890625, avg loss = 65.50439453125
client 1, data condensation 9400, total loss = 63.85467529296875, avg loss = 31.927337646484375
client 1, data condensation 9600, total loss = 47.346435546875, avg loss = 23.6732177734375
client 1, data condensation 9800, total loss = 161.947998046875, avg loss = 80.9739990234375
client 1, data condensation 10000, total loss = 112.79022216796875, avg loss = 56.395111083984375
Round 7, client 1 condense time: 578.3927137851715
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2951.06MB, free 21624.94MB
total 24576.0MB, used 2951.06MB, free 21624.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [152   8   4   3   7   7   3   1  11  46], bin edged: [0.0033811  0.00364352 0.00390594 0.00416836 0.00443079 0.00469321
 0.00495563 0.00521805 0.00548047 0.00574289 0.00600531]
client 2, data condensation 0, total loss = 102.04925537109375, avg loss = 102.04925537109375
client 2, data condensation 200, total loss = 49.435302734375, avg loss = 49.435302734375
client 2, data condensation 400, total loss = 17.741455078125, avg loss = 17.741455078125
client 2, data condensation 600, total loss = 61.78399658203125, avg loss = 61.78399658203125
client 2, data condensation 800, total loss = 35.419677734375, avg loss = 35.419677734375
client 2, data condensation 1000, total loss = 48.02923583984375, avg loss = 48.02923583984375
client 2, data condensation 1200, total loss = 30.48675537109375, avg loss = 30.48675537109375
client 2, data condensation 1400, total loss = 75.77288818359375, avg loss = 75.77288818359375
client 2, data condensation 1600, total loss = 157.9700927734375, avg loss = 157.9700927734375
client 2, data condensation 1800, total loss = 102.73785400390625, avg loss = 102.73785400390625
client 2, data condensation 2000, total loss = 51.60406494140625, avg loss = 51.60406494140625
client 2, data condensation 2200, total loss = 40.44891357421875, avg loss = 40.44891357421875
client 2, data condensation 2400, total loss = 105.5107421875, avg loss = 105.5107421875
client 2, data condensation 2600, total loss = 55.96356201171875, avg loss = 55.96356201171875
client 2, data condensation 2800, total loss = 72.11968994140625, avg loss = 72.11968994140625
client 2, data condensation 3000, total loss = 71.00994873046875, avg loss = 71.00994873046875
client 2, data condensation 3200, total loss = 129.00518798828125, avg loss = 129.00518798828125
client 2, data condensation 3400, total loss = 2175.149169921875, avg loss = 2175.149169921875
client 2, data condensation 3600, total loss = 50.48492431640625, avg loss = 50.48492431640625
client 2, data condensation 3800, total loss = 90.7078857421875, avg loss = 90.7078857421875
client 2, data condensation 4000, total loss = 42.29205322265625, avg loss = 42.29205322265625
client 2, data condensation 4200, total loss = 120.504638671875, avg loss = 120.504638671875
client 2, data condensation 4400, total loss = 41.62469482421875, avg loss = 41.62469482421875
client 2, data condensation 4600, total loss = 125.2015380859375, avg loss = 125.2015380859375
client 2, data condensation 4800, total loss = 114.6961669921875, avg loss = 114.6961669921875
client 2, data condensation 5000, total loss = 73.53692626953125, avg loss = 73.53692626953125
client 2, data condensation 5200, total loss = 60.99884033203125, avg loss = 60.99884033203125
client 2, data condensation 5400, total loss = 47.50701904296875, avg loss = 47.50701904296875
client 2, data condensation 5600, total loss = 80.128662109375, avg loss = 80.128662109375
client 2, data condensation 5800, total loss = 58.2657470703125, avg loss = 58.2657470703125
client 2, data condensation 6000, total loss = 43.113037109375, avg loss = 43.113037109375
client 2, data condensation 6200, total loss = 386.7894287109375, avg loss = 386.7894287109375
client 2, data condensation 6400, total loss = 50.05120849609375, avg loss = 50.05120849609375
client 2, data condensation 6600, total loss = 72.92913818359375, avg loss = 72.92913818359375
client 2, data condensation 6800, total loss = 57.9176025390625, avg loss = 57.9176025390625
client 2, data condensation 7000, total loss = 44.814453125, avg loss = 44.814453125
client 2, data condensation 7200, total loss = 42.5977783203125, avg loss = 42.5977783203125
client 2, data condensation 7400, total loss = 82.78564453125, avg loss = 82.78564453125
client 2, data condensation 7600, total loss = 67.1317138671875, avg loss = 67.1317138671875
client 2, data condensation 7800, total loss = 85.98779296875, avg loss = 85.98779296875
client 2, data condensation 8000, total loss = 22.2122802734375, avg loss = 22.2122802734375
client 2, data condensation 8200, total loss = 70.85980224609375, avg loss = 70.85980224609375
client 2, data condensation 8400, total loss = 67.17620849609375, avg loss = 67.17620849609375
client 2, data condensation 8600, total loss = 72.3533935546875, avg loss = 72.3533935546875
client 2, data condensation 8800, total loss = 286.9766845703125, avg loss = 286.9766845703125
client 2, data condensation 9000, total loss = 350.45086669921875, avg loss = 350.45086669921875
client 2, data condensation 9200, total loss = 63.39697265625, avg loss = 63.39697265625
client 2, data condensation 9400, total loss = 109.3704833984375, avg loss = 109.3704833984375
client 2, data condensation 9600, total loss = 70.32659912109375, avg loss = 70.32659912109375
client 2, data condensation 9800, total loss = 95.5001220703125, avg loss = 95.5001220703125
client 2, data condensation 10000, total loss = 57.08538818359375, avg loss = 57.08538818359375
Round 7, client 2 condense time: 281.8723728656769
client 2, class 9 have 242 samples
total 24576.0MB, used 2563.06MB, free 22012.94MB
total 24576.0MB, used 2563.06MB, free 22012.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [493  27  15  16  16  13  21  20  26 200], bin edged: [0.00093992 0.00101287 0.00108582 0.00115877 0.00123172 0.00130467
 0.00137762 0.00145057 0.00152352 0.00159647 0.00166943]
class 2 have 1094 samples, histogram: [294  54  26  31  20  22  34  35  54 524], bin edged: [0.00062151 0.00066975 0.00071798 0.00076622 0.00081446 0.0008627
 0.00091094 0.00095917 0.00100741 0.00105565 0.00110389]
client 3, data condensation 0, total loss = 70.22100830078125, avg loss = 35.110504150390625
client 3, data condensation 200, total loss = 11.8577880859375, avg loss = 5.92889404296875
client 3, data condensation 400, total loss = 14.0377197265625, avg loss = 7.01885986328125
client 3, data condensation 600, total loss = 5.16455078125, avg loss = 2.582275390625
client 3, data condensation 800, total loss = 7.34222412109375, avg loss = 3.671112060546875
client 3, data condensation 1000, total loss = 9.1622314453125, avg loss = 4.58111572265625
client 3, data condensation 1200, total loss = 13.31390380859375, avg loss = 6.656951904296875
client 3, data condensation 1400, total loss = 13.1497802734375, avg loss = 6.57489013671875
client 3, data condensation 1600, total loss = 11.21319580078125, avg loss = 5.606597900390625
client 3, data condensation 1800, total loss = 18.5428466796875, avg loss = 9.27142333984375
client 3, data condensation 2000, total loss = 8.82012939453125, avg loss = 4.410064697265625
client 3, data condensation 2200, total loss = 16.7099609375, avg loss = 8.35498046875
client 3, data condensation 2400, total loss = 11.0362548828125, avg loss = 5.51812744140625
client 3, data condensation 2600, total loss = 12.53814697265625, avg loss = 6.269073486328125
client 3, data condensation 2800, total loss = 58.1380615234375, avg loss = 29.06903076171875
client 3, data condensation 3000, total loss = 11.75823974609375, avg loss = 5.879119873046875
client 3, data condensation 3200, total loss = 15.424072265625, avg loss = 7.7120361328125
client 3, data condensation 3400, total loss = 7.450927734375, avg loss = 3.7254638671875
client 3, data condensation 3600, total loss = 14.420654296875, avg loss = 7.2103271484375
client 3, data condensation 3800, total loss = 19.83233642578125, avg loss = 9.916168212890625
client 3, data condensation 4000, total loss = 13.22332763671875, avg loss = 6.611663818359375
client 3, data condensation 4200, total loss = 22.6094970703125, avg loss = 11.30474853515625
client 3, data condensation 4400, total loss = 7.713623046875, avg loss = 3.8568115234375
client 3, data condensation 4600, total loss = 12.9423828125, avg loss = 6.47119140625
client 3, data condensation 4800, total loss = 17.94097900390625, avg loss = 8.970489501953125
client 3, data condensation 5000, total loss = 31.25042724609375, avg loss = 15.625213623046875
client 3, data condensation 5200, total loss = 22.31158447265625, avg loss = 11.155792236328125
client 3, data condensation 5400, total loss = 15.237060546875, avg loss = 7.6185302734375
client 3, data condensation 5600, total loss = 10.702880859375, avg loss = 5.3514404296875
client 3, data condensation 5800, total loss = 16.79302978515625, avg loss = 8.396514892578125
client 3, data condensation 6000, total loss = 42.7725830078125, avg loss = 21.38629150390625
client 3, data condensation 6200, total loss = 12.82879638671875, avg loss = 6.414398193359375
client 3, data condensation 6400, total loss = 7.6231689453125, avg loss = 3.81158447265625
client 3, data condensation 6600, total loss = 13.8641357421875, avg loss = 6.93206787109375
client 3, data condensation 6800, total loss = 29.80621337890625, avg loss = 14.903106689453125
client 3, data condensation 7000, total loss = 9.53387451171875, avg loss = 4.766937255859375
client 3, data condensation 7200, total loss = 8.30133056640625, avg loss = 4.150665283203125
client 3, data condensation 7400, total loss = 18.3985595703125, avg loss = 9.19927978515625
client 3, data condensation 7600, total loss = 14.279052734375, avg loss = 7.1395263671875
client 3, data condensation 7800, total loss = 56.03607177734375, avg loss = 28.018035888671875
client 3, data condensation 8000, total loss = 5.472412109375, avg loss = 2.7362060546875
client 3, data condensation 8200, total loss = 9.967529296875, avg loss = 4.9837646484375
client 3, data condensation 8400, total loss = 31.16864013671875, avg loss = 15.584320068359375
client 3, data condensation 8600, total loss = 5.8216552734375, avg loss = 2.91082763671875
client 3, data condensation 8800, total loss = 43.3408203125, avg loss = 21.67041015625
client 3, data condensation 9000, total loss = 17.3472900390625, avg loss = 8.67364501953125
client 3, data condensation 9200, total loss = 15.738037109375, avg loss = 7.8690185546875
client 3, data condensation 9400, total loss = 13.30755615234375, avg loss = 6.653778076171875
client 3, data condensation 9600, total loss = 6.2342529296875, avg loss = 3.11712646484375
client 3, data condensation 9800, total loss = 10.89666748046875, avg loss = 5.448333740234375
client 3, data condensation 10000, total loss = 10.5335693359375, avg loss = 5.26678466796875
Round 7, client 3 condense time: 511.0388708114624
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2947.06MB, free 21628.94MB
total 24576.0MB, used 2947.06MB, free 21628.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2456  159  106   80   73   79   71   78  107  943], bin edged: [0.00019378 0.00020883 0.00022387 0.00023891 0.00025395 0.00026899
 0.00028403 0.00029907 0.00031411 0.00032915 0.00034419]
class 5 have 307 samples, histogram: [119  13  11  13   8   4   7   6  14 112], bin edged: [0.00236824 0.00255205 0.00273586 0.00291967 0.00310348 0.00328728
 0.00347109 0.0036549  0.00383871 0.00402252 0.00420633]
client 4, data condensation 0, total loss = 92.366943359375, avg loss = 46.1834716796875
client 4, data condensation 200, total loss = 13.56976318359375, avg loss = 6.784881591796875
client 4, data condensation 400, total loss = 43.83660888671875, avg loss = 21.918304443359375
client 4, data condensation 600, total loss = 140.9443359375, avg loss = 70.47216796875
client 4, data condensation 800, total loss = 44.13323974609375, avg loss = 22.066619873046875
client 4, data condensation 1000, total loss = 43.648193359375, avg loss = 21.8240966796875
client 4, data condensation 1200, total loss = 54.84832763671875, avg loss = 27.424163818359375
client 4, data condensation 1400, total loss = 50.1466064453125, avg loss = 25.07330322265625
client 4, data condensation 1600, total loss = 130.10882568359375, avg loss = 65.05441284179688
client 4, data condensation 1800, total loss = 72.696044921875, avg loss = 36.3480224609375
client 4, data condensation 2000, total loss = 215.164794921875, avg loss = 107.5823974609375
client 4, data condensation 2200, total loss = 52.8739013671875, avg loss = 26.43695068359375
client 4, data condensation 2400, total loss = 96.3040771484375, avg loss = 48.15203857421875
client 4, data condensation 2600, total loss = 68.786865234375, avg loss = 34.3934326171875
client 4, data condensation 2800, total loss = 69.9686279296875, avg loss = 34.98431396484375
client 4, data condensation 3000, total loss = 39.404052734375, avg loss = 19.7020263671875
client 4, data condensation 3200, total loss = 34.455810546875, avg loss = 17.2279052734375
client 4, data condensation 3400, total loss = 79.537841796875, avg loss = 39.7689208984375
client 4, data condensation 3600, total loss = 222.34368896484375, avg loss = 111.17184448242188
client 4, data condensation 3800, total loss = 34.8018798828125, avg loss = 17.40093994140625
client 4, data condensation 4000, total loss = 35.20037841796875, avg loss = 17.600189208984375
client 4, data condensation 4200, total loss = 43.0843505859375, avg loss = 21.54217529296875
client 4, data condensation 4400, total loss = 114.50936889648438, avg loss = 57.25468444824219
client 4, data condensation 4600, total loss = 42.2244873046875, avg loss = 21.11224365234375
client 4, data condensation 4800, total loss = 24.578125, avg loss = 12.2890625
client 4, data condensation 5000, total loss = 22.484619140625, avg loss = 11.2423095703125
client 4, data condensation 5200, total loss = 44.18402099609375, avg loss = 22.092010498046875
client 4, data condensation 5400, total loss = 60.567626953125, avg loss = 30.2838134765625
client 4, data condensation 5600, total loss = 61.6826171875, avg loss = 30.84130859375
client 4, data condensation 5800, total loss = 25.580322265625, avg loss = 12.7901611328125
client 4, data condensation 6000, total loss = 87.24090576171875, avg loss = 43.620452880859375
client 4, data condensation 6200, total loss = 51.6029052734375, avg loss = 25.80145263671875
client 4, data condensation 6400, total loss = 34.75927734375, avg loss = 17.379638671875
client 4, data condensation 6600, total loss = 30.47039794921875, avg loss = 15.235198974609375
client 4, data condensation 6800, total loss = 11.91845703125, avg loss = 5.959228515625
client 4, data condensation 7000, total loss = 33.82147216796875, avg loss = 16.910736083984375
client 4, data condensation 7200, total loss = 27.388671875, avg loss = 13.6943359375
client 4, data condensation 7400, total loss = 54.2120361328125, avg loss = 27.10601806640625
client 4, data condensation 7600, total loss = 59.443115234375, avg loss = 29.7215576171875
client 4, data condensation 7800, total loss = 43.49151611328125, avg loss = 21.745758056640625
client 4, data condensation 8000, total loss = 2071.89013671875, avg loss = 1035.945068359375
client 4, data condensation 8200, total loss = 60.38482666015625, avg loss = 30.192413330078125
client 4, data condensation 8400, total loss = 105.14288330078125, avg loss = 52.571441650390625
client 4, data condensation 8600, total loss = 62.4737548828125, avg loss = 31.23687744140625
client 4, data condensation 8800, total loss = 28.2607421875, avg loss = 14.13037109375
client 4, data condensation 9000, total loss = 94.55078125, avg loss = 47.275390625
client 4, data condensation 9200, total loss = 258.718994140625, avg loss = 129.3594970703125
client 4, data condensation 9400, total loss = 60.75103759765625, avg loss = 30.375518798828125
client 4, data condensation 9600, total loss = 58.3804931640625, avg loss = 29.19024658203125
client 4, data condensation 9800, total loss = 45.8731689453125, avg loss = 22.93658447265625
client 4, data condensation 10000, total loss = 50.14324951171875, avg loss = 25.071624755859375
Round 7, client 4 condense time: 536.8521773815155
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [2392  217  138  119   96   88   93  123  153 1580], bin edged: [0.00015142 0.00016317 0.00017492 0.00018668 0.00019843 0.00021018
 0.00022193 0.00023368 0.00024544 0.00025719 0.00026894]
client 5, data condensation 0, total loss = 49.312255859375, avg loss = 49.312255859375
client 5, data condensation 200, total loss = 4.82574462890625, avg loss = 4.82574462890625
client 5, data condensation 400, total loss = 5.75628662109375, avg loss = 5.75628662109375
client 5, data condensation 600, total loss = 12.5106201171875, avg loss = 12.5106201171875
client 5, data condensation 800, total loss = 3.8394775390625, avg loss = 3.8394775390625
client 5, data condensation 1000, total loss = 6.07806396484375, avg loss = 6.07806396484375
client 5, data condensation 1200, total loss = 5.321533203125, avg loss = 5.321533203125
client 5, data condensation 1400, total loss = 4.181640625, avg loss = 4.181640625
client 5, data condensation 1600, total loss = 11.52294921875, avg loss = 11.52294921875
client 5, data condensation 1800, total loss = 4.95111083984375, avg loss = 4.95111083984375
client 5, data condensation 2000, total loss = 6.025146484375, avg loss = 6.025146484375
client 5, data condensation 2200, total loss = 8.12432861328125, avg loss = 8.12432861328125
client 5, data condensation 2400, total loss = 5.1820068359375, avg loss = 5.1820068359375
client 5, data condensation 2600, total loss = 16.20220947265625, avg loss = 16.20220947265625
client 5, data condensation 2800, total loss = 2.810546875, avg loss = 2.810546875
client 5, data condensation 3000, total loss = 2.62384033203125, avg loss = 2.62384033203125
client 5, data condensation 3200, total loss = 4.2796630859375, avg loss = 4.2796630859375
client 5, data condensation 3400, total loss = 10.22906494140625, avg loss = 10.22906494140625
client 5, data condensation 3600, total loss = 9.0374755859375, avg loss = 9.0374755859375
client 5, data condensation 3800, total loss = 5.55908203125, avg loss = 5.55908203125
client 5, data condensation 4000, total loss = 3.9307861328125, avg loss = 3.9307861328125
client 5, data condensation 4200, total loss = 17.31024169921875, avg loss = 17.31024169921875
client 5, data condensation 4400, total loss = 18.68994140625, avg loss = 18.68994140625
client 5, data condensation 4600, total loss = 4.528076171875, avg loss = 4.528076171875
client 5, data condensation 4800, total loss = 2.01953125, avg loss = 2.01953125
client 5, data condensation 5000, total loss = 48.2938232421875, avg loss = 48.2938232421875
client 5, data condensation 5200, total loss = 10.95391845703125, avg loss = 10.95391845703125
client 5, data condensation 5400, total loss = 8.3760986328125, avg loss = 8.3760986328125
client 5, data condensation 5600, total loss = 3.96966552734375, avg loss = 3.96966552734375
client 5, data condensation 5800, total loss = 6.990234375, avg loss = 6.990234375
client 5, data condensation 6000, total loss = 37.948486328125, avg loss = 37.948486328125
client 5, data condensation 6200, total loss = 7.08966064453125, avg loss = 7.08966064453125
client 5, data condensation 6400, total loss = 51.97821044921875, avg loss = 51.97821044921875
client 5, data condensation 6600, total loss = 4.4803466796875, avg loss = 4.4803466796875
client 5, data condensation 6800, total loss = 9.3502197265625, avg loss = 9.3502197265625
client 5, data condensation 7000, total loss = 9.48004150390625, avg loss = 9.48004150390625
client 5, data condensation 7200, total loss = 3.32574462890625, avg loss = 3.32574462890625
client 5, data condensation 7400, total loss = 5.1343994140625, avg loss = 5.1343994140625
client 5, data condensation 7600, total loss = 3.18499755859375, avg loss = 3.18499755859375
client 5, data condensation 7800, total loss = 2.73309326171875, avg loss = 2.73309326171875
client 5, data condensation 8000, total loss = 2.913818359375, avg loss = 2.913818359375
client 5, data condensation 8200, total loss = 4.0164794921875, avg loss = 4.0164794921875
client 5, data condensation 8400, total loss = 3.4990234375, avg loss = 3.4990234375
client 5, data condensation 8600, total loss = 3.8455810546875, avg loss = 3.8455810546875
client 5, data condensation 8800, total loss = 2.5093994140625, avg loss = 2.5093994140625
client 5, data condensation 9000, total loss = 2.38238525390625, avg loss = 2.38238525390625
client 5, data condensation 9200, total loss = 8.33489990234375, avg loss = 8.33489990234375
client 5, data condensation 9400, total loss = 56.642578125, avg loss = 56.642578125
client 5, data condensation 9600, total loss = 32.83355712890625, avg loss = 32.83355712890625
client 5, data condensation 9800, total loss = 8.23187255859375, avg loss = 8.23187255859375
client 5, data condensation 10000, total loss = 23.05804443359375, avg loss = 23.05804443359375
Round 7, client 5 condense time: 352.7789041996002
client 5, class 8 have 4999 samples
total 24576.0MB, used 2569.06MB, free 22006.94MB
total 24576.0MB, used 2569.06MB, free 22006.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1682  209  135  105  107   88  117  136  173 1613], bin edged: [0.00016601 0.0001789  0.00019178 0.00020467 0.00021755 0.00023044
 0.00024332 0.00025621 0.00026909 0.00028198 0.00029486]
class 6 have 3914 samples, histogram: [2534  132   86   68   60   55   56   69   80  774], bin edged: [0.00021114 0.00022752 0.00024391 0.0002603  0.00027668 0.00029307
 0.00030946 0.00032585 0.00034223 0.00035862 0.00037501]
client 6, data condensation 0, total loss = 80.14068603515625, avg loss = 40.070343017578125
client 6, data condensation 200, total loss = 15.5584716796875, avg loss = 7.77923583984375
client 6, data condensation 400, total loss = 7.17498779296875, avg loss = 3.587493896484375
client 6, data condensation 600, total loss = 5.17010498046875, avg loss = 2.585052490234375
client 6, data condensation 800, total loss = 19.10760498046875, avg loss = 9.553802490234375
client 6, data condensation 1000, total loss = 17.2056884765625, avg loss = 8.60284423828125
client 6, data condensation 1200, total loss = 40.904052734375, avg loss = 20.4520263671875
client 6, data condensation 1400, total loss = 15.81439208984375, avg loss = 7.907196044921875
client 6, data condensation 1600, total loss = 12.541259765625, avg loss = 6.2706298828125
client 6, data condensation 1800, total loss = 8.93206787109375, avg loss = 4.466033935546875
client 6, data condensation 2000, total loss = 6.235107421875, avg loss = 3.1175537109375
client 6, data condensation 2200, total loss = 14.35980224609375, avg loss = 7.179901123046875
client 6, data condensation 2400, total loss = 10.14892578125, avg loss = 5.074462890625
client 6, data condensation 2600, total loss = 20.316650390625, avg loss = 10.1583251953125
client 6, data condensation 2800, total loss = 10.935546875, avg loss = 5.4677734375
client 6, data condensation 3000, total loss = 4.89056396484375, avg loss = 2.445281982421875
client 6, data condensation 3200, total loss = 6.88946533203125, avg loss = 3.444732666015625
client 6, data condensation 3400, total loss = 10.01513671875, avg loss = 5.007568359375
client 6, data condensation 3600, total loss = 7.696044921875, avg loss = 3.8480224609375
client 6, data condensation 3800, total loss = 9.40325927734375, avg loss = 4.701629638671875
client 6, data condensation 4000, total loss = 6.63299560546875, avg loss = 3.316497802734375
client 6, data condensation 4200, total loss = 6.11529541015625, avg loss = 3.057647705078125
client 6, data condensation 4400, total loss = 7.20343017578125, avg loss = 3.601715087890625
client 6, data condensation 4600, total loss = 12.00250244140625, avg loss = 6.001251220703125
client 6, data condensation 4800, total loss = 4.49676513671875, avg loss = 2.248382568359375
client 6, data condensation 5000, total loss = 8.8973388671875, avg loss = 4.44866943359375
client 6, data condensation 5200, total loss = 13.20477294921875, avg loss = 6.602386474609375
client 6, data condensation 5400, total loss = 10.6773681640625, avg loss = 5.33868408203125
client 6, data condensation 5600, total loss = 57.42840576171875, avg loss = 28.714202880859375
client 6, data condensation 5800, total loss = 9.1341552734375, avg loss = 4.56707763671875
client 6, data condensation 6000, total loss = 40.97833251953125, avg loss = 20.489166259765625
client 6, data condensation 6200, total loss = 6.8121337890625, avg loss = 3.40606689453125
client 6, data condensation 6400, total loss = 5.50762939453125, avg loss = 2.753814697265625
client 6, data condensation 6600, total loss = 5.32293701171875, avg loss = 2.661468505859375
client 6, data condensation 6800, total loss = 16.68035888671875, avg loss = 8.340179443359375
client 6, data condensation 7000, total loss = 9.5, avg loss = 4.75
client 6, data condensation 7200, total loss = 50.3555908203125, avg loss = 25.17779541015625
client 6, data condensation 7400, total loss = 7.25726318359375, avg loss = 3.628631591796875
client 6, data condensation 7600, total loss = 6.2799072265625, avg loss = 3.13995361328125
client 6, data condensation 7800, total loss = 14.36785888671875, avg loss = 7.183929443359375
client 6, data condensation 8000, total loss = 13.04541015625, avg loss = 6.522705078125
client 6, data condensation 8200, total loss = 9.5867919921875, avg loss = 4.79339599609375
client 6, data condensation 8400, total loss = 9.55517578125, avg loss = 4.777587890625
client 6, data condensation 8600, total loss = 13.783447265625, avg loss = 6.8917236328125
client 6, data condensation 8800, total loss = 7.25518798828125, avg loss = 3.627593994140625
client 6, data condensation 9000, total loss = 5.87591552734375, avg loss = 2.937957763671875
client 6, data condensation 9200, total loss = 38.836669921875, avg loss = 19.4183349609375
client 6, data condensation 9400, total loss = 61.898193359375, avg loss = 30.9490966796875
client 6, data condensation 9600, total loss = 5.09625244140625, avg loss = 2.548126220703125
client 6, data condensation 9800, total loss = 6.9669189453125, avg loss = 3.48345947265625
client 6, data condensation 10000, total loss = 6.95806884765625, avg loss = 3.479034423828125
Round 7, client 6 condense time: 656.3998091220856
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 14632.0MB, free 9944.0MB
total 24576.0MB, used 14632.0MB, free 9944.0MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3217  164   99   60   66   69   65   75  100  690], bin edged: [0.00018523 0.00019961 0.00021399 0.00022837 0.00024274 0.00025712
 0.0002715  0.00028587 0.00030025 0.00031463 0.000329  ]
class 3 have 4999 samples, histogram: [1750  271  190  146  143  127  141  169  246 1816], bin edged: [0.00014378 0.00015494 0.0001661  0.00017725 0.00018841 0.00019957
 0.00021073 0.00022189 0.00023305 0.00024421 0.00025537]
client 7, data condensation 0, total loss = 45.32513427734375, avg loss = 22.662567138671875
client 7, data condensation 200, total loss = 9.74334716796875, avg loss = 4.871673583984375
client 7, data condensation 400, total loss = 14.88348388671875, avg loss = 7.441741943359375
client 7, data condensation 600, total loss = 7.8388671875, avg loss = 3.91943359375
client 7, data condensation 800, total loss = 9.8560791015625, avg loss = 4.92803955078125
client 7, data condensation 1000, total loss = 15.49920654296875, avg loss = 7.749603271484375
client 7, data condensation 1200, total loss = 5.85760498046875, avg loss = 2.928802490234375
client 7, data condensation 1400, total loss = 6.6588134765625, avg loss = 3.32940673828125
client 7, data condensation 1600, total loss = 11.82879638671875, avg loss = 5.914398193359375
client 7, data condensation 1800, total loss = 49.9593505859375, avg loss = 24.97967529296875
client 7, data condensation 2000, total loss = 9.3692626953125, avg loss = 4.68463134765625
client 7, data condensation 2200, total loss = 5.7996826171875, avg loss = 2.89984130859375
client 7, data condensation 2400, total loss = 10.93548583984375, avg loss = 5.467742919921875
client 7, data condensation 2600, total loss = 15.1495361328125, avg loss = 7.57476806640625
client 7, data condensation 2800, total loss = 9.925048828125, avg loss = 4.9625244140625
client 7, data condensation 3000, total loss = 26.41815185546875, avg loss = 13.209075927734375
client 7, data condensation 3200, total loss = 7.73309326171875, avg loss = 3.866546630859375
client 7, data condensation 3400, total loss = 6.34881591796875, avg loss = 3.174407958984375
client 7, data condensation 3600, total loss = 10.64068603515625, avg loss = 5.320343017578125
client 7, data condensation 3800, total loss = 18.02392578125, avg loss = 9.011962890625
client 7, data condensation 4000, total loss = 8.883056640625, avg loss = 4.4415283203125
client 7, data condensation 4200, total loss = 34.96484375, avg loss = 17.482421875
client 7, data condensation 4400, total loss = 7.57208251953125, avg loss = 3.786041259765625
client 7, data condensation 4600, total loss = 10.8065185546875, avg loss = 5.40325927734375
client 7, data condensation 4800, total loss = 28.07830810546875, avg loss = 14.039154052734375
client 7, data condensation 5000, total loss = 5.58343505859375, avg loss = 2.791717529296875
client 7, data condensation 5200, total loss = 20.83258056640625, avg loss = 10.416290283203125
client 7, data condensation 5400, total loss = 9.13677978515625, avg loss = 4.568389892578125
client 7, data condensation 5600, total loss = 6.21258544921875, avg loss = 3.106292724609375
client 7, data condensation 5800, total loss = 7.31658935546875, avg loss = 3.658294677734375
client 7, data condensation 6000, total loss = 7.1400146484375, avg loss = 3.57000732421875
client 7, data condensation 6200, total loss = 10.13714599609375, avg loss = 5.068572998046875
client 7, data condensation 6400, total loss = 6.24017333984375, avg loss = 3.120086669921875
client 7, data condensation 6600, total loss = 12.89788818359375, avg loss = 6.448944091796875
client 7, data condensation 6800, total loss = 11.70001220703125, avg loss = 5.850006103515625
client 7, data condensation 7000, total loss = 9.0240478515625, avg loss = 4.51202392578125
client 7, data condensation 7200, total loss = 7.4573974609375, avg loss = 3.72869873046875
client 7, data condensation 7400, total loss = 7.8870849609375, avg loss = 3.94354248046875
client 7, data condensation 7600, total loss = 10.753662109375, avg loss = 5.3768310546875
client 7, data condensation 7800, total loss = 12.7230224609375, avg loss = 6.36151123046875
client 7, data condensation 8000, total loss = 9.32867431640625, avg loss = 4.664337158203125
client 7, data condensation 8200, total loss = 7.33013916015625, avg loss = 3.665069580078125
client 7, data condensation 8400, total loss = 9.045654296875, avg loss = 4.5228271484375
client 7, data condensation 8600, total loss = 7.64837646484375, avg loss = 3.824188232421875
client 7, data condensation 8800, total loss = 8.8997802734375, avg loss = 4.44989013671875
client 7, data condensation 9000, total loss = 6.57427978515625, avg loss = 3.287139892578125
client 7, data condensation 9200, total loss = 8.4239501953125, avg loss = 4.21197509765625
client 7, data condensation 9400, total loss = 9.47137451171875, avg loss = 4.735687255859375
client 7, data condensation 9600, total loss = 5.48297119140625, avg loss = 2.741485595703125
client 7, data condensation 9800, total loss = 14.3878173828125, avg loss = 7.19390869140625
client 7, data condensation 10000, total loss = 6.5938720703125, avg loss = 3.29693603515625
Round 7, client 7 condense time: 869.848938703537
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [249  11  12   5   8   3   8   4   6  58], bin edged: [0.00233171 0.00251269 0.00269366 0.00287463 0.00305561 0.00323658
 0.00341755 0.00359853 0.0037795  0.00396047 0.00414145]
class 5 have 135 samples, histogram: [53  4  6  4  4  1  2  6  3 52], bin edged: [0.0053648  0.00578118 0.00619757 0.00661395 0.00703033 0.00744672
 0.0078631  0.00827949 0.00869587 0.00911225 0.00952864]
class 9 have 4727 samples, histogram: [3145  177  105   81   64   75   71   84  117  808], bin edged: [0.00017724 0.00019099 0.00020475 0.00021851 0.00023226 0.00024602
 0.00025978 0.00027353 0.00028729 0.00030104 0.0003148 ]
client 8, data condensation 0, total loss = 122.897216796875, avg loss = 40.965738932291664
client 8, data condensation 200, total loss = 92.59405517578125, avg loss = 30.86468505859375
client 8, data condensation 400, total loss = 73.9940185546875, avg loss = 24.6646728515625
client 8, data condensation 600, total loss = 561.9591674804688, avg loss = 187.3197224934896
client 8, data condensation 800, total loss = 212.0345458984375, avg loss = 70.67818196614583
client 8, data condensation 1000, total loss = 222.631591796875, avg loss = 74.21053059895833
client 8, data condensation 1200, total loss = 108.98114013671875, avg loss = 36.327046712239586
client 8, data condensation 1400, total loss = 231.6177978515625, avg loss = 77.2059326171875
client 8, data condensation 1600, total loss = 1169.552734375, avg loss = 389.8509114583333
client 8, data condensation 1800, total loss = 50.05145263671875, avg loss = 16.683817545572918
client 8, data condensation 2000, total loss = 82.345703125, avg loss = 27.448567708333332
client 8, data condensation 2200, total loss = 109.18994140625, avg loss = 36.396647135416664
client 8, data condensation 2400, total loss = 213.5615234375, avg loss = 71.18717447916667
client 8, data condensation 2600, total loss = 98.091064453125, avg loss = 32.697021484375
client 8, data condensation 2800, total loss = 126.84576416015625, avg loss = 42.28192138671875
client 8, data condensation 3000, total loss = 53.3416748046875, avg loss = 17.780558268229168
client 8, data condensation 3200, total loss = 89.83685302734375, avg loss = 29.94561767578125
client 8, data condensation 3400, total loss = 68.42938232421875, avg loss = 22.809794108072918
client 8, data condensation 3600, total loss = 236.7816162109375, avg loss = 78.92720540364583
client 8, data condensation 3800, total loss = 89.03240966796875, avg loss = 29.677469889322918
client 8, data condensation 4000, total loss = 80.6212158203125, avg loss = 26.873738606770832
client 8, data condensation 4200, total loss = 53.9609375, avg loss = 17.986979166666668
client 8, data condensation 4400, total loss = 105.5062255859375, avg loss = 35.168741861979164
client 8, data condensation 4600, total loss = 66.21875, avg loss = 22.072916666666668
client 8, data condensation 4800, total loss = 104.9658203125, avg loss = 34.988606770833336
client 8, data condensation 5000, total loss = 172.4886474609375, avg loss = 57.4962158203125
client 8, data condensation 5200, total loss = 94.8743896484375, avg loss = 31.624796549479168
client 8, data condensation 5400, total loss = 223.1783447265625, avg loss = 74.39278157552083
client 8, data condensation 5600, total loss = 131.8255615234375, avg loss = 43.941853841145836
client 8, data condensation 5800, total loss = 61.8892822265625, avg loss = 20.6297607421875
client 8, data condensation 6000, total loss = 71.13482666015625, avg loss = 23.71160888671875
client 8, data condensation 6200, total loss = 534.2870483398438, avg loss = 178.0956827799479
client 8, data condensation 6400, total loss = 76.682373046875, avg loss = 25.560791015625
client 8, data condensation 6600, total loss = 145.79681396484375, avg loss = 48.59893798828125
client 8, data condensation 6800, total loss = 60.15911865234375, avg loss = 20.05303955078125
client 8, data condensation 7000, total loss = 98.37664794921875, avg loss = 32.792215983072914
client 8, data condensation 7200, total loss = 156.79937744140625, avg loss = 52.266459147135414
client 8, data condensation 7400, total loss = 105.0213623046875, avg loss = 35.007120768229164
client 8, data condensation 7600, total loss = 53.4493408203125, avg loss = 17.816446940104168
client 8, data condensation 7800, total loss = 90.526611328125, avg loss = 30.175537109375
client 8, data condensation 8000, total loss = 91.88427734375, avg loss = 30.628092447916668
client 8, data condensation 8200, total loss = 64.9808349609375, avg loss = 21.6602783203125
client 8, data condensation 8400, total loss = 119.1788330078125, avg loss = 39.726277669270836
client 8, data condensation 8600, total loss = 70.02532958984375, avg loss = 23.341776529947918
client 8, data condensation 8800, total loss = 37.32952880859375, avg loss = 12.44317626953125
client 8, data condensation 9000, total loss = 84.86553955078125, avg loss = 28.28851318359375
client 8, data condensation 9200, total loss = 60.17681884765625, avg loss = 20.058939615885418
client 8, data condensation 9400, total loss = 170.41046142578125, avg loss = 56.803487141927086
client 8, data condensation 9600, total loss = 65.3817138671875, avg loss = 21.793904622395832
client 8, data condensation 9800, total loss = 175.41497802734375, avg loss = 58.471659342447914
client 8, data condensation 10000, total loss = 205.16650390625, avg loss = 68.38883463541667
Round 7, client 8 condense time: 808.0160672664642
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3207.06MB, free 21368.94MB
total 24576.0MB, used 3207.06MB, free 21368.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [51  3  2  3  2  3  2  1  5 48], bin edged: [0.00603555 0.006504   0.00697244 0.00744088 0.00790933 0.00837777
 0.00884621 0.00931466 0.0097831  0.01025154 0.01071999]
class 5 have 192 samples, histogram: [81  4  4  3  4  2  5  5  8 76], bin edged: [0.00375869 0.00405042 0.00434215 0.00463388 0.0049256  0.00521733
 0.00550906 0.00580079 0.00609251 0.00638424 0.00667597]
class 6 have 1075 samples, histogram: [686  39  24  27  17  16  24  20  30 192], bin edged: [0.00077053 0.00083034 0.00089014 0.00094995 0.00100975 0.00106956
 0.00112936 0.00118916 0.00124897 0.00130877 0.00136858]
client 9, data condensation 0, total loss = 536.7162475585938, avg loss = 178.9054158528646
client 9, data condensation 200, total loss = 195.1285400390625, avg loss = 65.0428466796875
client 9, data condensation 400, total loss = 194.9267578125, avg loss = 64.9755859375
client 9, data condensation 600, total loss = 214.1444091796875, avg loss = 71.3814697265625
client 9, data condensation 800, total loss = 687.8765258789062, avg loss = 229.29217529296875
client 9, data condensation 1000, total loss = 328.724365234375, avg loss = 109.57478841145833
client 9, data condensation 1200, total loss = 219.343505859375, avg loss = 73.114501953125
client 9, data condensation 1400, total loss = 255.544189453125, avg loss = 85.181396484375
client 9, data condensation 1600, total loss = 874.5576782226562, avg loss = 291.51922607421875
client 9, data condensation 1800, total loss = 271.71966552734375, avg loss = 90.57322184244792
client 9, data condensation 2000, total loss = 164.3367919921875, avg loss = 54.7789306640625
client 9, data condensation 2200, total loss = 219.37652587890625, avg loss = 73.12550862630208
client 9, data condensation 2400, total loss = 230.93896484375, avg loss = 76.97965494791667
client 9, data condensation 2600, total loss = 141.99298095703125, avg loss = 47.33099365234375
client 9, data condensation 2800, total loss = 756.027587890625, avg loss = 252.00919596354166
client 9, data condensation 3000, total loss = 219.34735107421875, avg loss = 73.11578369140625
client 9, data condensation 3200, total loss = 107.51904296875, avg loss = 35.839680989583336
client 9, data condensation 3400, total loss = 269.058349609375, avg loss = 89.68611653645833
client 9, data condensation 3600, total loss = 152.48663330078125, avg loss = 50.828877766927086
client 9, data condensation 3800, total loss = 145.01263427734375, avg loss = 48.337544759114586
client 9, data condensation 4000, total loss = 672.89208984375, avg loss = 224.29736328125
client 9, data condensation 4200, total loss = 309.99310302734375, avg loss = 103.33103434244792
client 9, data condensation 4400, total loss = 166.44866943359375, avg loss = 55.482889811197914
client 9, data condensation 4600, total loss = 112.14056396484375, avg loss = 37.38018798828125
client 9, data condensation 4800, total loss = 139.87799072265625, avg loss = 46.625996907552086
client 9, data condensation 5000, total loss = 401.02862548828125, avg loss = 133.67620849609375
client 9, data condensation 5200, total loss = 159.52703857421875, avg loss = 53.175679524739586
client 9, data condensation 5400, total loss = 135.9815673828125, avg loss = 45.327189127604164
client 9, data condensation 5600, total loss = 457.458251953125, avg loss = 152.486083984375
client 9, data condensation 5800, total loss = 136.98699951171875, avg loss = 45.662333170572914
client 9, data condensation 6000, total loss = 181.22589111328125, avg loss = 60.40863037109375
client 9, data condensation 6200, total loss = 342.15277099609375, avg loss = 114.05092366536458
client 9, data condensation 6400, total loss = 114.77728271484375, avg loss = 38.25909423828125
client 9, data condensation 6600, total loss = 202.02728271484375, avg loss = 67.34242757161458
client 9, data condensation 6800, total loss = 130.1658935546875, avg loss = 43.388631184895836
client 9, data condensation 7000, total loss = 311.65106201171875, avg loss = 103.88368733723958
client 9, data condensation 7200, total loss = 177.6351318359375, avg loss = 59.211710611979164
client 9, data condensation 7400, total loss = 1682.7109375, avg loss = 560.9036458333334
client 9, data condensation 7600, total loss = 164.725830078125, avg loss = 54.908610026041664
client 9, data condensation 7800, total loss = 134.05450439453125, avg loss = 44.684834798177086
client 9, data condensation 8000, total loss = 126.2901611328125, avg loss = 42.096720377604164
client 9, data condensation 8200, total loss = 154.0673828125, avg loss = 51.355794270833336
client 9, data condensation 8400, total loss = 186.73004150390625, avg loss = 62.24334716796875
client 9, data condensation 8600, total loss = 672.0624389648438, avg loss = 224.02081298828125
client 9, data condensation 8800, total loss = 131.22528076171875, avg loss = 43.74176025390625
client 9, data condensation 9000, total loss = 145.21966552734375, avg loss = 48.40655517578125
client 9, data condensation 9200, total loss = 176.3330078125, avg loss = 58.777669270833336
client 9, data condensation 9400, total loss = 123.5765380859375, avg loss = 41.192179361979164
client 9, data condensation 9600, total loss = 131.9552001953125, avg loss = 43.985066731770836
client 9, data condensation 9800, total loss = 123.9951171875, avg loss = 41.331705729166664
client 9, data condensation 10000, total loss = 178.4368896484375, avg loss = 59.478963216145836
Round 7, client 9 condense time: 716.2223579883575
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3207.06MB, free 21368.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 1.7076e+01,  1.2777e+00,  3.2046e+00, -4.4845e+00, -8.9395e-01,
         -9.8127e+00, -1.1048e+01, -2.8457e+00,  6.1729e+00,  1.8402e+00],
        [ 1.1173e+00,  1.8020e+01, -5.5197e+00, -2.8278e+00, -4.8778e+00,
         -6.8842e+00, -6.1121e+00, -1.6751e+00,  4.2536e-01,  9.6128e+00],
        [ 3.4308e-01, -5.2528e+00,  1.0351e+01,  2.0268e+00,  4.0263e+00,
          1.5896e+00,  2.7772e-01,  1.2364e+00, -8.5344e+00, -5.1094e+00],
        [-4.5527e+00, -3.3930e+00,  1.7503e+00,  1.0958e+01, -3.5880e-01,
          6.5761e+00,  2.5162e+00,  5.8880e-01, -9.7040e+00, -3.3923e+00],
        [-3.2450e+00, -4.7279e+00,  5.0906e+00, -2.2839e-01,  1.1123e+01,
          9.4375e-01,  3.3497e+00,  4.3972e+00, -1.0699e+01, -5.2629e+00],
        [-6.7708e+00, -4.7833e+00,  3.0149e+00,  9.5070e+00, -1.4822e-02,
          1.2239e+01,  6.6899e-01,  2.9391e+00, -1.1128e+01, -4.4817e+00],
        [-7.1633e+00, -3.0123e+00,  3.3335e+00,  4.6770e+00,  5.3808e+00,
          1.9175e+00,  1.6149e+01, -2.0077e+00, -1.3788e+01, -4.6097e+00],
        [-4.3256e+00, -3.3025e+00,  1.7607e+00,  1.5527e-01,  4.9948e+00,
          1.2246e+00, -2.2048e+00,  1.4968e+01, -1.1369e+01, -5.8704e-01],
        [ 1.0830e+01,  5.1907e+00, -1.7827e+00, -4.3057e+00, -3.6959e+00,
         -9.9574e+00, -1.2563e+01, -4.7084e+00,  1.6277e+01,  5.5545e+00],
        [ 9.5875e-02,  8.1363e+00, -5.0735e+00, -2.2936e+00, -4.5048e+00,
         -6.7867e+00, -6.6026e+00,  1.3997e+00,  1.6884e-02,  1.6713e+01]],
       device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 5, 7],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 2, 5, 3],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
8
Round 7: # synthetic sample: 8096
total 24576.0MB, used 3207.06MB, free 21368.94MB
{0: {0: 680, 1: 44, 2: 41, 3: 35, 4: 9, 5: 7, 6: 13, 7: 16, 8: 94, 9: 61}, 1: {0: 23, 1: 751, 2: 8, 3: 18, 4: 8, 5: 6, 6: 17, 7: 13, 8: 22, 9: 134}, 2: {0: 126, 1: 8, 2: 399, 3: 124, 4: 104, 5: 57, 6: 90, 7: 49, 8: 21, 9: 22}, 3: {0: 26, 1: 23, 2: 76, 3: 507, 4: 53, 5: 140, 6: 99, 7: 31, 8: 12, 9: 33}, 4: {0: 58, 1: 10, 2: 87, 3: 62, 4: 510, 5: 28, 6: 111, 7: 101, 8: 14, 9: 19}, 5: {0: 18, 1: 13, 2: 48, 3: 251, 4: 71, 5: 459, 6: 50, 7: 51, 8: 18, 9: 21}, 6: {0: 6, 1: 3, 2: 50, 3: 75, 4: 41, 5: 25, 6: 780, 7: 5, 8: 4, 9: 11}, 7: {0: 31, 1: 10, 2: 38, 3: 75, 4: 52, 5: 76, 6: 23, 7: 645, 8: 5, 9: 45}, 8: {0: 133, 1: 86, 2: 10, 3: 24, 4: 4, 5: 4, 6: 11, 7: 10, 8: 635, 9: 83}, 9: {0: 37, 1: 116, 2: 2, 3: 28, 4: 10, 5: 7, 6: 11, 7: 13, 8: 23, 9: 753}}
round 7 evaluation: test acc is 0.6119, test loss = 2.512489
{0: {0: 631, 1: 22, 2: 87, 3: 51, 4: 12, 5: 10, 6: 8, 7: 14, 8: 99, 9: 66}, 1: {0: 38, 1: 671, 2: 10, 3: 40, 4: 9, 5: 16, 6: 9, 7: 8, 8: 34, 9: 165}, 2: {0: 94, 1: 5, 2: 438, 3: 158, 4: 103, 5: 106, 6: 33, 7: 19, 8: 24, 9: 20}, 3: {0: 19, 1: 4, 2: 81, 3: 579, 4: 41, 5: 192, 6: 31, 7: 16, 8: 14, 9: 23}, 4: {0: 45, 1: 8, 2: 125, 3: 113, 4: 532, 5: 67, 6: 27, 7: 42, 8: 20, 9: 21}, 5: {0: 14, 1: 4, 2: 64, 3: 263, 4: 60, 5: 540, 6: 12, 7: 18, 8: 11, 9: 14}, 6: {0: 6, 1: 4, 2: 78, 3: 182, 4: 116, 5: 75, 6: 521, 7: 4, 8: 4, 9: 10}, 7: {0: 27, 1: 6, 2: 48, 3: 111, 4: 68, 5: 134, 6: 4, 7: 544, 8: 12, 9: 46}, 8: {0: 128, 1: 53, 2: 17, 3: 45, 4: 3, 5: 9, 6: 5, 7: 6, 8: 653, 9: 81}, 9: {0: 35, 1: 99, 2: 7, 3: 44, 4: 9, 5: 19, 6: 3, 7: 5, 8: 22, 9: 757}}
epoch 0, train loss avg now = 0.035601, train contrast loss now = 1.567991, test acc now = 0.5866, test loss now = 2.696722
{0: {0: 702, 1: 38, 2: 44, 3: 31, 4: 4, 5: 3, 6: 14, 7: 15, 8: 88, 9: 61}, 1: {0: 38, 1: 736, 2: 9, 3: 16, 4: 6, 5: 9, 6: 22, 7: 12, 8: 16, 9: 136}, 2: {0: 122, 1: 9, 2: 451, 3: 104, 4: 62, 5: 51, 6: 108, 7: 45, 8: 26, 9: 22}, 3: {0: 37, 1: 32, 2: 98, 3: 479, 4: 36, 5: 148, 6: 91, 7: 33, 8: 11, 9: 35}, 4: {0: 65, 1: 7, 2: 129, 3: 56, 4: 435, 5: 31, 6: 145, 7: 97, 8: 17, 9: 18}, 5: {0: 26, 1: 11, 2: 85, 3: 227, 4: 50, 5: 458, 6: 52, 7: 47, 8: 21, 9: 23}, 6: {0: 9, 1: 4, 2: 57, 3: 65, 4: 21, 5: 18, 6: 801, 7: 6, 8: 5, 9: 14}, 7: {0: 36, 1: 11, 2: 48, 3: 62, 4: 49, 5: 79, 6: 26, 7: 632, 8: 7, 9: 50}, 8: {0: 140, 1: 87, 2: 11, 3: 17, 4: 4, 5: 3, 6: 11, 7: 9, 8: 643, 9: 75}, 9: {0: 46, 1: 114, 2: 6, 3: 24, 4: 5, 5: 5, 6: 17, 7: 13, 8: 27, 9: 743}}
epoch 100, train loss avg now = 0.014629, train contrast loss now = 0.409862, test acc now = 0.6080, test loss now = 2.476173
{0: {0: 684, 1: 38, 2: 45, 3: 41, 4: 11, 5: 5, 6: 13, 7: 15, 8: 94, 9: 54}, 1: {0: 35, 1: 731, 2: 7, 3: 23, 4: 11, 5: 3, 6: 22, 7: 13, 8: 21, 9: 134}, 2: {0: 121, 1: 6, 2: 391, 3: 119, 4: 121, 5: 53, 6: 88, 7: 54, 8: 24, 9: 23}, 3: {0: 34, 1: 16, 2: 80, 3: 517, 4: 59, 5: 125, 6: 104, 7: 30, 8: 11, 9: 24}, 4: {0: 59, 1: 9, 2: 81, 3: 71, 4: 519, 5: 23, 6: 97, 7: 107, 8: 14, 9: 20}, 5: {0: 18, 1: 11, 2: 59, 3: 259, 4: 69, 5: 431, 6: 50, 7: 61, 8: 19, 9: 23}, 6: {0: 10, 1: 3, 2: 48, 3: 72, 4: 56, 5: 19, 6: 774, 7: 7, 8: 4, 9: 7}, 7: {0: 34, 1: 8, 2: 47, 3: 82, 4: 50, 5: 76, 6: 23, 7: 638, 8: 3, 9: 39}, 8: {0: 135, 1: 85, 2: 9, 3: 33, 4: 4, 5: 4, 6: 11, 7: 8, 8: 637, 9: 74}, 9: {0: 41, 1: 119, 2: 6, 3: 26, 4: 9, 5: 6, 6: 16, 7: 13, 8: 20, 9: 744}}
epoch 200, train loss avg now = 0.007996, train contrast loss now = 0.403905, test acc now = 0.6066, test loss now = 2.538155
{0: {0: 682, 1: 36, 2: 46, 3: 41, 4: 13, 5: 3, 6: 11, 7: 15, 8: 89, 9: 64}, 1: {0: 30, 1: 744, 2: 4, 3: 20, 4: 9, 5: 5, 6: 17, 7: 13, 8: 27, 9: 131}, 2: {0: 117, 1: 11, 2: 409, 3: 130, 4: 110, 5: 47, 6: 83, 7: 50, 8: 18, 9: 25}, 3: {0: 31, 1: 30, 2: 67, 3: 520, 4: 54, 5: 118, 6: 89, 7: 41, 8: 14, 9: 36}, 4: {0: 62, 1: 12, 2: 97, 3: 78, 4: 514, 5: 18, 6: 98, 7: 92, 8: 11, 9: 18}, 5: {0: 20, 1: 11, 2: 62, 3: 287, 4: 63, 5: 413, 6: 50, 7: 55, 8: 14, 9: 25}, 6: {0: 7, 1: 6, 2: 51, 3: 79, 4: 42, 5: 16, 6: 773, 7: 5, 8: 7, 9: 14}, 7: {0: 35, 1: 11, 2: 31, 3: 78, 4: 58, 5: 61, 6: 19, 7: 651, 8: 10, 9: 46}, 8: {0: 130, 1: 84, 2: 6, 3: 25, 4: 6, 5: 3, 6: 10, 7: 12, 8: 634, 9: 90}, 9: {0: 39, 1: 116, 2: 5, 3: 24, 4: 6, 5: 3, 6: 13, 7: 13, 8: 22, 9: 759}}
epoch 300, train loss avg now = 0.005355, train contrast loss now = 0.402436, test acc now = 0.6099, test loss now = 2.536662
{0: {0: 682, 1: 46, 2: 47, 3: 45, 4: 6, 5: 4, 6: 20, 7: 20, 8: 69, 9: 61}, 1: {0: 19, 1: 770, 2: 6, 3: 22, 4: 7, 5: 3, 6: 26, 7: 11, 8: 13, 9: 123}, 2: {0: 112, 1: 17, 2: 423, 3: 131, 4: 94, 5: 50, 6: 97, 7: 43, 8: 14, 9: 19}, 3: {0: 24, 1: 21, 2: 74, 3: 530, 4: 54, 5: 135, 6: 99, 7: 30, 8: 9, 9: 24}, 4: {0: 55, 1: 11, 2: 104, 3: 71, 4: 503, 5: 31, 6: 107, 7: 85, 8: 14, 9: 19}, 5: {0: 15, 1: 12, 2: 62, 3: 278, 4: 60, 5: 439, 6: 48, 7: 50, 8: 11, 9: 25}, 6: {0: 4, 1: 3, 2: 52, 3: 85, 4: 37, 5: 19, 6: 782, 7: 5, 8: 3, 9: 10}, 7: {0: 28, 1: 10, 2: 36, 3: 85, 4: 70, 5: 74, 6: 28, 7: 626, 8: 4, 9: 39}, 8: {0: 145, 1: 107, 2: 9, 3: 38, 4: 1, 5: 3, 6: 12, 7: 9, 8: 587, 9: 89}, 9: {0: 34, 1: 116, 2: 5, 3: 27, 4: 6, 5: 7, 6: 16, 7: 12, 8: 19, 9: 758}}
epoch 400, train loss avg now = 0.005980, train contrast loss now = 0.401197, test acc now = 0.6100, test loss now = 2.535722
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 713, 1: 33, 2: 49, 3: 31, 4: 17, 5: 2, 6: 12, 7: 14, 8: 81, 9: 48}, 1: {0: 37, 1: 743, 2: 7, 3: 16, 4: 8, 5: 2, 6: 19, 7: 20, 8: 28, 9: 120}, 2: {0: 122, 1: 11, 2: 423, 3: 105, 4: 112, 5: 47, 6: 85, 7: 56, 8: 22, 9: 17}, 3: {0: 37, 1: 24, 2: 96, 3: 470, 4: 58, 5: 135, 6: 97, 7: 43, 8: 12, 9: 28}, 4: {0: 64, 1: 8, 2: 91, 3: 53, 4: 547, 5: 24, 6: 90, 7: 96, 8: 12, 9: 15}, 5: {0: 22, 1: 10, 2: 76, 3: 244, 4: 74, 5: 428, 6: 48, 7: 60, 8: 16, 9: 22}, 6: {0: 7, 1: 6, 2: 50, 3: 70, 4: 54, 5: 19, 6: 770, 7: 7, 8: 7, 9: 10}, 7: {0: 40, 1: 5, 2: 32, 3: 67, 4: 68, 5: 71, 6: 28, 7: 648, 8: 5, 9: 36}, 8: {0: 135, 1: 81, 2: 11, 3: 24, 4: 4, 5: 1, 6: 10, 7: 12, 8: 650, 9: 72}, 9: {0: 49, 1: 122, 2: 3, 3: 18, 4: 10, 5: 5, 6: 13, 7: 17, 8: 27, 9: 736}}
epoch 500, train loss avg now = 0.003390, train contrast loss now = 0.400588, test acc now = 0.6128, test loss now = 2.575416
{0: {0: 704, 1: 43, 2: 38, 3: 40, 4: 8, 5: 3, 6: 12, 7: 15, 8: 82, 9: 55}, 1: {0: 28, 1: 753, 2: 4, 3: 17, 4: 8, 5: 3, 6: 17, 7: 15, 8: 27, 9: 128}, 2: {0: 128, 1: 15, 2: 383, 3: 123, 4: 114, 5: 57, 6: 92, 7: 44, 8: 22, 9: 22}, 3: {0: 35, 1: 28, 2: 69, 3: 509, 4: 49, 5: 128, 6: 100, 7: 31, 8: 15, 9: 36}, 4: {0: 65, 1: 9, 2: 86, 3: 65, 4: 518, 5: 24, 6: 107, 7: 94, 8: 13, 9: 19}, 5: {0: 22, 1: 11, 2: 60, 3: 258, 4: 65, 5: 439, 6: 45, 7: 57, 8: 19, 9: 24}, 6: {0: 10, 1: 5, 2: 47, 3: 69, 4: 51, 5: 21, 6: 775, 7: 5, 8: 5, 9: 12}, 7: {0: 37, 1: 8, 2: 25, 3: 70, 4: 61, 5: 76, 6: 24, 7: 646, 8: 6, 9: 47}, 8: {0: 134, 1: 89, 2: 6, 3: 22, 4: 3, 5: 3, 6: 10, 7: 9, 8: 648, 9: 76}, 9: {0: 40, 1: 120, 2: 1, 3: 18, 4: 8, 5: 3, 6: 13, 7: 12, 8: 24, 9: 761}}
epoch 600, train loss avg now = 0.001941, train contrast loss now = 0.400067, test acc now = 0.6136, test loss now = 2.568890
{0: {0: 696, 1: 44, 2: 40, 3: 42, 4: 8, 5: 5, 6: 10, 7: 16, 8: 79, 9: 60}, 1: {0: 29, 1: 754, 2: 3, 3: 19, 4: 7, 5: 3, 6: 19, 7: 14, 8: 24, 9: 128}, 2: {0: 128, 1: 12, 2: 389, 3: 121, 4: 107, 5: 64, 6: 91, 7: 46, 8: 22, 9: 20}, 3: {0: 32, 1: 24, 2: 70, 3: 517, 4: 51, 5: 137, 6: 90, 7: 33, 8: 14, 9: 32}, 4: {0: 64, 1: 9, 2: 88, 3: 67, 4: 516, 5: 29, 6: 102, 7: 93, 8: 12, 9: 20}, 5: {0: 22, 1: 10, 2: 57, 3: 259, 4: 62, 5: 451, 6: 40, 7: 56, 8: 18, 9: 25}, 6: {0: 9, 1: 4, 2: 46, 3: 76, 4: 48, 5: 26, 6: 768, 7: 5, 8: 5, 9: 13}, 7: {0: 36, 1: 9, 2: 27, 3: 70, 4: 57, 5: 76, 6: 25, 7: 649, 8: 5, 9: 46}, 8: {0: 135, 1: 87, 2: 7, 3: 26, 4: 3, 5: 3, 6: 9, 7: 9, 8: 641, 9: 80}, 9: {0: 38, 1: 121, 2: 2, 3: 22, 4: 8, 5: 4, 6: 12, 7: 12, 8: 20, 9: 761}}
epoch 700, train loss avg now = 0.001818, train contrast loss now = 0.400525, test acc now = 0.6142, test loss now = 2.541260
{0: {0: 709, 1: 46, 2: 38, 3: 41, 4: 8, 5: 4, 6: 11, 7: 16, 8: 68, 9: 59}, 1: {0: 29, 1: 765, 2: 3, 3: 19, 4: 6, 5: 4, 6: 20, 7: 15, 8: 19, 9: 120}, 2: {0: 125, 1: 15, 2: 388, 3: 125, 4: 111, 5: 63, 6: 96, 7: 41, 8: 16, 9: 20}, 3: {0: 33, 1: 28, 2: 66, 3: 517, 4: 46, 5: 139, 6: 94, 7: 35, 8: 10, 9: 32}, 4: {0: 64, 1: 10, 2: 80, 3: 69, 4: 507, 5: 30, 6: 114, 7: 93, 8: 13, 9: 20}, 5: {0: 20, 1: 12, 2: 59, 3: 262, 4: 58, 5: 457, 6: 43, 7: 49, 8: 16, 9: 24}, 6: {0: 10, 1: 4, 2: 44, 3: 77, 4: 38, 5: 24, 6: 784, 7: 5, 8: 4, 9: 10}, 7: {0: 37, 1: 10, 2: 25, 3: 78, 4: 50, 5: 79, 6: 23, 7: 646, 8: 5, 9: 47}, 8: {0: 140, 1: 93, 2: 6, 3: 25, 4: 2, 5: 3, 6: 11, 7: 10, 8: 633, 9: 77}, 9: {0: 36, 1: 128, 2: 2, 3: 23, 4: 7, 5: 5, 6: 14, 7: 12, 8: 21, 9: 752}}
epoch 800, train loss avg now = 0.002085, train contrast loss now = 0.400623, test acc now = 0.6158, test loss now = 2.607850
{0: {0: 698, 1: 45, 2: 40, 3: 43, 4: 7, 5: 3, 6: 14, 7: 16, 8: 74, 9: 60}, 1: {0: 28, 1: 757, 2: 4, 3: 19, 4: 7, 5: 4, 6: 20, 7: 14, 8: 16, 9: 131}, 2: {0: 122, 1: 12, 2: 392, 3: 124, 4: 107, 5: 63, 6: 99, 7: 42, 8: 18, 9: 21}, 3: {0: 30, 1: 28, 2: 72, 3: 520, 4: 48, 5: 136, 6: 94, 7: 31, 8: 10, 9: 31}, 4: {0: 61, 1: 11, 2: 86, 3: 66, 4: 505, 5: 30, 6: 112, 7: 96, 8: 14, 9: 19}, 5: {0: 19, 1: 12, 2: 57, 3: 266, 4: 60, 5: 450, 6: 43, 7: 54, 8: 15, 9: 24}, 6: {0: 9, 1: 4, 2: 44, 3: 78, 4: 37, 5: 20, 6: 788, 7: 6, 8: 5, 9: 9}, 7: {0: 37, 1: 10, 2: 27, 3: 77, 4: 54, 5: 77, 6: 24, 7: 645, 8: 5, 9: 44}, 8: {0: 139, 1: 92, 2: 8, 3: 26, 4: 2, 5: 3, 6: 11, 7: 10, 8: 627, 9: 82}, 9: {0: 35, 1: 120, 2: 2, 3: 22, 4: 7, 5: 5, 6: 14, 7: 12, 8: 20, 9: 763}}
epoch 900, train loss avg now = 0.002405, train contrast loss now = 0.400625, test acc now = 0.6145, test loss now = 2.575537
{0: {0: 691, 1: 45, 2: 40, 3: 43, 4: 8, 5: 3, 6: 14, 7: 14, 8: 83, 9: 59}, 1: {0: 29, 1: 755, 2: 4, 3: 19, 4: 8, 5: 4, 6: 21, 7: 15, 8: 23, 9: 122}, 2: {0: 119, 1: 13, 2: 399, 3: 119, 4: 110, 5: 56, 6: 103, 7: 41, 8: 20, 9: 20}, 3: {0: 32, 1: 26, 2: 74, 3: 507, 4: 51, 5: 137, 6: 102, 7: 28, 8: 12, 9: 31}, 4: {0: 61, 1: 10, 2: 82, 3: 63, 4: 514, 5: 24, 6: 119, 7: 95, 8: 15, 9: 17}, 5: {0: 20, 1: 11, 2: 58, 3: 258, 4: 60, 5: 452, 6: 47, 7: 52, 8: 17, 9: 25}, 6: {0: 8, 1: 5, 2: 45, 3: 74, 4: 39, 5: 19, 6: 793, 7: 4, 8: 4, 9: 9}, 7: {0: 35, 1: 8, 2: 28, 3: 70, 4: 54, 5: 73, 6: 33, 7: 648, 8: 5, 9: 46}, 8: {0: 130, 1: 92, 2: 8, 3: 25, 4: 2, 5: 3, 6: 12, 7: 10, 8: 643, 9: 75}, 9: {0: 36, 1: 119, 2: 2, 3: 22, 4: 7, 5: 5, 6: 14, 7: 12, 8: 24, 9: 759}}
epoch 1000, train loss avg now = 0.001433, train contrast loss now = 0.400287, test acc now = 0.6161, test loss now = 2.602673
epoch avg loss = 1.433099063275301e-06, total time = 9204.642660140991
total 24576.0MB, used 3207.06MB, free 21368.94MB
Round 7 finish, update the prev_syn_proto
torch.Size([808, 3, 32, 32])
torch.Size([808, 3, 32, 32])
torch.Size([832, 3, 32, 32])
torch.Size([800, 3, 32, 32])
torch.Size([800, 3, 32, 32])
torch.Size([840, 3, 32, 32])
torch.Size([808, 3, 32, 32])
torch.Size([800, 3, 32, 32])
torch.Size([800, 3, 32, 32])
torch.Size([800, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 691, 1: 45, 2: 40, 3: 43, 4: 8, 5: 3, 6: 14, 7: 14, 8: 83, 9: 59}, 1: {0: 29, 1: 755, 2: 4, 3: 19, 4: 8, 5: 4, 6: 21, 7: 15, 8: 23, 9: 122}, 2: {0: 119, 1: 13, 2: 399, 3: 119, 4: 110, 5: 56, 6: 103, 7: 41, 8: 20, 9: 20}, 3: {0: 32, 1: 26, 2: 74, 3: 507, 4: 51, 5: 137, 6: 102, 7: 28, 8: 12, 9: 31}, 4: {0: 61, 1: 10, 2: 82, 3: 63, 4: 514, 5: 24, 6: 119, 7: 95, 8: 15, 9: 17}, 5: {0: 20, 1: 11, 2: 58, 3: 258, 4: 60, 5: 452, 6: 47, 7: 52, 8: 17, 9: 25}, 6: {0: 8, 1: 5, 2: 45, 3: 74, 4: 39, 5: 19, 6: 793, 7: 4, 8: 4, 9: 9}, 7: {0: 35, 1: 8, 2: 28, 3: 70, 4: 54, 5: 73, 6: 33, 7: 648, 8: 5, 9: 46}, 8: {0: 130, 1: 92, 2: 8, 3: 25, 4: 2, 5: 3, 6: 12, 7: 10, 8: 643, 9: 75}, 9: {0: 36, 1: 119, 2: 2, 3: 22, 4: 7, 5: 5, 6: 14, 7: 12, 8: 24, 9: 759}}
round 7 evaluation: test acc is 0.6161, test loss = 2.602673
 ====== round 8 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3207.06MB, free 21368.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1210  137  103   95   62   51   81   93  175 1586], bin edged: [0.00019461 0.00020971 0.00022482 0.00023992 0.00025503 0.00027013
 0.00028524 0.00030034 0.00031544 0.00033055 0.00034565]
class 7 have 4999 samples, histogram: [2872  163  107   89   74   78  102   94  145 1275], bin edged: [0.00015833 0.00017062 0.00018291 0.0001952  0.00020749 0.00021977
 0.00023206 0.00024435 0.00025664 0.00026893 0.00028122]
client 0, data condensation 0, total loss = 43.0994873046875, avg loss = 21.54974365234375
client 0, data condensation 200, total loss = 8.40533447265625, avg loss = 4.202667236328125
client 0, data condensation 400, total loss = 4.386474609375, avg loss = 2.1932373046875
client 0, data condensation 600, total loss = 12.00006103515625, avg loss = 6.000030517578125
client 0, data condensation 800, total loss = 13.82220458984375, avg loss = 6.911102294921875
client 0, data condensation 1000, total loss = 11.16741943359375, avg loss = 5.583709716796875
client 0, data condensation 1200, total loss = 4.19580078125, avg loss = 2.097900390625
client 0, data condensation 1400, total loss = 13.0816650390625, avg loss = 6.54083251953125
client 0, data condensation 1600, total loss = 58.436767578125, avg loss = 29.2183837890625
client 0, data condensation 1800, total loss = 7.0548095703125, avg loss = 3.52740478515625
client 0, data condensation 2000, total loss = 6.7093505859375, avg loss = 3.35467529296875
client 0, data condensation 2200, total loss = 21.75775146484375, avg loss = 10.878875732421875
client 0, data condensation 2400, total loss = 5.5087890625, avg loss = 2.75439453125
client 0, data condensation 2600, total loss = 25.99005126953125, avg loss = 12.995025634765625
client 0, data condensation 2800, total loss = 7.1484375, avg loss = 3.57421875
client 0, data condensation 3000, total loss = 10.27142333984375, avg loss = 5.135711669921875
client 0, data condensation 3200, total loss = 28.4337158203125, avg loss = 14.21685791015625
client 0, data condensation 3400, total loss = 6.98052978515625, avg loss = 3.490264892578125
client 0, data condensation 3600, total loss = 8.635986328125, avg loss = 4.3179931640625
client 0, data condensation 3800, total loss = 72.99264526367188, avg loss = 36.49632263183594
client 0, data condensation 4000, total loss = 15.937255859375, avg loss = 7.9686279296875
client 0, data condensation 4200, total loss = 6.3941650390625, avg loss = 3.19708251953125
client 0, data condensation 4400, total loss = 6.080322265625, avg loss = 3.0401611328125
client 0, data condensation 4600, total loss = 9.16876220703125, avg loss = 4.584381103515625
client 0, data condensation 4800, total loss = 12.76739501953125, avg loss = 6.383697509765625
client 0, data condensation 5000, total loss = 4.96978759765625, avg loss = 2.484893798828125
client 0, data condensation 5200, total loss = 7.41845703125, avg loss = 3.709228515625
client 0, data condensation 5400, total loss = 5.71728515625, avg loss = 2.858642578125
client 0, data condensation 5600, total loss = 9.639892578125, avg loss = 4.8199462890625
client 0, data condensation 5800, total loss = 5.2855224609375, avg loss = 2.64276123046875
client 0, data condensation 6000, total loss = 7.1400146484375, avg loss = 3.57000732421875
client 0, data condensation 6200, total loss = 12.5169677734375, avg loss = 6.25848388671875
client 0, data condensation 6400, total loss = 13.4658203125, avg loss = 6.73291015625
client 0, data condensation 6600, total loss = 7.64178466796875, avg loss = 3.820892333984375
client 0, data condensation 6800, total loss = 8.48309326171875, avg loss = 4.241546630859375
client 0, data condensation 7000, total loss = 7.68182373046875, avg loss = 3.840911865234375
client 0, data condensation 7200, total loss = 8.27789306640625, avg loss = 4.138946533203125
client 0, data condensation 7400, total loss = 14.523681640625, avg loss = 7.2618408203125
client 0, data condensation 7600, total loss = 7.70892333984375, avg loss = 3.854461669921875
client 0, data condensation 7800, total loss = 18.50909423828125, avg loss = 9.254547119140625
client 0, data condensation 8000, total loss = 7.15496826171875, avg loss = 3.577484130859375
client 0, data condensation 8200, total loss = 6.61370849609375, avg loss = 3.306854248046875
client 0, data condensation 8400, total loss = 8.31475830078125, avg loss = 4.157379150390625
client 0, data condensation 8600, total loss = 11.79791259765625, avg loss = 5.898956298828125
client 0, data condensation 8800, total loss = 9.8106689453125, avg loss = 4.90533447265625
client 0, data condensation 9000, total loss = 31.80364990234375, avg loss = 15.901824951171875
client 0, data condensation 9200, total loss = 6.5596923828125, avg loss = 3.27984619140625
client 0, data condensation 9400, total loss = 9.92578125, avg loss = 4.962890625
client 0, data condensation 9600, total loss = 4.29425048828125, avg loss = 2.147125244140625
client 0, data condensation 9800, total loss = 8.0755615234375, avg loss = 4.03778076171875
client 0, data condensation 10000, total loss = 9.8648681640625, avg loss = 4.93243408203125
Round 8, client 0 condense time: 746.454628944397
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [54  5  6  7  6  2  7  8  7 73], bin edged: [0.00398443 0.00429368 0.00460293 0.00491217 0.00522142 0.00553067
 0.00583992 0.00614916 0.00645841 0.00676766 0.00707691]
class 4 have 4958 samples, histogram: [1904  235  161  129  100  108  126  132  226 1837], bin edged: [0.0001459  0.00015723 0.00016855 0.00017988 0.0001912  0.00020252
 0.00021385 0.00022517 0.0002365  0.00024782 0.00025914]
client 1, data condensation 0, total loss = 193.71295166015625, avg loss = 96.85647583007812
client 1, data condensation 200, total loss = 270.6197509765625, avg loss = 135.30987548828125
client 1, data condensation 400, total loss = 105.50872802734375, avg loss = 52.754364013671875
client 1, data condensation 600, total loss = 2813.11767578125, avg loss = 1406.558837890625
client 1, data condensation 800, total loss = 1149.603759765625, avg loss = 574.8018798828125
client 1, data condensation 1000, total loss = 304.74591064453125, avg loss = 152.37295532226562
client 1, data condensation 1200, total loss = 347.29931640625, avg loss = 173.649658203125
client 1, data condensation 1400, total loss = 169.24371337890625, avg loss = 84.62185668945312
client 1, data condensation 1600, total loss = 220.4649658203125, avg loss = 110.23248291015625
client 1, data condensation 1800, total loss = 1122.65478515625, avg loss = 561.327392578125
client 1, data condensation 2000, total loss = 107.39599609375, avg loss = 53.697998046875
client 1, data condensation 2200, total loss = 113.783203125, avg loss = 56.8916015625
client 1, data condensation 2400, total loss = 2970.1748046875, avg loss = 1485.08740234375
client 1, data condensation 2600, total loss = 1803.37939453125, avg loss = 901.689697265625
client 1, data condensation 2800, total loss = 870.1453247070312, avg loss = 435.0726623535156
client 1, data condensation 3000, total loss = 175.9725341796875, avg loss = 87.98626708984375
client 1, data condensation 3200, total loss = 437.25506591796875, avg loss = 218.62753295898438
client 1, data condensation 3400, total loss = 283.30120849609375, avg loss = 141.65060424804688
client 1, data condensation 3600, total loss = 90.3021240234375, avg loss = 45.15106201171875
client 1, data condensation 3800, total loss = 249.6314697265625, avg loss = 124.81573486328125
client 1, data condensation 4000, total loss = 165.69024658203125, avg loss = 82.84512329101562
client 1, data condensation 4200, total loss = 661.105712890625, avg loss = 330.5528564453125
client 1, data condensation 4400, total loss = 147.205810546875, avg loss = 73.6029052734375
client 1, data condensation 4600, total loss = 120.61712646484375, avg loss = 60.308563232421875
client 1, data condensation 4800, total loss = 2073.12841796875, avg loss = 1036.564208984375
client 1, data condensation 5000, total loss = 366.0980224609375, avg loss = 183.04901123046875
client 1, data condensation 5200, total loss = 444.2890625, avg loss = 222.14453125
client 1, data condensation 5400, total loss = 101.3975830078125, avg loss = 50.69879150390625
client 1, data condensation 5600, total loss = 234.8955078125, avg loss = 117.44775390625
client 1, data condensation 5800, total loss = 1377.35595703125, avg loss = 688.677978515625
client 1, data condensation 6000, total loss = 153.7598876953125, avg loss = 76.87994384765625
client 1, data condensation 6200, total loss = 117.0263671875, avg loss = 58.51318359375
client 1, data condensation 6400, total loss = 96.11968994140625, avg loss = 48.059844970703125
client 1, data condensation 6600, total loss = 126.188232421875, avg loss = 63.0941162109375
client 1, data condensation 6800, total loss = 608.2154541015625, avg loss = 304.10772705078125
client 1, data condensation 7000, total loss = 100.45379638671875, avg loss = 50.226898193359375
client 1, data condensation 7200, total loss = 194.53955078125, avg loss = 97.269775390625
client 1, data condensation 7400, total loss = 637.75830078125, avg loss = 318.879150390625
client 1, data condensation 7600, total loss = 241.87017822265625, avg loss = 120.93508911132812
client 1, data condensation 7800, total loss = 205.087646484375, avg loss = 102.5438232421875
client 1, data condensation 8000, total loss = 166.1090087890625, avg loss = 83.05450439453125
client 1, data condensation 8200, total loss = 212.2451171875, avg loss = 106.12255859375
client 1, data condensation 8400, total loss = 154.47723388671875, avg loss = 77.23861694335938
client 1, data condensation 8600, total loss = 1722.156005859375, avg loss = 861.0780029296875
client 1, data condensation 8800, total loss = 815.0791625976562, avg loss = 407.5395812988281
client 1, data condensation 9000, total loss = 158.9925537109375, avg loss = 79.49627685546875
client 1, data condensation 9200, total loss = 110.1712646484375, avg loss = 55.08563232421875
client 1, data condensation 9400, total loss = 120.64306640625, avg loss = 60.321533203125
client 1, data condensation 9600, total loss = 138.36322021484375, avg loss = 69.18161010742188
client 1, data condensation 9800, total loss = 202.249755859375, avg loss = 101.1248779296875
client 1, data condensation 10000, total loss = 1191.69775390625, avg loss = 595.848876953125
Round 8, client 1 condense time: 561.1643798351288
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2951.06MB, free 21624.94MB
total 24576.0MB, used 2951.06MB, free 21624.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [152   9   5   4   3   2   3   4  11  49], bin edged: [0.00337381 0.00363566 0.00389752 0.00415937 0.00442123 0.00468308
 0.00494494 0.00520679 0.00546865 0.0057305  0.00599236]
client 2, data condensation 0, total loss = 300.88006591796875, avg loss = 300.88006591796875
client 2, data condensation 200, total loss = 142.56781005859375, avg loss = 142.56781005859375
client 2, data condensation 400, total loss = 184.81243896484375, avg loss = 184.81243896484375
client 2, data condensation 600, total loss = 222.78118896484375, avg loss = 222.78118896484375
client 2, data condensation 800, total loss = 251.4581298828125, avg loss = 251.4581298828125
client 2, data condensation 1000, total loss = 617.43017578125, avg loss = 617.43017578125
client 2, data condensation 1200, total loss = 267.47601318359375, avg loss = 267.47601318359375
client 2, data condensation 1400, total loss = 5305.27587890625, avg loss = 5305.27587890625
client 2, data condensation 1600, total loss = 57.29376220703125, avg loss = 57.29376220703125
client 2, data condensation 1800, total loss = 583.8050537109375, avg loss = 583.8050537109375
client 2, data condensation 2000, total loss = 971.09228515625, avg loss = 971.09228515625
client 2, data condensation 2200, total loss = 226.128662109375, avg loss = 226.128662109375
client 2, data condensation 2400, total loss = 881.208984375, avg loss = 881.208984375
client 2, data condensation 2600, total loss = 124.4617919921875, avg loss = 124.4617919921875
client 2, data condensation 2800, total loss = 76.18194580078125, avg loss = 76.18194580078125
client 2, data condensation 3000, total loss = 77.76416015625, avg loss = 77.76416015625
client 2, data condensation 3200, total loss = 98.6009521484375, avg loss = 98.6009521484375
client 2, data condensation 3400, total loss = 456.443359375, avg loss = 456.443359375
client 2, data condensation 3600, total loss = 217.41558837890625, avg loss = 217.41558837890625
client 2, data condensation 3800, total loss = 118.2923583984375, avg loss = 118.2923583984375
client 2, data condensation 4000, total loss = 100.4857177734375, avg loss = 100.4857177734375
client 2, data condensation 4200, total loss = 79.30084228515625, avg loss = 79.30084228515625
client 2, data condensation 4400, total loss = 248.38006591796875, avg loss = 248.38006591796875
client 2, data condensation 4600, total loss = 57.6895751953125, avg loss = 57.6895751953125
client 2, data condensation 4800, total loss = 106.9630126953125, avg loss = 106.9630126953125
client 2, data condensation 5000, total loss = 138.006103515625, avg loss = 138.006103515625
client 2, data condensation 5200, total loss = 70.47869873046875, avg loss = 70.47869873046875
client 2, data condensation 5400, total loss = 292.3646240234375, avg loss = 292.3646240234375
client 2, data condensation 5600, total loss = 1511.259033203125, avg loss = 1511.259033203125
client 2, data condensation 5800, total loss = 63.22430419921875, avg loss = 63.22430419921875
client 2, data condensation 6000, total loss = 110.03387451171875, avg loss = 110.03387451171875
client 2, data condensation 6200, total loss = 166.34130859375, avg loss = 166.34130859375
client 2, data condensation 6400, total loss = 45.04498291015625, avg loss = 45.04498291015625
client 2, data condensation 6600, total loss = 210.08349609375, avg loss = 210.08349609375
client 2, data condensation 6800, total loss = 473.503173828125, avg loss = 473.503173828125
client 2, data condensation 7000, total loss = 105.14434814453125, avg loss = 105.14434814453125
client 2, data condensation 7200, total loss = 947.7701416015625, avg loss = 947.7701416015625
client 2, data condensation 7400, total loss = 154.19024658203125, avg loss = 154.19024658203125
client 2, data condensation 7600, total loss = 104.05487060546875, avg loss = 104.05487060546875
client 2, data condensation 7800, total loss = 98.0357666015625, avg loss = 98.0357666015625
client 2, data condensation 8000, total loss = 135.8988037109375, avg loss = 135.8988037109375
client 2, data condensation 8200, total loss = 78.07135009765625, avg loss = 78.07135009765625
client 2, data condensation 8400, total loss = 244.1199951171875, avg loss = 244.1199951171875
client 2, data condensation 8600, total loss = 149.2374267578125, avg loss = 149.2374267578125
client 2, data condensation 8800, total loss = 106.1552734375, avg loss = 106.1552734375
client 2, data condensation 9000, total loss = 184.43182373046875, avg loss = 184.43182373046875
client 2, data condensation 9200, total loss = 181.09912109375, avg loss = 181.09912109375
client 2, data condensation 9400, total loss = 63.265380859375, avg loss = 63.265380859375
client 2, data condensation 9600, total loss = 982.302978515625, avg loss = 982.302978515625
client 2, data condensation 9800, total loss = 130.25579833984375, avg loss = 130.25579833984375
client 2, data condensation 10000, total loss = 703.1396484375, avg loss = 703.1396484375
Round 8, client 2 condense time: 270.55962920188904
client 2, class 9 have 242 samples
total 24576.0MB, used 2565.06MB, free 22010.94MB
total 24576.0MB, used 2565.06MB, free 22010.94MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [515  25  14  11  11  18  16  22  25 190], bin edged: [0.00094928 0.00102295 0.00109663 0.00117031 0.00124399 0.00131766
 0.00139134 0.00146502 0.0015387  0.00161237 0.00168605]
class 2 have 1094 samples, histogram: [309  53  31  35  29  16  29  28  51 513], bin edged: [0.0006273  0.00067599 0.00072468 0.00077337 0.00082205 0.00087074
 0.00091943 0.00096812 0.0010168  0.00106549 0.00111418]
client 3, data condensation 0, total loss = 63.8475341796875, avg loss = 31.92376708984375
client 3, data condensation 200, total loss = 40.922119140625, avg loss = 20.4610595703125
client 3, data condensation 400, total loss = 23.40728759765625, avg loss = 11.703643798828125
client 3, data condensation 600, total loss = 7.38311767578125, avg loss = 3.691558837890625
client 3, data condensation 800, total loss = 30.493896484375, avg loss = 15.2469482421875
client 3, data condensation 1000, total loss = 24.3182373046875, avg loss = 12.15911865234375
client 3, data condensation 1200, total loss = 18.26495361328125, avg loss = 9.132476806640625
client 3, data condensation 1400, total loss = 21.45391845703125, avg loss = 10.726959228515625
client 3, data condensation 1600, total loss = 6.6943359375, avg loss = 3.34716796875
client 3, data condensation 1800, total loss = 12.7672119140625, avg loss = 6.38360595703125
client 3, data condensation 2000, total loss = 15.6593017578125, avg loss = 7.82965087890625
client 3, data condensation 2200, total loss = 27.32672119140625, avg loss = 13.663360595703125
client 3, data condensation 2400, total loss = 17.2476806640625, avg loss = 8.62384033203125
client 3, data condensation 2600, total loss = 14.68359375, avg loss = 7.341796875
client 3, data condensation 2800, total loss = 20.5015869140625, avg loss = 10.25079345703125
client 3, data condensation 3000, total loss = 20.07080078125, avg loss = 10.035400390625
client 3, data condensation 3200, total loss = 17.0360107421875, avg loss = 8.51800537109375
client 3, data condensation 3400, total loss = 33.7022705078125, avg loss = 16.85113525390625
client 3, data condensation 3600, total loss = 76.588134765625, avg loss = 38.2940673828125
client 3, data condensation 3800, total loss = 14.6209716796875, avg loss = 7.31048583984375
client 3, data condensation 4000, total loss = 6.30450439453125, avg loss = 3.152252197265625
client 3, data condensation 4200, total loss = 10.89471435546875, avg loss = 5.447357177734375
client 3, data condensation 4400, total loss = 32.87786865234375, avg loss = 16.438934326171875
client 3, data condensation 4600, total loss = 9.070068359375, avg loss = 4.5350341796875
client 3, data condensation 4800, total loss = 18.603271484375, avg loss = 9.3016357421875
client 3, data condensation 5000, total loss = 19.96124267578125, avg loss = 9.980621337890625
client 3, data condensation 5200, total loss = 14.8592529296875, avg loss = 7.42962646484375
client 3, data condensation 5400, total loss = 26.44073486328125, avg loss = 13.220367431640625
client 3, data condensation 5600, total loss = 33.05029296875, avg loss = 16.525146484375
client 3, data condensation 5800, total loss = 24.48394775390625, avg loss = 12.241973876953125
client 3, data condensation 6000, total loss = 8.71893310546875, avg loss = 4.359466552734375
client 3, data condensation 6200, total loss = 10.69586181640625, avg loss = 5.347930908203125
client 3, data condensation 6400, total loss = 22.13861083984375, avg loss = 11.069305419921875
client 3, data condensation 6600, total loss = 207.28173828125, avg loss = 103.640869140625
client 3, data condensation 6800, total loss = 13.46441650390625, avg loss = 6.732208251953125
client 3, data condensation 7000, total loss = 65.54888916015625, avg loss = 32.774444580078125
client 3, data condensation 7200, total loss = 18.58770751953125, avg loss = 9.293853759765625
client 3, data condensation 7400, total loss = 8.24847412109375, avg loss = 4.124237060546875
client 3, data condensation 7600, total loss = 16.56671142578125, avg loss = 8.283355712890625
client 3, data condensation 7800, total loss = 17.88873291015625, avg loss = 8.944366455078125
client 3, data condensation 8000, total loss = 13.668701171875, avg loss = 6.8343505859375
client 3, data condensation 8200, total loss = 48.328125, avg loss = 24.1640625
client 3, data condensation 8400, total loss = 52.223724365234375, avg loss = 26.111862182617188
client 3, data condensation 8600, total loss = 17.1663818359375, avg loss = 8.58319091796875
client 3, data condensation 8800, total loss = 13.34332275390625, avg loss = 6.671661376953125
client 3, data condensation 9000, total loss = 36.85565185546875, avg loss = 18.427825927734375
client 3, data condensation 9200, total loss = 128.31640625, avg loss = 64.158203125
client 3, data condensation 9400, total loss = 18.42315673828125, avg loss = 9.211578369140625
client 3, data condensation 9600, total loss = 17.09075927734375, avg loss = 8.545379638671875
client 3, data condensation 9800, total loss = 8.5006103515625, avg loss = 4.25030517578125
client 3, data condensation 10000, total loss = 23.935791015625, avg loss = 11.9678955078125
Round 8, client 3 condense time: 637.5627496242523
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 2947.06MB, free 21628.94MB
total 24576.0MB, used 2947.06MB, free 21628.94MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2519  156   78   75   83   71   63   68   98  941], bin edged: [0.00019469 0.0002098  0.00022491 0.00024002 0.00025513 0.00027024
 0.00028536 0.00030047 0.00031558 0.00033069 0.0003458 ]
class 5 have 307 samples, histogram: [110  21  13  11   6   7   4   8   9 118], bin edged: [0.00235719 0.00254014 0.0027231  0.00290605 0.003089   0.00327195
 0.0034549  0.00363785 0.0038208  0.00400375 0.0041867 ]
client 4, data condensation 0, total loss = 90.73028564453125, avg loss = 45.365142822265625
client 4, data condensation 200, total loss = 1620.6905517578125, avg loss = 810.3452758789062
client 4, data condensation 400, total loss = 86.527099609375, avg loss = 43.2635498046875
client 4, data condensation 600, total loss = 214.2071533203125, avg loss = 107.10357666015625
client 4, data condensation 800, total loss = 177.6414794921875, avg loss = 88.82073974609375
client 4, data condensation 1000, total loss = 163.5069580078125, avg loss = 81.75347900390625
client 4, data condensation 1200, total loss = 163.9425048828125, avg loss = 81.97125244140625
client 4, data condensation 1400, total loss = 889.2388305664062, avg loss = 444.6194152832031
client 4, data condensation 1600, total loss = 137.484130859375, avg loss = 68.7420654296875
client 4, data condensation 1800, total loss = 76.889404296875, avg loss = 38.4447021484375
client 4, data condensation 2000, total loss = 25.949462890625, avg loss = 12.9747314453125
client 4, data condensation 2200, total loss = 116.0811767578125, avg loss = 58.04058837890625
client 4, data condensation 2400, total loss = 76.0289306640625, avg loss = 38.01446533203125
client 4, data condensation 2600, total loss = 67.92510986328125, avg loss = 33.962554931640625
client 4, data condensation 2800, total loss = 151.88671875, avg loss = 75.943359375
client 4, data condensation 3000, total loss = 115.99359130859375, avg loss = 57.996795654296875
client 4, data condensation 3200, total loss = 70.0400390625, avg loss = 35.02001953125
client 4, data condensation 3400, total loss = 795.3843994140625, avg loss = 397.69219970703125
client 4, data condensation 3600, total loss = 172.1702880859375, avg loss = 86.08514404296875
client 4, data condensation 3800, total loss = 202.70355224609375, avg loss = 101.35177612304688
client 4, data condensation 4000, total loss = 77.71038818359375, avg loss = 38.855194091796875
client 4, data condensation 4200, total loss = 116.19866943359375, avg loss = 58.099334716796875
client 4, data condensation 4400, total loss = 81.0966796875, avg loss = 40.54833984375
client 4, data condensation 4600, total loss = 104.93280029296875, avg loss = 52.466400146484375
client 4, data condensation 4800, total loss = 107.91650390625, avg loss = 53.958251953125
client 4, data condensation 5000, total loss = 125.73358154296875, avg loss = 62.866790771484375
client 4, data condensation 5200, total loss = 162.0234375, avg loss = 81.01171875
client 4, data condensation 5400, total loss = 199.01312255859375, avg loss = 99.50656127929688
client 4, data condensation 5600, total loss = 71.70648193359375, avg loss = 35.853240966796875
client 4, data condensation 5800, total loss = 92.74444580078125, avg loss = 46.372222900390625
client 4, data condensation 6000, total loss = 10414.2041015625, avg loss = 5207.10205078125
client 4, data condensation 6200, total loss = 114.57940673828125, avg loss = 57.289703369140625
client 4, data condensation 6400, total loss = 69.01959228515625, avg loss = 34.509796142578125
client 4, data condensation 6600, total loss = 126.094970703125, avg loss = 63.0474853515625
client 4, data condensation 6800, total loss = 147.5440673828125, avg loss = 73.77203369140625
client 4, data condensation 7000, total loss = 125.9586181640625, avg loss = 62.97930908203125
client 4, data condensation 7200, total loss = 76.0657958984375, avg loss = 38.03289794921875
client 4, data condensation 7400, total loss = 581.0199584960938, avg loss = 290.5099792480469
client 4, data condensation 7600, total loss = 71.82000732421875, avg loss = 35.910003662109375
client 4, data condensation 7800, total loss = 66.71038818359375, avg loss = 33.355194091796875
client 4, data condensation 8000, total loss = 415.57220458984375, avg loss = 207.78610229492188
client 4, data condensation 8200, total loss = 160.56134033203125, avg loss = 80.28067016601562
client 4, data condensation 8400, total loss = 1950.623779296875, avg loss = 975.3118896484375
client 4, data condensation 8600, total loss = 63.932861328125, avg loss = 31.9664306640625
client 4, data condensation 8800, total loss = 1306.0858154296875, avg loss = 653.0429077148438
client 4, data condensation 9000, total loss = 748.53271484375, avg loss = 374.266357421875
client 4, data condensation 9200, total loss = 130.386474609375, avg loss = 65.1932373046875
client 4, data condensation 9400, total loss = 104.96856689453125, avg loss = 52.484283447265625
client 4, data condensation 9600, total loss = 73.638671875, avg loss = 36.8193359375
client 4, data condensation 9800, total loss = 122.24017333984375, avg loss = 61.120086669921875
client 4, data condensation 10000, total loss = 103.94061279296875, avg loss = 51.970306396484375
Round 8, client 4 condense time: 736.47185754776
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [2494  187  136  100  101   84   91   98  166 1542], bin edged: [0.00015244 0.00016427 0.00017611 0.00018794 0.00019977 0.0002116
 0.00022343 0.00023526 0.0002471  0.00025893 0.00027076]
client 5, data condensation 0, total loss = 56.8446044921875, avg loss = 56.8446044921875
client 5, data condensation 200, total loss = 5.9481201171875, avg loss = 5.9481201171875
client 5, data condensation 400, total loss = 6.038818359375, avg loss = 6.038818359375
client 5, data condensation 600, total loss = 5.2381591796875, avg loss = 5.2381591796875
client 5, data condensation 800, total loss = 5.88104248046875, avg loss = 5.88104248046875
client 5, data condensation 1000, total loss = 8.91680908203125, avg loss = 8.91680908203125
client 5, data condensation 1200, total loss = 7.33648681640625, avg loss = 7.33648681640625
client 5, data condensation 1400, total loss = 6.1739501953125, avg loss = 6.1739501953125
client 5, data condensation 1600, total loss = 2.7998046875, avg loss = 2.7998046875
client 5, data condensation 1800, total loss = 3.5986328125, avg loss = 3.5986328125
client 5, data condensation 2000, total loss = 6.30908203125, avg loss = 6.30908203125
client 5, data condensation 2200, total loss = 6.71026611328125, avg loss = 6.71026611328125
client 5, data condensation 2400, total loss = 3.06719970703125, avg loss = 3.06719970703125
client 5, data condensation 2600, total loss = 10.50909423828125, avg loss = 10.50909423828125
client 5, data condensation 2800, total loss = 8.579833984375, avg loss = 8.579833984375
client 5, data condensation 3000, total loss = 4.24627685546875, avg loss = 4.24627685546875
client 5, data condensation 3200, total loss = 6.26226806640625, avg loss = 6.26226806640625
client 5, data condensation 3400, total loss = 11.9891357421875, avg loss = 11.9891357421875
client 5, data condensation 3600, total loss = 8.8734130859375, avg loss = 8.8734130859375
client 5, data condensation 3800, total loss = 3.4464111328125, avg loss = 3.4464111328125
client 5, data condensation 4000, total loss = 7.927001953125, avg loss = 7.927001953125
client 5, data condensation 4200, total loss = 42.5438232421875, avg loss = 42.5438232421875
client 5, data condensation 4400, total loss = 18.2430419921875, avg loss = 18.2430419921875
client 5, data condensation 4600, total loss = 2.98748779296875, avg loss = 2.98748779296875
client 5, data condensation 4800, total loss = 10.55474853515625, avg loss = 10.55474853515625
client 5, data condensation 5000, total loss = 24.9696044921875, avg loss = 24.9696044921875
client 5, data condensation 5200, total loss = 4.81976318359375, avg loss = 4.81976318359375
client 5, data condensation 5400, total loss = 5.0633544921875, avg loss = 5.0633544921875
client 5, data condensation 5600, total loss = 40.808837890625, avg loss = 40.808837890625
client 5, data condensation 5800, total loss = 19.6134033203125, avg loss = 19.6134033203125
client 5, data condensation 6000, total loss = 19.6519775390625, avg loss = 19.6519775390625
client 5, data condensation 6200, total loss = 17.4183349609375, avg loss = 17.4183349609375
client 5, data condensation 6400, total loss = 80.6683349609375, avg loss = 80.6683349609375
client 5, data condensation 6600, total loss = 11.15283203125, avg loss = 11.15283203125
client 5, data condensation 6800, total loss = 5.87115478515625, avg loss = 5.87115478515625
client 5, data condensation 7000, total loss = 4.54278564453125, avg loss = 4.54278564453125
client 5, data condensation 7200, total loss = 2.44793701171875, avg loss = 2.44793701171875
client 5, data condensation 7400, total loss = 7.11883544921875, avg loss = 7.11883544921875
client 5, data condensation 7600, total loss = 7.72772216796875, avg loss = 7.72772216796875
client 5, data condensation 7800, total loss = 30.05450439453125, avg loss = 30.05450439453125
client 5, data condensation 8000, total loss = 2.5511474609375, avg loss = 2.5511474609375
client 5, data condensation 8200, total loss = 5.74444580078125, avg loss = 5.74444580078125
client 5, data condensation 8400, total loss = 11.51519775390625, avg loss = 11.51519775390625
client 5, data condensation 8600, total loss = 7.569580078125, avg loss = 7.569580078125
client 5, data condensation 8800, total loss = 3.49114990234375, avg loss = 3.49114990234375
client 5, data condensation 9000, total loss = 4.7288818359375, avg loss = 4.7288818359375
client 5, data condensation 9200, total loss = 33.72369384765625, avg loss = 33.72369384765625
client 5, data condensation 9400, total loss = 4.0924072265625, avg loss = 4.0924072265625
client 5, data condensation 9600, total loss = 9.530029296875, avg loss = 9.530029296875
client 5, data condensation 9800, total loss = 3.0291748046875, avg loss = 3.0291748046875
client 5, data condensation 10000, total loss = 7.56536865234375, avg loss = 7.56536865234375
Round 8, client 5 condense time: 398.90029740333557
client 5, class 8 have 4999 samples
total 24576.0MB, used 2569.06MB, free 22006.94MB
total 24576.0MB, used 2569.06MB, free 22006.94MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1617  179  122  100  107   84  116  126  189 1725], bin edged: [0.00016379 0.00017651 0.00018922 0.00020193 0.00021465 0.00022736
 0.00024007 0.00025278 0.0002655  0.00027821 0.00029092]
class 6 have 3914 samples, histogram: [2657  117   89   58   50   43   62   55   75  708], bin edged: [0.00021434 0.00023097 0.00024761 0.00026424 0.00028088 0.00029751
 0.00031415 0.00033078 0.00034742 0.00036405 0.00038069]
client 6, data condensation 0, total loss = 59.60382080078125, avg loss = 29.801910400390625
client 6, data condensation 200, total loss = 27.4642333984375, avg loss = 13.73211669921875
client 6, data condensation 400, total loss = 20.39007568359375, avg loss = 10.195037841796875
client 6, data condensation 600, total loss = 14.99346923828125, avg loss = 7.496734619140625
client 6, data condensation 800, total loss = 7.74151611328125, avg loss = 3.870758056640625
client 6, data condensation 1000, total loss = 13.1475830078125, avg loss = 6.57379150390625
client 6, data condensation 1200, total loss = 37.60443115234375, avg loss = 18.802215576171875
client 6, data condensation 1400, total loss = 16.12078857421875, avg loss = 8.060394287109375
client 6, data condensation 1600, total loss = 8.3953857421875, avg loss = 4.19769287109375
client 6, data condensation 1800, total loss = 8.73602294921875, avg loss = 4.368011474609375
client 6, data condensation 2000, total loss = 7.9429931640625, avg loss = 3.97149658203125
client 6, data condensation 2200, total loss = 7.18402099609375, avg loss = 3.592010498046875
client 6, data condensation 2400, total loss = 10.00506591796875, avg loss = 5.002532958984375
client 6, data condensation 2600, total loss = 4.61895751953125, avg loss = 2.309478759765625
client 6, data condensation 2800, total loss = 6.63092041015625, avg loss = 3.315460205078125
client 6, data condensation 3000, total loss = 17.33349609375, avg loss = 8.666748046875
client 6, data condensation 3200, total loss = 6.8419189453125, avg loss = 3.42095947265625
client 6, data condensation 3400, total loss = 10.13397216796875, avg loss = 5.066986083984375
client 6, data condensation 3600, total loss = 11.64971923828125, avg loss = 5.824859619140625
client 6, data condensation 3800, total loss = 17.972412109375, avg loss = 8.9862060546875
client 6, data condensation 4000, total loss = 18.34722900390625, avg loss = 9.173614501953125
client 6, data condensation 4200, total loss = 6.25390625, avg loss = 3.126953125
client 6, data condensation 4400, total loss = 6.58074951171875, avg loss = 3.290374755859375
client 6, data condensation 4600, total loss = 15.1307373046875, avg loss = 7.56536865234375
client 6, data condensation 4800, total loss = 7.91448974609375, avg loss = 3.957244873046875
client 6, data condensation 5000, total loss = 19.2921142578125, avg loss = 9.64605712890625
client 6, data condensation 5200, total loss = 5.7015380859375, avg loss = 2.85076904296875
client 6, data condensation 5400, total loss = 6.8363037109375, avg loss = 3.41815185546875
client 6, data condensation 5600, total loss = 5.1260986328125, avg loss = 2.56304931640625
client 6, data condensation 5800, total loss = 20.95269775390625, avg loss = 10.476348876953125
client 6, data condensation 6000, total loss = 31.930908203125, avg loss = 15.9654541015625
client 6, data condensation 6200, total loss = 10.11212158203125, avg loss = 5.056060791015625
client 6, data condensation 6400, total loss = 3.82501220703125, avg loss = 1.912506103515625
client 6, data condensation 6600, total loss = 5.16961669921875, avg loss = 2.584808349609375
client 6, data condensation 6800, total loss = 5.9493408203125, avg loss = 2.97467041015625
client 6, data condensation 7000, total loss = 25.2193603515625, avg loss = 12.60968017578125
client 6, data condensation 7200, total loss = 10.791259765625, avg loss = 5.3956298828125
client 6, data condensation 7400, total loss = 5.75714111328125, avg loss = 2.878570556640625
client 6, data condensation 7600, total loss = 26.445068359375, avg loss = 13.2225341796875
client 6, data condensation 7800, total loss = 6.560791015625, avg loss = 3.2803955078125
client 6, data condensation 8000, total loss = 30.74041748046875, avg loss = 15.370208740234375
client 6, data condensation 8200, total loss = 14.36572265625, avg loss = 7.182861328125
client 6, data condensation 8400, total loss = 4.2021484375, avg loss = 2.10107421875
client 6, data condensation 8600, total loss = 5.45953369140625, avg loss = 2.729766845703125
client 6, data condensation 8800, total loss = 22.79547119140625, avg loss = 11.397735595703125
client 6, data condensation 9000, total loss = 7.3638916015625, avg loss = 3.68194580078125
client 6, data condensation 9200, total loss = 4.572509765625, avg loss = 2.2862548828125
client 6, data condensation 9400, total loss = 4.97210693359375, avg loss = 2.486053466796875
client 6, data condensation 9600, total loss = 28.88604736328125, avg loss = 14.443023681640625
client 6, data condensation 9800, total loss = 6.62896728515625, avg loss = 3.314483642578125
client 6, data condensation 10000, total loss = 29.15118408203125, avg loss = 14.575592041015625
Round 8, client 6 condense time: 668.1318583488464
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3306  134  103   59   50   58   65   74   94  662], bin edged: [0.00018663 0.00020111 0.0002156  0.00023008 0.00024457 0.00025905
 0.00027354 0.00028802 0.00030251 0.00031699 0.00033148]
class 3 have 4999 samples, histogram: [1805  257  174  124  143  124  135  163  227 1847], bin edged: [0.00014392 0.00015509 0.00016626 0.00017743 0.0001886  0.00019977
 0.00021094 0.00022211 0.00023328 0.00024445 0.00025562]
client 7, data condensation 0, total loss = 32.194580078125, avg loss = 16.0972900390625
client 7, data condensation 200, total loss = 39.3873291015625, avg loss = 19.69366455078125
client 7, data condensation 400, total loss = 16.66595458984375, avg loss = 8.332977294921875
client 7, data condensation 600, total loss = 16.19610595703125, avg loss = 8.098052978515625
client 7, data condensation 800, total loss = 34.5511474609375, avg loss = 17.27557373046875
client 7, data condensation 1000, total loss = 10.14385986328125, avg loss = 5.071929931640625
client 7, data condensation 1200, total loss = 12.3189697265625, avg loss = 6.15948486328125
client 7, data condensation 1400, total loss = 14.861328125, avg loss = 7.4306640625
client 7, data condensation 1600, total loss = 12.0089111328125, avg loss = 6.00445556640625
client 7, data condensation 1800, total loss = 27.49029541015625, avg loss = 13.745147705078125
client 7, data condensation 2000, total loss = 7.83935546875, avg loss = 3.919677734375
client 7, data condensation 2200, total loss = 90.6300048828125, avg loss = 45.31500244140625
client 7, data condensation 2400, total loss = 9.2027587890625, avg loss = 4.60137939453125
client 7, data condensation 2600, total loss = 11.403076171875, avg loss = 5.7015380859375
client 7, data condensation 2800, total loss = 12.669921875, avg loss = 6.3349609375
client 7, data condensation 3000, total loss = 14.7412109375, avg loss = 7.37060546875
client 7, data condensation 3200, total loss = 33.9647216796875, avg loss = 16.98236083984375
client 7, data condensation 3400, total loss = 27.68475341796875, avg loss = 13.842376708984375
client 7, data condensation 3600, total loss = 17.28717041015625, avg loss = 8.643585205078125
client 7, data condensation 3800, total loss = 19.956298828125, avg loss = 9.9781494140625
client 7, data condensation 4000, total loss = 4.91876220703125, avg loss = 2.459381103515625
client 7, data condensation 4200, total loss = 51.89996337890625, avg loss = 25.949981689453125
client 7, data condensation 4400, total loss = 21.54345703125, avg loss = 10.771728515625
client 7, data condensation 4600, total loss = 6.4188232421875, avg loss = 3.20941162109375
client 7, data condensation 4800, total loss = 5.3802490234375, avg loss = 2.69012451171875
client 7, data condensation 5000, total loss = 46.88653564453125, avg loss = 23.443267822265625
client 7, data condensation 5200, total loss = 7.1497802734375, avg loss = 3.57489013671875
client 7, data condensation 5400, total loss = 8.11273193359375, avg loss = 4.056365966796875
client 7, data condensation 5600, total loss = 51.9097900390625, avg loss = 25.95489501953125
client 7, data condensation 5800, total loss = 7.06964111328125, avg loss = 3.534820556640625
client 7, data condensation 6000, total loss = 11.75225830078125, avg loss = 5.876129150390625
client 7, data condensation 6200, total loss = 12.82794189453125, avg loss = 6.413970947265625
client 7, data condensation 6400, total loss = 7.8575439453125, avg loss = 3.92877197265625
client 7, data condensation 6600, total loss = 12.52374267578125, avg loss = 6.261871337890625
client 7, data condensation 6800, total loss = 21.30657958984375, avg loss = 10.653289794921875
client 7, data condensation 7000, total loss = 18.09381103515625, avg loss = 9.046905517578125
client 7, data condensation 7200, total loss = 7.7342529296875, avg loss = 3.86712646484375
client 7, data condensation 7400, total loss = 16.16595458984375, avg loss = 8.082977294921875
client 7, data condensation 7600, total loss = 7.84735107421875, avg loss = 3.923675537109375
client 7, data condensation 7800, total loss = 6.98358154296875, avg loss = 3.491790771484375
client 7, data condensation 8000, total loss = 6.695556640625, avg loss = 3.3477783203125
client 7, data condensation 8200, total loss = 7.13299560546875, avg loss = 3.566497802734375
client 7, data condensation 8400, total loss = 41.64239501953125, avg loss = 20.821197509765625
client 7, data condensation 8600, total loss = 38.19805908203125, avg loss = 19.099029541015625
client 7, data condensation 8800, total loss = 15.1009521484375, avg loss = 7.55047607421875
client 7, data condensation 9000, total loss = 15.2930908203125, avg loss = 7.64654541015625
client 7, data condensation 9200, total loss = 9.24090576171875, avg loss = 4.620452880859375
client 7, data condensation 9400, total loss = 12.7283935546875, avg loss = 6.36419677734375
client 7, data condensation 9600, total loss = 41.8424072265625, avg loss = 20.92120361328125
client 7, data condensation 9800, total loss = 7.9195556640625, avg loss = 3.95977783203125
client 7, data condensation 10000, total loss = 14.65374755859375, avg loss = 7.326873779296875
Round 8, client 7 condense time: 709.1371483802795
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [254  12   6   6   5   4   6   5   7  59], bin edged: [0.00233298 0.00251405 0.00269512 0.00287619 0.00305727 0.00323834
 0.00341941 0.00360048 0.00378155 0.00396263 0.0041437 ]
class 5 have 135 samples, histogram: [56  6  2  2  3  1  4  3  8 50], bin edged: [0.00536451 0.00578087 0.00619723 0.00661359 0.00702996 0.00744632
 0.00786268 0.00827904 0.0086954  0.00911176 0.00952812]
class 9 have 4727 samples, histogram: [3220  156  103   75   57   69   72   73  124  778], bin edged: [0.00017833 0.00019217 0.00020601 0.00021985 0.00023369 0.00024753
 0.00026138 0.00027522 0.00028906 0.0003029  0.00031674]
client 8, data condensation 0, total loss = 226.1763916015625, avg loss = 75.39213053385417
client 8, data condensation 200, total loss = 142.3753662109375, avg loss = 47.458455403645836
client 8, data condensation 400, total loss = 127.95977783203125, avg loss = 42.65325927734375
client 8, data condensation 600, total loss = 152.08636474609375, avg loss = 50.695454915364586
client 8, data condensation 800, total loss = 510.90618896484375, avg loss = 170.30206298828125
client 8, data condensation 1000, total loss = 161.29302978515625, avg loss = 53.76434326171875
client 8, data condensation 1200, total loss = 489.21051025390625, avg loss = 163.0701700846354
client 8, data condensation 1400, total loss = 241.67950439453125, avg loss = 80.55983479817708
client 8, data condensation 1600, total loss = 769.7576904296875, avg loss = 256.5858968098958
client 8, data condensation 1800, total loss = 2037.792236328125, avg loss = 679.2640787760416
client 8, data condensation 2000, total loss = 405.3975830078125, avg loss = 135.13252766927084
client 8, data condensation 2200, total loss = 320.364990234375, avg loss = 106.788330078125
client 8, data condensation 2400, total loss = 180.64447021484375, avg loss = 60.214823404947914
client 8, data condensation 2600, total loss = 748.3722534179688, avg loss = 249.4574178059896
client 8, data condensation 2800, total loss = 219.9385986328125, avg loss = 73.3128662109375
client 8, data condensation 3000, total loss = 924.5371704101562, avg loss = 308.17905680338544
client 8, data condensation 3200, total loss = 90.154052734375, avg loss = 30.051350911458332
client 8, data condensation 3400, total loss = 433.73980712890625, avg loss = 144.5799357096354
client 8, data condensation 3600, total loss = 132.387939453125, avg loss = 44.129313151041664
client 8, data condensation 3800, total loss = 98.6102294921875, avg loss = 32.870076497395836
client 8, data condensation 4000, total loss = 581.4375610351562, avg loss = 193.8125203450521
client 8, data condensation 4200, total loss = 121.42572021484375, avg loss = 40.475240071614586
client 8, data condensation 4400, total loss = 114.2918701171875, avg loss = 38.0972900390625
client 8, data condensation 4600, total loss = 1016.84716796875, avg loss = 338.9490559895833
client 8, data condensation 4800, total loss = 181.688232421875, avg loss = 60.562744140625
client 8, data condensation 5000, total loss = 201.4852294921875, avg loss = 67.1617431640625
client 8, data condensation 5200, total loss = 125.68255615234375, avg loss = 41.894185384114586
client 8, data condensation 5400, total loss = 140.78167724609375, avg loss = 46.927225748697914
client 8, data condensation 5600, total loss = 196.83935546875, avg loss = 65.61311848958333
client 8, data condensation 5800, total loss = 851.0455322265625, avg loss = 283.6818440755208
client 8, data condensation 6000, total loss = 180.0745849609375, avg loss = 60.024861653645836
client 8, data condensation 6200, total loss = 153.83111572265625, avg loss = 51.27703857421875
client 8, data condensation 6400, total loss = 170.45599365234375, avg loss = 56.81866455078125
client 8, data condensation 6600, total loss = 238.9688720703125, avg loss = 79.65629069010417
client 8, data condensation 6800, total loss = 555.0787963867188, avg loss = 185.0262654622396
client 8, data condensation 7000, total loss = 213.7330322265625, avg loss = 71.24434407552083
client 8, data condensation 7200, total loss = 1580.02734375, avg loss = 526.67578125
client 8, data condensation 7400, total loss = 70.94964599609375, avg loss = 23.649881998697918
client 8, data condensation 7600, total loss = 251.74053955078125, avg loss = 83.91351318359375
client 8, data condensation 7800, total loss = 154.5389404296875, avg loss = 51.512980143229164
client 8, data condensation 8000, total loss = 114.6397705078125, avg loss = 38.2132568359375
client 8, data condensation 8200, total loss = 130.25750732421875, avg loss = 43.419169108072914
client 8, data condensation 8400, total loss = 794.8069458007812, avg loss = 264.93564860026044
client 8, data condensation 8600, total loss = 170.9302978515625, avg loss = 56.976765950520836
client 8, data condensation 8800, total loss = 350.73486328125, avg loss = 116.91162109375
client 8, data condensation 9000, total loss = 711.3943481445312, avg loss = 237.1314493815104
client 8, data condensation 9200, total loss = 135.0126953125, avg loss = 45.004231770833336
client 8, data condensation 9400, total loss = 160.96875, avg loss = 53.65625
client 8, data condensation 9600, total loss = 544.320556640625, avg loss = 181.440185546875
client 8, data condensation 9800, total loss = 148.03131103515625, avg loss = 49.343770345052086
client 8, data condensation 10000, total loss = 195.5479736328125, avg loss = 65.18265787760417
Round 8, client 8 condense time: 762.1381781101227
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 3209.06MB, free 21366.94MB
total 24576.0MB, used 3209.06MB, free 21366.94MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [48  6  1  4  0  4  3  0  6 48], bin edged: [0.00602464 0.00649224 0.00695983 0.00742743 0.00789503 0.00836262
 0.00883022 0.00929781 0.00976541 0.01023301 0.0107006 ]
class 5 have 192 samples, histogram: [76  6  2  4  4  3  3  7 10 77], bin edged: [0.00372137 0.0040102  0.00429903 0.00458786 0.00487669 0.00516552
 0.00545435 0.00574318 0.00603201 0.00632084 0.00660967]
class 6 have 1075 samples, histogram: [713  41  25  34  12  13   7  16  26 188], bin edged: [0.00078019 0.00084074 0.0009013  0.00096185 0.0010224  0.00108296
 0.00114351 0.00120406 0.00126462 0.00132517 0.00138572]
client 9, data condensation 0, total loss = 267.25921630859375, avg loss = 89.08640543619792
client 9, data condensation 200, total loss = 309.802978515625, avg loss = 103.26765950520833
client 9, data condensation 400, total loss = 314.95867919921875, avg loss = 104.98622639973958
client 9, data condensation 600, total loss = 230.5821533203125, avg loss = 76.8607177734375
client 9, data condensation 800, total loss = 430.89605712890625, avg loss = 143.63201904296875
client 9, data condensation 1000, total loss = 1473.26416015625, avg loss = 491.0880533854167
client 9, data condensation 1200, total loss = 803.2181396484375, avg loss = 267.7393798828125
client 9, data condensation 1400, total loss = 3297.581787109375, avg loss = 1099.1939290364583
client 9, data condensation 1600, total loss = 299.21429443359375, avg loss = 99.73809814453125
client 9, data condensation 1800, total loss = 5138.44091796875, avg loss = 1712.8136393229167
client 9, data condensation 2000, total loss = 294.8345947265625, avg loss = 98.2781982421875
client 9, data condensation 2200, total loss = 330.77154541015625, avg loss = 110.25718180338542
client 9, data condensation 2400, total loss = 2448.65478515625, avg loss = 816.21826171875
client 9, data condensation 2600, total loss = 476.5570068359375, avg loss = 158.85233561197916
client 9, data condensation 2800, total loss = 225.01092529296875, avg loss = 75.00364176432292
client 9, data condensation 3000, total loss = 182.583251953125, avg loss = 60.861083984375
client 9, data condensation 3200, total loss = 4241.60205078125, avg loss = 1413.8673502604167
client 9, data condensation 3400, total loss = 279.50372314453125, avg loss = 93.16790771484375
client 9, data condensation 3600, total loss = 594.1220092773438, avg loss = 198.0406697591146
client 9, data condensation 3800, total loss = 220.18743896484375, avg loss = 73.39581298828125
client 9, data condensation 4000, total loss = 1802.220703125, avg loss = 600.740234375
client 9, data condensation 4200, total loss = 286.23681640625, avg loss = 95.41227213541667
client 9, data condensation 4400, total loss = 361.84063720703125, avg loss = 120.61354573567708
client 9, data condensation 4600, total loss = 531.3602294921875, avg loss = 177.12007649739584
client 9, data condensation 4800, total loss = 806.0326538085938, avg loss = 268.67755126953125
client 9, data condensation 5000, total loss = 244.56610107421875, avg loss = 81.52203369140625
client 9, data condensation 5200, total loss = 1180.5550537109375, avg loss = 393.5183512369792
client 9, data condensation 5400, total loss = 595.2802734375, avg loss = 198.4267578125
client 9, data condensation 5600, total loss = 398.84625244140625, avg loss = 132.9487508138021
client 9, data condensation 5800, total loss = 249.0888671875, avg loss = 83.02962239583333
client 9, data condensation 6000, total loss = 1369.012939453125, avg loss = 456.337646484375
client 9, data condensation 6200, total loss = 376.66790771484375, avg loss = 125.55596923828125
client 9, data condensation 6400, total loss = 751.1744384765625, avg loss = 250.3914794921875
client 9, data condensation 6600, total loss = 216.76983642578125, avg loss = 72.25661214192708
client 9, data condensation 6800, total loss = 211.96771240234375, avg loss = 70.65590413411458
client 9, data condensation 7000, total loss = 527.2284545898438, avg loss = 175.7428181966146
client 9, data condensation 7200, total loss = 306.06121826171875, avg loss = 102.02040608723958
client 9, data condensation 7400, total loss = 303.68182373046875, avg loss = 101.22727457682292
client 9, data condensation 7600, total loss = 240.86431884765625, avg loss = 80.28810628255208
client 9, data condensation 7800, total loss = 320.4056396484375, avg loss = 106.8018798828125
client 9, data condensation 8000, total loss = 410.39373779296875, avg loss = 136.79791259765625
client 9, data condensation 8200, total loss = 287.3792724609375, avg loss = 95.7930908203125
client 9, data condensation 8400, total loss = 518.0670776367188, avg loss = 172.68902587890625
client 9, data condensation 8600, total loss = 300.63739013671875, avg loss = 100.21246337890625
client 9, data condensation 8800, total loss = 158.93597412109375, avg loss = 52.978658040364586
client 9, data condensation 9000, total loss = 3256.041015625, avg loss = 1085.3470052083333
client 9, data condensation 9200, total loss = 285.86328125, avg loss = 95.28776041666667
client 9, data condensation 9400, total loss = 261.763427734375, avg loss = 87.25447591145833
client 9, data condensation 9600, total loss = 736.79345703125, avg loss = 245.59781901041666
client 9, data condensation 9800, total loss = 397.95111083984375, avg loss = 132.6503702799479
client 9, data condensation 10000, total loss = 793.41845703125, avg loss = 264.4728190104167
Round 8, client 9 condense time: 614.7209243774414
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3207.06MB, free 21368.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 18.1799,   1.5773,   3.5993,  -4.6371,  -1.2492, -10.4408, -11.5530,
          -3.3920,   6.4973,   1.9002],
        [  1.3203,  19.1038,  -6.0609,  -2.7706,  -5.2120,  -7.6411,  -6.0367,
          -1.9579,   0.4448,  10.1068],
        [  0.4331,  -5.3948,  10.8284,   2.3765,   4.2974,   1.4590,   0.3466,
           1.0296,  -8.7537,  -5.7283],
        [ -4.6040,  -3.2980,   1.6389,  11.5608,  -0.3333,   6.8224,   2.8751,
           0.5223, -10.3906,  -3.9054],
        [ -3.7613,  -4.8602,   5.4293,   0.2835,  11.6131,   0.9153,   3.8055,
           4.2811, -11.1421,  -5.9027],
        [ -6.9549,  -4.7244,   3.0896,  10.1698,  -0.0288,  12.7537,   0.8928,
           2.8980, -11.8477,  -5.1129],
        [ -7.6959,  -2.9325,   3.2141,   5.3515,   6.0371,   1.6617,  17.4788,
          -2.3058, -14.6909,  -5.4513],
        [ -4.6590,  -2.8764,   1.9076,   0.5715,   5.2210,   1.3442,  -2.3439,
          15.0399, -12.0714,  -0.9274],
        [ 11.8212,   5.5388,  -1.6811,  -4.8819,  -4.1625, -10.7166, -13.0684,
          -5.4521,  17.5742,   5.9298],
        [  0.3684,   8.9634,  -5.6154,  -2.3183,  -5.3305,  -7.0456,  -6.8737,
           1.2316,  -0.0548,  17.7069]], device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 5, 7],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 2, 5, 3],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
9
Round 8: # synthetic sample: 9108
total 24576.0MB, used 3207.06MB, free 21368.94MB
{0: {0: 691, 1: 45, 2: 40, 3: 43, 4: 8, 5: 3, 6: 14, 7: 14, 8: 83, 9: 59}, 1: {0: 29, 1: 755, 2: 4, 3: 19, 4: 8, 5: 4, 6: 21, 7: 15, 8: 23, 9: 122}, 2: {0: 119, 1: 13, 2: 399, 3: 119, 4: 110, 5: 56, 6: 103, 7: 41, 8: 20, 9: 20}, 3: {0: 32, 1: 26, 2: 74, 3: 507, 4: 51, 5: 137, 6: 102, 7: 28, 8: 12, 9: 31}, 4: {0: 61, 1: 10, 2: 82, 3: 63, 4: 514, 5: 24, 6: 119, 7: 95, 8: 15, 9: 17}, 5: {0: 20, 1: 11, 2: 58, 3: 258, 4: 60, 5: 452, 6: 47, 7: 52, 8: 17, 9: 25}, 6: {0: 8, 1: 5, 2: 45, 3: 74, 4: 39, 5: 19, 6: 793, 7: 4, 8: 4, 9: 9}, 7: {0: 35, 1: 8, 2: 28, 3: 70, 4: 54, 5: 73, 6: 33, 7: 648, 8: 5, 9: 46}, 8: {0: 130, 1: 92, 2: 8, 3: 25, 4: 2, 5: 3, 6: 12, 7: 10, 8: 643, 9: 75}, 9: {0: 36, 1: 119, 2: 2, 3: 22, 4: 7, 5: 5, 6: 14, 7: 12, 8: 24, 9: 759}}
round 8 evaluation: test acc is 0.6161, test loss = 2.602673
{0: {0: 670, 1: 24, 2: 47, 3: 24, 4: 11, 5: 4, 6: 17, 7: 19, 8: 148, 9: 36}, 1: {0: 37, 1: 689, 2: 5, 3: 14, 4: 16, 5: 11, 6: 16, 7: 16, 8: 66, 9: 130}, 2: {0: 125, 1: 5, 2: 430, 3: 80, 4: 112, 5: 49, 6: 97, 7: 40, 8: 40, 9: 22}, 3: {0: 39, 1: 25, 2: 102, 3: 395, 4: 58, 5: 165, 6: 116, 7: 36, 8: 27, 9: 37}, 4: {0: 63, 1: 7, 2: 97, 3: 40, 4: 534, 5: 24, 6: 131, 7: 65, 8: 27, 9: 12}, 5: {0: 21, 1: 6, 2: 94, 3: 190, 4: 71, 5: 457, 6: 56, 7: 53, 8: 31, 9: 21}, 6: {0: 8, 1: 6, 2: 76, 3: 63, 4: 49, 5: 25, 6: 741, 7: 6, 8: 12, 9: 14}, 7: {0: 41, 1: 6, 2: 54, 3: 50, 4: 81, 5: 67, 6: 25, 7: 627, 8: 16, 9: 33}, 8: {0: 108, 1: 56, 2: 8, 3: 14, 4: 4, 5: 2, 6: 9, 7: 6, 8: 740, 9: 53}, 9: {0: 51, 1: 111, 2: 7, 3: 15, 4: 10, 5: 8, 6: 12, 7: 17, 8: 57, 9: 712}}
epoch 0, train loss avg now = 0.021251, train contrast loss now = 1.592125, test acc now = 0.5995, test loss now = 2.723388
{0: {0: 663, 1: 37, 2: 35, 3: 48, 4: 6, 5: 11, 6: 11, 7: 19, 8: 102, 9: 68}, 1: {0: 21, 1: 719, 2: 7, 3: 26, 4: 6, 5: 12, 6: 17, 7: 14, 8: 21, 9: 157}, 2: {0: 129, 1: 11, 2: 369, 3: 134, 4: 93, 5: 110, 6: 67, 7: 41, 8: 18, 9: 28}, 3: {0: 21, 1: 21, 2: 60, 3: 513, 4: 37, 5: 214, 6: 53, 7: 26, 8: 18, 9: 37}, 4: {0: 52, 1: 5, 2: 94, 3: 89, 4: 464, 5: 62, 6: 90, 7: 107, 8: 18, 9: 19}, 5: {0: 14, 1: 7, 2: 45, 3: 219, 4: 31, 5: 570, 6: 23, 7: 45, 8: 20, 9: 26}, 6: {0: 6, 1: 4, 2: 47, 3: 103, 4: 33, 5: 57, 6: 724, 7: 5, 8: 6, 9: 15}, 7: {0: 35, 1: 10, 2: 26, 3: 78, 4: 45, 5: 114, 6: 13, 7: 625, 8: 4, 9: 50}, 8: {0: 100, 1: 80, 2: 7, 3: 30, 4: 3, 5: 6, 6: 7, 7: 10, 8: 670, 9: 87}, 9: {0: 33, 1: 100, 2: 4, 3: 29, 4: 3, 5: 11, 6: 10, 7: 14, 8: 17, 9: 779}}
epoch 100, train loss avg now = 0.010274, train contrast loss now = 0.451278, test acc now = 0.6096, test loss now = 2.572213
{0: {0: 692, 1: 35, 2: 53, 3: 37, 4: 10, 5: 5, 6: 18, 7: 15, 8: 62, 9: 73}, 1: {0: 24, 1: 698, 2: 4, 3: 16, 4: 14, 5: 7, 6: 24, 7: 16, 8: 20, 9: 177}, 2: {0: 121, 1: 7, 2: 381, 3: 107, 4: 137, 5: 65, 6: 103, 7: 34, 8: 20, 9: 25}, 3: {0: 27, 1: 21, 2: 66, 3: 468, 4: 67, 5: 174, 6: 100, 7: 23, 8: 13, 9: 41}, 4: {0: 55, 1: 6, 2: 78, 3: 64, 4: 549, 5: 31, 6: 113, 7: 73, 8: 11, 9: 20}, 5: {0: 17, 1: 6, 2: 53, 3: 215, 4: 75, 5: 502, 6: 50, 7: 40, 8: 17, 9: 25}, 6: {0: 6, 1: 3, 2: 39, 3: 62, 4: 55, 5: 24, 6: 787, 7: 7, 8: 5, 9: 12}, 7: {0: 33, 1: 5, 2: 23, 3: 69, 4: 75, 5: 79, 6: 27, 7: 634, 8: 4, 9: 51}, 8: {0: 172, 1: 72, 2: 12, 3: 27, 4: 7, 5: 3, 6: 11, 7: 6, 8: 580, 9: 110}, 9: {0: 41, 1: 80, 2: 2, 3: 24, 4: 6, 5: 9, 6: 14, 7: 12, 8: 15, 9: 797}}
epoch 200, train loss avg now = 0.005503, train contrast loss now = 0.435289, test acc now = 0.6088, test loss now = 2.580703
{0: {0: 676, 1: 41, 2: 56, 3: 35, 4: 12, 5: 6, 6: 15, 7: 21, 8: 87, 9: 51}, 1: {0: 27, 1: 749, 2: 6, 3: 19, 4: 7, 5: 4, 6: 24, 7: 19, 8: 26, 9: 119}, 2: {0: 105, 1: 13, 2: 408, 3: 98, 4: 112, 5: 66, 6: 112, 7: 49, 8: 19, 9: 18}, 3: {0: 23, 1: 24, 2: 84, 3: 416, 4: 57, 5: 201, 6: 116, 7: 40, 8: 12, 9: 27}, 4: {0: 50, 1: 7, 2: 100, 3: 54, 4: 506, 5: 36, 6: 120, 7: 98, 8: 13, 9: 16}, 5: {0: 18, 1: 10, 2: 57, 3: 172, 4: 56, 5: 530, 6: 57, 7: 69, 8: 11, 9: 20}, 6: {0: 5, 1: 4, 2: 48, 3: 54, 4: 29, 5: 32, 6: 805, 7: 8, 8: 5, 9: 10}, 7: {0: 33, 1: 5, 2: 29, 3: 47, 4: 59, 5: 91, 6: 25, 7: 670, 8: 7, 9: 34}, 8: {0: 115, 1: 79, 2: 8, 3: 27, 4: 7, 5: 5, 6: 11, 7: 13, 8: 672, 9: 63}, 9: {0: 41, 1: 109, 2: 5, 3: 23, 4: 5, 5: 10, 6: 17, 7: 15, 8: 25, 9: 750}}
epoch 300, train loss avg now = 0.004757, train contrast loss now = 0.429179, test acc now = 0.6182, test loss now = 2.533612
{0: {0: 695, 1: 34, 2: 51, 3: 44, 4: 10, 5: 3, 6: 17, 7: 19, 8: 78, 9: 49}, 1: {0: 29, 1: 722, 2: 3, 3: 31, 4: 13, 5: 8, 6: 28, 7: 15, 8: 19, 9: 132}, 2: {0: 123, 1: 11, 2: 355, 3: 122, 4: 132, 5: 71, 6: 119, 7: 30, 8: 19, 9: 18}, 3: {0: 28, 1: 15, 2: 70, 3: 516, 4: 50, 5: 166, 6: 101, 7: 22, 8: 12, 9: 20}, 4: {0: 54, 1: 7, 2: 64, 3: 76, 4: 536, 5: 30, 6: 138, 7: 68, 8: 14, 9: 13}, 5: {0: 15, 1: 8, 2: 47, 3: 234, 4: 79, 5: 483, 6: 55, 7: 45, 8: 14, 9: 20}, 6: {0: 5, 1: 3, 2: 38, 3: 85, 4: 35, 5: 19, 6: 798, 7: 5, 8: 5, 9: 7}, 7: {0: 34, 1: 6, 2: 31, 3: 87, 4: 77, 5: 91, 6: 28, 7: 612, 8: 4, 9: 30}, 8: {0: 142, 1: 81, 2: 9, 3: 35, 4: 8, 5: 6, 6: 11, 7: 8, 8: 626, 9: 74}, 9: {0: 39, 1: 89, 2: 5, 3: 32, 4: 9, 5: 13, 6: 17, 7: 15, 8: 18, 9: 763}}
epoch 400, train loss avg now = 0.003136, train contrast loss now = 0.427586, test acc now = 0.6106, test loss now = 2.681440
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 693, 1: 44, 2: 46, 3: 35, 4: 11, 5: 4, 6: 17, 7: 15, 8: 83, 9: 52}, 1: {0: 32, 1: 758, 2: 3, 3: 16, 4: 11, 5: 4, 6: 20, 7: 10, 8: 20, 9: 126}, 2: {0: 130, 1: 14, 2: 360, 3: 108, 4: 122, 5: 60, 6: 125, 7: 40, 8: 19, 9: 22}, 3: {0: 34, 1: 30, 2: 61, 3: 463, 4: 54, 5: 175, 6: 119, 7: 24, 8: 12, 9: 28}, 4: {0: 59, 1: 9, 2: 77, 3: 61, 4: 508, 5: 33, 6: 148, 7: 73, 8: 15, 9: 17}, 5: {0: 21, 1: 13, 2: 43, 3: 204, 4: 65, 5: 508, 6: 69, 7: 40, 8: 12, 9: 25}, 6: {0: 7, 1: 4, 2: 36, 3: 57, 4: 27, 5: 25, 6: 819, 7: 8, 8: 5, 9: 12}, 7: {0: 40, 1: 12, 2: 25, 3: 60, 4: 58, 5: 82, 6: 29, 7: 639, 8: 7, 9: 48}, 8: {0: 136, 1: 101, 2: 4, 3: 27, 4: 6, 5: 2, 6: 10, 7: 7, 8: 635, 9: 72}, 9: {0: 47, 1: 122, 2: 2, 3: 24, 4: 6, 5: 6, 6: 16, 7: 14, 8: 22, 9: 741}}
epoch 500, train loss avg now = 0.002080, train contrast loss now = 0.426244, test acc now = 0.6124, test loss now = 2.669027
{0: {0: 696, 1: 39, 2: 45, 3: 34, 4: 7, 5: 4, 6: 16, 7: 16, 8: 87, 9: 56}, 1: {0: 24, 1: 742, 2: 4, 3: 18, 4: 12, 5: 5, 6: 19, 7: 16, 8: 22, 9: 138}, 2: {0: 125, 1: 12, 2: 384, 3: 113, 4: 109, 5: 66, 6: 102, 7: 45, 8: 19, 9: 25}, 3: {0: 29, 1: 25, 2: 73, 3: 486, 4: 47, 5: 167, 6: 94, 7: 30, 8: 16, 9: 33}, 4: {0: 60, 1: 6, 2: 84, 3: 74, 4: 503, 5: 29, 6: 122, 7: 90, 8: 15, 9: 17}, 5: {0: 20, 1: 10, 2: 54, 3: 208, 4: 55, 5: 504, 6: 52, 7: 50, 8: 16, 9: 31}, 6: {0: 6, 1: 3, 2: 42, 3: 74, 4: 38, 5: 28, 6: 785, 7: 6, 8: 5, 9: 13}, 7: {0: 32, 1: 8, 2: 30, 3: 57, 4: 55, 5: 81, 6: 24, 7: 660, 8: 6, 9: 47}, 8: {0: 126, 1: 84, 2: 6, 3: 24, 4: 4, 5: 2, 6: 10, 7: 9, 8: 658, 9: 77}, 9: {0: 40, 1: 106, 2: 3, 3: 23, 4: 5, 5: 11, 6: 13, 7: 14, 8: 19, 9: 766}}
epoch 600, train loss avg now = 0.001781, train contrast loss now = 0.425099, test acc now = 0.6184, test loss now = 2.602884
{0: {0: 695, 1: 36, 2: 49, 3: 34, 4: 7, 5: 4, 6: 15, 7: 16, 8: 90, 9: 54}, 1: {0: 28, 1: 751, 2: 4, 3: 19, 4: 11, 5: 5, 6: 18, 7: 16, 8: 26, 9: 122}, 2: {0: 123, 1: 13, 2: 402, 3: 113, 4: 117, 5: 59, 6: 92, 7: 38, 8: 24, 9: 19}, 3: {0: 31, 1: 25, 2: 74, 3: 479, 4: 52, 5: 162, 6: 95, 7: 35, 8: 18, 9: 29}, 4: {0: 63, 1: 7, 2: 90, 3: 69, 4: 517, 5: 31, 6: 106, 7: 86, 8: 17, 9: 14}, 5: {0: 20, 1: 9, 2: 62, 3: 225, 4: 58, 5: 480, 6: 51, 7: 50, 8: 19, 9: 26}, 6: {0: 8, 1: 3, 2: 51, 3: 73, 4: 32, 5: 26, 6: 782, 7: 8, 8: 5, 9: 12}, 7: {0: 37, 1: 7, 2: 32, 3: 59, 4: 59, 5: 78, 6: 22, 7: 653, 8: 6, 9: 47}, 8: {0: 128, 1: 80, 2: 7, 3: 26, 4: 4, 5: 2, 6: 10, 7: 8, 8: 668, 9: 67}, 9: {0: 43, 1: 112, 2: 3, 3: 25, 4: 5, 5: 10, 6: 13, 7: 15, 8: 26, 9: 748}}
epoch 700, train loss avg now = 0.001504, train contrast loss now = 0.425395, test acc now = 0.6175, test loss now = 2.584312
{0: {0: 703, 1: 36, 2: 49, 3: 35, 4: 8, 5: 4, 6: 16, 7: 17, 8: 77, 9: 55}, 1: {0: 27, 1: 748, 2: 4, 3: 19, 4: 11, 5: 5, 6: 20, 7: 17, 8: 23, 9: 126}, 2: {0: 124, 1: 13, 2: 397, 3: 108, 4: 114, 5: 61, 6: 101, 7: 42, 8: 18, 9: 22}, 3: {0: 29, 1: 24, 2: 73, 3: 476, 4: 55, 5: 163, 6: 98, 7: 36, 8: 16, 9: 30}, 4: {0: 60, 1: 6, 2: 89, 3: 65, 4: 519, 5: 28, 6: 112, 7: 91, 8: 14, 9: 16}, 5: {0: 18, 1: 9, 2: 59, 3: 220, 4: 58, 5: 483, 6: 55, 7: 52, 8: 17, 9: 29}, 6: {0: 9, 1: 3, 2: 43, 3: 71, 4: 36, 5: 25, 6: 790, 7: 8, 8: 5, 9: 10}, 7: {0: 35, 1: 6, 2: 31, 3: 58, 4: 59, 5: 77, 6: 22, 7: 664, 8: 5, 9: 43}, 8: {0: 135, 1: 84, 2: 7, 3: 26, 4: 4, 5: 2, 6: 10, 7: 8, 8: 652, 9: 72}, 9: {0: 40, 1: 107, 2: 3, 3: 26, 4: 6, 5: 10, 6: 12, 7: 18, 8: 23, 9: 755}}
epoch 800, train loss avg now = 0.001687, train contrast loss now = 0.424548, test acc now = 0.6187, test loss now = 2.567009
{0: {0: 703, 1: 37, 2: 51, 3: 32, 4: 6, 5: 3, 6: 16, 7: 14, 8: 83, 9: 55}, 1: {0: 30, 1: 743, 2: 5, 3: 20, 4: 10, 5: 3, 6: 19, 7: 13, 8: 26, 9: 131}, 2: {0: 127, 1: 10, 2: 394, 3: 110, 4: 108, 5: 56, 6: 107, 7: 43, 8: 22, 9: 23}, 3: {0: 30, 1: 26, 2: 78, 3: 480, 4: 49, 5: 156, 6: 103, 7: 34, 8: 18, 9: 26}, 4: {0: 61, 1: 7, 2: 89, 3: 67, 4: 507, 5: 27, 6: 129, 7: 84, 8: 16, 9: 13}, 5: {0: 21, 1: 9, 2: 63, 3: 236, 4: 60, 5: 460, 6: 55, 7: 50, 8: 20, 9: 26}, 6: {0: 8, 1: 4, 2: 45, 3: 70, 4: 32, 5: 22, 6: 796, 7: 7, 8: 5, 9: 11}, 7: {0: 38, 1: 6, 2: 34, 3: 65, 4: 62, 5: 71, 6: 25, 7: 643, 8: 7, 9: 49}, 8: {0: 133, 1: 80, 2: 7, 3: 24, 4: 4, 5: 2, 6: 9, 7: 7, 8: 664, 9: 70}, 9: {0: 40, 1: 112, 2: 3, 3: 25, 4: 6, 5: 8, 6: 13, 7: 15, 8: 26, 9: 752}}
epoch 900, train loss avg now = 0.000998, train contrast loss now = 0.424940, test acc now = 0.6142, test loss now = 2.616819
{0: {0: 700, 1: 38, 2: 49, 3: 33, 4: 8, 5: 3, 6: 16, 7: 15, 8: 82, 9: 56}, 1: {0: 28, 1: 747, 2: 4, 3: 20, 4: 11, 5: 4, 6: 17, 7: 15, 8: 23, 9: 131}, 2: {0: 121, 1: 10, 2: 400, 3: 108, 4: 115, 5: 61, 6: 98, 7: 41, 8: 23, 9: 23}, 3: {0: 31, 1: 27, 2: 77, 3: 473, 4: 53, 5: 158, 6: 98, 7: 37, 8: 16, 9: 30}, 4: {0: 60, 1: 6, 2: 86, 3: 65, 4: 519, 5: 28, 6: 112, 7: 92, 8: 16, 9: 16}, 5: {0: 19, 1: 9, 2: 62, 3: 223, 4: 62, 5: 469, 6: 52, 7: 53, 8: 20, 9: 31}, 6: {0: 9, 1: 3, 2: 49, 3: 70, 4: 42, 5: 26, 6: 777, 7: 7, 8: 5, 9: 12}, 7: {0: 37, 1: 7, 2: 29, 3: 59, 4: 60, 5: 77, 6: 22, 7: 652, 8: 5, 9: 52}, 8: {0: 131, 1: 82, 2: 7, 3: 25, 4: 3, 5: 2, 6: 10, 7: 8, 8: 659, 9: 73}, 9: {0: 40, 1: 107, 2: 3, 3: 23, 4: 6, 5: 9, 6: 13, 7: 17, 8: 23, 9: 759}}
epoch 1000, train loss avg now = 0.001127, train contrast loss now = 0.424893, test acc now = 0.6155, test loss now = 2.626914
epoch avg loss = 1.1268239109938476e-06, total time = 9499.993654489517
total 24576.0MB, used 3207.06MB, free 21368.94MB
Round 8 finish, update the prev_syn_proto
torch.Size([909, 3, 32, 32])
torch.Size([909, 3, 32, 32])
torch.Size([936, 3, 32, 32])
torch.Size([900, 3, 32, 32])
torch.Size([900, 3, 32, 32])
torch.Size([945, 3, 32, 32])
torch.Size([909, 3, 32, 32])
torch.Size([900, 3, 32, 32])
torch.Size([900, 3, 32, 32])
torch.Size([900, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 700, 1: 38, 2: 49, 3: 33, 4: 8, 5: 3, 6: 16, 7: 15, 8: 82, 9: 56}, 1: {0: 28, 1: 747, 2: 4, 3: 20, 4: 11, 5: 4, 6: 17, 7: 15, 8: 23, 9: 131}, 2: {0: 121, 1: 10, 2: 400, 3: 108, 4: 115, 5: 61, 6: 98, 7: 41, 8: 23, 9: 23}, 3: {0: 31, 1: 27, 2: 77, 3: 473, 4: 53, 5: 158, 6: 98, 7: 37, 8: 16, 9: 30}, 4: {0: 60, 1: 6, 2: 86, 3: 65, 4: 519, 5: 28, 6: 112, 7: 92, 8: 16, 9: 16}, 5: {0: 19, 1: 9, 2: 62, 3: 223, 4: 62, 5: 469, 6: 52, 7: 53, 8: 20, 9: 31}, 6: {0: 9, 1: 3, 2: 49, 3: 70, 4: 42, 5: 26, 6: 777, 7: 7, 8: 5, 9: 12}, 7: {0: 37, 1: 7, 2: 29, 3: 59, 4: 60, 5: 77, 6: 22, 7: 652, 8: 5, 9: 52}, 8: {0: 131, 1: 82, 2: 7, 3: 25, 4: 3, 5: 2, 6: 10, 7: 8, 8: 659, 9: 73}, 9: {0: 40, 1: 107, 2: 3, 3: 23, 4: 6, 5: 9, 6: 13, 7: 17, 8: 23, 9: 759}}
round 8 evaluation: test acc is 0.6155, test loss = 2.626914
 ====== round 9 ======
---------- client training ----------
selected clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
total 24576.0MB, used 3207.06MB, free 21368.94MB
initialized by random noise
client 0 have real samples [3593, 4999]
client 0 will condense {2: 72, 7: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 3593 samples, histogram: [1215  148   93   92   86   73   95  108  166 1517], bin edged: [0.00019556 0.00021074 0.00022592 0.0002411  0.00025628 0.00027146
 0.00028663 0.00030181 0.00031699 0.00033217 0.00034735]
class 7 have 4999 samples, histogram: [2909  154   88   96   85   84   85  106  141 1251], bin edged: [0.00015877 0.00017109 0.00018341 0.00019574 0.00020806 0.00022038
 0.0002327  0.00024503 0.00025735 0.00026967 0.000282  ]
client 0, data condensation 0, total loss = 61.19268798828125, avg loss = 30.596343994140625
client 0, data condensation 200, total loss = 14.3931884765625, avg loss = 7.19659423828125
client 0, data condensation 400, total loss = 10.20220947265625, avg loss = 5.101104736328125
client 0, data condensation 600, total loss = 10.71673583984375, avg loss = 5.358367919921875
client 0, data condensation 800, total loss = 15.26141357421875, avg loss = 7.630706787109375
client 0, data condensation 1000, total loss = 4.741455078125, avg loss = 2.3707275390625
client 0, data condensation 1200, total loss = 7.4237060546875, avg loss = 3.71185302734375
client 0, data condensation 1400, total loss = 17.7506103515625, avg loss = 8.87530517578125
client 0, data condensation 1600, total loss = 7.66485595703125, avg loss = 3.832427978515625
client 0, data condensation 1800, total loss = 6.7642822265625, avg loss = 3.38214111328125
client 0, data condensation 2000, total loss = 8.73828125, avg loss = 4.369140625
client 0, data condensation 2200, total loss = 12.03057861328125, avg loss = 6.015289306640625
client 0, data condensation 2400, total loss = 9.71282958984375, avg loss = 4.856414794921875
client 0, data condensation 2600, total loss = 33.59124755859375, avg loss = 16.795623779296875
client 0, data condensation 2800, total loss = 31.77880859375, avg loss = 15.889404296875
client 0, data condensation 3000, total loss = 6.92242431640625, avg loss = 3.461212158203125
client 0, data condensation 3200, total loss = 10.494873046875, avg loss = 5.2474365234375
client 0, data condensation 3400, total loss = 8.40911865234375, avg loss = 4.204559326171875
client 0, data condensation 3600, total loss = 10.0338134765625, avg loss = 5.01690673828125
client 0, data condensation 3800, total loss = 7.68023681640625, avg loss = 3.840118408203125
client 0, data condensation 4000, total loss = 7.64886474609375, avg loss = 3.824432373046875
client 0, data condensation 4200, total loss = 10.6077880859375, avg loss = 5.30389404296875
client 0, data condensation 4400, total loss = 25.79620361328125, avg loss = 12.898101806640625
client 0, data condensation 4600, total loss = 6.36151123046875, avg loss = 3.180755615234375
client 0, data condensation 4800, total loss = 8.12115478515625, avg loss = 4.060577392578125
client 0, data condensation 5000, total loss = 6.57623291015625, avg loss = 3.288116455078125
client 0, data condensation 5200, total loss = 7.03179931640625, avg loss = 3.515899658203125
client 0, data condensation 5400, total loss = 6.37677001953125, avg loss = 3.188385009765625
client 0, data condensation 5600, total loss = 7.32696533203125, avg loss = 3.663482666015625
client 0, data condensation 5800, total loss = 8.7720947265625, avg loss = 4.38604736328125
client 0, data condensation 6000, total loss = 10.579833984375, avg loss = 5.2899169921875
client 0, data condensation 6200, total loss = 11.78802490234375, avg loss = 5.894012451171875
client 0, data condensation 6400, total loss = 9.360107421875, avg loss = 4.6800537109375
client 0, data condensation 6600, total loss = 8.17193603515625, avg loss = 4.085968017578125
client 0, data condensation 6800, total loss = 7.74530029296875, avg loss = 3.872650146484375
client 0, data condensation 7000, total loss = 8.58990478515625, avg loss = 4.294952392578125
client 0, data condensation 7200, total loss = 19.37298583984375, avg loss = 9.686492919921875
client 0, data condensation 7400, total loss = 14.7452392578125, avg loss = 7.37261962890625
client 0, data condensation 7600, total loss = 14.70513916015625, avg loss = 7.352569580078125
client 0, data condensation 7800, total loss = 12.95458984375, avg loss = 6.477294921875
client 0, data condensation 8000, total loss = 8.80841064453125, avg loss = 4.404205322265625
client 0, data condensation 8200, total loss = 68.9521484375, avg loss = 34.47607421875
client 0, data condensation 8400, total loss = 24.39300537109375, avg loss = 12.196502685546875
client 0, data condensation 8600, total loss = 12.9658203125, avg loss = 6.48291015625
client 0, data condensation 8800, total loss = 20.828857421875, avg loss = 10.4144287109375
client 0, data condensation 9000, total loss = 22.198974609375, avg loss = 11.0994873046875
client 0, data condensation 9200, total loss = 32.6883544921875, avg loss = 16.34417724609375
client 0, data condensation 9400, total loss = 3.66680908203125, avg loss = 1.833404541015625
client 0, data condensation 9600, total loss = 14.5755615234375, avg loss = 7.28778076171875
client 0, data condensation 9800, total loss = 6.44866943359375, avg loss = 3.224334716796875
client 0, data condensation 10000, total loss = 17.40264892578125, avg loss = 8.701324462890625
Round 9, client 0 condense time: 801.1766777038574
client 0, class 2 have 3593 samples
client 0, class 7 have 4999 samples
total 24576.0MB, used 2953.06MB, free 21622.94MB
total 24576.0MB, used 2953.06MB, free 21622.94MB
initialized by random noise
client 1 have real samples [175, 4958]
client 1 will condense {2: 5, 4: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 175 samples, histogram: [56  6  4  6  6  4  4  5  7 77], bin edged: [0.00398674 0.00429616 0.00460559 0.00491502 0.00522444 0.00553387
 0.00584329 0.00615272 0.00646215 0.00677157 0.007081  ]
class 4 have 4958 samples, histogram: [1970  237  169  105  102  129  121  125  190 1810], bin edged: [0.0001469  0.00015831 0.00016971 0.00018111 0.00019251 0.00020391
 0.00021531 0.00022672 0.00023812 0.00024952 0.00026092]
client 1, data condensation 0, total loss = 232.074462890625, avg loss = 116.0372314453125
client 1, data condensation 200, total loss = 553.1162109375, avg loss = 276.55810546875
client 1, data condensation 400, total loss = 389.99560546875, avg loss = 194.997802734375
client 1, data condensation 600, total loss = 2210.652587890625, avg loss = 1105.3262939453125
client 1, data condensation 800, total loss = 504.74786376953125, avg loss = 252.37393188476562
client 1, data condensation 1000, total loss = 135.2720947265625, avg loss = 67.63604736328125
client 1, data condensation 1200, total loss = 300.55902099609375, avg loss = 150.27951049804688
client 1, data condensation 1400, total loss = 189.79803466796875, avg loss = 94.89901733398438
client 1, data condensation 1600, total loss = 691.1470947265625, avg loss = 345.57354736328125
client 1, data condensation 1800, total loss = 154.3621826171875, avg loss = 77.18109130859375
client 1, data condensation 2000, total loss = 381.876220703125, avg loss = 190.9381103515625
client 1, data condensation 2200, total loss = 509.16973876953125, avg loss = 254.58486938476562
client 1, data condensation 2400, total loss = 160.833740234375, avg loss = 80.4168701171875
client 1, data condensation 2600, total loss = 722.4183349609375, avg loss = 361.20916748046875
client 1, data condensation 2800, total loss = 2440.78125, avg loss = 1220.390625
client 1, data condensation 3000, total loss = 169.17755126953125, avg loss = 84.58877563476562
client 1, data condensation 3200, total loss = 357.76556396484375, avg loss = 178.88278198242188
client 1, data condensation 3400, total loss = 1323.1934814453125, avg loss = 661.5967407226562
client 1, data condensation 3600, total loss = 1873.248779296875, avg loss = 936.6243896484375
client 1, data condensation 3800, total loss = 2558.047607421875, avg loss = 1279.0238037109375
client 1, data condensation 4000, total loss = 3102.939208984375, avg loss = 1551.4696044921875
client 1, data condensation 4200, total loss = 188.6387939453125, avg loss = 94.31939697265625
client 1, data condensation 4400, total loss = 504.6002197265625, avg loss = 252.30010986328125
client 1, data condensation 4600, total loss = 375.83599853515625, avg loss = 187.91799926757812
client 1, data condensation 4800, total loss = 223.4022216796875, avg loss = 111.70111083984375
client 1, data condensation 5000, total loss = 174.47479248046875, avg loss = 87.23739624023438
client 1, data condensation 5200, total loss = 178.1136474609375, avg loss = 89.05682373046875
client 1, data condensation 5400, total loss = 166.1824951171875, avg loss = 83.09124755859375
client 1, data condensation 5600, total loss = 811.7952270507812, avg loss = 405.8976135253906
client 1, data condensation 5800, total loss = 3229.720947265625, avg loss = 1614.8604736328125
client 1, data condensation 6000, total loss = 382.5673828125, avg loss = 191.28369140625
client 1, data condensation 6200, total loss = 804.5320434570312, avg loss = 402.2660217285156
client 1, data condensation 6400, total loss = 1983.9188232421875, avg loss = 991.9594116210938
client 1, data condensation 6600, total loss = 621.133056640625, avg loss = 310.5665283203125
client 1, data condensation 6800, total loss = 150.9998779296875, avg loss = 75.49993896484375
client 1, data condensation 7000, total loss = 2199.164306640625, avg loss = 1099.5821533203125
client 1, data condensation 7200, total loss = 174.74676513671875, avg loss = 87.37338256835938
client 1, data condensation 7400, total loss = 1402.749267578125, avg loss = 701.3746337890625
client 1, data condensation 7600, total loss = 261.79864501953125, avg loss = 130.89932250976562
client 1, data condensation 7800, total loss = 230.222900390625, avg loss = 115.1114501953125
client 1, data condensation 8000, total loss = 495.69720458984375, avg loss = 247.84860229492188
client 1, data condensation 8200, total loss = 171.8406982421875, avg loss = 85.92034912109375
client 1, data condensation 8400, total loss = 427.83111572265625, avg loss = 213.91555786132812
client 1, data condensation 8600, total loss = 150.3851318359375, avg loss = 75.19256591796875
client 1, data condensation 8800, total loss = 270.1690673828125, avg loss = 135.08453369140625
client 1, data condensation 9000, total loss = 234.6787109375, avg loss = 117.33935546875
client 1, data condensation 9200, total loss = 289.5723876953125, avg loss = 144.78619384765625
client 1, data condensation 9400, total loss = 175.856201171875, avg loss = 87.9281005859375
client 1, data condensation 9600, total loss = 245.0418701171875, avg loss = 122.52093505859375
client 1, data condensation 9800, total loss = 223.74786376953125, avg loss = 111.87393188476562
client 1, data condensation 10000, total loss = 1337.1165771484375, avg loss = 668.5582885742188
Round 9, client 1 condense time: 717.4428462982178
client 1, class 2 have 175 samples
client 1, class 4 have 4958 samples
total 24576.0MB, used 2951.06MB, free 21624.94MB
total 24576.0MB, used 2951.06MB, free 21624.94MB
initialized by random noise
client 2 have real samples [242]
client 2 will condense {9: 5} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 9 have 242 samples, histogram: [154   6   3   4   5   1   4   8   6  51], bin edged: [0.00336844 0.00362987 0.00389131 0.00415275 0.00441419 0.00467562
 0.00493706 0.0051985  0.00545994 0.00572138 0.00598281]
client 2, data condensation 0, total loss = 180.47314453125, avg loss = 180.47314453125
client 2, data condensation 200, total loss = 219.6068115234375, avg loss = 219.6068115234375
client 2, data condensation 400, total loss = 129.6651611328125, avg loss = 129.6651611328125
client 2, data condensation 600, total loss = 994.4627685546875, avg loss = 994.4627685546875
client 2, data condensation 800, total loss = 313.7803955078125, avg loss = 313.7803955078125
client 2, data condensation 1000, total loss = 252.87109375, avg loss = 252.87109375
client 2, data condensation 1200, total loss = 565.0247802734375, avg loss = 565.0247802734375
client 2, data condensation 1400, total loss = 108.7880859375, avg loss = 108.7880859375
client 2, data condensation 1600, total loss = 304.777099609375, avg loss = 304.777099609375
client 2, data condensation 1800, total loss = 467.3426513671875, avg loss = 467.3426513671875
client 2, data condensation 2000, total loss = 295.83642578125, avg loss = 295.83642578125
client 2, data condensation 2200, total loss = 98.84771728515625, avg loss = 98.84771728515625
client 2, data condensation 2400, total loss = 241.3294677734375, avg loss = 241.3294677734375
client 2, data condensation 2600, total loss = 180.40704345703125, avg loss = 180.40704345703125
client 2, data condensation 2800, total loss = 148.816162109375, avg loss = 148.816162109375
client 2, data condensation 3000, total loss = 414.6373291015625, avg loss = 414.6373291015625
client 2, data condensation 3200, total loss = 125.5037841796875, avg loss = 125.5037841796875
client 2, data condensation 3400, total loss = 71.91180419921875, avg loss = 71.91180419921875
client 2, data condensation 3600, total loss = 689.1153564453125, avg loss = 689.1153564453125
client 2, data condensation 3800, total loss = 735.21826171875, avg loss = 735.21826171875
client 2, data condensation 4000, total loss = 193.21240234375, avg loss = 193.21240234375
client 2, data condensation 4200, total loss = 113.61480712890625, avg loss = 113.61480712890625
client 2, data condensation 4400, total loss = 1419.622314453125, avg loss = 1419.622314453125
client 2, data condensation 4600, total loss = 200.51141357421875, avg loss = 200.51141357421875
client 2, data condensation 4800, total loss = 65.89794921875, avg loss = 65.89794921875
client 2, data condensation 5000, total loss = 282.84765625, avg loss = 282.84765625
client 2, data condensation 5200, total loss = 182.471435546875, avg loss = 182.471435546875
client 2, data condensation 5400, total loss = 161.29302978515625, avg loss = 161.29302978515625
client 2, data condensation 5600, total loss = 108.36102294921875, avg loss = 108.36102294921875
client 2, data condensation 5800, total loss = 120.06634521484375, avg loss = 120.06634521484375
client 2, data condensation 6000, total loss = 94.814453125, avg loss = 94.814453125
client 2, data condensation 6200, total loss = 60.95953369140625, avg loss = 60.95953369140625
client 2, data condensation 6400, total loss = 468.154052734375, avg loss = 468.154052734375
client 2, data condensation 6600, total loss = 93.2159423828125, avg loss = 93.2159423828125
client 2, data condensation 6800, total loss = 208.2679443359375, avg loss = 208.2679443359375
client 2, data condensation 7000, total loss = 108.86181640625, avg loss = 108.86181640625
client 2, data condensation 7200, total loss = 2496.260498046875, avg loss = 2496.260498046875
client 2, data condensation 7400, total loss = 271.2410888671875, avg loss = 271.2410888671875
client 2, data condensation 7600, total loss = 137.96075439453125, avg loss = 137.96075439453125
client 2, data condensation 7800, total loss = 125.8482666015625, avg loss = 125.8482666015625
client 2, data condensation 8000, total loss = 1111.387939453125, avg loss = 1111.387939453125
client 2, data condensation 8200, total loss = 143.8251953125, avg loss = 143.8251953125
client 2, data condensation 8400, total loss = 112.29510498046875, avg loss = 112.29510498046875
client 2, data condensation 8600, total loss = 102.4730224609375, avg loss = 102.4730224609375
client 2, data condensation 8800, total loss = 462.67437744140625, avg loss = 462.67437744140625
client 2, data condensation 9000, total loss = 103.6512451171875, avg loss = 103.6512451171875
client 2, data condensation 9200, total loss = 148.596923828125, avg loss = 148.596923828125
client 2, data condensation 9400, total loss = 129.64019775390625, avg loss = 129.64019775390625
client 2, data condensation 9600, total loss = 384.4937744140625, avg loss = 384.4937744140625
client 2, data condensation 9800, total loss = 194.46484375, avg loss = 194.46484375
client 2, data condensation 10000, total loss = 370.834716796875, avg loss = 370.834716796875
Round 9, client 2 condense time: 255.96042275428772
client 2, class 9 have 242 samples
total 24576.0MB, used 15952.0MB, free 8624.0MB
total 24576.0MB, used 15952.0MB, free 8624.0MB
initialized by random noise
client 3 have real samples [847, 1094]
client 3 will condense {0: 17, 2: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 847 samples, histogram: [513  25  13   8  10  20  18  19  21 200], bin edged: [0.00094658 0.00102005 0.00109351 0.00116698 0.00124045 0.00131392
 0.00138739 0.00146085 0.00153432 0.00160779 0.00168126]
class 2 have 1094 samples, histogram: [307  49  30  22  25  18  25  41  44 533], bin edged: [0.00062225 0.00067055 0.00071884 0.00076714 0.00081544 0.00086373
 0.00091203 0.00096032 0.00100862 0.00105691 0.00110521]
client 3, data condensation 0, total loss = 89.93707275390625, avg loss = 44.968536376953125
client 3, data condensation 200, total loss = 34.874755859375, avg loss = 17.4373779296875
client 3, data condensation 400, total loss = 14.79638671875, avg loss = 7.398193359375
client 3, data condensation 600, total loss = 45.14202880859375, avg loss = 22.571014404296875
client 3, data condensation 800, total loss = 14.2445068359375, avg loss = 7.12225341796875
client 3, data condensation 1000, total loss = 23.04876708984375, avg loss = 11.524383544921875
client 3, data condensation 1200, total loss = 35.79266357421875, avg loss = 17.896331787109375
client 3, data condensation 1400, total loss = 13.32110595703125, avg loss = 6.660552978515625
client 3, data condensation 1600, total loss = 30.28955078125, avg loss = 15.144775390625
client 3, data condensation 1800, total loss = 17.69354248046875, avg loss = 8.846771240234375
client 3, data condensation 2000, total loss = 13.38232421875, avg loss = 6.691162109375
client 3, data condensation 2200, total loss = 11.7501220703125, avg loss = 5.87506103515625
client 3, data condensation 2400, total loss = 22.74261474609375, avg loss = 11.371307373046875
client 3, data condensation 2600, total loss = 219.17755126953125, avg loss = 109.58877563476562
client 3, data condensation 2800, total loss = 45.766357421875, avg loss = 22.8831787109375
client 3, data condensation 3000, total loss = 21.4150390625, avg loss = 10.70751953125
client 3, data condensation 3200, total loss = 18.40057373046875, avg loss = 9.200286865234375
client 3, data condensation 3400, total loss = 43.78668212890625, avg loss = 21.893341064453125
client 3, data condensation 3600, total loss = 37.93280029296875, avg loss = 18.966400146484375
client 3, data condensation 3800, total loss = 6.18951416015625, avg loss = 3.094757080078125
client 3, data condensation 4000, total loss = 16.1507568359375, avg loss = 8.07537841796875
client 3, data condensation 4200, total loss = 14.656982421875, avg loss = 7.3284912109375
client 3, data condensation 4400, total loss = 14.67816162109375, avg loss = 7.339080810546875
client 3, data condensation 4600, total loss = 65.4197998046875, avg loss = 32.70989990234375
client 3, data condensation 4800, total loss = 19.9879150390625, avg loss = 9.99395751953125
client 3, data condensation 5000, total loss = 33.27203369140625, avg loss = 16.636016845703125
client 3, data condensation 5200, total loss = 67.60491943359375, avg loss = 33.802459716796875
client 3, data condensation 5400, total loss = 32.94329833984375, avg loss = 16.471649169921875
client 3, data condensation 5600, total loss = 18.20184326171875, avg loss = 9.100921630859375
client 3, data condensation 5800, total loss = 35.1724853515625, avg loss = 17.58624267578125
client 3, data condensation 6000, total loss = 21.54302978515625, avg loss = 10.771514892578125
client 3, data condensation 6200, total loss = 27.60845947265625, avg loss = 13.804229736328125
client 3, data condensation 6400, total loss = 36.43798828125, avg loss = 18.218994140625
client 3, data condensation 6600, total loss = 18.4710693359375, avg loss = 9.23553466796875
client 3, data condensation 6800, total loss = 62.07244873046875, avg loss = 31.036224365234375
client 3, data condensation 7000, total loss = 9.499755859375, avg loss = 4.7498779296875
client 3, data condensation 7200, total loss = 7.6990966796875, avg loss = 3.84954833984375
client 3, data condensation 7400, total loss = 10.42205810546875, avg loss = 5.211029052734375
client 3, data condensation 7600, total loss = 7.277587890625, avg loss = 3.6387939453125
client 3, data condensation 7800, total loss = 18.84222412109375, avg loss = 9.421112060546875
client 3, data condensation 8000, total loss = 25.21929931640625, avg loss = 12.609649658203125
client 3, data condensation 8200, total loss = 22.8150634765625, avg loss = 11.40753173828125
client 3, data condensation 8400, total loss = 21.17938232421875, avg loss = 10.589691162109375
client 3, data condensation 8600, total loss = 33.97802734375, avg loss = 16.989013671875
client 3, data condensation 8800, total loss = 8.67303466796875, avg loss = 4.336517333984375
client 3, data condensation 9000, total loss = 11.4173583984375, avg loss = 5.70867919921875
client 3, data condensation 9200, total loss = 23.72418212890625, avg loss = 11.862091064453125
client 3, data condensation 9400, total loss = 102.2349853515625, avg loss = 51.11749267578125
client 3, data condensation 9600, total loss = 24.35540771484375, avg loss = 12.177703857421875
client 3, data condensation 9800, total loss = 103.44293212890625, avg loss = 51.721466064453125
client 3, data condensation 10000, total loss = 39.13818359375, avg loss = 19.569091796875
Round 9, client 3 condense time: 637.9410660266876
client 3, class 0 have 847 samples
client 3, class 2 have 1094 samples
total 24576.0MB, used 16358.0MB, free 8218.0MB
total 24576.0MB, used 16358.0MB, free 8218.0MB
initialized by random noise
client 4 have real samples [4152, 307]
client 4 will condense {0: 84, 5: 7} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 0 have 4152 samples, histogram: [2536  146   79   79   86   59   59   78   99  931], bin edged: [0.00019505 0.00021019 0.00022533 0.00024047 0.0002556  0.00027074
 0.00028588 0.00030102 0.00031616 0.0003313  0.00034644]
class 5 have 307 samples, histogram: [128  10   9   8   9   4  14   4  14 107], bin edged: [0.00238832 0.00257369 0.00275905 0.00294442 0.00312979 0.00331515
 0.00350052 0.00368589 0.00387125 0.00405662 0.00424199]
client 4, data condensation 0, total loss = 135.594482421875, avg loss = 67.7972412109375
client 4, data condensation 200, total loss = 193.02294921875, avg loss = 96.511474609375
client 4, data condensation 400, total loss = 87.373291015625, avg loss = 43.6866455078125
client 4, data condensation 600, total loss = 633.0657348632812, avg loss = 316.5328674316406
client 4, data condensation 800, total loss = 739.7499389648438, avg loss = 369.8749694824219
client 4, data condensation 1000, total loss = 389.57733154296875, avg loss = 194.78866577148438
client 4, data condensation 1200, total loss = 88.4970703125, avg loss = 44.24853515625
client 4, data condensation 1400, total loss = 1059.86474609375, avg loss = 529.932373046875
client 4, data condensation 1600, total loss = 105.39715576171875, avg loss = 52.698577880859375
client 4, data condensation 1800, total loss = 89.68194580078125, avg loss = 44.840972900390625
client 4, data condensation 2000, total loss = 1178.728515625, avg loss = 589.3642578125
client 4, data condensation 2200, total loss = 510.0350341796875, avg loss = 255.01751708984375
client 4, data condensation 2400, total loss = 971.2960815429688, avg loss = 485.6480407714844
client 4, data condensation 2600, total loss = 124.14605712890625, avg loss = 62.073028564453125
client 4, data condensation 2800, total loss = 85.97662353515625, avg loss = 42.988311767578125
client 4, data condensation 3000, total loss = 80.51861572265625, avg loss = 40.259307861328125
client 4, data condensation 3200, total loss = 94.47711181640625, avg loss = 47.238555908203125
client 4, data condensation 3400, total loss = 63.436279296875, avg loss = 31.7181396484375
client 4, data condensation 3600, total loss = 196.424560546875, avg loss = 98.2122802734375
client 4, data condensation 3800, total loss = 397.3167724609375, avg loss = 198.65838623046875
client 4, data condensation 4000, total loss = 248.763427734375, avg loss = 124.3817138671875
client 4, data condensation 4200, total loss = 77.6038818359375, avg loss = 38.80194091796875
client 4, data condensation 4400, total loss = 56.62249755859375, avg loss = 28.311248779296875
client 4, data condensation 4600, total loss = 194.8056640625, avg loss = 97.40283203125
client 4, data condensation 4800, total loss = 217.465087890625, avg loss = 108.7325439453125
client 4, data condensation 5000, total loss = 79.93328857421875, avg loss = 39.966644287109375
client 4, data condensation 5200, total loss = 198.2725830078125, avg loss = 99.13629150390625
client 4, data condensation 5400, total loss = 910.9400024414062, avg loss = 455.4700012207031
client 4, data condensation 5600, total loss = 27.701171875, avg loss = 13.8505859375
client 4, data condensation 5800, total loss = 1594.47021484375, avg loss = 797.235107421875
client 4, data condensation 6000, total loss = 394.6258544921875, avg loss = 197.31292724609375
client 4, data condensation 6200, total loss = 89.777099609375, avg loss = 44.8885498046875
client 4, data condensation 6400, total loss = 223.86383056640625, avg loss = 111.93191528320312
client 4, data condensation 6600, total loss = 287.89190673828125, avg loss = 143.94595336914062
client 4, data condensation 6800, total loss = 493.7459716796875, avg loss = 246.87298583984375
client 4, data condensation 7000, total loss = 188.8380126953125, avg loss = 94.41900634765625
client 4, data condensation 7200, total loss = 92.0791015625, avg loss = 46.03955078125
client 4, data condensation 7400, total loss = 72.13763427734375, avg loss = 36.068817138671875
client 4, data condensation 7600, total loss = 68.72930908203125, avg loss = 34.364654541015625
client 4, data condensation 7800, total loss = 156.9202880859375, avg loss = 78.46014404296875
client 4, data condensation 8000, total loss = 385.40362548828125, avg loss = 192.70181274414062
client 4, data condensation 8200, total loss = 87.59173583984375, avg loss = 43.795867919921875
client 4, data condensation 8400, total loss = 366.39422607421875, avg loss = 183.19711303710938
client 4, data condensation 8600, total loss = 154.2037353515625, avg loss = 77.10186767578125
client 4, data condensation 8800, total loss = 346.37664794921875, avg loss = 173.18832397460938
client 4, data condensation 9000, total loss = 138.36407470703125, avg loss = 69.18203735351562
client 4, data condensation 9200, total loss = 63.4818115234375, avg loss = 31.74090576171875
client 4, data condensation 9400, total loss = 69.08782958984375, avg loss = 34.543914794921875
client 4, data condensation 9600, total loss = 110.86474609375, avg loss = 55.432373046875
client 4, data condensation 9800, total loss = 912.4248046875, avg loss = 456.21240234375
client 4, data condensation 10000, total loss = 189.10845947265625, avg loss = 94.55422973632812
Round 9, client 4 condense time: 753.9618155956268
client 4, class 0 have 4152 samples
client 4, class 5 have 307 samples
total 24576.0MB, used 16364.0MB, free 8212.0MB
total 24576.0MB, used 16364.0MB, free 8212.0MB
initialized by random noise
client 5 have real samples [4999]
client 5 will condense {8: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 8 have 4999 samples, histogram: [2437  189  147  101   87   82   88  109  152 1607], bin edged: [0.00015146 0.00016322 0.00017497 0.00018673 0.00019848 0.00021024
 0.00022199 0.00023375 0.0002455  0.00025726 0.00026901]
client 5, data condensation 0, total loss = 49.57830810546875, avg loss = 49.57830810546875
client 5, data condensation 200, total loss = 4.96124267578125, avg loss = 4.96124267578125
client 5, data condensation 400, total loss = 10.328125, avg loss = 10.328125
client 5, data condensation 600, total loss = 8.54888916015625, avg loss = 8.54888916015625
client 5, data condensation 800, total loss = 137.82293701171875, avg loss = 137.82293701171875
client 5, data condensation 1000, total loss = 6.00067138671875, avg loss = 6.00067138671875
client 5, data condensation 1200, total loss = 5.45538330078125, avg loss = 5.45538330078125
client 5, data condensation 1400, total loss = 2.4234619140625, avg loss = 2.4234619140625
client 5, data condensation 1600, total loss = 6.5919189453125, avg loss = 6.5919189453125
client 5, data condensation 1800, total loss = 4.822265625, avg loss = 4.822265625
client 5, data condensation 2000, total loss = 4.680908203125, avg loss = 4.680908203125
client 5, data condensation 2200, total loss = 8.106689453125, avg loss = 8.106689453125
client 5, data condensation 2400, total loss = 3.21826171875, avg loss = 3.21826171875
client 5, data condensation 2600, total loss = 11.1385498046875, avg loss = 11.1385498046875
client 5, data condensation 2800, total loss = 9.048095703125, avg loss = 9.048095703125
client 5, data condensation 3000, total loss = 37.318115234375, avg loss = 37.318115234375
client 5, data condensation 3200, total loss = 5.37249755859375, avg loss = 5.37249755859375
client 5, data condensation 3400, total loss = 29.1099853515625, avg loss = 29.1099853515625
client 5, data condensation 3600, total loss = 8.83795166015625, avg loss = 8.83795166015625
client 5, data condensation 3800, total loss = 11.18048095703125, avg loss = 11.18048095703125
client 5, data condensation 4000, total loss = 6.97320556640625, avg loss = 6.97320556640625
client 5, data condensation 4200, total loss = 53.8187255859375, avg loss = 53.8187255859375
client 5, data condensation 4400, total loss = 2.96490478515625, avg loss = 2.96490478515625
client 5, data condensation 4600, total loss = 66.860107421875, avg loss = 66.860107421875
client 5, data condensation 4800, total loss = 28.52581787109375, avg loss = 28.52581787109375
client 5, data condensation 5000, total loss = 24.1507568359375, avg loss = 24.1507568359375
client 5, data condensation 5200, total loss = 5.3382568359375, avg loss = 5.3382568359375
client 5, data condensation 5400, total loss = 3.0941162109375, avg loss = 3.0941162109375
client 5, data condensation 5600, total loss = 3.826416015625, avg loss = 3.826416015625
client 5, data condensation 5800, total loss = 11.1763916015625, avg loss = 11.1763916015625
client 5, data condensation 6000, total loss = 4.66473388671875, avg loss = 4.66473388671875
client 5, data condensation 6200, total loss = 25.3343505859375, avg loss = 25.3343505859375
client 5, data condensation 6400, total loss = 23.51214599609375, avg loss = 23.51214599609375
client 5, data condensation 6600, total loss = 9.89337158203125, avg loss = 9.89337158203125
client 5, data condensation 6800, total loss = 6.08270263671875, avg loss = 6.08270263671875
client 5, data condensation 7000, total loss = 11.602783203125, avg loss = 11.602783203125
client 5, data condensation 7200, total loss = 7.324951171875, avg loss = 7.324951171875
client 5, data condensation 7400, total loss = 3.7265625, avg loss = 3.7265625
client 5, data condensation 7600, total loss = 5.67352294921875, avg loss = 5.67352294921875
client 5, data condensation 7800, total loss = 13.196044921875, avg loss = 13.196044921875
client 5, data condensation 8000, total loss = 3.08868408203125, avg loss = 3.08868408203125
client 5, data condensation 8200, total loss = 11.1856689453125, avg loss = 11.1856689453125
client 5, data condensation 8400, total loss = 42.42645263671875, avg loss = 42.42645263671875
client 5, data condensation 8600, total loss = 4.88909912109375, avg loss = 4.88909912109375
client 5, data condensation 8800, total loss = 5.73992919921875, avg loss = 5.73992919921875
client 5, data condensation 9000, total loss = 34.154052734375, avg loss = 34.154052734375
client 5, data condensation 9200, total loss = 10.72705078125, avg loss = 10.72705078125
client 5, data condensation 9400, total loss = 6.4259033203125, avg loss = 6.4259033203125
client 5, data condensation 9600, total loss = 7.51824951171875, avg loss = 7.51824951171875
client 5, data condensation 9800, total loss = 6.9075927734375, avg loss = 6.9075927734375
client 5, data condensation 10000, total loss = 6.68316650390625, avg loss = 6.68316650390625
Round 9, client 5 condense time: 458.1821758747101
client 5, class 8 have 4999 samples
total 24576.0MB, used 15980.0MB, free 8596.0MB
total 24576.0MB, used 15980.0MB, free 8596.0MB
initialized by random noise
client 6 have real samples [4365, 3914]
client 6 will condense {5: 88, 6: 79} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 5 have 4365 samples, histogram: [1766  175  135   94   90   80   92  127  179 1627], bin edged: [0.00016652 0.00017945 0.00019237 0.0002053  0.00021822 0.00023114
 0.00024407 0.00025699 0.00026992 0.00028284 0.00029577]
class 6 have 3914 samples, histogram: [2597  118   78   66   43   46   64   50   88  764], bin edged: [0.0002121  0.00022856 0.00024502 0.00026148 0.00027794 0.0002944
 0.00031086 0.00032733 0.00034379 0.00036025 0.00037671]
client 6, data condensation 0, total loss = 106.4942626953125, avg loss = 53.24713134765625
client 6, data condensation 200, total loss = 19.02947998046875, avg loss = 9.514739990234375
client 6, data condensation 400, total loss = 12.94061279296875, avg loss = 6.470306396484375
client 6, data condensation 600, total loss = 9.24554443359375, avg loss = 4.622772216796875
client 6, data condensation 800, total loss = 8.9080810546875, avg loss = 4.45404052734375
client 6, data condensation 1000, total loss = 6.615966796875, avg loss = 3.3079833984375
client 6, data condensation 1200, total loss = 7.20782470703125, avg loss = 3.603912353515625
client 6, data condensation 1400, total loss = 7.5478515625, avg loss = 3.77392578125
client 6, data condensation 1600, total loss = 9.4681396484375, avg loss = 4.73406982421875
client 6, data condensation 1800, total loss = 17.7508544921875, avg loss = 8.87542724609375
client 6, data condensation 2000, total loss = 7.4031982421875, avg loss = 3.70159912109375
client 6, data condensation 2200, total loss = 11.8477783203125, avg loss = 5.92388916015625
client 6, data condensation 2400, total loss = 19.52642822265625, avg loss = 9.763214111328125
client 6, data condensation 2600, total loss = 17.5723876953125, avg loss = 8.78619384765625
client 6, data condensation 2800, total loss = 17.5594482421875, avg loss = 8.77972412109375
client 6, data condensation 3000, total loss = 6.3114013671875, avg loss = 3.15570068359375
client 6, data condensation 3200, total loss = 6.1048583984375, avg loss = 3.05242919921875
client 6, data condensation 3400, total loss = 5.0841064453125, avg loss = 2.54205322265625
client 6, data condensation 3600, total loss = 30.19659423828125, avg loss = 15.098297119140625
client 6, data condensation 3800, total loss = 10.44970703125, avg loss = 5.224853515625
client 6, data condensation 4000, total loss = 29.63067626953125, avg loss = 14.815338134765625
client 6, data condensation 4200, total loss = 24.31597900390625, avg loss = 12.157989501953125
client 6, data condensation 4400, total loss = 18.194091796875, avg loss = 9.0970458984375
client 6, data condensation 4600, total loss = 4.94244384765625, avg loss = 2.471221923828125
client 6, data condensation 4800, total loss = 9.4324951171875, avg loss = 4.71624755859375
client 6, data condensation 5000, total loss = 7.75018310546875, avg loss = 3.875091552734375
client 6, data condensation 5200, total loss = 7.56329345703125, avg loss = 3.781646728515625
client 6, data condensation 5400, total loss = 9.33447265625, avg loss = 4.667236328125
client 6, data condensation 5600, total loss = 23.4609375, avg loss = 11.73046875
client 6, data condensation 5800, total loss = 41.62060546875, avg loss = 20.810302734375
client 6, data condensation 6000, total loss = 9.408447265625, avg loss = 4.7042236328125
client 6, data condensation 6200, total loss = 9.74200439453125, avg loss = 4.871002197265625
client 6, data condensation 6400, total loss = 8.1588134765625, avg loss = 4.07940673828125
client 6, data condensation 6600, total loss = 16.9818115234375, avg loss = 8.49090576171875
client 6, data condensation 6800, total loss = 5.1505126953125, avg loss = 2.57525634765625
client 6, data condensation 7000, total loss = 7.6029052734375, avg loss = 3.80145263671875
client 6, data condensation 7200, total loss = 6.031494140625, avg loss = 3.0157470703125
client 6, data condensation 7400, total loss = 18.10009765625, avg loss = 9.050048828125
client 6, data condensation 7600, total loss = 12.29266357421875, avg loss = 6.146331787109375
client 6, data condensation 7800, total loss = 16.5377197265625, avg loss = 8.26885986328125
client 6, data condensation 8000, total loss = 9.30450439453125, avg loss = 4.652252197265625
client 6, data condensation 8200, total loss = 6.11962890625, avg loss = 3.059814453125
client 6, data condensation 8400, total loss = 6.3846435546875, avg loss = 3.19232177734375
client 6, data condensation 8600, total loss = 9.73187255859375, avg loss = 4.865936279296875
client 6, data condensation 8800, total loss = 34.44415283203125, avg loss = 17.222076416015625
client 6, data condensation 9000, total loss = 11.03338623046875, avg loss = 5.516693115234375
client 6, data condensation 9200, total loss = 6.6146240234375, avg loss = 3.30731201171875
client 6, data condensation 9400, total loss = 9.35357666015625, avg loss = 4.676788330078125
client 6, data condensation 9600, total loss = 19.66961669921875, avg loss = 9.834808349609375
client 6, data condensation 9800, total loss = 10.6224365234375, avg loss = 5.31121826171875
client 6, data condensation 10000, total loss = 5.0997314453125, avg loss = 2.54986572265625
Round 9, client 6 condense time: 804.0242502689362
client 6, class 5 have 4365 samples
client 6, class 6 have 3914 samples
total 24576.0MB, used 16368.0MB, free 8208.0MB
total 24576.0MB, used 16368.0MB, free 8208.0MB
initialized by random noise
client 7 have real samples [4605, 4999]
client 7 will condense {1: 93, 3: 100} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 4605 samples, histogram: [3306  161   97   58   52   51   68   54  109  649], bin edged: [0.00018709 0.00020161 0.00021613 0.00023065 0.00024517 0.00025969
 0.00027421 0.00028873 0.00030325 0.00031778 0.0003323 ]
class 3 have 4999 samples, histogram: [1837  262  175  137  113  103  140  157  230 1845], bin edged: [0.00014417 0.00015536 0.00016655 0.00017774 0.00018893 0.00020012
 0.00021131 0.0002225  0.00023369 0.00024488 0.00025607]
client 7, data condensation 0, total loss = 37.8577880859375, avg loss = 18.92889404296875
client 7, data condensation 200, total loss = 7.6221923828125, avg loss = 3.81109619140625
client 7, data condensation 400, total loss = 5.8934326171875, avg loss = 2.94671630859375
client 7, data condensation 600, total loss = 5.8123779296875, avg loss = 2.90618896484375
client 7, data condensation 800, total loss = 22.39019775390625, avg loss = 11.195098876953125
client 7, data condensation 1000, total loss = 5.83929443359375, avg loss = 2.919647216796875
client 7, data condensation 1200, total loss = 36.1463623046875, avg loss = 18.07318115234375
client 7, data condensation 1400, total loss = 5.09393310546875, avg loss = 2.546966552734375
client 7, data condensation 1600, total loss = 36.646728515625, avg loss = 18.3233642578125
client 7, data condensation 1800, total loss = 19.93182373046875, avg loss = 9.965911865234375
client 7, data condensation 2000, total loss = 7.15899658203125, avg loss = 3.579498291015625
client 7, data condensation 2200, total loss = 21.90545654296875, avg loss = 10.952728271484375
client 7, data condensation 2400, total loss = 13.0386962890625, avg loss = 6.51934814453125
client 7, data condensation 2600, total loss = 10.64849853515625, avg loss = 5.324249267578125
client 7, data condensation 2800, total loss = 7.308837890625, avg loss = 3.6544189453125
client 7, data condensation 3000, total loss = 13.193115234375, avg loss = 6.5965576171875
client 7, data condensation 3200, total loss = 9.87451171875, avg loss = 4.937255859375
client 7, data condensation 3400, total loss = 18.7396240234375, avg loss = 9.36981201171875
client 7, data condensation 3600, total loss = 11.8094482421875, avg loss = 5.90472412109375
client 7, data condensation 3800, total loss = 24.14520263671875, avg loss = 12.072601318359375
client 7, data condensation 4000, total loss = 31.84210205078125, avg loss = 15.921051025390625
client 7, data condensation 4200, total loss = 58.88409423828125, avg loss = 29.442047119140625
client 7, data condensation 4400, total loss = 6.569580078125, avg loss = 3.2847900390625
client 7, data condensation 4600, total loss = 16.6070556640625, avg loss = 8.30352783203125
client 7, data condensation 4800, total loss = 5.42718505859375, avg loss = 2.713592529296875
client 7, data condensation 5000, total loss = 13.618896484375, avg loss = 6.8094482421875
client 7, data condensation 5200, total loss = 24.76458740234375, avg loss = 12.382293701171875
client 7, data condensation 5400, total loss = 11.6531982421875, avg loss = 5.82659912109375
client 7, data condensation 5600, total loss = 7.84747314453125, avg loss = 3.923736572265625
client 7, data condensation 5800, total loss = 10.82928466796875, avg loss = 5.414642333984375
client 7, data condensation 6000, total loss = 25.655029296875, avg loss = 12.8275146484375
client 7, data condensation 6200, total loss = 7.4749755859375, avg loss = 3.73748779296875
client 7, data condensation 6400, total loss = 15.891845703125, avg loss = 7.9459228515625
client 7, data condensation 6600, total loss = 16.0064697265625, avg loss = 8.00323486328125
client 7, data condensation 6800, total loss = 7.70745849609375, avg loss = 3.853729248046875
client 7, data condensation 7000, total loss = 8.51617431640625, avg loss = 4.258087158203125
client 7, data condensation 7200, total loss = 10.92425537109375, avg loss = 5.462127685546875
client 7, data condensation 7400, total loss = 11.75299072265625, avg loss = 5.876495361328125
client 7, data condensation 7600, total loss = 13.43951416015625, avg loss = 6.719757080078125
client 7, data condensation 7800, total loss = 6.25958251953125, avg loss = 3.129791259765625
client 7, data condensation 8000, total loss = 7.89007568359375, avg loss = 3.945037841796875
client 7, data condensation 8200, total loss = 8.37274169921875, avg loss = 4.186370849609375
client 7, data condensation 8400, total loss = 9.3831787109375, avg loss = 4.69158935546875
client 7, data condensation 8600, total loss = 6.48516845703125, avg loss = 3.242584228515625
client 7, data condensation 8800, total loss = 6.302001953125, avg loss = 3.1510009765625
client 7, data condensation 9000, total loss = 32.7271728515625, avg loss = 16.36358642578125
client 7, data condensation 9200, total loss = 14.58441162109375, avg loss = 7.292205810546875
client 7, data condensation 9400, total loss = 8.56683349609375, avg loss = 4.283416748046875
client 7, data condensation 9600, total loss = 16.9569091796875, avg loss = 8.47845458984375
client 7, data condensation 9800, total loss = 7.85302734375, avg loss = 3.926513671875
client 7, data condensation 10000, total loss = 5.60162353515625, avg loss = 2.800811767578125
Round 9, client 7 condense time: 848.0768992900848
client 7, class 1 have 4605 samples
client 7, class 3 have 4999 samples
total 24576.0MB, used 16368.0MB, free 8208.0MB
total 24576.0MB, used 16368.0MB, free 8208.0MB
initialized by random noise
client 8 have real samples [364, 135, 4727]
client 8 will condense {1: 8, 5: 5, 9: 95} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 1 have 364 samples, histogram: [258  11   5   7   3   1   9   5   8  57], bin edged: [0.0023407  0.00252237 0.00270404 0.00288571 0.00306738 0.00324905
 0.00343072 0.00361239 0.00379406 0.00397573 0.0041574 ]
class 5 have 135 samples, histogram: [59  2  4  1  4  0  1  5  4 55], bin edged: [0.00536082 0.00577689 0.00619297 0.00660904 0.00702511 0.00744119
 0.00785726 0.00827334 0.00868941 0.00910549 0.00952156]
class 9 have 4727 samples, histogram: [3234  170   91   57   54   48   80   83  118  792], bin edged: [0.00017832 0.00019216 0.000206   0.00021984 0.00023368 0.00024752
 0.00026136 0.0002752  0.00028904 0.00030288 0.00031672]
client 8, data condensation 0, total loss = 352.05792236328125, avg loss = 117.35264078776042
client 8, data condensation 200, total loss = 697.7899169921875, avg loss = 232.59663899739584
client 8, data condensation 400, total loss = 378.42578125, avg loss = 126.14192708333333
client 8, data condensation 600, total loss = 1346.260986328125, avg loss = 448.753662109375
client 8, data condensation 800, total loss = 502.7510986328125, avg loss = 167.58369954427084
client 8, data condensation 1000, total loss = 189.93316650390625, avg loss = 63.311055501302086
client 8, data condensation 1200, total loss = 284.8902587890625, avg loss = 94.96341959635417
client 8, data condensation 1400, total loss = 1665.1925048828125, avg loss = 555.0641682942709
client 8, data condensation 1600, total loss = 399.31207275390625, avg loss = 133.1040242513021
client 8, data condensation 1800, total loss = 160.99462890625, avg loss = 53.664876302083336
client 8, data condensation 2000, total loss = 148.24969482421875, avg loss = 49.41656494140625
client 8, data condensation 2200, total loss = 197.53387451171875, avg loss = 65.84462483723958
client 8, data condensation 2400, total loss = 128.598388671875, avg loss = 42.866129557291664
client 8, data condensation 2600, total loss = 798.2686157226562, avg loss = 266.08953857421875
client 8, data condensation 2800, total loss = 268.01409912109375, avg loss = 89.33803304036458
client 8, data condensation 3000, total loss = 187.64788818359375, avg loss = 62.549296061197914
client 8, data condensation 3200, total loss = 1528.329833984375, avg loss = 509.4432779947917
client 8, data condensation 3400, total loss = 286.87652587890625, avg loss = 95.62550862630208
client 8, data condensation 3600, total loss = 137.752685546875, avg loss = 45.917561848958336
client 8, data condensation 3800, total loss = 240.838134765625, avg loss = 80.27937825520833
client 8, data condensation 4000, total loss = 234.83294677734375, avg loss = 78.27764892578125
client 8, data condensation 4200, total loss = 998.3303833007812, avg loss = 332.77679443359375
client 8, data condensation 4400, total loss = 174.79876708984375, avg loss = 58.266255696614586
client 8, data condensation 4600, total loss = 157.646484375, avg loss = 52.548828125
client 8, data condensation 4800, total loss = 150.45147705078125, avg loss = 50.150492350260414
client 8, data condensation 5000, total loss = 604.2852172851562, avg loss = 201.42840576171875
client 8, data condensation 5200, total loss = 143.24224853515625, avg loss = 47.747416178385414
client 8, data condensation 5400, total loss = 164.32275390625, avg loss = 54.774251302083336
client 8, data condensation 5600, total loss = 448.89422607421875, avg loss = 149.63140869140625
client 8, data condensation 5800, total loss = 217.29742431640625, avg loss = 72.43247477213542
client 8, data condensation 6000, total loss = 121.79132080078125, avg loss = 40.59710693359375
client 8, data condensation 6200, total loss = 110.40362548828125, avg loss = 36.80120849609375
client 8, data condensation 6400, total loss = 2312.298828125, avg loss = 770.7662760416666
client 8, data condensation 6600, total loss = 602.5453491210938, avg loss = 200.84844970703125
client 8, data condensation 6800, total loss = 268.5313720703125, avg loss = 89.51045735677083
client 8, data condensation 7000, total loss = 793.658935546875, avg loss = 264.552978515625
client 8, data condensation 7200, total loss = 142.26336669921875, avg loss = 47.421122233072914
client 8, data condensation 7400, total loss = 114.37603759765625, avg loss = 38.125345865885414
client 8, data condensation 7600, total loss = 442.13287353515625, avg loss = 147.37762451171875
client 8, data condensation 7800, total loss = 131.8140869140625, avg loss = 43.938028971354164
client 8, data condensation 8000, total loss = 831.5403442382812, avg loss = 277.18011474609375
client 8, data condensation 8200, total loss = 340.56927490234375, avg loss = 113.52309163411458
client 8, data condensation 8400, total loss = 141.63330078125, avg loss = 47.211100260416664
client 8, data condensation 8600, total loss = 274.5009765625, avg loss = 91.50032552083333
client 8, data condensation 8800, total loss = 2556.05859375, avg loss = 852.01953125
client 8, data condensation 9000, total loss = 282.783203125, avg loss = 94.26106770833333
client 8, data condensation 9200, total loss = 118.8914794921875, avg loss = 39.6304931640625
client 8, data condensation 9400, total loss = 190.20953369140625, avg loss = 63.403177897135414
client 8, data condensation 9600, total loss = 161.48614501953125, avg loss = 53.828715006510414
client 8, data condensation 9800, total loss = 243.08428955078125, avg loss = 81.02809651692708
client 8, data condensation 10000, total loss = 820.3384399414062, avg loss = 273.44614664713544
Round 9, client 8 condense time: 1019.4168388843536
client 8, class 1 have 364 samples
client 8, class 5 have 135 samples
client 8, class 9 have 4727 samples
total 24576.0MB, used 16622.0MB, free 7954.0MB
total 24576.0MB, used 16622.0MB, free 7954.0MB
initialized by random noise
client 9 have real samples [120, 192, 1075]
client 9 will condense {2: 5, 5: 5, 6: 22} samples for each class it owns
get_feature_prototype
get_logit_prototype
loss weighted matching the samples
class 2 have 120 samples, histogram: [48  4  3  5  2  1  2  7  4 44], bin edged: [0.0060465  0.00651579 0.00698508 0.00745437 0.00792367 0.00839296
 0.00886225 0.00933155 0.00980084 0.01027013 0.01073942]
class 5 have 192 samples, histogram: [79  6  6  1  5  0  3  6  8 78], bin edged: [0.00374865 0.0040396  0.00433055 0.00462149 0.00491244 0.00520339
 0.00549434 0.00578528 0.00607623 0.00636718 0.00665813]
class 6 have 1075 samples, histogram: [721  36  24  15  12  12  15  22  27 191], bin edged: [0.00077776 0.00083813 0.00089849 0.00095886 0.00101923 0.00107959
 0.00113996 0.00120032 0.00126069 0.00132105 0.00138142]
client 9, data condensation 0, total loss = 356.16143798828125, avg loss = 118.72047932942708
client 9, data condensation 200, total loss = 250.1854248046875, avg loss = 83.3951416015625
client 9, data condensation 400, total loss = 1021.9873046875, avg loss = 340.6624348958333
client 9, data condensation 600, total loss = 1687.58935546875, avg loss = 562.52978515625
client 9, data condensation 800, total loss = 540.6760864257812, avg loss = 180.2253621419271
client 9, data condensation 1000, total loss = 1822.25, avg loss = 607.4166666666666
client 9, data condensation 1200, total loss = 965.4623413085938, avg loss = 321.82078043619794
client 9, data condensation 1400, total loss = 1209.915283203125, avg loss = 403.3050944010417
client 9, data condensation 1600, total loss = 409.6143798828125, avg loss = 136.53812662760416
client 9, data condensation 1800, total loss = 527.8694458007812, avg loss = 175.95648193359375
client 9, data condensation 2000, total loss = 1365.070068359375, avg loss = 455.0233561197917
client 9, data condensation 2200, total loss = 216.701416015625, avg loss = 72.23380533854167
client 9, data condensation 2400, total loss = 2815.038330078125, avg loss = 938.3461100260416
client 9, data condensation 2600, total loss = 794.7210083007812, avg loss = 264.90700276692706
client 9, data condensation 2800, total loss = 349.8331298828125, avg loss = 116.61104329427083
client 9, data condensation 3000, total loss = 390.77880859375, avg loss = 130.25960286458334
client 9, data condensation 3200, total loss = 1810.85546875, avg loss = 603.6184895833334
client 9, data condensation 3400, total loss = 1349.42138671875, avg loss = 449.80712890625
client 9, data condensation 3600, total loss = 237.547607421875, avg loss = 79.18253580729167
client 9, data condensation 3800, total loss = 593.2271118164062, avg loss = 197.74237060546875
client 9, data condensation 4000, total loss = 495.93475341796875, avg loss = 165.31158447265625
client 9, data condensation 4200, total loss = 3558.23583984375, avg loss = 1186.07861328125
client 9, data condensation 4400, total loss = 311.42340087890625, avg loss = 103.80780029296875
client 9, data condensation 4600, total loss = 3186.119873046875, avg loss = 1062.0399576822917
client 9, data condensation 4800, total loss = 3658.2373046875, avg loss = 1219.4124348958333
client 9, data condensation 5000, total loss = 337.10870361328125, avg loss = 112.36956787109375
client 9, data condensation 5200, total loss = 227.009765625, avg loss = 75.669921875
client 9, data condensation 5400, total loss = 221.18597412109375, avg loss = 73.72865804036458
client 9, data condensation 5600, total loss = 207.2255859375, avg loss = 69.0751953125
client 9, data condensation 5800, total loss = 888.5464477539062, avg loss = 296.18214925130206
client 9, data condensation 6000, total loss = 1347.636962890625, avg loss = 449.2123209635417
client 9, data condensation 6200, total loss = 515.064697265625, avg loss = 171.688232421875
client 9, data condensation 6400, total loss = 466.66143798828125, avg loss = 155.5538126627604
client 9, data condensation 6600, total loss = 761.2031860351562, avg loss = 253.7343953450521
client 9, data condensation 6800, total loss = 409.77020263671875, avg loss = 136.5900675455729
client 9, data condensation 7000, total loss = 1272.261962890625, avg loss = 424.0873209635417
client 9, data condensation 7200, total loss = 283.5242919921875, avg loss = 94.50809733072917
client 9, data condensation 7400, total loss = 1558.9683837890625, avg loss = 519.6561279296875
client 9, data condensation 7600, total loss = 283.222412109375, avg loss = 94.407470703125
client 9, data condensation 7800, total loss = 644.6769409179688, avg loss = 214.8923136393229
client 9, data condensation 8000, total loss = 504.38262939453125, avg loss = 168.1275431315104
client 9, data condensation 8200, total loss = 587.5121459960938, avg loss = 195.8373819986979
client 9, data condensation 8400, total loss = 343.70343017578125, avg loss = 114.56781005859375
client 9, data condensation 8600, total loss = 310.91900634765625, avg loss = 103.63966878255208
client 9, data condensation 8800, total loss = 611.888427734375, avg loss = 203.96280924479166
client 9, data condensation 9000, total loss = 757.744140625, avg loss = 252.58138020833334
client 9, data condensation 9200, total loss = 289.1290283203125, avg loss = 96.3763427734375
client 9, data condensation 9400, total loss = 2682.430419921875, avg loss = 894.1434733072916
client 9, data condensation 9600, total loss = 589.9810791015625, avg loss = 196.66035970052084
client 9, data condensation 9800, total loss = 408.8128662109375, avg loss = 136.27095540364584
client 9, data condensation 10000, total loss = 401.2340087890625, avg loss = 133.74466959635416
Round 9, client 9 condense time: 782.3423035144806
client 9, class 2 have 120 samples
client 9, class 5 have 192 samples
client 9, class 6 have 1075 samples
total 24576.0MB, used 3207.06MB, free 21368.94MB
server receives {0: 101, 1: 101, 2: 104, 3: 100, 4: 100, 5: 105, 6: 101, 7: 100, 8: 100, 9: 100} condensed samples for each class
logit_proto before softmax: tensor([[ 1.8940e+01,  1.7067e+00,  3.8013e+00, -5.2200e+00, -6.0852e-01,
         -1.0843e+01, -1.2557e+01, -3.5950e+00,  6.8429e+00,  2.0443e+00],
        [ 1.4370e+00,  1.9602e+01, -5.9373e+00, -2.8913e+00, -5.4957e+00,
         -8.2202e+00, -6.6376e+00, -2.0895e+00,  1.1505e+00,  1.0348e+01],
        [ 3.6500e-01, -5.9666e+00,  1.1359e+01,  1.8147e+00,  4.9200e+00,
          1.4225e+00,  2.6884e-01,  1.3700e+00, -8.8485e+00, -5.7576e+00],
        [-4.9549e+00, -3.8285e+00,  2.1887e+00,  1.1322e+01, -1.2838e-02,
          7.2243e+00,  2.5232e+00,  7.0386e-01, -1.0268e+01, -3.9352e+00],
        [-3.8582e+00, -5.5156e+00,  6.0698e+00, -4.4346e-01,  1.2218e+01,
          9.5266e-01,  3.8044e+00,  4.7134e+00, -1.1257e+01, -5.9014e+00],
        [-7.4424e+00, -5.2988e+00,  3.6098e+00,  9.7350e+00,  2.5258e-01,
          1.3318e+01,  5.7616e-01,  3.2485e+00, -1.1707e+01, -5.1070e+00],
        [-8.0378e+00, -3.4278e+00,  4.2072e+00,  4.8206e+00,  6.4365e+00,
          1.8923e+00,  1.7860e+01, -1.9451e+00, -1.5299e+01, -5.7048e+00],
        [-4.3745e+00, -3.2685e+00,  2.2630e+00, -2.4154e-01,  5.5032e+00,
          1.1337e+00, -2.8866e+00,  1.5878e+01, -1.1987e+01, -7.8607e-01],
        [ 1.2409e+01,  5.8394e+00, -1.7010e+00, -5.0504e+00, -3.8890e+00,
         -1.1370e+01, -1.3736e+01, -6.1467e+00,  1.8556e+01,  6.0521e+00],
        [ 6.3130e-01,  9.2067e+00, -5.4620e+00, -2.3188e+00, -5.8248e+00,
         -7.3450e+00, -7.7862e+00,  9.5805e-01,  5.7545e-01,  1.8433e+01]],
       device='cuda:2')
shape of prototypes in tensor: torch.Size([10, 2048])
shape of logit prototypes in tensor: torch.Size([10, 10])
relation tensor: tensor([[0, 8, 2, 9, 1],
        [1, 9, 0, 8, 7],
        [2, 4, 3, 5, 7],
        [3, 5, 6, 2, 7],
        [4, 2, 7, 6, 5],
        [5, 3, 2, 7, 6],
        [6, 4, 3, 2, 5],
        [7, 4, 2, 5, 3],
        [8, 0, 9, 1, 2],
        [9, 1, 7, 0, 8]], device='cuda:2')
---------- update global model ----------
1012
preserve threshold: 10
10
Round 9: # synthetic sample: 10120
total 24576.0MB, used 3207.06MB, free 21368.94MB
{0: {0: 700, 1: 38, 2: 49, 3: 33, 4: 8, 5: 3, 6: 16, 7: 15, 8: 82, 9: 56}, 1: {0: 28, 1: 747, 2: 4, 3: 20, 4: 11, 5: 4, 6: 17, 7: 15, 8: 23, 9: 131}, 2: {0: 121, 1: 10, 2: 400, 3: 108, 4: 115, 5: 61, 6: 98, 7: 41, 8: 23, 9: 23}, 3: {0: 31, 1: 27, 2: 77, 3: 473, 4: 53, 5: 158, 6: 98, 7: 37, 8: 16, 9: 30}, 4: {0: 60, 1: 6, 2: 86, 3: 65, 4: 519, 5: 28, 6: 112, 7: 92, 8: 16, 9: 16}, 5: {0: 19, 1: 9, 2: 62, 3: 223, 4: 62, 5: 469, 6: 52, 7: 53, 8: 20, 9: 31}, 6: {0: 9, 1: 3, 2: 49, 3: 70, 4: 42, 5: 26, 6: 777, 7: 7, 8: 5, 9: 12}, 7: {0: 37, 1: 7, 2: 29, 3: 59, 4: 60, 5: 77, 6: 22, 7: 652, 8: 5, 9: 52}, 8: {0: 131, 1: 82, 2: 7, 3: 25, 4: 3, 5: 2, 6: 10, 7: 8, 8: 659, 9: 73}, 9: {0: 40, 1: 107, 2: 3, 3: 23, 4: 6, 5: 9, 6: 13, 7: 17, 8: 23, 9: 759}}
round 9 evaluation: test acc is 0.6155, test loss = 2.626914
{0: {0: 724, 1: 28, 2: 51, 3: 22, 4: 13, 5: 5, 6: 14, 7: 19, 8: 74, 9: 50}, 1: {0: 45, 1: 698, 2: 11, 3: 21, 4: 13, 5: 3, 6: 21, 7: 21, 8: 30, 9: 137}, 2: {0: 126, 1: 6, 2: 456, 3: 70, 4: 108, 5: 79, 6: 81, 7: 41, 8: 17, 9: 16}, 3: {0: 42, 1: 18, 2: 104, 3: 393, 4: 61, 5: 186, 6: 108, 7: 43, 8: 17, 9: 28}, 4: {0: 65, 1: 5, 2: 122, 3: 40, 4: 513, 5: 37, 6: 108, 7: 87, 8: 15, 9: 8}, 5: {0: 23, 1: 4, 2: 75, 3: 155, 4: 66, 5: 524, 6: 51, 7: 59, 8: 22, 9: 21}, 6: {0: 9, 1: 4, 2: 75, 3: 53, 4: 32, 5: 33, 6: 772, 7: 7, 8: 4, 9: 11}, 7: {0: 36, 1: 5, 2: 55, 3: 46, 4: 67, 5: 81, 6: 24, 7: 650, 8: 8, 9: 28}, 8: {0: 155, 1: 75, 2: 9, 3: 19, 4: 7, 5: 3, 6: 10, 7: 9, 8: 644, 9: 69}, 9: {0: 50, 1: 100, 2: 11, 3: 20, 4: 6, 5: 7, 6: 15, 7: 23, 8: 23, 9: 745}}
epoch 0, train loss avg now = 0.024501, train contrast loss now = 1.592263, test acc now = 0.6119, test loss now = 2.811885
{0: {0: 650, 1: 42, 2: 52, 3: 40, 4: 13, 5: 4, 6: 19, 7: 15, 8: 94, 9: 71}, 1: {0: 27, 1: 719, 2: 3, 3: 24, 4: 13, 5: 3, 6: 20, 7: 7, 8: 21, 9: 163}, 2: {0: 100, 1: 18, 2: 344, 3: 131, 4: 158, 5: 39, 6: 115, 7: 50, 8: 18, 9: 27}, 3: {0: 25, 1: 20, 2: 67, 3: 542, 4: 56, 5: 108, 6: 107, 7: 29, 8: 14, 9: 32}, 4: {0: 48, 1: 9, 2: 58, 3: 83, 4: 530, 5: 19, 6: 133, 7: 89, 8: 17, 9: 14}, 5: {0: 13, 1: 12, 2: 58, 3: 350, 4: 70, 5: 351, 6: 59, 7: 44, 8: 18, 9: 25}, 6: {0: 5, 1: 7, 2: 41, 3: 68, 4: 47, 5: 19, 6: 793, 7: 4, 8: 5, 9: 11}, 7: {0: 35, 1: 9, 2: 31, 3: 97, 4: 66, 5: 52, 6: 24, 7: 626, 8: 7, 9: 53}, 8: {0: 122, 1: 85, 2: 7, 3: 28, 4: 5, 5: 1, 6: 16, 7: 8, 8: 630, 9: 98}, 9: {0: 32, 1: 90, 2: 4, 3: 27, 4: 6, 5: 4, 6: 12, 7: 11, 8: 18, 9: 796}}
epoch 100, train loss avg now = 0.008289, train contrast loss now = 0.592517, test acc now = 0.5981, test loss now = 2.672997
{0: {0: 668, 1: 42, 2: 60, 3: 30, 4: 9, 5: 7, 6: 14, 7: 18, 8: 86, 9: 66}, 1: {0: 29, 1: 723, 2: 7, 3: 14, 4: 10, 5: 6, 6: 17, 7: 14, 8: 22, 9: 158}, 2: {0: 114, 1: 12, 2: 404, 3: 91, 4: 123, 5: 63, 6: 99, 7: 48, 8: 22, 9: 24}, 3: {0: 28, 1: 25, 2: 78, 3: 432, 4: 57, 5: 185, 6: 90, 7: 39, 8: 21, 9: 45}, 4: {0: 58, 1: 9, 2: 88, 3: 53, 4: 500, 5: 34, 6: 108, 7: 116, 8: 18, 9: 16}, 5: {0: 20, 1: 9, 2: 56, 3: 190, 4: 60, 5: 507, 6: 39, 7: 71, 8: 20, 9: 28}, 6: {0: 7, 1: 6, 2: 43, 3: 53, 4: 49, 5: 40, 6: 770, 7: 8, 8: 7, 9: 17}, 7: {0: 40, 1: 10, 2: 35, 3: 44, 4: 54, 5: 79, 6: 15, 7: 672, 8: 8, 9: 43}, 8: {0: 136, 1: 85, 2: 8, 3: 22, 4: 7, 5: 3, 6: 9, 7: 8, 8: 634, 9: 88}, 9: {0: 41, 1: 95, 2: 1, 3: 20, 4: 7, 5: 8, 6: 11, 7: 19, 8: 17, 9: 781}}
epoch 200, train loss avg now = 0.005646, train contrast loss now = 0.479571, test acc now = 0.6091, test loss now = 2.727878
{0: {0: 678, 1: 42, 2: 51, 3: 23, 4: 12, 5: 6, 6: 21, 7: 12, 8: 100, 9: 55}, 1: {0: 27, 1: 787, 2: 4, 3: 18, 4: 13, 5: 3, 6: 15, 7: 5, 8: 26, 9: 102}, 2: {0: 112, 1: 20, 2: 410, 3: 89, 4: 132, 5: 51, 6: 103, 7: 33, 8: 31, 9: 19}, 3: {0: 38, 1: 31, 2: 70, 3: 482, 4: 55, 5: 146, 6: 105, 7: 19, 8: 23, 9: 31}, 4: {0: 62, 1: 7, 2: 77, 3: 65, 4: 531, 5: 25, 6: 131, 7: 66, 8: 22, 9: 14}, 5: {0: 23, 1: 12, 2: 64, 3: 222, 4: 68, 5: 464, 6: 56, 7: 42, 8: 24, 9: 25}, 6: {0: 9, 1: 5, 2: 51, 3: 62, 4: 36, 5: 26, 6: 787, 7: 5, 8: 7, 9: 12}, 7: {0: 41, 1: 17, 2: 34, 3: 64, 4: 73, 5: 76, 6: 27, 7: 612, 8: 7, 9: 49}, 8: {0: 121, 1: 94, 2: 6, 3: 19, 4: 6, 5: 2, 6: 8, 7: 5, 8: 681, 9: 58}, 9: {0: 39, 1: 145, 2: 5, 3: 19, 4: 5, 5: 7, 6: 15, 7: 10, 8: 25, 9: 730}}
epoch 300, train loss avg now = 0.003868, train contrast loss now = 0.460982, test acc now = 0.6162, test loss now = 2.724568
{0: {0: 679, 1: 41, 2: 47, 3: 34, 4: 12, 5: 5, 6: 15, 7: 12, 8: 97, 9: 58}, 1: {0: 26, 1: 751, 2: 2, 3: 21, 4: 9, 5: 5, 6: 19, 7: 9, 8: 22, 9: 136}, 2: {0: 118, 1: 15, 2: 386, 3: 108, 4: 125, 5: 66, 6: 95, 7: 42, 8: 18, 9: 27}, 3: {0: 33, 1: 22, 2: 69, 3: 478, 4: 45, 5: 179, 6: 92, 7: 31, 8: 16, 9: 35}, 4: {0: 59, 1: 9, 2: 80, 3: 78, 4: 507, 5: 42, 6: 114, 7: 82, 8: 13, 9: 16}, 5: {0: 19, 1: 6, 2: 50, 3: 222, 4: 56, 5: 513, 6: 48, 7: 45, 8: 15, 9: 26}, 6: {0: 8, 1: 5, 2: 45, 3: 67, 4: 31, 5: 41, 6: 779, 7: 8, 8: 5, 9: 11}, 7: {0: 40, 1: 11, 2: 25, 3: 56, 4: 61, 5: 97, 6: 23, 7: 630, 8: 6, 9: 51}, 8: {0: 127, 1: 94, 2: 5, 3: 22, 4: 5, 5: 2, 6: 11, 7: 8, 8: 652, 9: 74}, 9: {0: 41, 1: 99, 2: 4, 3: 20, 4: 6, 5: 9, 6: 11, 7: 10, 8: 17, 9: 783}}
epoch 400, train loss avg now = 0.002397, train contrast loss now = 0.454217, test acc now = 0.6158, test loss now = 2.751430
At epoch 500, decay the con_beta with 0.1 factor
{0: {0: 684, 1: 33, 2: 52, 3: 27, 4: 11, 5: 7, 6: 20, 7: 17, 8: 84, 9: 65}, 1: {0: 24, 1: 740, 2: 4, 3: 20, 4: 10, 5: 5, 6: 17, 7: 12, 8: 21, 9: 147}, 2: {0: 130, 1: 13, 2: 386, 3: 98, 4: 127, 5: 60, 6: 103, 7: 41, 8: 18, 9: 24}, 3: {0: 31, 1: 27, 2: 70, 3: 435, 4: 59, 5: 178, 6: 111, 7: 34, 8: 19, 9: 36}, 4: {0: 57, 1: 8, 2: 74, 3: 57, 4: 527, 5: 30, 6: 121, 7: 88, 8: 16, 9: 22}, 5: {0: 19, 1: 8, 2: 50, 3: 216, 4: 68, 5: 494, 6: 49, 7: 48, 8: 19, 9: 29}, 6: {0: 5, 1: 5, 2: 43, 3: 58, 4: 46, 5: 27, 6: 787, 7: 9, 8: 7, 9: 13}, 7: {0: 36, 1: 8, 2: 32, 3: 54, 4: 70, 5: 80, 6: 20, 7: 645, 8: 8, 9: 47}, 8: {0: 133, 1: 77, 2: 8, 3: 19, 4: 5, 5: 3, 6: 11, 7: 11, 8: 652, 9: 81}, 9: {0: 40, 1: 92, 2: 3, 3: 22, 4: 8, 5: 8, 6: 14, 7: 13, 8: 16, 9: 784}}
epoch 500, train loss avg now = 0.002947, train contrast loss now = 0.451543, test acc now = 0.6134, test loss now = 2.735366
{0: {0: 693, 1: 33, 2: 50, 3: 27, 4: 11, 5: 6, 6: 16, 7: 14, 8: 91, 9: 59}, 1: {0: 30, 1: 743, 2: 4, 3: 21, 4: 8, 5: 4, 6: 18, 7: 14, 8: 25, 9: 133}, 2: {0: 130, 1: 12, 2: 386, 3: 103, 4: 127, 5: 56, 6: 105, 7: 41, 8: 22, 9: 18}, 3: {0: 32, 1: 27, 2: 69, 3: 472, 4: 51, 5: 166, 6: 99, 7: 33, 8: 17, 9: 34}, 4: {0: 60, 1: 6, 2: 76, 3: 70, 4: 498, 5: 37, 6: 127, 7: 95, 8: 16, 9: 15}, 5: {0: 22, 1: 8, 2: 52, 3: 224, 4: 58, 5: 494, 6: 45, 7: 50, 8: 20, 9: 27}, 6: {0: 5, 1: 5, 2: 50, 3: 60, 4: 38, 5: 35, 6: 778, 7: 9, 8: 7, 9: 13}, 7: {0: 35, 1: 8, 2: 30, 3: 56, 4: 63, 5: 87, 6: 24, 7: 645, 8: 8, 9: 44}, 8: {0: 126, 1: 85, 2: 7, 3: 20, 4: 4, 5: 2, 6: 10, 7: 9, 8: 670, 9: 67}, 9: {0: 44, 1: 98, 2: 3, 3: 21, 4: 6, 5: 9, 6: 13, 7: 10, 8: 20, 9: 776}}
epoch 600, train loss avg now = 0.001897, train contrast loss now = 0.450389, test acc now = 0.6155, test loss now = 2.739991
{0: {0: 699, 1: 36, 2: 47, 3: 27, 4: 9, 5: 6, 6: 17, 7: 15, 8: 87, 9: 57}, 1: {0: 32, 1: 756, 2: 4, 3: 23, 4: 7, 5: 5, 6: 15, 7: 11, 8: 26, 9: 121}, 2: {0: 129, 1: 15, 2: 392, 3: 101, 4: 128, 5: 59, 6: 96, 7: 40, 8: 21, 9: 19}, 3: {0: 33, 1: 29, 2: 72, 3: 470, 4: 50, 5: 166, 6: 89, 7: 38, 8: 20, 9: 33}, 4: {0: 58, 1: 7, 2: 84, 3: 69, 4: 517, 5: 35, 6: 104, 7: 95, 8: 17, 9: 14}, 5: {0: 20, 1: 9, 2: 54, 3: 227, 4: 57, 5: 496, 6: 44, 7: 48, 8: 19, 9: 26}, 6: {0: 7, 1: 5, 2: 50, 3: 67, 4: 40, 5: 36, 6: 768, 7: 9, 8: 6, 9: 12}, 7: {0: 37, 1: 8, 2: 29, 3: 58, 4: 66, 5: 88, 6: 18, 7: 647, 8: 8, 9: 41}, 8: {0: 137, 1: 83, 2: 9, 3: 22, 4: 5, 5: 2, 6: 10, 7: 8, 8: 658, 9: 66}, 9: {0: 45, 1: 110, 2: 3, 3: 22, 4: 8, 5: 8, 6: 12, 7: 10, 8: 20, 9: 762}}
epoch 700, train loss avg now = 0.001201, train contrast loss now = 0.450544, test acc now = 0.6165, test loss now = 2.641069
{0: {0: 691, 1: 37, 2: 52, 3: 27, 4: 12, 5: 6, 6: 17, 7: 13, 8: 85, 9: 60}, 1: {0: 30, 1: 744, 2: 4, 3: 24, 4: 10, 5: 5, 6: 17, 7: 10, 8: 23, 9: 133}, 2: {0: 127, 1: 12, 2: 395, 3: 97, 4: 132, 5: 60, 6: 102, 7: 39, 8: 17, 9: 19}, 3: {0: 31, 1: 28, 2: 71, 3: 468, 4: 51, 5: 172, 6: 99, 7: 32, 8: 17, 9: 31}, 4: {0: 56, 1: 7, 2: 83, 3: 68, 4: 517, 5: 37, 6: 112, 7: 90, 8: 15, 9: 15}, 5: {0: 21, 1: 9, 2: 49, 3: 221, 4: 60, 5: 504, 6: 49, 7: 46, 8: 14, 9: 27}, 6: {0: 4, 1: 5, 2: 51, 3: 63, 4: 38, 5: 36, 6: 779, 7: 7, 8: 6, 9: 11}, 7: {0: 36, 1: 8, 2: 29, 3: 54, 4: 68, 5: 91, 6: 24, 7: 643, 8: 7, 9: 40}, 8: {0: 133, 1: 86, 2: 9, 3: 23, 4: 5, 5: 2, 6: 10, 7: 9, 8: 656, 9: 67}, 9: {0: 43, 1: 110, 2: 2, 3: 22, 4: 9, 5: 9, 6: 12, 7: 13, 8: 19, 9: 761}}
epoch 800, train loss avg now = 0.001188, train contrast loss now = 0.449647, test acc now = 0.6158, test loss now = 2.717311
{0: {0: 691, 1: 38, 2: 47, 3: 29, 4: 12, 5: 6, 6: 19, 7: 13, 8: 85, 9: 60}, 1: {0: 31, 1: 748, 2: 3, 3: 24, 4: 9, 5: 4, 6: 17, 7: 10, 8: 23, 9: 131}, 2: {0: 123, 1: 12, 2: 389, 3: 99, 4: 132, 5: 61, 6: 108, 7: 38, 8: 19, 9: 19}, 3: {0: 28, 1: 26, 2: 67, 3: 470, 4: 51, 5: 172, 6: 105, 7: 33, 8: 16, 9: 32}, 4: {0: 57, 1: 6, 2: 74, 3: 71, 4: 503, 5: 34, 6: 131, 7: 96, 8: 15, 9: 13}, 5: {0: 19, 1: 10, 2: 51, 3: 225, 4: 54, 5: 502, 6: 53, 7: 46, 8: 13, 9: 27}, 6: {0: 6, 1: 6, 2: 46, 3: 61, 4: 32, 5: 33, 6: 791, 7: 6, 8: 6, 9: 13}, 7: {0: 35, 1: 8, 2: 27, 3: 56, 4: 64, 5: 90, 6: 24, 7: 646, 8: 8, 9: 42}, 8: {0: 127, 1: 88, 2: 9, 3: 24, 4: 4, 5: 2, 6: 11, 7: 8, 8: 660, 9: 67}, 9: {0: 43, 1: 107, 2: 2, 3: 21, 4: 7, 5: 10, 6: 13, 7: 12, 8: 17, 9: 768}}
epoch 900, train loss avg now = 0.001196, train contrast loss now = 0.449276, test acc now = 0.6168, test loss now = 2.741086
{0: {0: 693, 1: 38, 2: 45, 3: 26, 4: 12, 5: 6, 6: 19, 7: 14, 8: 86, 9: 61}, 1: {0: 30, 1: 754, 2: 3, 3: 21, 4: 10, 5: 4, 6: 16, 7: 11, 8: 25, 9: 126}, 2: {0: 128, 1: 12, 2: 385, 3: 105, 4: 133, 5: 50, 6: 112, 7: 39, 8: 20, 9: 16}, 3: {0: 33, 1: 27, 2: 69, 3: 466, 4: 52, 5: 167, 6: 103, 7: 35, 8: 17, 9: 31}, 4: {0: 58, 1: 6, 2: 77, 3: 70, 4: 512, 5: 29, 6: 128, 7: 92, 8: 16, 9: 12}, 5: {0: 20, 1: 8, 2: 53, 3: 219, 4: 60, 5: 485, 6: 53, 7: 56, 8: 19, 9: 27}, 6: {0: 7, 1: 6, 2: 46, 3: 57, 4: 33, 5: 28, 6: 796, 7: 7, 8: 6, 9: 14}, 7: {0: 36, 1: 8, 2: 30, 3: 55, 4: 61, 5: 82, 6: 25, 7: 652, 8: 8, 9: 43}, 8: {0: 129, 1: 85, 2: 8, 3: 22, 4: 4, 5: 2, 6: 11, 7: 8, 8: 662, 9: 69}, 9: {0: 42, 1: 104, 2: 3, 3: 20, 4: 8, 5: 8, 6: 14, 7: 10, 8: 19, 9: 772}}
epoch 1000, train loss avg now = 0.001269, train contrast loss now = 0.449441, test acc now = 0.6177, test loss now = 2.754359
epoch avg loss = 1.2691609839367654e-06, total time = 11490.296433210373
total 24576.0MB, used 16596.0MB, free 7980.0MB
Round 9 finish, update the prev_syn_proto
torch.Size([1010, 3, 32, 32])
torch.Size([1010, 3, 32, 32])
torch.Size([1040, 3, 32, 32])
torch.Size([1000, 3, 32, 32])
torch.Size([1000, 3, 32, 32])
torch.Size([1050, 3, 32, 32])
torch.Size([1010, 3, 32, 32])
torch.Size([1000, 3, 32, 32])
torch.Size([1000, 3, 32, 32])
torch.Size([1000, 3, 32, 32])
shape of prev_syn_proto: torch.Size([10, 2048])
{0: {0: 693, 1: 38, 2: 45, 3: 26, 4: 12, 5: 6, 6: 19, 7: 14, 8: 86, 9: 61}, 1: {0: 30, 1: 754, 2: 3, 3: 21, 4: 10, 5: 4, 6: 16, 7: 11, 8: 25, 9: 126}, 2: {0: 128, 1: 12, 2: 385, 3: 105, 4: 133, 5: 50, 6: 112, 7: 39, 8: 20, 9: 16}, 3: {0: 33, 1: 27, 2: 69, 3: 466, 4: 52, 5: 167, 6: 103, 7: 35, 8: 17, 9: 31}, 4: {0: 58, 1: 6, 2: 77, 3: 70, 4: 512, 5: 29, 6: 128, 7: 92, 8: 16, 9: 12}, 5: {0: 20, 1: 8, 2: 53, 3: 219, 4: 60, 5: 485, 6: 53, 7: 56, 8: 19, 9: 27}, 6: {0: 7, 1: 6, 2: 46, 3: 57, 4: 33, 5: 28, 6: 796, 7: 7, 8: 6, 9: 14}, 7: {0: 36, 1: 8, 2: 30, 3: 55, 4: 61, 5: 82, 6: 25, 7: 652, 8: 8, 9: 43}, 8: {0: 129, 1: 85, 2: 8, 3: 22, 4: 4, 5: 2, 6: 11, 7: 8, 8: 662, 9: 69}, 9: {0: 42, 1: 104, 2: 3, 3: 20, 4: 8, 5: 8, 6: 14, 7: 10, 8: 19, 9: 772}}
round 9 evaluation: test acc is 0.6177, test loss = 2.754359
[0.3718, 0.5081, 0.5716, 0.5856, 0.597, 0.6068, 0.6119, 0.6161, 0.6155, 0.6177]
{0: [18.158666676330565, 5.026687046813965, 5.259934803771973, 5.542360331726075, 5.8676357940673824, 6.198517114257813, 6.33680131225586, 6.518613966369629, 6.925741114807129, 6.899510302734375], 1: [14.01381662979126, 9.65345260925293, 22.86277348022461, 26.627806452941893, 76.28346012878418, 55.950385952758786, 78.17399600982667, 104.29679906311036, 274.0606840301514, 333.2979879486084], 2: [16.719959101867676, 19.86891809387207, 26.629328494262694, 33.234283389282226, 92.74974861450195, 82.36773834838867, 104.94271525268555, 126.22028746948243, 328.0686318359375, 380.91676837768557], 3: [11.424321787261963, 5.233091000366211, 5.84036594543457, 6.476556375122071, 7.213838702392578, 7.691343383789063, 8.258309516906738, 8.662878495788574, 17.019865168762205, 24.081922412109375], 4: [15.252624771118164, 7.422619264221192, 9.029819367980958, 11.292661405944823, 29.15157230682373, 21.96363692626953, 25.89587788848877, 44.760888806152344, 147.0645906097412, 167.0469443145752], 5: [26.093468673706056, 12.513700994873046, 10.647611474609375, 10.432605676269532, 10.340029742431641, 10.96398472290039, 11.053293029785156, 10.92506693725586, 11.397384008789063, 11.577173358154297], 6: [15.781656328582764, 5.7899534744262695, 5.723426358032227, 6.1061467330932615, 6.598287696838379, 6.649048098754883, 6.899101725769043, 6.859660870361328, 7.189920492553711, 7.282883090209961], 7: [19.27908719177246, 5.651318069458008, 5.612095680236816, 5.857131135559082, 6.3569746032714844, 6.5905265045166015, 6.911256907653809, 7.02769175567627, 7.311107539367676, 7.160249771118164], 8: [12.282036475118002, 8.379791524251303, 12.3936738474528, 18.596657489013673, 47.21072578531901, 36.938667193603514, 41.48913041992188, 49.01924554239909, 128.35291331278484, 157.0741583190918], 9: [9.687569127909342, 10.12192523803711, 20.713909310913085, 24.91516829325358, 61.63628386332194, 58.49636799214681, 84.12899453328451, 123.92180026855469, 311.0967645772298, 381.0891774617513]}
{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}
{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}
